[
  {
    "id": "5044",
    "manifest_path": "data/manifests/the_stack_sample/sample_1768.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failed-pod\nspec:\n  containers:\n  - name: motor\n    image: busybox\n    resources:\n      limits:\n        cpu: 100m\n        memory: 16Mi\n      requests:\n        cpu: 100m\n        memory: 16Mi\n    command:\n    - sh\n    - -c\n    args:\n    - echo \"This is failed phase\"; exit 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"motor\" is not set to runAsNonRoot"
  },
  {
    "id": "5045",
    "manifest_path": "data/manifests/the_stack_sample/sample_1769.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: tdedahaks-7d3a\n  labels:\n    app: tdedahaks-7d3a\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 4000\n    targetPort: 4000\n    protocol: TCP\n    name: http\n  selector:\n    app: tdedahaks-7d3a\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:tdedahaks-7d3a])"
  },
  {
    "id": "5046",
    "manifest_path": "data/manifests/the_stack_sample/sample_1771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: salesforce-connector\n  name: salesforce-connector\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: salesforce-connector\n  template:\n    metadata:\n      labels:\n        name: salesforce-connector\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - gfw\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '9500'\n        - name: LOGGER_TYPE\n          value: console\n        - name: LOGGER_LEVEL\n          value: debug\n        - name: NODE_ENV\n          value: staging\n        - name: NODE_PATH\n          value: app/src\n        - name: MICROSERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: MICROSERVICE_TOKEN\n              name: mssecrets\n        - name: GATEWAY_URL\n          valueFrom:\n            secretKeyRef:\n              key: GATEWAY_URL\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: SALESFORCE_URL\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_URL\n              name: mssecrets\n        - name: SALESFORCE_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_USERNAME\n              name: mssecrets\n        - name: SALESFORCE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_PASSWORD\n              name: mssecrets\n        image: gfwdockerhub/salesforce-connector\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: salesforce-connector\n        ports:\n        - containerPort: 9500\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: 350m\n            memory: 256M\n      securityContext: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"salesforce-connector\" is using an invalid container image, \"gfwdockerhub/salesforce-connector\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5047",
    "manifest_path": "data/manifests/the_stack_sample/sample_1771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: salesforce-connector\n  name: salesforce-connector\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: salesforce-connector\n  template:\n    metadata:\n      labels:\n        name: salesforce-connector\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - gfw\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '9500'\n        - name: LOGGER_TYPE\n          value: console\n        - name: LOGGER_LEVEL\n          value: debug\n        - name: NODE_ENV\n          value: staging\n        - name: NODE_PATH\n          value: app/src\n        - name: MICROSERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: MICROSERVICE_TOKEN\n              name: mssecrets\n        - name: GATEWAY_URL\n          valueFrom:\n            secretKeyRef:\n              key: GATEWAY_URL\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: SALESFORCE_URL\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_URL\n              name: mssecrets\n        - name: SALESFORCE_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_USERNAME\n              name: mssecrets\n        - name: SALESFORCE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_PASSWORD\n              name: mssecrets\n        image: gfwdockerhub/salesforce-connector\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: salesforce-connector\n        ports:\n        - containerPort: 9500\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: 350m\n            memory: 256M\n      securityContext: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"salesforce-connector\" does not have a read-only root file system"
  },
  {
    "id": "5048",
    "manifest_path": "data/manifests/the_stack_sample/sample_1771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: salesforce-connector\n  name: salesforce-connector\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: salesforce-connector\n  template:\n    metadata:\n      labels:\n        name: salesforce-connector\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - gfw\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '9500'\n        - name: LOGGER_TYPE\n          value: console\n        - name: LOGGER_LEVEL\n          value: debug\n        - name: NODE_ENV\n          value: staging\n        - name: NODE_PATH\n          value: app/src\n        - name: MICROSERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: MICROSERVICE_TOKEN\n              name: mssecrets\n        - name: GATEWAY_URL\n          valueFrom:\n            secretKeyRef:\n              key: GATEWAY_URL\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: SALESFORCE_URL\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_URL\n              name: mssecrets\n        - name: SALESFORCE_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_USERNAME\n              name: mssecrets\n        - name: SALESFORCE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_PASSWORD\n              name: mssecrets\n        image: gfwdockerhub/salesforce-connector\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: salesforce-connector\n        ports:\n        - containerPort: 9500\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: 350m\n            memory: 256M\n      securityContext: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"salesforce-connector\" is not set to runAsNonRoot"
  },
  {
    "id": "5049",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"systemize-user-api\" is using an invalid container image, \"systemize-user-api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5050",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5051",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"systemize-user-api\" does not have a read-only root file system"
  },
  {
    "id": "5052",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"systemize-user-api\" is not set to runAsNonRoot"
  },
  {
    "id": "5053",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"systemize-user-api\" has cpu request 0"
  },
  {
    "id": "5054",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"systemize-user-api\" has memory limit 0"
  },
  {
    "id": "5055",
    "manifest_path": "data/manifests/the_stack_sample/sample_1775.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: petclinic\n  namespace: default\nspec:\n  ports:\n  - name: http-ui\n    port: 80\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: petclinic\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:petclinic])"
  },
  {
    "id": "5056",
    "manifest_path": "data/manifests/the_stack_sample/sample_1779.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: xray\n    chart: xray-103.41.4\n    component: xray\n    heritage: Helm\n    release: RELEASE-NAME\n  name: RELEASE-NAME-xray\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 8000\n  - name: http-router\n    port: 8082\n    protocol: TCP\n    targetPort: 8082\n  selector:\n    app: xray\n    component: xray\n    release: RELEASE-NAME\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:xray component:xray release:RELEASE-NAME])"
  },
  {
    "id": "5057",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5058",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5059",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5060",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5061",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5062",
    "manifest_path": "data/manifests/the_stack_sample/sample_1782.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sensordeploy\n  labels:\n    app: sensorapp\nspec:\n  selector:\n    matchLabels:\n      app: sensorapp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sensorapp\n    spec:\n      containers:\n      - name: sensorapp\n        image: 03021994/sensor:latest\n        command:\n        - python\n        - ./app/sensor-test.py\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 8083\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 50m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sensorapp\" is using an invalid container image, \"03021994/sensor:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5063",
    "manifest_path": "data/manifests/the_stack_sample/sample_1782.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sensordeploy\n  labels:\n    app: sensorapp\nspec:\n  selector:\n    matchLabels:\n      app: sensorapp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sensorapp\n    spec:\n      containers:\n      - name: sensorapp\n        image: 03021994/sensor:latest\n        command:\n        - python\n        - ./app/sensor-test.py\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 8083\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 50m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sensorapp\" does not have a read-only root file system"
  },
  {
    "id": "5064",
    "manifest_path": "data/manifests/the_stack_sample/sample_1782.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sensordeploy\n  labels:\n    app: sensorapp\nspec:\n  selector:\n    matchLabels:\n      app: sensorapp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sensorapp\n    spec:\n      containers:\n      - name: sensorapp\n        image: 03021994/sensor:latest\n        command:\n        - python\n        - ./app/sensor-test.py\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 8083\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 50m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sensorapp\" is not set to runAsNonRoot"
  },
  {
    "id": "5065",
    "manifest_path": "data/manifests/the_stack_sample/sample_1784.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: router-https\nspec:\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app: router-proxy\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:router-proxy])"
  },
  {
    "id": "5066",
    "manifest_path": "data/manifests/the_stack_sample/sample_1785.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: percona-xtradb-cluster-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: operator\n      app.kubernetes.io/instance: percona-xtradb-cluster-operator\n      app.kubernetes.io/name: percona-xtradb-cluster-operator\n      app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: operator\n        app.kubernetes.io/instance: percona-xtradb-cluster-operator\n        app.kubernetes.io/name: percona-xtradb-cluster-operator\n        app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n    spec:\n      containers:\n      - command:\n        - percona-xtradb-cluster-operator\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: percona-xtradb-cluster-operator\n        image: perconalab/percona-xtradb-cluster-operator:main\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /metrics\n            port: metrics\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        name: percona-xtradb-cluster-operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n      serviceAccountName: percona-xtradb-cluster-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"percona-xtradb-cluster-operator\" does not have a read-only root file system"
  },
  {
    "id": "5067",
    "manifest_path": "data/manifests/the_stack_sample/sample_1785.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: percona-xtradb-cluster-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: operator\n      app.kubernetes.io/instance: percona-xtradb-cluster-operator\n      app.kubernetes.io/name: percona-xtradb-cluster-operator\n      app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: operator\n        app.kubernetes.io/instance: percona-xtradb-cluster-operator\n        app.kubernetes.io/name: percona-xtradb-cluster-operator\n        app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n    spec:\n      containers:\n      - command:\n        - percona-xtradb-cluster-operator\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: percona-xtradb-cluster-operator\n        image: perconalab/percona-xtradb-cluster-operator:main\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /metrics\n            port: metrics\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        name: percona-xtradb-cluster-operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n      serviceAccountName: percona-xtradb-cluster-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"percona-xtradb-cluster-operator\" not found"
  },
  {
    "id": "5068",
    "manifest_path": "data/manifests/the_stack_sample/sample_1785.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: percona-xtradb-cluster-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: operator\n      app.kubernetes.io/instance: percona-xtradb-cluster-operator\n      app.kubernetes.io/name: percona-xtradb-cluster-operator\n      app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: operator\n        app.kubernetes.io/instance: percona-xtradb-cluster-operator\n        app.kubernetes.io/name: percona-xtradb-cluster-operator\n        app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n    spec:\n      containers:\n      - command:\n        - percona-xtradb-cluster-operator\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: percona-xtradb-cluster-operator\n        image: perconalab/percona-xtradb-cluster-operator:main\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /metrics\n            port: metrics\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        name: percona-xtradb-cluster-operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n      serviceAccountName: percona-xtradb-cluster-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"percona-xtradb-cluster-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "5069",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "5070",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5071",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "5072",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"deck\" not found"
  },
  {
    "id": "5073",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "5074",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "5075",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "5076",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "5077",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"my-nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5078",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5079",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"my-nginx\" does not have a read-only root file system"
  },
  {
    "id": "5080",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"my-nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5081",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"my-nginx\" has cpu request 0"
  },
  {
    "id": "5082",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"my-nginx\" has memory limit 0"
  },
  {
    "id": "5083",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ibms\" is using an invalid container image, \"ghcr.io/dbca-wa/ibms:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5084",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ibms\" does not have a read-only root file system"
  },
  {
    "id": "5085",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ibms\" is not set to runAsNonRoot"
  },
  {
    "id": "5086",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ibms\" has cpu request 0"
  },
  {
    "id": "5087",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ibms\" has memory limit 0"
  },
  {
    "id": "5088",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5089",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5090",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5091",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5092",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5093",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"cilium-agent\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "5094",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"clean-cilium-state\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "5095",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "5096",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cilium-agent\" is using an invalid container image, \"docker.io/cilium/cilium:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5097",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "5098",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "5099",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium\" not found"
  },
  {
    "id": "5100",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5101",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"clean-cilium-state\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5102",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cilium-agent\" is privileged"
  },
  {
    "id": "5103",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"clean-cilium-state\" is privileged"
  },
  {
    "id": "5104",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "5105",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "5106",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "5107",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "5108",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "5109",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "5110",
    "manifest_path": "data/manifests/the_stack_sample/sample_1794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: a337134d0e64ec21fde04859d87d697b47666d6c4d404b7e8f1eb39a41ffc05d\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: ranggaperwiratama\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 46bcf702811226edac90b77c43cdbfec99bc6fe6\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "5111",
    "manifest_path": "data/manifests/the_stack_sample/sample_1794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: a337134d0e64ec21fde04859d87d697b47666d6c4d404b7e8f1eb39a41ffc05d\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: ranggaperwiratama\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 46bcf702811226edac90b77c43cdbfec99bc6fe6\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-foghorn\" not found"
  },
  {
    "id": "5112",
    "manifest_path": "data/manifests/the_stack_sample/sample_1794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: a337134d0e64ec21fde04859d87d697b47666d6c4d404b7e8f1eb39a41ffc05d\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: ranggaperwiratama\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 46bcf702811226edac90b77c43cdbfec99bc6fe6\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "5113",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5114",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5115",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5116",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5117",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5118",
    "manifest_path": "data/manifests/the_stack_sample/sample_1800.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nginx-svc\n  labels:\n    app: nginx\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n  selector:\n    app: nginx\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nginx])"
  },
  {
    "id": "5119",
    "manifest_path": "data/manifests/the_stack_sample/sample_1801.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-20-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-20\n        image: lemtzas/rollem-discord:2.6.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2021-04-20'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '20'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '50'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rollem-shard-20\" does not have a read-only root file system"
  },
  {
    "id": "5120",
    "manifest_path": "data/manifests/the_stack_sample/sample_1801.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-20-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-20\n        image: lemtzas/rollem-discord:2.6.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2021-04-20'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '20'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '50'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rollem-shard-20\" is not set to runAsNonRoot"
  },
  {
    "id": "5121",
    "manifest_path": "data/manifests/the_stack_sample/sample_1801.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-20-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-20\n        image: lemtzas/rollem-discord:2.6.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2021-04-20'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '20'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '50'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rollem-shard-20\" has memory limit 0"
  },
  {
    "id": "5122",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"digilocker-support-api\" is using an invalid container image, \"REGISTRY/digilocker_support_api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5123",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"digilocker-support-api\" does not have a read-only root file system"
  },
  {
    "id": "5124",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"digilocker-support-api\" is not set to runAsNonRoot"
  },
  {
    "id": "5125",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"digilocker-support-api\" has cpu request 0"
  },
  {
    "id": "5126",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"digilocker-support-api\" has memory limit 0"
  },
  {
    "id": "5127",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "5128",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "5129",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "5130",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "5131",
    "manifest_path": "data/manifests/the_stack_sample/sample_1810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: smtp\n  name: smtp\n  namespace: utils\nspec:\n  selector:\n    matchLabels:\n      app: smtp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: smtp\n    spec:\n      containers:\n      - image: devture/exim-relay:4.94.2-r0-3\n        imagePullPolicy: Always\n        name: smtp\n        env:\n        - name: TZ\n          value: Europe/Zurich\n        - name: smtp.int.eighty-three.me\n        - name: SMARTHOST\n          value: smtp.sendgrid.net::587\n        - name: SMTP_USERNAME\n          value: apikey\n        - name: SMTP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: smtp-password\n              key: password\n        resources:\n          requests:\n            cpu: 10m\n            memory: 10Mi\n          limits:\n            cpu: 400m\n            memory: 100Mi\n        ports:\n        - containerPort: 8025\n        livenessProbe:\n          exec:\n            command:\n            - exim\n            - -bt\n            - test@tuxpeople.org\n          initialDelaySeconds: 10\n          periodSeconds: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"smtp\" does not have a read-only root file system"
  },
  {
    "id": "5132",
    "manifest_path": "data/manifests/the_stack_sample/sample_1810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: smtp\n  name: smtp\n  namespace: utils\nspec:\n  selector:\n    matchLabels:\n      app: smtp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: smtp\n    spec:\n      containers:\n      - image: devture/exim-relay:4.94.2-r0-3\n        imagePullPolicy: Always\n        name: smtp\n        env:\n        - name: TZ\n          value: Europe/Zurich\n        - name: smtp.int.eighty-three.me\n        - name: SMARTHOST\n          value: smtp.sendgrid.net::587\n        - name: SMTP_USERNAME\n          value: apikey\n        - name: SMTP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: smtp-password\n              key: password\n        resources:\n          requests:\n            cpu: 10m\n            memory: 10Mi\n          limits:\n            cpu: 400m\n            memory: 100Mi\n        ports:\n        - containerPort: 8025\n        livenessProbe:\n          exec:\n            command:\n            - exim\n            - -bt\n            - test@tuxpeople.org\n          initialDelaySeconds: 10\n          periodSeconds: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"smtp\" is not set to runAsNonRoot"
  },
  {
    "id": "5133",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"motive-back-end-container\" is using an invalid container image, \"$FULL_IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5134",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5135",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"motive-back-end-container\" does not have a read-only root file system"
  },
  {
    "id": "5136",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"motive-back-end-container\" is not set to runAsNonRoot"
  },
  {
    "id": "5137",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"motive-back-end-container\" has cpu request 0"
  },
  {
    "id": "5138",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"motive-back-end-container\" has memory limit 0"
  },
  {
    "id": "5139",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "5140",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "5141",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-sriovdp\" does not have a read-only root file system"
  },
  {
    "id": "5142",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"sriov-device-plugin\" not found"
  },
  {
    "id": "5143",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"kube-sriovdp\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5144",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"kube-sriovdp\" is privileged"
  },
  {
    "id": "5145",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-sriovdp\" is not set to runAsNonRoot"
  },
  {
    "id": "5146",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-sriovdp\" has cpu request 0"
  },
  {
    "id": "5147",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-sriovdp\" has memory limit 0"
  },
  {
    "id": "5148",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"testground-daemon\""
  },
  {
    "id": "5149",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "5150",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "5151",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"goproxy\" does not have a read-only root file system"
  },
  {
    "id": "5152",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"iproute-add\" does not have a read-only root file system"
  },
  {
    "id": "5153",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"testground-daemon\" does not have a read-only root file system"
  },
  {
    "id": "5154",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"testground-daemon\" not found"
  },
  {
    "id": "5155",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"iproute-add\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5156",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"testground-daemon\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5157",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"iproute-add\" is privileged"
  },
  {
    "id": "5158",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"testground-daemon\" is privileged"
  },
  {
    "id": "5159",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"goproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "5160",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"iproute-add\" is not set to runAsNonRoot"
  },
  {
    "id": "5161",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"testground-daemon\" is not set to runAsNonRoot"
  },
  {
    "id": "5162",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"iproute-add\" has cpu request 0"
  },
  {
    "id": "5163",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"iproute-add\" has memory limit 0"
  },
  {
    "id": "5164",
    "manifest_path": "data/manifests/the_stack_sample/sample_1819.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shippingservice\nspec:\n  selector:\n    matchLabels:\n      app: shippingservice\n  template:\n    metadata:\n      labels:\n        app: shippingservice\n    spec:\n      containers:\n      - name: server\n        image: shippingservice\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        readinessProbe:\n          periodSeconds: 5\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"shippingservice\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5165",
    "manifest_path": "data/manifests/the_stack_sample/sample_1819.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shippingservice\nspec:\n  selector:\n    matchLabels:\n      app: shippingservice\n  template:\n    metadata:\n      labels:\n        app: shippingservice\n    spec:\n      containers:\n      - name: server\n        image: shippingservice\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        readinessProbe:\n          periodSeconds: 5\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "5166",
    "manifest_path": "data/manifests/the_stack_sample/sample_1819.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shippingservice\nspec:\n  selector:\n    matchLabels:\n      app: shippingservice\n  template:\n    metadata:\n      labels:\n        app: shippingservice\n    spec:\n      containers:\n      - name: server\n        image: shippingservice\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        readinessProbe:\n          periodSeconds: 5\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "5167",
    "manifest_path": "data/manifests/the_stack_sample/sample_1821.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: prometheus\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 2.25.0\n    prometheus: k8s\n  name: prometheus-k8s\n  namespace: monitoring\nspec:\n  ports:\n  - name: web\n    port: 9090\n    targetPort: web\n  selector:\n    app: prometheus\n    app.kubernetes.io/component: prometheus\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/part-of: kube-prometheus\n    prometheus: k8s\n  sessionAffinity: ClientIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:prometheus app.kubernetes.io/component:prometheus app.kubernetes.io/name:prometheus app.kubernetes.io/part-of:kube-prometheus prometheus:k8s])"
  },
  {
    "id": "5168",
    "manifest_path": "data/manifests/the_stack_sample/sample_1822.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: slack-service\n  name: slack-service\n  namespace: slack-plugin\nspec:\n  ports:\n  - port: 1234\n    targetPort: 1234\n  selector:\n    name: slack\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:slack])"
  },
  {
    "id": "5169",
    "manifest_path": "data/manifests/the_stack_sample/sample_1823.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-user-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-user-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-user-operator\n    spec:\n      serviceAccountName: strimzi-user-operator\n      containers:\n      - name: strimzi-user-operator\n        image: quay.io/strimzi/operator:0.25.0\n        args:\n        - /opt/strimzi/bin/user_operator_run.sh\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LABELS\n          value: strimzi.io/cluster=my-cluster\n        - name: STRIMZI_CA_CERT_NAME\n          value: my-cluster-clients-ca-cert\n        - name: STRIMZI_CA_KEY_NAME\n          value: my-cluster-clients-ca\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_LOG_LEVEL\n          value: INFO\n        - name: STRIMZI_GC_LOG_ENABLED\n          value: 'true'\n        - name: STRIMZI_CA_VALIDITY\n          value: '365'\n        - name: STRIMZI_CA_RENEWAL\n          value: '30'\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 500m\n          requests:\n            memory: 256Mi\n            cpu: 100m\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"strimzi-user-operator\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "5170",
    "manifest_path": "data/manifests/the_stack_sample/sample_1823.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-user-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-user-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-user-operator\n    spec:\n      serviceAccountName: strimzi-user-operator\n      containers:\n      - name: strimzi-user-operator\n        image: quay.io/strimzi/operator:0.25.0\n        args:\n        - /opt/strimzi/bin/user_operator_run.sh\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LABELS\n          value: strimzi.io/cluster=my-cluster\n        - name: STRIMZI_CA_CERT_NAME\n          value: my-cluster-clients-ca-cert\n        - name: STRIMZI_CA_KEY_NAME\n          value: my-cluster-clients-ca\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_LOG_LEVEL\n          value: INFO\n        - name: STRIMZI_GC_LOG_ENABLED\n          value: 'true'\n        - name: STRIMZI_CA_VALIDITY\n          value: '365'\n        - name: STRIMZI_CA_RENEWAL\n          value: '30'\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 500m\n          requests:\n            memory: 256Mi\n            cpu: 100m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"strimzi-user-operator\" does not have a read-only root file system"
  },
  {
    "id": "5171",
    "manifest_path": "data/manifests/the_stack_sample/sample_1823.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-user-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-user-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-user-operator\n    spec:\n      serviceAccountName: strimzi-user-operator\n      containers:\n      - name: strimzi-user-operator\n        image: quay.io/strimzi/operator:0.25.0\n        args:\n        - /opt/strimzi/bin/user_operator_run.sh\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LABELS\n          value: strimzi.io/cluster=my-cluster\n        - name: STRIMZI_CA_CERT_NAME\n          value: my-cluster-clients-ca-cert\n        - name: STRIMZI_CA_KEY_NAME\n          value: my-cluster-clients-ca\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_LOG_LEVEL\n          value: INFO\n        - name: STRIMZI_GC_LOG_ENABLED\n          value: 'true'\n        - name: STRIMZI_CA_VALIDITY\n          value: '365'\n        - name: STRIMZI_CA_RENEWAL\n          value: '30'\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 500m\n          requests:\n            memory: 256Mi\n            cpu: 100m\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"strimzi-user-operator\" not found"
  },
  {
    "id": "5172",
    "manifest_path": "data/manifests/the_stack_sample/sample_1823.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-user-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-user-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-user-operator\n    spec:\n      serviceAccountName: strimzi-user-operator\n      containers:\n      - name: strimzi-user-operator\n        image: quay.io/strimzi/operator:0.25.0\n        args:\n        - /opt/strimzi/bin/user_operator_run.sh\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LABELS\n          value: strimzi.io/cluster=my-cluster\n        - name: STRIMZI_CA_CERT_NAME\n          value: my-cluster-clients-ca-cert\n        - name: STRIMZI_CA_KEY_NAME\n          value: my-cluster-clients-ca\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_LOG_LEVEL\n          value: INFO\n        - name: STRIMZI_GC_LOG_ENABLED\n          value: 'true'\n        - name: STRIMZI_CA_VALIDITY\n          value: '365'\n        - name: STRIMZI_CA_RENEWAL\n          value: '30'\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 500m\n          requests:\n            memory: 256Mi\n            cpu: 100m\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"strimzi-user-operator\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "5173",
    "manifest_path": "data/manifests/the_stack_sample/sample_1823.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-user-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-user-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-user-operator\n    spec:\n      serviceAccountName: strimzi-user-operator\n      containers:\n      - name: strimzi-user-operator\n        image: quay.io/strimzi/operator:0.25.0\n        args:\n        - /opt/strimzi/bin/user_operator_run.sh\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LABELS\n          value: strimzi.io/cluster=my-cluster\n        - name: STRIMZI_CA_CERT_NAME\n          value: my-cluster-clients-ca-cert\n        - name: STRIMZI_CA_KEY_NAME\n          value: my-cluster-clients-ca\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_LOG_LEVEL\n          value: INFO\n        - name: STRIMZI_GC_LOG_ENABLED\n          value: 'true'\n        - name: STRIMZI_CA_VALIDITY\n          value: '365'\n        - name: STRIMZI_CA_RENEWAL\n          value: '30'\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 500m\n          requests:\n            memory: 256Mi\n            cpu: 100m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"strimzi-user-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "5174",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sistema-noticias\" does not have a read-only root file system"
  },
  {
    "id": "5175",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sistema-noticias\" is not set to runAsNonRoot"
  },
  {
    "id": "5176",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sistema-noticias\" has cpu request 0"
  },
  {
    "id": "5177",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sistema-noticias\" has memory limit 0"
  },
  {
    "id": "5178",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"api\" is using an invalid container image, \"agus7fauzi/hari\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5179",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5180",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api\" does not have a read-only root file system"
  },
  {
    "id": "5181",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"api\" is not set to runAsNonRoot"
  },
  {
    "id": "5182",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api\" has cpu request 0"
  },
  {
    "id": "5183",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api\" has memory limit 0"
  },
  {
    "id": "5184",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5185",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5186",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5187",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5188",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5189",
    "manifest_path": "data/manifests/the_stack_sample/sample_1837.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: web-client-service\n  labels:\n    app: web-client\nspec:\n  ports:\n  - port: 5500\n    protocol: TCP\n  selector:\n    app: web-client\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:web-client])"
  },
  {
    "id": "5190",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"downloader\" is using an invalid container image, \"$CORTEX_IMAGE_DOWNLOADER\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5191",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"python-predictor-cpu\" is using an invalid container image, \"$CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5192",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tensorflow-predictor\" is using an invalid container image, \"$CORTEX_IMAGE_TENSORFLOW_PREDICTOR\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5193",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tensorflow-serving-cpu\" is using an invalid container image, \"$CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5194",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"downloader\" does not have a read-only root file system"
  },
  {
    "id": "5195",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"python-predictor-cpu\" does not have a read-only root file system"
  },
  {
    "id": "5196",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tensorflow-predictor\" does not have a read-only root file system"
  },
  {
    "id": "5197",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tensorflow-serving-cpu\" does not have a read-only root file system"
  },
  {
    "id": "5198",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"downloader\" is not set to runAsNonRoot"
  },
  {
    "id": "5199",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"python-predictor-cpu\" is not set to runAsNonRoot"
  },
  {
    "id": "5200",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tensorflow-predictor\" is not set to runAsNonRoot"
  },
  {
    "id": "5201",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tensorflow-serving-cpu\" is not set to runAsNonRoot"
  },
  {
    "id": "5202",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"downloader\" has cpu request 0"
  },
  {
    "id": "5203",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"python-predictor-cpu\" has cpu request 0"
  },
  {
    "id": "5204",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tensorflow-predictor\" has cpu request 0"
  },
  {
    "id": "5205",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tensorflow-serving-cpu\" has cpu request 0"
  },
  {
    "id": "5206",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"downloader\" has memory limit 0"
  },
  {
    "id": "5207",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"python-predictor-cpu\" has memory limit 0"
  },
  {
    "id": "5208",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tensorflow-predictor\" has memory limit 0"
  },
  {
    "id": "5209",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tensorflow-serving-cpu\" has memory limit 0"
  },
  {
    "id": "5210",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5211",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"yeah-it-works\" does not have a read-only root file system"
  },
  {
    "id": "5212",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"yeah-it-works\" is not set to runAsNonRoot"
  },
  {
    "id": "5213",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"yeah-it-works\" has cpu request 0"
  },
  {
    "id": "5214",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"yeah-it-works\" has memory limit 0"
  },
  {
    "id": "5215",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"modeldb-artifact-store\" does not have a read-only root file system"
  },
  {
    "id": "5216",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"modeldb-artifact-store\" is not set to runAsNonRoot"
  },
  {
    "id": "5217",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"modeldb-artifact-store\" has cpu request 0"
  },
  {
    "id": "5218",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"modeldb-artifact-store\" has memory limit 0"
  },
  {
    "id": "5219",
    "manifest_path": "data/manifests/the_stack_sample/sample_1848.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose -f ../../docker-compose.yml convert\n    kompose.version: 1.21.0 ()\n  labels:\n    io.kompose.service: customersrv\n  name: customersrv\nspec:\n  ports:\n  - name: '50057'\n    port: 50057\n    targetPort: 50057\n  - name: '2112'\n    port: 2112\n    targetPort: 2112\n  selector:\n    io.kompose.service: customersrv\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[io.kompose.service:customersrv])"
  },
  {
    "id": "5220",
    "manifest_path": "data/manifests/the_stack_sample/sample_1853.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: compliance-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: compliance-operator\n  template:\n    metadata:\n      labels:\n        name: compliance-operator\n      annotations:\n        workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n    spec:\n      serviceAccountName: compliance-operator\n      containers:\n      - name: compliance-operator\n        image: quay.io/compliance-operator/compliance-operator:latest\n        command:\n        - compliance-operator\n        - operator\n        imagePullPolicy: Always\n        securityContext:\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n          limits:\n            memory: 200Mi\n            cpu: 100m\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: compliance-operator\n        - name: RELATED_IMAGE_OPENSCAP\n          value: quay.io/compliance-operator/openscap-ocp:1.3.5\n        - name: RELATED_IMAGE_OPERATOR\n          value: quay.io/compliance-operator/compliance-operator:latest\n        - name: RELATED_IMAGE_PROFILE\n          value: quay.io/complianceascode/ocp4:latest\n        volumeMounts:\n        - name: serving-cert\n          mountPath: /var/run/secrets/serving-cert\n          readOnly: true\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: compliance-operator-serving-cert\n          optional: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"compliance-operator\" is using an invalid container image, \"quay.io/compliance-operator/compliance-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5221",
    "manifest_path": "data/manifests/the_stack_sample/sample_1853.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: compliance-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: compliance-operator\n  template:\n    metadata:\n      labels:\n        name: compliance-operator\n      annotations:\n        workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n    spec:\n      serviceAccountName: compliance-operator\n      containers:\n      - name: compliance-operator\n        image: quay.io/compliance-operator/compliance-operator:latest\n        command:\n        - compliance-operator\n        - operator\n        imagePullPolicy: Always\n        securityContext:\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n          limits:\n            memory: 200Mi\n            cpu: 100m\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: compliance-operator\n        - name: RELATED_IMAGE_OPENSCAP\n          value: quay.io/compliance-operator/openscap-ocp:1.3.5\n        - name: RELATED_IMAGE_OPERATOR\n          value: quay.io/compliance-operator/compliance-operator:latest\n        - name: RELATED_IMAGE_PROFILE\n          value: quay.io/complianceascode/ocp4:latest\n        volumeMounts:\n        - name: serving-cert\n          mountPath: /var/run/secrets/serving-cert\n          readOnly: true\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: compliance-operator-serving-cert\n          optional: true\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"compliance-operator\" not found"
  },
  {
    "id": "5222",
    "manifest_path": "data/manifests/the_stack_sample/sample_1853.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: compliance-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: compliance-operator\n  template:\n    metadata:\n      labels:\n        name: compliance-operator\n      annotations:\n        workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n    spec:\n      serviceAccountName: compliance-operator\n      containers:\n      - name: compliance-operator\n        image: quay.io/compliance-operator/compliance-operator:latest\n        command:\n        - compliance-operator\n        - operator\n        imagePullPolicy: Always\n        securityContext:\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n          limits:\n            memory: 200Mi\n            cpu: 100m\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: compliance-operator\n        - name: RELATED_IMAGE_OPENSCAP\n          value: quay.io/compliance-operator/openscap-ocp:1.3.5\n        - name: RELATED_IMAGE_OPERATOR\n          value: quay.io/compliance-operator/compliance-operator:latest\n        - name: RELATED_IMAGE_PROFILE\n          value: quay.io/complianceascode/ocp4:latest\n        volumeMounts:\n        - name: serving-cert\n          mountPath: /var/run/secrets/serving-cert\n          readOnly: true\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: compliance-operator-serving-cert\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"compliance-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "5223",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (csi-cinder-node-sa), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "5224",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"cinder-csi-plugin\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "5225",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "5226",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cinder-csi-plugin\" is using an invalid container image, \"docker.io/k8scloudprovider/cinder-csi-plugin:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5227",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cinder-csi-plugin\" does not have a read-only root file system"
  },
  {
    "id": "5228",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "5229",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"csi-cinder-node-sa\" not found"
  },
  {
    "id": "5230",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cinder-csi-plugin\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "5231",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cinder-csi-plugin\" is privileged"
  },
  {
    "id": "5232",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cinder-csi-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "5233",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "5234",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"cinder-csi-plugin\""
  },
  {
    "id": "5235",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cinder-csi-plugin\" has cpu request 0"
  },
  {
    "id": "5236",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-driver-registrar\" has cpu request 0"
  },
  {
    "id": "5237",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cinder-csi-plugin\" has memory limit 0"
  },
  {
    "id": "5238",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-driver-registrar\" has memory limit 0"
  },
  {
    "id": "5239",
    "manifest_path": "data/manifests/the_stack_sample/sample_1855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: podinfo-ds\n  labels:\n    app.kubernetes.io/name: podinfo-ds\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: podinfo-ds\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app.kubernetes.io/name: podinfo-ds\n    spec:\n      containers:\n      - name: podinfod\n        image: ghcr.io/stefanprodan/podinfo:6.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        livenessProbe:\n          httpGet:\n            port: 9898\n            path: /healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            port: 9898\n            path: /readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 1m\n            memory: 16Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "5240",
    "manifest_path": "data/manifests/the_stack_sample/sample_1855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: podinfo-ds\n  labels:\n    app.kubernetes.io/name: podinfo-ds\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: podinfo-ds\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app.kubernetes.io/name: podinfo-ds\n    spec:\n      containers:\n      - name: podinfod\n        image: ghcr.io/stefanprodan/podinfo:6.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        livenessProbe:\n          httpGet:\n            port: 9898\n            path: /healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            port: 9898\n            path: /readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 1m\n            memory: 16Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "5241",
    "manifest_path": "data/manifests/the_stack_sample/sample_1858.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: transacao\n  name: transacao\nspec:\n  ports:\n  - name: http\n    port: 2222\n    protocol: TCP\n    targetPort: 2222\n  type: LoadBalancer\n  selector:\n    app: transacao\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:transacao])"
  },
  {
    "id": "5242",
    "manifest_path": "data/manifests/the_stack_sample/sample_1859.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: myservice\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n  selector:\n    app: myapp\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:myapp])"
  },
  {
    "id": "5243",
    "manifest_path": "data/manifests/the_stack_sample/sample_1863.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: sp-test-version-1\n  name: sp-test-version-1\nspec:\n  type: NodePort\n  sessionAffinity: ClientIP\n  ports:\n  - name: servlet-http\n    port: 9090\n    targetPort: 9090\n  - name: msf4j-http\n    port: 8080\n    targetPort: 8080\n  selector:\n    name: sp-test-version-1\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:sp-test-version-1])"
  },
  {
    "id": "5244",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5245",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5246",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5247",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5248",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5249",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5250",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"minio-client\" is using an invalid container image, \"minio/mc\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5251",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio-client\" does not have a read-only root file system"
  },
  {
    "id": "5252",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio-client\" is not set to runAsNonRoot"
  },
  {
    "id": "5253",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio-client\" has cpu request 0"
  },
  {
    "id": "5254",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio-client\" has memory limit 0"
  },
  {
    "id": "5255",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"base\" is using an invalid container image, \"dummy_image\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5256",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"base\" does not have a read-only root file system"
  },
  {
    "id": "5257",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"RELEASE-NAME-worker-serviceaccount\" not found"
  },
  {
    "id": "5258",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"base\" has cpu request 0"
  },
  {
    "id": "5259",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"base\" has memory limit 0"
  },
  {
    "id": "5260",
    "manifest_path": "data/manifests/the_stack_sample/sample_1867.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: tigergraph\n  name: tigergraph\n  namespace: default\nspec:\n  clusterIP: None\n  selector:\n    app: tigergraph\n  ports:\n  - port: 9000\n    name: rest\n    targetPort: 9000\n  - port: 14240\n    name: graphstudio\n    targetPort: 14240\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:tigergraph])"
  },
  {
    "id": "5261",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5262",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vote\" does not have a read-only root file system"
  },
  {
    "id": "5263",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vote\" is not set to runAsNonRoot"
  },
  {
    "id": "5264",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vote\" has cpu request 0"
  },
  {
    "id": "5265",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vote\" has memory limit 0"
  },
  {
    "id": "5266",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5267",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5268",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5269",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5270",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5271",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5272",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "5273",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "5274",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "5275",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "5276",
    "manifest_path": "data/manifests/the_stack_sample/sample_1873.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 8080\n    targetPort: http-metrics\n  - name: telemetry\n    port: 8081\n    targetPort: telemetry\n  selector:\n    app.kubernetes.io/name: kube-state-metrics\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:kube-state-metrics])"
  },
  {
    "id": "5277",
    "manifest_path": "data/manifests/the_stack_sample/sample_1874.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: v3-data-examples-tls-enabled-cert-issuer\n    fiaas/deployed_by: ''\n    fiaas/deployment_id: DEPLOYMENT_ID\n    fiaas/version: VERSION\n  name: v3-data-examples-tls-enabled-cert-issuer\n  namespace: default\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: '8080'\n  selector:\n    app: v3-data-examples-tls-enabled-cert-issuer\n  sessionAffinity: None\n  type: SERVICE_TYPE\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:v3-data-examples-tls-enabled-cert-issuer])"
  },
  {
    "id": "5278",
    "manifest_path": "data/manifests/the_stack_sample/sample_1874.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: v3-data-examples-tls-enabled-cert-issuer\n    fiaas/deployed_by: ''\n    fiaas/deployment_id: DEPLOYMENT_ID\n    fiaas/version: VERSION\n  name: v3-data-examples-tls-enabled-cert-issuer\n  namespace: default\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: '8080'\n  selector:\n    app: v3-data-examples-tls-enabled-cert-issuer\n  sessionAffinity: None\n  type: SERVICE_TYPE\n",
    "policy_id": "invalid-target-ports",
    "violation_text": "port targetPort \"8080\" in service \"v3-data-examples-tls-enabled-cert-issuer\" must contain at least one letter (a-z)"
  },
  {
    "id": "5279",
    "manifest_path": "data/manifests/the_stack_sample/sample_1878.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: flux-system\n    app.kubernetes.io/version: 0.2.2\n    control-plane: controller\n  name: notification-controller\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  selector:\n    app: notification-controller\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:notification-controller])"
  },
  {
    "id": "5280",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"k3s-cluster-docs\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5281",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "5282",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "5283",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "5284",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "5285",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5286",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5287",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5288",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5289",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5290",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "5291",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "5292",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "5293",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sfs-turbo-csi-plugin\" does not have a read-only root file system"
  },
  {
    "id": "5294",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"csi-sfs-turbo-controller-sa\" not found"
  },
  {
    "id": "5295",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "5296",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "5297",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "5298",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sfs-turbo-csi-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "5299",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-attacher\" has cpu request 0"
  },
  {
    "id": "5300",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-provisioner\" has cpu request 0"
  },
  {
    "id": "5301",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness-probe\" has cpu request 0"
  },
  {
    "id": "5302",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sfs-turbo-csi-plugin\" has cpu request 0"
  },
  {
    "id": "5303",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-attacher\" has memory limit 0"
  },
  {
    "id": "5304",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-provisioner\" has memory limit 0"
  },
  {
    "id": "5305",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness-probe\" has memory limit 0"
  },
  {
    "id": "5306",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sfs-turbo-csi-plugin\" has memory limit 0"
  },
  {
    "id": "5307",
    "manifest_path": "data/manifests/the_stack_sample/sample_1885.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: rush-rpc-service\n  namespace: default\n  labels:\n    app: rush-rpc\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    nodePort: 30000\n    targetPort: 80\n  selector:\n    app: rush-rpc\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:rush-rpc])"
  },
  {
    "id": "5308",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5309",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"soak\" is using an invalid container image, \"docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5310",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"soak\" does not have a read-only root file system"
  },
  {
    "id": "5311",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"soak\" is not set to runAsNonRoot"
  },
  {
    "id": "5312",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"soak\" has cpu request 0"
  },
  {
    "id": "5313",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"soak\" has memory limit 0"
  },
  {
    "id": "5314",
    "manifest_path": "data/manifests/the_stack_sample/sample_1890.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: trips\n  namespace: api\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 80\n  selector:\n    app: trips\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:trips])"
  },
  {
    "id": "5315",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"msg-path-demo-container\" is using an invalid container image, \"debian\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5316",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"msg-path-demo-container\" does not have a read-only root file system"
  },
  {
    "id": "5317",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"msg-path-demo-container\" is not set to runAsNonRoot"
  },
  {
    "id": "5318",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"msg-path-demo-container\" has cpu request 0"
  },
  {
    "id": "5319",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"msg-path-demo-container\" has memory limit 0"
  },
  {
    "id": "5320",
    "manifest_path": "data/manifests/the_stack_sample/sample_1893.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: web\n  labels:\n    name: web\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 3000\n    protocol: TCP\n  selector:\n    name: web\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:web])"
  },
  {
    "id": "5321",
    "manifest_path": "data/manifests/the_stack_sample/sample_1894.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ccsm-helm-test-tasklist\n  labels:\n    app: camunda-cloud-self-managed\n    app.kubernetes.io/name: tasklist\n    app.kubernetes.io/instance: ccsm-helm-test\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: camunda-cloud-self-managed\n    app.kubernetes.io/version: 1.3.4\n    app.kubernetes.io/component: tasklist\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    name: http\n    targetPort: 8080\n    protocol: TCP\n  selector:\n    app: camunda-cloud-self-managed\n    app.kubernetes.io/name: tasklist\n    app.kubernetes.io/instance: ccsm-helm-test\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: camunda-cloud-self-managed\n    app.kubernetes.io/component: tasklist\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:camunda-cloud-self-managed app.kubernetes.io/component:tasklist app.kubernetes.io/instance:ccsm-helm-test app.kubernetes.io/managed-by:Helm app.kubernetes.io/name:tasklist app.kubernetes.io/part-of:camunda-cloud-self-managed])"
  },
  {
    "id": "5322",
    "manifest_path": "data/manifests/the_stack_sample/sample_1897.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-loadbalancer\nspec:\n  selector:\n    app: nginx\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nginx])"
  },
  {
    "id": "5323",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cirros-vm\" is using an invalid container image, \"virtlet.cloud/cirros\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5324",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cirros-vm\" does not have a read-only root file system"
  },
  {
    "id": "5325",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cirros-vm\" is not set to runAsNonRoot"
  },
  {
    "id": "5326",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cirros-vm\" has cpu request 0"
  },
  {
    "id": "5327",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cirros-vm\" has memory limit 0"
  },
  {
    "id": "5328",
    "manifest_path": "data/manifests/the_stack_sample/sample_1904.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: guestbook-ui\n  labels:\n    app: guestbook-ui\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 5000\n  selector:\n    app: guestbook-ui\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:guestbook-ui])"
  },
  {
    "id": "5329",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"test-rc\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5330",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-rc\" does not have a read-only root file system"
  },
  {
    "id": "5331",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-rc\" is not set to runAsNonRoot"
  },
  {
    "id": "5332",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-rc\" has cpu request 0"
  },
  {
    "id": "5333",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-rc\" has memory limit 0"
  },
  {
    "id": "5334",
    "manifest_path": "data/manifests/the_stack_sample/sample_1910.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch-logging\n  namespace: evreka\n  labels:\n    k8s-app: elasticsearch-logging\n    kubernetes.io/cluster-service: 'true'\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: Elasticsearch\nspec:\n  ports:\n  - port: 9200\n    protocol: TCP\n    targetPort: db\n  selector:\n    k8s-app: elasticsearch-logging\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:elasticsearch-logging])"
  },
  {
    "id": "5335",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"yolo\" is using an invalid container image, \"github.com/josephburnett/kubecon-seattle-2018/yolo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5336",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"yolo\" does not have a read-only root file system"
  },
  {
    "id": "5337",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"controller\" not found"
  },
  {
    "id": "5338",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"yolo\" is not set to runAsNonRoot"
  },
  {
    "id": "5339",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"yolo\" has cpu request 0"
  },
  {
    "id": "5340",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"yolo\" has memory limit 0"
  },
  {
    "id": "5341",
    "manifest_path": "data/manifests/the_stack_sample/sample_1912.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-qualifiers-nrfin00009-pov1\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-qualifiers-nrfin00009-pov1\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py qualifiers NRFIN_00009\n      pov_1 3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cyborg-seeker-qualifiers-nrfin00009-pov1\" does not have a read-only root file system"
  },
  {
    "id": "5342",
    "manifest_path": "data/manifests/the_stack_sample/sample_1912.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-qualifiers-nrfin00009-pov1\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-qualifiers-nrfin00009-pov1\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py qualifiers NRFIN_00009\n      pov_1 3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cyborg-seeker-qualifiers-nrfin00009-pov1\" is not set to runAsNonRoot"
  },
  {
    "id": "5343",
    "manifest_path": "data/manifests/the_stack_sample/sample_1913.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: mongo\n  name: mongo\nspec:\n  ports:\n  - port: 27017\n    targetPort: 27017\n  selector:\n    name: mongo\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:mongo])"
  },
  {
    "id": "5344",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5345",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5346",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5347",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5348",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5349",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sleeping-container\" is using an invalid container image, \"sergiofgonzalez/sleeping-container:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5350",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sleeping-container\" does not have a read-only root file system"
  },
  {
    "id": "5351",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sleeping-container\" is not set to runAsNonRoot"
  },
  {
    "id": "5352",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sleeping-container\" has cpu request 0"
  },
  {
    "id": "5353",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sleeping-container\" has memory limit 0"
  },
  {
    "id": "5354",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "5355",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"crier\" not found"
  },
  {
    "id": "5356",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "5357",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "5358",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "5359",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5360",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5361",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5362",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5363",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5364",
    "manifest_path": "data/manifests/the_stack_sample/sample_1920.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: zk-headless\n  labels:\n    role: zk-headless\nspec:\n  ports:\n  - port: 2181\n    name: client\n    targetPort: 2181\n    protocol: TCP\n  - port: 2888\n    name: server\n    targetPort: 2888\n    protocol: TCP\n  - port: 3888\n    name: leader-election\n    targetPort: 3888\n    protocol: TCP\n  clusterIP: None\n  selector:\n    role: zk\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[role:zk])"
  },
  {
    "id": "5365",
    "manifest_path": "data/manifests/the_stack_sample/sample_1921.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kubernetes-dashboard-lb\n  namespace: kube-system\n  labels:\n    k8s-app: kubernetes-dashboard\nspec:\n  selector:\n    k8s-app: kubernetes-dashboard\n  ports:\n  - port: 443\n    targetPort: 8443\n    name: kubernetes-dashboard\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kubernetes-dashboard])"
  },
  {
    "id": "5366",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-driver-registrar\" is using an invalid container image, \"MUSTPATCHWITHKUSTOMIZE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5367",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"iks-vpc-block-node-driver\" is using an invalid container image, \"MUSTPATCHWITHKUSTOMIZE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5368",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"liveness-probe\" is using an invalid container image, \"MUSTPATCHWITHKUSTOMIZE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5369",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "5370",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"iks-vpc-block-node-driver\" does not have a read-only root file system"
  },
  {
    "id": "5371",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "5372",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ibm-vpc-block-node-sa\" not found"
  },
  {
    "id": "5373",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"iks-vpc-block-node-driver\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5374",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"iks-vpc-block-node-driver\" is privileged"
  },
  {
    "id": "5375",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "5376",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"iks-vpc-block-node-driver\" is not set to runAsNonRoot"
  },
  {
    "id": "5377",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "5378",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"iks-vpc-block-node-driver\""
  },
  {
    "id": "5379",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"iks-vpc-block-node-driver\""
  },
  {
    "id": "5380",
    "manifest_path": "data/manifests/the_stack_sample/sample_1924.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nodejs-service\n  labels:\n    name: nodejs-service\nspec:\n  type: LoadBalancer\n  ports:\n  - name: koa-port\n    targetPort: koa-port\n    port: 3000\n  selector:\n    name: nodejs-pod\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:nodejs-pod])"
  },
  {
    "id": "5381",
    "manifest_path": "data/manifests/the_stack_sample/sample_1926.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: enterprise-metrics-ruler\n    app.kubernetes.io/managed-by: Helmraiser\n    chart: enterprise-metrics-1.4.3\n    heritage: Helm\n    release: enterprise-metrics\n  name: enterprise-metrics-ruler\n  namespace: enterprise-metrics\nspec:\n  ports:\n  - name: http-metrics\n    port: 8080\n    protocol: TCP\n    targetPort: http-metrics\n  selector:\n    app: enterprise-metrics-ruler\n    release: enterprise-metrics\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:enterprise-metrics-ruler release:enterprise-metrics])"
  },
  {
    "id": "5382",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"bitnami/nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5383",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5384",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5385",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5386",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5387",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5388",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5389",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5390",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5391",
    "manifest_path": "data/manifests/the_stack_sample/sample_1931.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus\n  namespace: cloud-paas\n  labels:\n    app: prometheus\nspec:\n  ports:\n  - name: http-9090\n    protocol: TCP\n    port: 9090\n    targetPort: 9090\n  selector:\n    app: prometheus\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:prometheus])"
  },
  {
    "id": "5392",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable STANDARD_AUTHSERVICE_CERT_SECRET_NAME in container \"address-space-controller\" found"
  },
  {
    "id": "5393",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME in container \"address-space-controller\" found"
  },
  {
    "id": "5394",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"address-space-controller\" does not have a read-only root file system"
  },
  {
    "id": "5395",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"address-space-controller\" not found"
  },
  {
    "id": "5396",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"address-space-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "5397",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"address-space-controller\" has cpu request 0"
  },
  {
    "id": "5398",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dc-api\" is using an invalid container image, \"gcr.io/neural-pattern-278618/dc-api\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5399",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5400",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dc-api\" does not have a read-only root file system"
  },
  {
    "id": "5401",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dc-api\" is not set to runAsNonRoot"
  },
  {
    "id": "5402",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dc-api\" has memory limit 0"
  },
  {
    "id": "5403",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"simpleapp\" is using an invalid container image, \"dgkanatsios/simpleapp\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5404",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5405",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"simpleapp\" does not have a read-only root file system"
  },
  {
    "id": "5406",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"simpleapp\" is not set to runAsNonRoot"
  },
  {
    "id": "5407",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"simpleapp\" has cpu request 0"
  },
  {
    "id": "5408",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"simpleapp\" has memory limit 0"
  },
  {
    "id": "5409",
    "manifest_path": "data/manifests/the_stack_sample/sample_1935.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: helloworldapp-frontend\nspec:\n  type: LoadBalancer\n  ports:\n  - name: http\n    port: 80\n    targetPort: 80\n    protocol: TCP\n  selector:\n    app: helloworldapp\n    role: frontend\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:helloworldapp role:frontend])"
  },
  {
    "id": "5410",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init-sysctl\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5411",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5412",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"banias-frontend\" does not have a read-only root file system"
  },
  {
    "id": "5413",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-sysctl\" does not have a read-only root file system"
  },
  {
    "id": "5414",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"init-sysctl\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5415",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"init-sysctl\" is privileged"
  },
  {
    "id": "5416",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"banias-frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "5417",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-sysctl\" is not set to runAsNonRoot"
  },
  {
    "id": "5418",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-sysctl\" has cpu request 0"
  },
  {
    "id": "5419",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-sysctl\" has memory limit 0"
  },
  {
    "id": "5420",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5421",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5422",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5423",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5424",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5425",
    "manifest_path": "data/manifests/the_stack_sample/sample_1941.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: greenblue-850c\n  labels:\n    app: greenblue-850c\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app: greenblue-850c\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:greenblue-850c])"
  },
  {
    "id": "5426",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "5427",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5428",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "5429",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"deck\" not found"
  },
  {
    "id": "5430",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "5431",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "5432",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "5433",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "5434",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-main\" does not have a read-only root file system"
  },
  {
    "id": "5435",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-self\" does not have a read-only root file system"
  },
  {
    "id": "5436",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "5437",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kube-state-metrics\" not found"
  },
  {
    "id": "5438",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy-main\" is not set to runAsNonRoot"
  },
  {
    "id": "5439",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy-self\" is not set to runAsNonRoot"
  },
  {
    "id": "5440",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-state-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "5441",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy-main\" has memory limit 0"
  },
  {
    "id": "5442",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy-self\" has memory limit 0"
  },
  {
    "id": "5443",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "5444",
    "manifest_path": "data/manifests/the_stack_sample/sample_1945.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-selectors-svc\nspec:\n  type: ClusterIP\n  selector:\n    tier: backend\n    region: us\n  ports:\n  - port: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[region:us tier:backend])"
  },
  {
    "id": "5445",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "5446",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "5447",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "5448",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "5449",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "5450",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "5451",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "5452",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "5453",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "5454",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "5455",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "5456",
    "manifest_path": "data/manifests/the_stack_sample/sample_1948.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: keep-ecdsa-1\n  namespace: default\n  labels:\n    app: keep\n    type: ecdsa\n    id: '1'\n    network: ropsten\n    chain: ethereum\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 3919\n    targetPort: 3919\n    name: tcp-3919\n  - port: 9601\n    targetPort: 9601\n    name: tcp-9601\n  selector:\n    app: keep\n    type: ecdsa\n    id: '1'\n    network: ropsten\n    chain: ethereum\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:keep chain:ethereum id:1 network:ropsten type:ecdsa])"
  },
  {
    "id": "5457",
    "manifest_path": "data/manifests/the_stack_sample/sample_1950.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: client-cluster-ip-service\nspec:\n  type: ClusterIP\n  selector:\n    component: web\n  ports:\n  - port: 3000\n    targetPort: 3000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:web])"
  },
  {
    "id": "5458",
    "manifest_path": "data/manifests/the_stack_sample/sample_1951.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-operator\n  namespace: istio-operator\n",
    "policy_id": "mismatching-selector",
    "violation_text": "object has no selector specified"
  },
  {
    "id": "5459",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"results-app\" is using an invalid container image, \"dockersamples/examplevotingapp_result\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5460",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"results-app\" does not have a read-only root file system"
  },
  {
    "id": "5461",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"results-app\" is not set to runAsNonRoot"
  },
  {
    "id": "5462",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"results-app\" has cpu request 0"
  },
  {
    "id": "5463",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"results-app\" has memory limit 0"
  },
  {
    "id": "5464",
    "manifest_path": "data/manifests/the_stack_sample/sample_1960.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: k8s-io\nspec:\n  selector:\n    app: k8s-io\n  type: LoadBalancer\n  ports:\n  - name: http\n    port: 80\n  - name: https\n    port: 443\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:k8s-io])"
  },
  {
    "id": "5465",
    "manifest_path": "data/manifests/the_stack_sample/sample_1961.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  selector:\n    app: mysql\n  ports:\n  - protocol: TCP\n    port: 3306\n    targetPort: 3306\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:mysql])"
  },
  {
    "id": "5466",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "5467",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "5468",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "5469",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "5470",
    "manifest_path": "data/manifests/the_stack_sample/sample_1963.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: kube-janitor\n  labels:\n    app: kube-janitor\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: kube-janitor\n        containers:\n        - name: kube-janitor\n          image: themagicalkarp/kube-janitor:v0.1.0\n          imagePullPolicy: Always\n          command:\n          - /kube-janitor\n          - -expiration=2880\n          - -annotation=kube.janitor.io\n          - -pendingJobExpiration=60\n          - -verbose\n          resources:\n            limits:\n              cpu: 200m\n              memory: 100Mi\n            requests:\n              cpu: 50m\n              memory: 50Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-janitor\" does not have a read-only root file system"
  },
  {
    "id": "5471",
    "manifest_path": "data/manifests/the_stack_sample/sample_1963.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: kube-janitor\n  labels:\n    app: kube-janitor\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: kube-janitor\n        containers:\n        - name: kube-janitor\n          image: themagicalkarp/kube-janitor:v0.1.0\n          imagePullPolicy: Always\n          command:\n          - /kube-janitor\n          - -expiration=2880\n          - -annotation=kube.janitor.io\n          - -pendingJobExpiration=60\n          - -verbose\n          resources:\n            limits:\n              cpu: 200m\n              memory: 100Mi\n            requests:\n              cpu: 50m\n              memory: 50Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kube-janitor\" not found"
  },
  {
    "id": "5472",
    "manifest_path": "data/manifests/the_stack_sample/sample_1963.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: kube-janitor\n  labels:\n    app: kube-janitor\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: kube-janitor\n        containers:\n        - name: kube-janitor\n          image: themagicalkarp/kube-janitor:v0.1.0\n          imagePullPolicy: Always\n          command:\n          - /kube-janitor\n          - -expiration=2880\n          - -annotation=kube.janitor.io\n          - -pendingJobExpiration=60\n          - -verbose\n          resources:\n            limits:\n              cpu: 200m\n              memory: 100Mi\n            requests:\n              cpu: 50m\n              memory: 50Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-janitor\" is not set to runAsNonRoot"
  },
  {
    "id": "5473",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"signalfx-agent\" does not have a read-only root file system"
  },
  {
    "id": "5474",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"signalfx-agent\" not found"
  },
  {
    "id": "5475",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"signalfx-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "5476",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"signalfx-agent\" has cpu request 0"
  },
  {
    "id": "5477",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"signalfx-agent\" has memory limit 0"
  },
  {
    "id": "5478",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "5479",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"vppagent-client\" is using an invalid container image, \"networkservicemesh/vpp-icmp-vppagent-client:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5480",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5481",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vppagent-client\" does not have a read-only root file system"
  },
  {
    "id": "5482",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vppagent-client\" is not set to runAsNonRoot"
  },
  {
    "id": "5483",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vppagent-client\" has cpu request 0"
  },
  {
    "id": "5484",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vppagent-client\" has memory limit 0"
  },
  {
    "id": "5485",
    "manifest_path": "data/manifests/the_stack_sample/sample_1967.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service-6\nspec:\n  selector:\n    app: MyApp\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 9375\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:MyApp])"
  },
  {
    "id": "5486",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (kurl-proxy), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "5487",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable TLS_SECRET_NAME in container \"proxy\" found"
  },
  {
    "id": "5488",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"proxy\" does not have a read-only root file system"
  },
  {
    "id": "5489",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kurl-proxy\" not found"
  },
  {
    "id": "5490",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "5491",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"proxy\" has cpu request 0"
  },
  {
    "id": "5492",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"proxy\" has memory limit 0"
  },
  {
    "id": "5493",
    "manifest_path": "data/manifests/the_stack_sample/sample_1969.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-producer\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 8081\n    name: http\n  - port: 443\n    protocol: TCP\n    targetPort: 8081\n    name: https\n  selector:\n    app: kafka-producer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kafka-producer])"
  },
  {
    "id": "5494",
    "manifest_path": "data/manifests/the_stack_sample/sample_1971.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: healthylinkx-api-service\n  labels:\n    app: healthylinkx-api\nspec:\n  type: NodePort\n  ports:\n  - port: 8081\n    nodePort: 30100\n    protocol: TCP\n    name: http\n  selector:\n    app: healthylinkx-api\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:healthylinkx-api])"
  },
  {
    "id": "5495",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"cilium-agent\""
  },
  {
    "id": "5496",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"cilium-agent\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "5497",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"clean-cilium-state\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "5498",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "5499",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cilium-agent\" is using an invalid container image, \"docker.io/cilium/cilium:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5500",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "5501",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "5502",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium\" not found"
  },
  {
    "id": "5503",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5504",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"clean-cilium-state\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5505",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cilium-agent\" is privileged"
  },
  {
    "id": "5506",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"clean-cilium-state\" is privileged"
  },
  {
    "id": "5507",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "5508",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "5509",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "5510",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "5511",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "5512",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "5513",
    "manifest_path": "data/manifests/the_stack_sample/sample_1976.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kubek\nspec:\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n  - name: https\n    port: 443\n    targetPort: https\n  selector:\n    app: kubek\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kubek])"
  },
  {
    "id": "5514",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "5515",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "5516",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "5517",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "5518",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5519",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "5520",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"litmus\" not found"
  },
  {
    "id": "5521",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "5522",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "5523",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "5524",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5525",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"helloworld\" does not have a read-only root file system"
  },
  {
    "id": "5526",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"helloworld\" is not set to runAsNonRoot"
  },
  {
    "id": "5527",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"helloworld\" has cpu request 0"
  },
  {
    "id": "5528",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"helloworld\" has memory limit 0"
  },
  {
    "id": "5529",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5530",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container\" does not have a read-only root file system"
  },
  {
    "id": "5531",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container\" is not set to runAsNonRoot"
  },
  {
    "id": "5532",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container\" has cpu request 0"
  },
  {
    "id": "5533",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container\" has memory limit 0"
  },
  {
    "id": "5534",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "5535",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "5536",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "5537",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "5538",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "5539",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "5540",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "5541",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "5542",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "5543",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "5544",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "5545",
    "manifest_path": "data/manifests/the_stack_sample/sample_1990.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: test-chain-node-port\nspec:\n  type: NodePort\n  ports:\n  - port: 50004\n    targetPort: 50004\n    nodePort: 30004\n    name: rpc\n  selector:\n    chain_name: chainconfig-sample\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[chain_name:chainconfig-sample])"
  },
  {
    "id": "5546",
    "manifest_path": "data/manifests/the_stack_sample/sample_1992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load\n  labels:\n    service: load\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: load\n  template:\n    metadata:\n      labels:\n        service: load\n    spec:\n      containers:\n      - name: load\n        env:\n        - name: HOST\n          value: http://payment.robotshop:8080/health\n        - name: NUM_CLIENTS\n          value: '15'\n        - name: SILENT\n          value: '1'\n        - name: ERROR\n          value: '1'\n        image: robotshop/rs-load:latest\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"load\" is using an invalid container image, \"robotshop/rs-load:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5547",
    "manifest_path": "data/manifests/the_stack_sample/sample_1992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load\n  labels:\n    service: load\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: load\n  template:\n    metadata:\n      labels:\n        service: load\n    spec:\n      containers:\n      - name: load\n        env:\n        - name: HOST\n          value: http://payment.robotshop:8080/health\n        - name: NUM_CLIENTS\n          value: '15'\n        - name: SILENT\n          value: '1'\n        - name: ERROR\n          value: '1'\n        image: robotshop/rs-load:latest\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"load\" does not have a read-only root file system"
  },
  {
    "id": "5548",
    "manifest_path": "data/manifests/the_stack_sample/sample_1992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load\n  labels:\n    service: load\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: load\n  template:\n    metadata:\n      labels:\n        service: load\n    spec:\n      containers:\n      - name: load\n        env:\n        - name: HOST\n          value: http://payment.robotshop:8080/health\n        - name: NUM_CLIENTS\n          value: '15'\n        - name: SILENT\n          value: '1'\n        - name: ERROR\n          value: '1'\n        image: robotshop/rs-load:latest\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"load\" is not set to runAsNonRoot"
  },
  {
    "id": "5549",
    "manifest_path": "data/manifests/the_stack_sample/sample_1994.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  type: NodePort\n  ports:\n  - port: 9292\n    protocol: TCP\n    targetPort: 9292\n  selector:\n    app: reddit\n    component: ui\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:reddit component:ui])"
  },
  {
    "id": "5550",
    "manifest_path": "data/manifests/the_stack_sample/sample_1997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: myservice\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9376\n",
    "policy_id": "dangling-service",
    "violation_text": "service has no selector specified"
  },
  {
    "id": "5551",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "host-ipc",
    "violation_text": "resource shares host's IPC namespace (via hostIPC=true)."
  },
  {
    "id": "5552",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5553",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mlperf-inference-container\" is using an invalid container image, \"aferikoglou/mlperf-inference:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5554",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mlperf-inference-container\" does not have a read-only root file system"
  },
  {
    "id": "5555",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mlperf-inference-container\" is not set to runAsNonRoot"
  },
  {
    "id": "5556",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mlperf-inference-container\" has cpu request 0"
  },
  {
    "id": "5557",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mlperf-inference-container\" has memory limit 0"
  },
  {
    "id": "5558",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "5559",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ebs-plugin\" is using an invalid container image, \"k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5560",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ebs-plugin\" does not have a read-only root file system"
  },
  {
    "id": "5561",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "5562",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "5563",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ebs-csi-node-sa\" not found"
  },
  {
    "id": "5564",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"ebs-plugin\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5565",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"ebs-plugin\" is privileged"
  },
  {
    "id": "5566",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ebs-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "5567",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "5568",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "5569",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"ebs-plugin\""
  },
  {
    "id": "5570",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ebs-plugin\" has cpu request 0"
  },
  {
    "id": "5571",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness-probe\" has cpu request 0"
  },
  {
    "id": "5572",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-driver-registrar\" has cpu request 0"
  },
  {
    "id": "5573",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ebs-plugin\" has memory limit 0"
  },
  {
    "id": "5574",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness-probe\" has memory limit 0"
  },
  {
    "id": "5575",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-driver-registrar\" has memory limit 0"
  },
  {
    "id": "5576",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"smi-adapter-istio\" is using an invalid container image, \"layer5/smi-istio:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5577",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"smi-adapter-istio\" does not have a read-only root file system"
  },
  {
    "id": "5578",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"smi-adapter-istio\" not found"
  },
  {
    "id": "5579",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"smi-adapter-istio\" is not set to runAsNonRoot"
  },
  {
    "id": "5580",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"smi-adapter-istio\" has cpu request 0"
  },
  {
    "id": "5581",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"smi-adapter-istio\" has memory limit 0"
  },
  {
    "id": "5582",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5583",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"certs\" does not have a read-only root file system"
  },
  {
    "id": "5584",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"certs\" is not set to runAsNonRoot"
  },
  {
    "id": "5585",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"certs\" has cpu request 0"
  },
  {
    "id": "5586",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"certs\" has memory limit 0"
  },
  {
    "id": "5587",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-operator\" does not have a read-only root file system"
  },
  {
    "id": "5588",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jaeger-operator\" not found"
  },
  {
    "id": "5589",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "5590",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-operator\" has cpu request 0"
  },
  {
    "id": "5591",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-operator\" has memory limit 0"
  },
  {
    "id": "5592",
    "manifest_path": "data/manifests/the_stack_sample/sample_2005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - uid_entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"argocd-repo-server\" is using an invalid container image, \"quay.io/argoproj/argocd:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5593",
    "manifest_path": "data/manifests/the_stack_sample/sample_2005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - uid_entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"argocd-repo-server\" does not have a read-only root file system"
  },
  {
    "id": "5594",
    "manifest_path": "data/manifests/the_stack_sample/sample_2005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - uid_entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"argocd-repo-server\" is not set to runAsNonRoot"
  },
  {
    "id": "5595",
    "manifest_path": "data/manifests/the_stack_sample/sample_2005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - uid_entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"argocd-repo-server\" has cpu request 0"
  },
  {
    "id": "5596",
    "manifest_path": "data/manifests/the_stack_sample/sample_2005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - uid_entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"argocd-repo-server\" has memory limit 0"
  },
  {
    "id": "5597",
    "manifest_path": "data/manifests/the_stack_sample/sample_2006.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: timeserver-lb\nspec:\n  selector:\n    app: timeserver\n  ports:\n  - port: 80\n    targetPort: 8080\n    protocol: TCP\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:timeserver])"
  },
  {
    "id": "5598",
    "manifest_path": "data/manifests/the_stack_sample/sample_2008.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200416-dd6c31b18\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "5599",
    "manifest_path": "data/manifests/the_stack_sample/sample_2008.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200416-dd6c31b18\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"crier\" not found"
  },
  {
    "id": "5600",
    "manifest_path": "data/manifests/the_stack_sample/sample_2008.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200416-dd6c31b18\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "5601",
    "manifest_path": "data/manifests/the_stack_sample/sample_2008.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200416-dd6c31b18\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "5602",
    "manifest_path": "data/manifests/the_stack_sample/sample_2008.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200416-dd6c31b18\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "5603",
    "manifest_path": "data/manifests/the_stack_sample/sample_2009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod03\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.1\n    volumeMounts:\n    - mountPath: /var/local/aaa\n      name: mydir\n  volumes:\n  - name: mydir\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5604",
    "manifest_path": "data/manifests/the_stack_sample/sample_2009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod03\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.1\n    volumeMounts:\n    - mountPath: /var/local/aaa\n      name: mydir\n  volumes:\n  - name: mydir\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5605",
    "manifest_path": "data/manifests/the_stack_sample/sample_2009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod03\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.1\n    volumeMounts:\n    - mountPath: /var/local/aaa\n      name: mydir\n  volumes:\n  - name: mydir\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5606",
    "manifest_path": "data/manifests/the_stack_sample/sample_2009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod03\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.1\n    volumeMounts:\n    - mountPath: /var/local/aaa\n      name: mydir\n  volumes:\n  - name: mydir\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5607",
    "manifest_path": "data/manifests/the_stack_sample/sample_2013.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: debian-debug\n  namespace: default\n  annotations:\n    injector.tumblr.com/request: test1\nspec:\n  containers:\n  - image: debian:jessie\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: debian-debug\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"debian-debug\" does not have a read-only root file system"
  },
  {
    "id": "5608",
    "manifest_path": "data/manifests/the_stack_sample/sample_2013.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: debian-debug\n  namespace: default\n  annotations:\n    injector.tumblr.com/request: test1\nspec:\n  containers:\n  - image: debian:jessie\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: debian-debug\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"debian-debug\" is not set to runAsNonRoot"
  },
  {
    "id": "5609",
    "manifest_path": "data/manifests/the_stack_sample/sample_2013.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: debian-debug\n  namespace: default\n  annotations:\n    injector.tumblr.com/request: test1\nspec:\n  containers:\n  - image: debian:jessie\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: debian-debug\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"debian-debug\" has cpu request 0"
  },
  {
    "id": "5610",
    "manifest_path": "data/manifests/the_stack_sample/sample_2013.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: debian-debug\n  namespace: default\n  annotations:\n    injector.tumblr.com/request: test1\nspec:\n  containers:\n  - image: debian:jessie\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: debian-debug\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"debian-debug\" has memory limit 0"
  },
  {
    "id": "5611",
    "manifest_path": "data/manifests/the_stack_sample/sample_2014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: myfirstapp-lb-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: myapp\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:myapp])"
  },
  {
    "id": "5612",
    "manifest_path": "data/manifests/the_stack_sample/sample_2015.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  selector:\n    matchLabels:\n      run: load-balancer-example\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: load-balancer-example\n    spec:\n      containers:\n      - name: hello-world\n        image: gcr.io/google-samples/node-hello:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5613",
    "manifest_path": "data/manifests/the_stack_sample/sample_2015.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  selector:\n    matchLabels:\n      run: load-balancer-example\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: load-balancer-example\n    spec:\n      containers:\n      - name: hello-world\n        image: gcr.io/google-samples/node-hello:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello-world\" does not have a read-only root file system"
  },
  {
    "id": "5614",
    "manifest_path": "data/manifests/the_stack_sample/sample_2015.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  selector:\n    matchLabels:\n      run: load-balancer-example\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: load-balancer-example\n    spec:\n      containers:\n      - name: hello-world\n        image: gcr.io/google-samples/node-hello:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello-world\" is not set to runAsNonRoot"
  },
  {
    "id": "5615",
    "manifest_path": "data/manifests/the_stack_sample/sample_2015.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  selector:\n    matchLabels:\n      run: load-balancer-example\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: load-balancer-example\n    spec:\n      containers:\n      - name: hello-world\n        image: gcr.io/google-samples/node-hello:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello-world\" has cpu request 0"
  },
  {
    "id": "5616",
    "manifest_path": "data/manifests/the_stack_sample/sample_2015.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  selector:\n    matchLabels:\n      run: load-balancer-example\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: load-balancer-example\n    spec:\n      containers:\n      - name: hello-world\n        image: gcr.io/google-samples/node-hello:1.0\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello-world\" has memory limit 0"
  },
  {
    "id": "5617",
    "manifest_path": "data/manifests/the_stack_sample/sample_2016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: kubernetes.io/hostname\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: topology.kubernetes.io/zone\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=kube-system\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        resources:\n          limits:\n            cpu: 200m\n            memory: 64Mi\n          requests:\n            cpu: 200m\n            memory: 64Mi\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pod-identity-webhook\" is using an invalid container image, \"IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5618",
    "manifest_path": "data/manifests/the_stack_sample/sample_2016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: kubernetes.io/hostname\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: topology.kubernetes.io/zone\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=kube-system\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        resources:\n          limits:\n            cpu: 200m\n            memory: 64Mi\n          requests:\n            cpu: 200m\n            memory: 64Mi\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pod-identity-webhook\" does not have a read-only root file system"
  },
  {
    "id": "5619",
    "manifest_path": "data/manifests/the_stack_sample/sample_2016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: kubernetes.io/hostname\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: topology.kubernetes.io/zone\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=kube-system\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        resources:\n          limits:\n            cpu: 200m\n            memory: 64Mi\n          requests:\n            cpu: 200m\n            memory: 64Mi\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"pod-identity-webhook\" not found"
  },
  {
    "id": "5620",
    "manifest_path": "data/manifests/the_stack_sample/sample_2016.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: kubernetes.io/hostname\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - pod-identity-webhook\n              topologyKey: topology.kubernetes.io/zone\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=kube-system\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        resources:\n          limits:\n            cpu: 200m\n            memory: 64Mi\n          requests:\n            cpu: 200m\n            memory: 64Mi\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pod-identity-webhook\" is not set to runAsNonRoot"
  },
  {
    "id": "5621",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"database\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5622",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"pgo-pg\" not found"
  },
  {
    "id": "5623",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"database\" is not set to runAsNonRoot"
  },
  {
    "id": "5624",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"database\" has cpu request 0"
  },
  {
    "id": "5625",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"database\" has memory limit 0"
  },
  {
    "id": "5626",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5627",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy\" does not have a read-only root file system"
  },
  {
    "id": "5628",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "5629",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "5630",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "5631",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-rbac-proxy\" has cpu request 0"
  },
  {
    "id": "5632",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy\" has memory limit 0"
  },
  {
    "id": "5633",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5634",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ibmmq-client\" is using an invalid container image, \"mqkeda/sample-app:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5635",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ibmmq-client\" does not have a read-only root file system"
  },
  {
    "id": "5636",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ibmmq-client\" is not set to runAsNonRoot"
  },
  {
    "id": "5637",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ibmmq-client\" has cpu request 0"
  },
  {
    "id": "5638",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ibmmq-client\" has memory limit 0"
  },
  {
    "id": "5639",
    "manifest_path": "data/manifests/the_stack_sample/sample_2025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: thanos-query\n  name: thanos-query\n  namespace: default\nspec:\n  ports:\n  - name: http\n    port: 10902\n    targetPort: http\n  selector:\n    app: thanos-query\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:thanos-query])"
  },
  {
    "id": "5640",
    "manifest_path": "data/manifests/the_stack_sample/sample_2028.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch-logging\n  namespace: kube-system\n  labels:\n    k8s-app: elasticsearch-logging\n    kubernetes.io/cluster-service: 'true'\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: Elasticsearch\nspec:\n  selector:\n    k8s-app: elasticsearch-logging\n    version: v6.3.0\n  ports:\n  - port: 9200\n    protocol: TCP\n    targetPort: db\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:elasticsearch-logging version:v6.3.0])"
  },
  {
    "id": "5641",
    "manifest_path": "data/manifests/the_stack_sample/sample_2029.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  namespace: healthcare-hlf\n  name: peer%NUM%-%ORG%-nlb\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\nspec:\n  selector:\n    name: peer%NUM%-%ORG%\n  ports:\n  - name: grpc\n    protocol: TCP\n    port: 7051\n    targetPort: 7051\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "service has invalid label selector: values[0][name]: Invalid value: \"peer%NUM%-%ORG%\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "5642",
    "manifest_path": "data/manifests/the_stack_sample/sample_2031.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: juiceservice\nspec:\n  type: ClusterIP\n  selector:\n    app: juiceservice\n  ports:\n  - name: http\n    port: 80\n    targetPort: 3000\n    appProtocol: https\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:juiceservice])"
  },
  {
    "id": "5643",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "5644",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "5645",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "5646",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "5647",
    "manifest_path": "data/manifests/the_stack_sample/sample_2033.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: project-manager-service\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - port: 50300\n    protocol: TCP\n    targetPort: 50300\n    name: tcp-http2\n  selector:\n    name: project-manager-server\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:project-manager-server])"
  },
  {
    "id": "5648",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5649",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5650",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "5651",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "5652",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "5653",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "5654",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "5655",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "5656",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"jaeger-agent\" is using an invalid container image, \"jaegertracing/jaeger-agent\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5657",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-agent\" does not have a read-only root file system"
  },
  {
    "id": "5658",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress\" does not have a read-only root file system"
  },
  {
    "id": "5659",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"nginx-ingress\" not found"
  },
  {
    "id": "5660",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nginx-ingress\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "5661",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "5662",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-agent\" has cpu request 0"
  },
  {
    "id": "5663",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress\" has cpu request 0"
  },
  {
    "id": "5664",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-agent\" has memory limit 0"
  },
  {
    "id": "5665",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress\" has memory limit 0"
  },
  {
    "id": "5666",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5667",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5668",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5669",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5670",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5671",
    "manifest_path": "data/manifests/the_stack_sample/sample_2040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  namespace: shg\n  labels:\n    app: shg-comet\n  name: shg-comet\nspec:\n  ports:\n  - name: http-comet-api\n    port: 9443\n    protocol: TCP\n    targetPort: comet-api\n  selector:\n    app: shg-comet\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:shg-comet])"
  },
  {
    "id": "5672",
    "manifest_path": "data/manifests/the_stack_sample/sample_2042.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-reflector-controller\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      app: image-reflector-controller\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: image-reflector-controller\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --enable-leader-election\n        image: squaremo/image-reflector-controller\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"squaremo/image-reflector-controller\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5673",
    "manifest_path": "data/manifests/the_stack_sample/sample_2042.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-reflector-controller\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      app: image-reflector-controller\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: image-reflector-controller\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --enable-leader-election\n        image: squaremo/image-reflector-controller\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "5674",
    "manifest_path": "data/manifests/the_stack_sample/sample_2042.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-reflector-controller\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      app: image-reflector-controller\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: image-reflector-controller\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --enable-leader-election\n        image: squaremo/image-reflector-controller\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "5675",
    "manifest_path": "data/manifests/the_stack_sample/sample_2043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: a042ef704bc21bfd8abcd79f32e9788ecad0bdb70e7653aff9458eda85bafe3b\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: skreet2k\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-keeper\" does not have a read-only root file system"
  },
  {
    "id": "5676",
    "manifest_path": "data/manifests/the_stack_sample/sample_2043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: a042ef704bc21bfd8abcd79f32e9788ecad0bdb70e7653aff9458eda85bafe3b\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: skreet2k\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-keeper\" not found"
  },
  {
    "id": "5677",
    "manifest_path": "data/manifests/the_stack_sample/sample_2043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: a042ef704bc21bfd8abcd79f32e9788ecad0bdb70e7653aff9458eda85bafe3b\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: skreet2k\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-keeper\" is not set to runAsNonRoot"
  },
  {
    "id": "5678",
    "manifest_path": "data/manifests/the_stack_sample/sample_2047.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: privileged\n    image: busybox\n    command:\n    - sleep\n    - '9999'\n    securityContext:\n      allowPrivilegeEscalation: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"privileged\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5679",
    "manifest_path": "data/manifests/the_stack_sample/sample_2047.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: privileged\n    image: busybox\n    command:\n    - sleep\n    - '9999'\n    securityContext:\n      allowPrivilegeEscalation: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"privileged\" does not have a read-only root file system"
  },
  {
    "id": "5680",
    "manifest_path": "data/manifests/the_stack_sample/sample_2047.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: privileged\n    image: busybox\n    command:\n    - sleep\n    - '9999'\n    securityContext:\n      allowPrivilegeEscalation: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"privileged\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "5681",
    "manifest_path": "data/manifests/the_stack_sample/sample_2047.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: privileged\n    image: busybox\n    command:\n    - sleep\n    - '9999'\n    securityContext:\n      allowPrivilegeEscalation: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"privileged\" has cpu request 0"
  },
  {
    "id": "5682",
    "manifest_path": "data/manifests/the_stack_sample/sample_2047.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged\nspec:\n  securityContext:\n    runAsNonRoot: true\n  containers:\n  - name: privileged\n    image: busybox\n    command:\n    - sleep\n    - '9999'\n    securityContext:\n      allowPrivilegeEscalation: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"privileged\" has memory limit 0"
  },
  {
    "id": "5683",
    "manifest_path": "data/manifests/the_stack_sample/sample_2049.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: rstudio-service\n  namespace: rstudio\nspec:\n  selector:\n    app: rstudio\n  ports:\n  - port: 10002\n    targetPort: 8787\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:rstudio])"
  },
  {
    "id": "5684",
    "manifest_path": "data/manifests/the_stack_sample/sample_2050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-cloud-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: cloud-controller-manager\n    image: mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0\n    imagePullPolicy: IfNotPresent\n    command:\n    - cloud-controller-manager\n    args:\n    - --cloud-provider=azure\n    - --controllers=cloud-node\n    - --kubeconfig=/etc/kubernetes/secrets/kubeconfig\n    - --cloud-config=/etc/kubernetes/configs/cloud.conf\n    - --leader-elect=false\n    - --port=10267\n    - -v=2\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10267\n      initialDelaySeconds: 20\n      periodSeconds: 10\n      timeoutSeconds: 5\n    volumeMounts:\n    - name: secrets\n      mountPath: /etc/kubernetes/secrets\n    - name: configs\n      mountPath: /etc/kubernetes/configs\n  volumes:\n  - name: secrets\n    hostPath:\n      path: /etc/kubernetes/bootstrap-secrets\n  - name: configs\n    hostPath:\n      path: /etc/kubernetes/bootstrap-configs\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "5685",
    "manifest_path": "data/manifests/the_stack_sample/sample_2050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-cloud-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: cloud-controller-manager\n    image: mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0\n    imagePullPolicy: IfNotPresent\n    command:\n    - cloud-controller-manager\n    args:\n    - --cloud-provider=azure\n    - --controllers=cloud-node\n    - --kubeconfig=/etc/kubernetes/secrets/kubeconfig\n    - --cloud-config=/etc/kubernetes/configs/cloud.conf\n    - --leader-elect=false\n    - --port=10267\n    - -v=2\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10267\n      initialDelaySeconds: 20\n      periodSeconds: 10\n      timeoutSeconds: 5\n    volumeMounts:\n    - name: secrets\n      mountPath: /etc/kubernetes/secrets\n    - name: configs\n      mountPath: /etc/kubernetes/configs\n  volumes:\n  - name: secrets\n    hostPath:\n      path: /etc/kubernetes/bootstrap-secrets\n  - name: configs\n    hostPath:\n      path: /etc/kubernetes/bootstrap-configs\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cloud-controller-manager\" does not expose port 10267 for the HTTPGet"
  },
  {
    "id": "5686",
    "manifest_path": "data/manifests/the_stack_sample/sample_2050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-cloud-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: cloud-controller-manager\n    image: mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0\n    imagePullPolicy: IfNotPresent\n    command:\n    - cloud-controller-manager\n    args:\n    - --cloud-provider=azure\n    - --controllers=cloud-node\n    - --kubeconfig=/etc/kubernetes/secrets/kubeconfig\n    - --cloud-config=/etc/kubernetes/configs/cloud.conf\n    - --leader-elect=false\n    - --port=10267\n    - -v=2\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10267\n      initialDelaySeconds: 20\n      periodSeconds: 10\n      timeoutSeconds: 5\n    volumeMounts:\n    - name: secrets\n      mountPath: /etc/kubernetes/secrets\n    - name: configs\n      mountPath: /etc/kubernetes/configs\n  volumes:\n  - name: secrets\n    hostPath:\n      path: /etc/kubernetes/bootstrap-secrets\n  - name: configs\n    hostPath:\n      path: /etc/kubernetes/bootstrap-configs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cloud-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "5687",
    "manifest_path": "data/manifests/the_stack_sample/sample_2050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-cloud-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: cloud-controller-manager\n    image: mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0\n    imagePullPolicy: IfNotPresent\n    command:\n    - cloud-controller-manager\n    args:\n    - --cloud-provider=azure\n    - --controllers=cloud-node\n    - --kubeconfig=/etc/kubernetes/secrets/kubeconfig\n    - --cloud-config=/etc/kubernetes/configs/cloud.conf\n    - --leader-elect=false\n    - --port=10267\n    - -v=2\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10267\n      initialDelaySeconds: 20\n      periodSeconds: 10\n      timeoutSeconds: 5\n    volumeMounts:\n    - name: secrets\n      mountPath: /etc/kubernetes/secrets\n    - name: configs\n      mountPath: /etc/kubernetes/configs\n  volumes:\n  - name: secrets\n    hostPath:\n      path: /etc/kubernetes/bootstrap-secrets\n  - name: configs\n    hostPath:\n      path: /etc/kubernetes/bootstrap-configs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cloud-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "5688",
    "manifest_path": "data/manifests/the_stack_sample/sample_2050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-cloud-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: cloud-controller-manager\n    image: mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0\n    imagePullPolicy: IfNotPresent\n    command:\n    - cloud-controller-manager\n    args:\n    - --cloud-provider=azure\n    - --controllers=cloud-node\n    - --kubeconfig=/etc/kubernetes/secrets/kubeconfig\n    - --cloud-config=/etc/kubernetes/configs/cloud.conf\n    - --leader-elect=false\n    - --port=10267\n    - -v=2\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10267\n      initialDelaySeconds: 20\n      periodSeconds: 10\n      timeoutSeconds: 5\n    volumeMounts:\n    - name: secrets\n      mountPath: /etc/kubernetes/secrets\n    - name: configs\n      mountPath: /etc/kubernetes/configs\n  volumes:\n  - name: secrets\n    hostPath:\n      path: /etc/kubernetes/bootstrap-secrets\n  - name: configs\n    hostPath:\n      path: /etc/kubernetes/bootstrap-configs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cloud-controller-manager\" has cpu request 0"
  },
  {
    "id": "5689",
    "manifest_path": "data/manifests/the_stack_sample/sample_2050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-cloud-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: cloud-controller-manager\n    image: mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0\n    imagePullPolicy: IfNotPresent\n    command:\n    - cloud-controller-manager\n    args:\n    - --cloud-provider=azure\n    - --controllers=cloud-node\n    - --kubeconfig=/etc/kubernetes/secrets/kubeconfig\n    - --cloud-config=/etc/kubernetes/configs/cloud.conf\n    - --leader-elect=false\n    - --port=10267\n    - -v=2\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10267\n      initialDelaySeconds: 20\n      periodSeconds: 10\n      timeoutSeconds: 5\n    volumeMounts:\n    - name: secrets\n      mountPath: /etc/kubernetes/secrets\n    - name: configs\n      mountPath: /etc/kubernetes/configs\n  volumes:\n  - name: secrets\n    hostPath:\n      path: /etc/kubernetes/bootstrap-secrets\n  - name: configs\n    hostPath:\n      path: /etc/kubernetes/bootstrap-configs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cloud-controller-manager\" has memory limit 0"
  },
  {
    "id": "5690",
    "manifest_path": "data/manifests/the_stack_sample/sample_2052.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220115-3e20513bd7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --supplemental-plugin-config-dir=/etc/plugins\n        - --config-path=/etc/config/config.yaml\n        - --supplemental-prow-config-dir=/etc/config\n        - --github-app-id=$(GITHUB_APP_ID)\n        - --github-app-private-key-path=/etc/github/cert\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        env:\n        - name: GITHUB_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: openshift-prow-github-app\n              key: appid\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: github-app-credentials\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: github-app-credentials\n        secret:\n          secretName: openshift-prow-github-app\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        projected:\n          sources:\n          - configMap:\n              name: job-config-misc\n          - configMap:\n              name: job-config-master-periodics\n          - configMap:\n              name: job-config-master-postsubmits\n          - configMap:\n              name: job-config-master-presubmits\n          - configMap:\n              name: job-config-3.x\n          - configMap:\n              name: job-config-4.1\n          - configMap:\n              name: job-config-4.2\n          - configMap:\n              name: job-config-4.3\n          - configMap:\n              name: job-config-4.4\n          - configMap:\n              name: job-config-4.5\n          - configMap:\n              name: job-config-4.6\n          - configMap:\n              name: job-config-4.7\n          - configMap:\n              name: job-config-4.8\n          - configMap:\n              name: job-config-4.9\n          - configMap:\n              name: job-config-4.10\n          - configMap:\n              name: job-config-4.11\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "5691",
    "manifest_path": "data/manifests/the_stack_sample/sample_2052.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220115-3e20513bd7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --supplemental-plugin-config-dir=/etc/plugins\n        - --config-path=/etc/config/config.yaml\n        - --supplemental-prow-config-dir=/etc/config\n        - --github-app-id=$(GITHUB_APP_ID)\n        - --github-app-private-key-path=/etc/github/cert\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        env:\n        - name: GITHUB_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: openshift-prow-github-app\n              key: appid\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: github-app-credentials\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: github-app-credentials\n        secret:\n          secretName: openshift-prow-github-app\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        projected:\n          sources:\n          - configMap:\n              name: job-config-misc\n          - configMap:\n              name: job-config-master-periodics\n          - configMap:\n              name: job-config-master-postsubmits\n          - configMap:\n              name: job-config-master-presubmits\n          - configMap:\n              name: job-config-3.x\n          - configMap:\n              name: job-config-4.1\n          - configMap:\n              name: job-config-4.2\n          - configMap:\n              name: job-config-4.3\n          - configMap:\n              name: job-config-4.4\n          - configMap:\n              name: job-config-4.5\n          - configMap:\n              name: job-config-4.6\n          - configMap:\n              name: job-config-4.7\n          - configMap:\n              name: job-config-4.8\n          - configMap:\n              name: job-config-4.9\n          - configMap:\n              name: job-config-4.10\n          - configMap:\n              name: job-config-4.11\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"statusreconciler\" not found"
  },
  {
    "id": "5692",
    "manifest_path": "data/manifests/the_stack_sample/sample_2052.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220115-3e20513bd7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --supplemental-plugin-config-dir=/etc/plugins\n        - --config-path=/etc/config/config.yaml\n        - --supplemental-prow-config-dir=/etc/config\n        - --github-app-id=$(GITHUB_APP_ID)\n        - --github-app-private-key-path=/etc/github/cert\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        env:\n        - name: GITHUB_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: openshift-prow-github-app\n              key: appid\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: github-app-credentials\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: github-app-credentials\n        secret:\n          secretName: openshift-prow-github-app\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        projected:\n          sources:\n          - configMap:\n              name: job-config-misc\n          - configMap:\n              name: job-config-master-periodics\n          - configMap:\n              name: job-config-master-postsubmits\n          - configMap:\n              name: job-config-master-presubmits\n          - configMap:\n              name: job-config-3.x\n          - configMap:\n              name: job-config-4.1\n          - configMap:\n              name: job-config-4.2\n          - configMap:\n              name: job-config-4.3\n          - configMap:\n              name: job-config-4.4\n          - configMap:\n              name: job-config-4.5\n          - configMap:\n              name: job-config-4.6\n          - configMap:\n              name: job-config-4.7\n          - configMap:\n              name: job-config-4.8\n          - configMap:\n              name: job-config-4.9\n          - configMap:\n              name: job-config-4.10\n          - configMap:\n              name: job-config-4.11\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "5693",
    "manifest_path": "data/manifests/the_stack_sample/sample_2052.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20220115-3e20513bd7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --supplemental-plugin-config-dir=/etc/plugins\n        - --config-path=/etc/config/config.yaml\n        - --supplemental-prow-config-dir=/etc/config\n        - --github-app-id=$(GITHUB_APP_ID)\n        - --github-app-private-key-path=/etc/github/cert\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        - --projected-token-file=/var/sa-token/token\n        env:\n        - name: GITHUB_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: openshift-prow-github-app\n              key: appid\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: service-account-token\n          mountPath: /var/sa-token\n        - name: github-app-credentials\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: service-account-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              path: token\n      - name: github-app-credentials\n        secret:\n          secretName: openshift-prow-github-app\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        projected:\n          sources:\n          - configMap:\n              name: job-config-misc\n          - configMap:\n              name: job-config-master-periodics\n          - configMap:\n              name: job-config-master-postsubmits\n          - configMap:\n              name: job-config-master-presubmits\n          - configMap:\n              name: job-config-3.x\n          - configMap:\n              name: job-config-4.1\n          - configMap:\n              name: job-config-4.2\n          - configMap:\n              name: job-config-4.3\n          - configMap:\n              name: job-config-4.4\n          - configMap:\n              name: job-config-4.5\n          - configMap:\n              name: job-config-4.6\n          - configMap:\n              name: job-config-4.7\n          - configMap:\n              name: job-config-4.8\n          - configMap:\n              name: job-config-4.9\n          - configMap:\n              name: job-config-4.10\n          - configMap:\n              name: job-config-4.11\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "5694",
    "manifest_path": "data/manifests/the_stack_sample/sample_2054.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-tools\n  namespace: rook-ceph\n  labels:\n    app: rook-ceph-tools\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-ceph-tools\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-tools\n    spec:\n      containers:\n      - name: rook-ceph-tools\n        image: rook/ceph:v1.2.2\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_ADMIN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: admin-secret\n        volumeMounts:\n        - mountPath: /etc/ceph\n          name: ceph-config\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n      - name: ceph-config\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rook-ceph-tools\" does not have a read-only root file system"
  },
  {
    "id": "5695",
    "manifest_path": "data/manifests/the_stack_sample/sample_2054.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-tools\n  namespace: rook-ceph\n  labels:\n    app: rook-ceph-tools\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-ceph-tools\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-tools\n    spec:\n      containers:\n      - name: rook-ceph-tools\n        image: rook/ceph:v1.2.2\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_ADMIN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: admin-secret\n        volumeMounts:\n        - mountPath: /etc/ceph\n          name: ceph-config\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n      - name: ceph-config\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rook-ceph-tools\" is not set to runAsNonRoot"
  },
  {
    "id": "5696",
    "manifest_path": "data/manifests/the_stack_sample/sample_2054.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-tools\n  namespace: rook-ceph\n  labels:\n    app: rook-ceph-tools\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-ceph-tools\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-tools\n    spec:\n      containers:\n      - name: rook-ceph-tools\n        image: rook/ceph:v1.2.2\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_ADMIN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: admin-secret\n        volumeMounts:\n        - mountPath: /etc/ceph\n          name: ceph-config\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n      - name: ceph-config\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rook-ceph-tools\" has cpu request 0"
  },
  {
    "id": "5697",
    "manifest_path": "data/manifests/the_stack_sample/sample_2054.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-ceph-tools\n  namespace: rook-ceph\n  labels:\n    app: rook-ceph-tools\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-ceph-tools\n  template:\n    metadata:\n      labels:\n        app: rook-ceph-tools\n    spec:\n      containers:\n      - name: rook-ceph-tools\n        image: rook/ceph:v1.2.2\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_ADMIN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: admin-secret\n        volumeMounts:\n        - mountPath: /etc/ceph\n          name: ceph-config\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n      - name: ceph-config\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rook-ceph-tools\" has memory limit 0"
  },
  {
    "id": "5698",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"gotest\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5699",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gotest\" does not have a read-only root file system"
  },
  {
    "id": "5700",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"pod-io-error-retval-sa\" not found"
  },
  {
    "id": "5701",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gotest\" is not set to runAsNonRoot"
  },
  {
    "id": "5702",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gotest\" has cpu request 0"
  },
  {
    "id": "5703",
    "manifest_path": "data/manifests/the_stack_sample/sample_2056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litmus-experiment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: litmus-experiment\n  template:\n    metadata:\n      labels:\n        app: litmus-experiment\n    spec:\n      serviceAccountName: pod-io-error-retval-sa\n      containers:\n      - name: gotest\n        image: busybox\n        command:\n        - sleep\n        - '3600'\n        env:\n        - name: APP_NAMESPACE\n          value: default\n        - name: APP_LABEL\n          value: run=nginx\n        - name: APP_KIND\n          value: deployment\n        - name: TOTAL_CHAOS_DURATION\n          value: '60'\n        - name: CHAOS_INTERVAL\n          value: '10'\n        - name: MEMORY_CONSUMPTION\n          value: '500'\n        - name: PODS_AFFECTED_PERC\n          value: '100'\n        - name: LIB\n          value: litmus\n        - name: TARGET_POD\n          value: ''\n        - name: TARGET_CONTAINER\n          value: ''\n        - name: SEQUENCE\n          value: parallel\n        - name: CHAOS_NAMESPACE\n          value: default\n        - name: RAMP_TIME\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gotest\" has memory limit 0"
  },
  {
    "id": "5704",
    "manifest_path": "data/manifests/the_stack_sample/sample_2057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5705",
    "manifest_path": "data/manifests/the_stack_sample/sample_2057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"php-redis\" does not have a read-only root file system"
  },
  {
    "id": "5706",
    "manifest_path": "data/manifests/the_stack_sample/sample_2057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"php-redis\" is not set to runAsNonRoot"
  },
  {
    "id": "5707",
    "manifest_path": "data/manifests/the_stack_sample/sample_2057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"php-redis\" has memory limit 0"
  },
  {
    "id": "5708",
    "manifest_path": "data/manifests/the_stack_sample/sample_2060.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: nuodb-app-chaos\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: nuodbns\n        - name: APP_LABEL\n          value: nodetype=sm\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/nuodb/chaos/app_pod_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5709",
    "manifest_path": "data/manifests/the_stack_sample/sample_2060.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: nuodb-app-chaos\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: nuodbns\n        - name: APP_LABEL\n          value: nodetype=sm\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/nuodb/chaos/app_pod_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "5710",
    "manifest_path": "data/manifests/the_stack_sample/sample_2060.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: nuodb-app-chaos\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: nuodbns\n        - name: APP_LABEL\n          value: nodetype=sm\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/nuodb/chaos/app_pod_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"litmus\" not found"
  },
  {
    "id": "5711",
    "manifest_path": "data/manifests/the_stack_sample/sample_2060.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: nuodb-app-chaos\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: nuodbns\n        - name: APP_LABEL\n          value: nodetype=sm\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/nuodb/chaos/app_pod_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "5712",
    "manifest_path": "data/manifests/the_stack_sample/sample_2060.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: nuodb-app-chaos\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: nuodbns\n        - name: APP_LABEL\n          value: nodetype=sm\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/nuodb/chaos/app_pod_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "5713",
    "manifest_path": "data/manifests/the_stack_sample/sample_2060.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      labels:\n        name: nuodb-app-chaos\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: APP_NAMESPACE\n          value: nuodbns\n        - name: APP_LABEL\n          value: nodetype=sm\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/nuodb/chaos/app_pod_failure/test.yml -i /etc/ansible/hosts\n          -vv; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "5714",
    "manifest_path": "data/manifests/the_stack_sample/sample_2062.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: demo\n  name: demo\nspec:\n  containers:\n  - image: in-cluster\n    name: demo\n    imagePullPolicy: Never\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"demo\" is using an invalid container image, \"in-cluster\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5715",
    "manifest_path": "data/manifests/the_stack_sample/sample_2062.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: demo\n  name: demo\nspec:\n  containers:\n  - image: in-cluster\n    name: demo\n    imagePullPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"demo\" does not have a read-only root file system"
  },
  {
    "id": "5716",
    "manifest_path": "data/manifests/the_stack_sample/sample_2062.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: demo\n  name: demo\nspec:\n  containers:\n  - image: in-cluster\n    name: demo\n    imagePullPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"demo\" is not set to runAsNonRoot"
  },
  {
    "id": "5717",
    "manifest_path": "data/manifests/the_stack_sample/sample_2062.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: demo\n  name: demo\nspec:\n  containers:\n  - image: in-cluster\n    name: demo\n    imagePullPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"demo\" has cpu request 0"
  },
  {
    "id": "5718",
    "manifest_path": "data/manifests/the_stack_sample/sample_2062.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: demo\n  name: demo\nspec:\n  containers:\n  - image: in-cluster\n    name: demo\n    imagePullPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"demo\" has memory limit 0"
  },
  {
    "id": "5719",
    "manifest_path": "data/manifests/the_stack_sample/sample_2063.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    service: mysql\n  name: mysql\nspec:\n  ports:\n  - name: mysql\n    port: 3306\n    targetPort: 3306\n  selector:\n    service: mysql\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:mysql])"
  },
  {
    "id": "5720",
    "manifest_path": "data/manifests/the_stack_sample/sample_2064.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-background-producer-deployment\nspec:\n  selector:\n    matchLabels:\n      app: kafka-background-producer\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: kafka-background-producer\n    spec:\n      containers:\n      - name: kafka-background-producer\n        image: ${BACKENDV2_IMAGE}\n        command:\n        - npm\n        - run\n        - background-producer\n        imagePullPolicy: Always\n        resources:\n          requests:\n            memory: 300Mi\n            cpu: 100m\n          limits:\n            memory: 500Mi\n            cpu: 500m\n        env:\n        - name: NODE_ENV\n          value: production\n        - name: REDIS_HOST\n          value: quizzes-backend-redis-master.default.svc.cluster.local\n        - name: REDIS_PORT\n          value: '6379'\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: quizzes-backend-redis\n              key: redis-password\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_DATABASE\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_HOST\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_PASSWORD\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_USERNAME\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: KAFKA_HOST\n        - name: MESSAGE_FORMAT_VERSION\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: MESSAGE_FORMAT_VERSION\n        - name: NEW_RELIC_LICENSE_KEY\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_LICENSE_KEY\n        - name: NEW_RELIC_APP_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_APP_NAME\n        - name: NEW_RELIC_NO_CONFIG_FILE\n          value: 'true'\n        - name: SERVICE_ID\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SERVICE_ID\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SENTRY_DSN\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kafka-background-producer\" is using an invalid container image, \"${BACKENDV2_IMAGE}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5721",
    "manifest_path": "data/manifests/the_stack_sample/sample_2064.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-background-producer-deployment\nspec:\n  selector:\n    matchLabels:\n      app: kafka-background-producer\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: kafka-background-producer\n    spec:\n      containers:\n      - name: kafka-background-producer\n        image: ${BACKENDV2_IMAGE}\n        command:\n        - npm\n        - run\n        - background-producer\n        imagePullPolicy: Always\n        resources:\n          requests:\n            memory: 300Mi\n            cpu: 100m\n          limits:\n            memory: 500Mi\n            cpu: 500m\n        env:\n        - name: NODE_ENV\n          value: production\n        - name: REDIS_HOST\n          value: quizzes-backend-redis-master.default.svc.cluster.local\n        - name: REDIS_PORT\n          value: '6379'\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: quizzes-backend-redis\n              key: redis-password\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_DATABASE\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_HOST\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_PASSWORD\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_USERNAME\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: KAFKA_HOST\n        - name: MESSAGE_FORMAT_VERSION\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: MESSAGE_FORMAT_VERSION\n        - name: NEW_RELIC_LICENSE_KEY\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_LICENSE_KEY\n        - name: NEW_RELIC_APP_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_APP_NAME\n        - name: NEW_RELIC_NO_CONFIG_FILE\n          value: 'true'\n        - name: SERVICE_ID\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SERVICE_ID\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SENTRY_DSN\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kafka-background-producer\" does not have a read-only root file system"
  },
  {
    "id": "5722",
    "manifest_path": "data/manifests/the_stack_sample/sample_2064.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-background-producer-deployment\nspec:\n  selector:\n    matchLabels:\n      app: kafka-background-producer\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: kafka-background-producer\n    spec:\n      containers:\n      - name: kafka-background-producer\n        image: ${BACKENDV2_IMAGE}\n        command:\n        - npm\n        - run\n        - background-producer\n        imagePullPolicy: Always\n        resources:\n          requests:\n            memory: 300Mi\n            cpu: 100m\n          limits:\n            memory: 500Mi\n            cpu: 500m\n        env:\n        - name: NODE_ENV\n          value: production\n        - name: REDIS_HOST\n          value: quizzes-backend-redis-master.default.svc.cluster.local\n        - name: REDIS_PORT\n          value: '6379'\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: quizzes-backend-redis\n              key: redis-password\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_DATABASE\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_HOST\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_PASSWORD\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: POSTGRES_USERNAME\n        - name: KAFKA_HOST\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: KAFKA_HOST\n        - name: MESSAGE_FORMAT_VERSION\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: MESSAGE_FORMAT_VERSION\n        - name: NEW_RELIC_LICENSE_KEY\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_LICENSE_KEY\n        - name: NEW_RELIC_APP_NAME\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: NEW_RELIC_APP_NAME\n        - name: NEW_RELIC_NO_CONFIG_FILE\n          value: 'true'\n        - name: SERVICE_ID\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SERVICE_ID\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: backend-database-secret\n              key: SENTRY_DSN\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kafka-background-producer\" is not set to runAsNonRoot"
  },
  {
    "id": "5723",
    "manifest_path": "data/manifests/the_stack_sample/sample_2067.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: vladimir44/microservices-demo-payment-service:0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5724",
    "manifest_path": "data/manifests/the_stack_sample/sample_2067.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: vladimir44/microservices-demo-payment-service:0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"paymentservice\" does not have a read-only root file system"
  },
  {
    "id": "5725",
    "manifest_path": "data/manifests/the_stack_sample/sample_2067.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: vladimir44/microservices-demo-payment-service:0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"paymentservice\" is not set to runAsNonRoot"
  },
  {
    "id": "5726",
    "manifest_path": "data/manifests/the_stack_sample/sample_2067.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: vladimir44/microservices-demo-payment-service:0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"paymentservice\" has cpu request 0"
  },
  {
    "id": "5727",
    "manifest_path": "data/manifests/the_stack_sample/sample_2067.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: vladimir44/microservices-demo-payment-service:0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"paymentservice\" has memory limit 0"
  },
  {
    "id": "5728",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (ebs-csi-controller-sa), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "5729",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ebs-plugin\" is using an invalid container image, \"amazon/aws-ebs-csi-driver:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5730",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5731",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "5732",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "5733",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ebs-plugin\" does not have a read-only root file system"
  },
  {
    "id": "5734",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "5735",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ebs-csi-controller-sa\" not found"
  },
  {
    "id": "5736",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "5737",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "5738",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ebs-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "5739",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "5740",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-attacher\" has cpu request 0"
  },
  {
    "id": "5741",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-provisioner\" has cpu request 0"
  },
  {
    "id": "5742",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ebs-plugin\" has cpu request 0"
  },
  {
    "id": "5743",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness-probe\" has cpu request 0"
  },
  {
    "id": "5744",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-attacher\" has memory limit 0"
  },
  {
    "id": "5745",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-provisioner\" has memory limit 0"
  },
  {
    "id": "5746",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ebs-plugin\" has memory limit 0"
  },
  {
    "id": "5747",
    "manifest_path": "data/manifests/the_stack_sample/sample_2068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n    spec:\n      serviceAccount: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: amazon/aws-ebs-csi-driver:latest\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.5.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --feature-gates=Topology=true\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v1.2.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=5\n        - --leader-election=true\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness-probe\" has memory limit 0"
  },
  {
    "id": "5748",
    "manifest_path": "data/manifests/the_stack_sample/sample_2069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v2\n  labels:\n    app: reviews\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v2\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: /tmp/logs\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reviews\" does not have a read-only root file system"
  },
  {
    "id": "5749",
    "manifest_path": "data/manifests/the_stack_sample/sample_2069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v2\n  labels:\n    app: reviews\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v2\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: /tmp/logs\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reviews\" is not set to runAsNonRoot"
  },
  {
    "id": "5750",
    "manifest_path": "data/manifests/the_stack_sample/sample_2069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v2\n  labels:\n    app: reviews\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v2\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: /tmp/logs\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"reviews\" has cpu request 0"
  },
  {
    "id": "5751",
    "manifest_path": "data/manifests/the_stack_sample/sample_2069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v2\n  labels:\n    app: reviews\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v2\n    spec:\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: LOG_DIR\n          value: /tmp/logs\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: wlp-output\n          mountPath: /opt/ibm/wlp/output\n      volumes:\n      - name: wlp-output\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"reviews\" has memory limit 0"
  },
  {
    "id": "5752",
    "manifest_path": "data/manifests/the_stack_sample/sample_2071.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis\n  namespace: nuvolaris\nspec:\n  type: NodePort\n  selector:\n    name: redis\n  ports:\n  - port: 6379\n    targetPort: 6379\n    nodePort: 30379\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:redis])"
  },
  {
    "id": "5753",
    "manifest_path": "data/manifests/the_stack_sample/sample_2072.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: burpsuite\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: burpsuite\n        app.kubernetes.io/instance: burpsuite\n        app.kubernetes.io/version: 2022.1.1\n    spec:\n      containers:\n      - name: burpsuite\n        image: ghcr.io/k4yt3x/burpsuite:2022.1.1\n        env:\n        - name: DISPLAY\n          value: :0\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: x11-unix\n          mountPath: /tmp/.X11-unix\n      volumes:\n      - name: x11-unix\n        hostPath:\n          path: /tmp/.X11-unix\n          type: Directory\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5754",
    "manifest_path": "data/manifests/the_stack_sample/sample_2072.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: burpsuite\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: burpsuite\n        app.kubernetes.io/instance: burpsuite\n        app.kubernetes.io/version: 2022.1.1\n    spec:\n      containers:\n      - name: burpsuite\n        image: ghcr.io/k4yt3x/burpsuite:2022.1.1\n        env:\n        - name: DISPLAY\n          value: :0\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: x11-unix\n          mountPath: /tmp/.X11-unix\n      volumes:\n      - name: x11-unix\n        hostPath:\n          path: /tmp/.X11-unix\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"burpsuite\" does not have a read-only root file system"
  },
  {
    "id": "5755",
    "manifest_path": "data/manifests/the_stack_sample/sample_2072.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: burpsuite\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: burpsuite\n        app.kubernetes.io/instance: burpsuite\n        app.kubernetes.io/version: 2022.1.1\n    spec:\n      containers:\n      - name: burpsuite\n        image: ghcr.io/k4yt3x/burpsuite:2022.1.1\n        env:\n        - name: DISPLAY\n          value: :0\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: x11-unix\n          mountPath: /tmp/.X11-unix\n      volumes:\n      - name: x11-unix\n        hostPath:\n          path: /tmp/.X11-unix\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"burpsuite\" is not set to runAsNonRoot"
  },
  {
    "id": "5756",
    "manifest_path": "data/manifests/the_stack_sample/sample_2072.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: burpsuite\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: burpsuite\n        app.kubernetes.io/instance: burpsuite\n        app.kubernetes.io/version: 2022.1.1\n    spec:\n      containers:\n      - name: burpsuite\n        image: ghcr.io/k4yt3x/burpsuite:2022.1.1\n        env:\n        - name: DISPLAY\n          value: :0\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: x11-unix\n          mountPath: /tmp/.X11-unix\n      volumes:\n      - name: x11-unix\n        hostPath:\n          path: /tmp/.X11-unix\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"burpsuite\" has cpu request 0"
  },
  {
    "id": "5757",
    "manifest_path": "data/manifests/the_stack_sample/sample_2072.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: burpsuite\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: burpsuite\n        app.kubernetes.io/instance: burpsuite\n        app.kubernetes.io/version: 2022.1.1\n    spec:\n      containers:\n      - name: burpsuite\n        image: ghcr.io/k4yt3x/burpsuite:2022.1.1\n        env:\n        - name: DISPLAY\n          value: :0\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: x11-unix\n          mountPath: /tmp/.X11-unix\n      volumes:\n      - name: x11-unix\n        hostPath:\n          path: /tmp/.X11-unix\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"burpsuite\" has memory limit 0"
  },
  {
    "id": "5758",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"project-app\" does not have a read-only root file system"
  },
  {
    "id": "5759",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"project-service\" does not have a read-only root file system"
  },
  {
    "id": "5760",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"project-app\" is not set to runAsNonRoot"
  },
  {
    "id": "5761",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"project-service\" is not set to runAsNonRoot"
  },
  {
    "id": "5762",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"project-app\" has cpu request 0"
  },
  {
    "id": "5763",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"project-service\" has cpu request 0"
  },
  {
    "id": "5764",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"project-app\" has memory limit 0"
  },
  {
    "id": "5765",
    "manifest_path": "data/manifests/the_stack_sample/sample_2073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: project-dep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: project-app\n  template:\n    metadata:\n      labels:\n        app: project-app\n    spec:\n      volumes:\n      - name: shared-data\n        persistentVolumeClaim:\n          claimName: local-claim\n      - name: config\n        configMap:\n          name: project-app-config\n          items:\n          - key: app.properties\n            path: config.js\n      containers:\n      - name: project-app\n        image: robsondepaula/project:1_13\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n        - name: config\n          mountPath: /usr/share/nginx/html/config.js\n          subPath: config.js\n      - name: project-service\n        image: robsondepaula/project-service:1_13\n        ports:\n        - containerPort: 3001\n        volumeMounts:\n        - name: shared-data\n          mountPath: /data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"project-service\" has memory limit 0"
  },
  {
    "id": "5766",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"buildkitd\" does not have a read-only root file system"
  },
  {
    "id": "5767",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"buildkitd\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5768",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"buildkitd\" is privileged"
  },
  {
    "id": "5769",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"buildkitd\" is not set to runAsNonRoot"
  },
  {
    "id": "5770",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"buildkitd\" has cpu request 0"
  },
  {
    "id": "5771",
    "manifest_path": "data/manifests/the_stack_sample/sample_2074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: buildkitd\nspec:\n  containers:\n  - name: buildkitd\n    image: moby/buildkit:master\n    readinessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    livenessProbe:\n      exec:\n        command:\n        - buildctl\n        - debug\n        - workers\n      initialDelaySeconds: 5\n      periodSeconds: 30\n    securityContext:\n      privileged: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"buildkitd\" has memory limit 0"
  },
  {
    "id": "5772",
    "manifest_path": "data/manifests/the_stack_sample/sample_2075.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-test\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql-test\n  template:\n    metadata:\n      labels:\n        app: mysql-test\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: key\n                operator: In\n                values:\n                - value\n      containers:\n      - name: mysql-test\n        image: statemood/mysql:5.7.21\n        imagePullPolicy: Always\n        env:\n        - name: MYSQL_CONFIG_FILE\n          value: /var/lib/mysql/my.cnf\n        - name: MYSQL_ROOT_PASSWORD\n          value: ''\n        volumeMounts:\n        - mountPath: /var/lib/mysql\n          name: data\n        ports:\n        - containerPort: 3306\n        resources:\n          limits:\n            cpu: 900m\n            memory: 2Gi\n          requests:\n            cpu: 900m\n            memory: 2Gi\n        livenessProbe:\n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          periodSeconds: 20\n        readinessProbe:\n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          periodSeconds: 20\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-mysql-test\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql-test\" does not have a read-only root file system"
  },
  {
    "id": "5773",
    "manifest_path": "data/manifests/the_stack_sample/sample_2075.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-test\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql-test\n  template:\n    metadata:\n      labels:\n        app: mysql-test\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: key\n                operator: In\n                values:\n                - value\n      containers:\n      - name: mysql-test\n        image: statemood/mysql:5.7.21\n        imagePullPolicy: Always\n        env:\n        - name: MYSQL_CONFIG_FILE\n          value: /var/lib/mysql/my.cnf\n        - name: MYSQL_ROOT_PASSWORD\n          value: ''\n        volumeMounts:\n        - mountPath: /var/lib/mysql\n          name: data\n        ports:\n        - containerPort: 3306\n        resources:\n          limits:\n            cpu: 900m\n            memory: 2Gi\n          requests:\n            cpu: 900m\n            memory: 2Gi\n        livenessProbe:\n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          periodSeconds: 20\n        readinessProbe:\n          tcpSocket:\n            port: 3306\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          periodSeconds: 20\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-mysql-test\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql-test\" is not set to runAsNonRoot"
  },
  {
    "id": "5774",
    "manifest_path": "data/manifests/the_stack_sample/sample_2076.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-client\n  labels:\n    app: echo-client\n    group: sample\n    version: 0.0.1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echo-client\n      group: sample\n      version: 0.0.1\n  template:\n    metadata:\n      labels:\n        app: echo-client\n        group: sample\n        version: 0.0.1\n    spec:\n      containers:\n      - name: echo-client\n        image: echo-client:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 7001\n        - name: http-metrics\n          containerPort: 7090\n        resources:\n          limits:\n            cpu: 250m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        livenessProbe:\n          httpGet:\n            path: /live\n            port: 7090\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 7090\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"echo-client\" is using an invalid container image, \"echo-client:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5775",
    "manifest_path": "data/manifests/the_stack_sample/sample_2076.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-client\n  labels:\n    app: echo-client\n    group: sample\n    version: 0.0.1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echo-client\n      group: sample\n      version: 0.0.1\n  template:\n    metadata:\n      labels:\n        app: echo-client\n        group: sample\n        version: 0.0.1\n    spec:\n      containers:\n      - name: echo-client\n        image: echo-client:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 7001\n        - name: http-metrics\n          containerPort: 7090\n        resources:\n          limits:\n            cpu: 250m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        livenessProbe:\n          httpGet:\n            path: /live\n            port: 7090\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 7090\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echo-client\" does not have a read-only root file system"
  },
  {
    "id": "5776",
    "manifest_path": "data/manifests/the_stack_sample/sample_2076.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-client\n  labels:\n    app: echo-client\n    group: sample\n    version: 0.0.1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echo-client\n      group: sample\n      version: 0.0.1\n  template:\n    metadata:\n      labels:\n        app: echo-client\n        group: sample\n        version: 0.0.1\n    spec:\n      containers:\n      - name: echo-client\n        image: echo-client:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 7001\n        - name: http-metrics\n          containerPort: 7090\n        resources:\n          limits:\n            cpu: 250m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        livenessProbe:\n          httpGet:\n            path: /live\n            port: 7090\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 7090\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echo-client\" is not set to runAsNonRoot"
  },
  {
    "id": "5777",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"read-du\" is using an invalid container image, \"giantswarm/tiny-tools\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5778",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"caddy\" does not have a read-only root file system"
  },
  {
    "id": "5779",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"read-du\" does not have a read-only root file system"
  },
  {
    "id": "5780",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"caddy\" is not set to runAsNonRoot"
  },
  {
    "id": "5781",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"read-du\" is not set to runAsNonRoot"
  },
  {
    "id": "5782",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"caddy\" has cpu request 0"
  },
  {
    "id": "5783",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"read-du\" has cpu request 0"
  },
  {
    "id": "5784",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"caddy\" has memory limit 0"
  },
  {
    "id": "5785",
    "manifest_path": "data/manifests/the_stack_sample/sample_2078.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: 'This `DaemonSet` provides metrics in Prometheus format about disk\n      usage on the nodes.\n\n      The container `read-du` reads in sizes of all directories below /mnt and writes\n      that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n\n      The other container `caddy` just hands out the contents of that file on request\n      via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n      These are scheduled on every node in the Kubernetes cluster.\n\n      To choose directories from the node to check, just mount them on the `read-du`\n      container below `/mnt`.\n\n      '\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: 'This `Pod` provides metrics in Prometheus format about disk\n          usage on the node.\n\n          The container `read-du` reads in sizes of all directories below /mnt and\n          writes that to `/tmp/metrics`. It only reports directories larger then `100M`\n          for now.\n\n          The other container `caddy` just hands out the contents of that file on\n          request on `/metrics` at port `9102` which are the defaults for Prometheus.\n\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n\n          To choose directories from the node to check just mount them on `read-du`\n          below `/mnt`.\n\n          '\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        command:\n        - fish\n        - --command\n        - \"touch /tmp/metrics-temp\\nwhile true\\n  for directory in (du --bytes --separate-dirs\\\n          \\ --threshold=100M /mnt)\\n    echo $directory | read size path\\n    echo\\\n          \\ \\\"node_directory_size_bytes{path=\\\\\\\"$path\\\\\\\"} $size\\\" \\\\\\n      >> /tmp/metrics-temp\\n\\\n          \\  end\\n  mv /tmp/metrics-temp /tmp/metrics\\n  sleep 300\\nend\\n\"\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"read-du\" has memory limit 0"
  },
  {
    "id": "5786",
    "manifest_path": "data/manifests/the_stack_sample/sample_2082.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: localingress-ds\n  namespace: localingress\n  labels:\n    component: localingress\nspec:\n  selector:\n    matchLabels:\n      component: localingress\n  template:\n    metadata:\n      labels:\n        component: localingress\n    spec:\n      volumes:\n      - name: localingress-config\n        configMap:\n          name: localingress-config\n      containers:\n      - name: ingress\n        image: docker.io/library/haproxy:2.2.4\n        volumeMounts:\n        - name: localingress-config\n          mountPath: /usr/local/etc/haproxy/\n        ports:\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 80\n          hostPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ingress\" does not have a read-only root file system"
  },
  {
    "id": "5787",
    "manifest_path": "data/manifests/the_stack_sample/sample_2082.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: localingress-ds\n  namespace: localingress\n  labels:\n    component: localingress\nspec:\n  selector:\n    matchLabels:\n      component: localingress\n  template:\n    metadata:\n      labels:\n        component: localingress\n    spec:\n      volumes:\n      - name: localingress-config\n        configMap:\n          name: localingress-config\n      containers:\n      - name: ingress\n        image: docker.io/library/haproxy:2.2.4\n        volumeMounts:\n        - name: localingress-config\n          mountPath: /usr/local/etc/haproxy/\n        ports:\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 80\n          hostPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ingress\" is not set to runAsNonRoot"
  },
  {
    "id": "5788",
    "manifest_path": "data/manifests/the_stack_sample/sample_2082.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: localingress-ds\n  namespace: localingress\n  labels:\n    component: localingress\nspec:\n  selector:\n    matchLabels:\n      component: localingress\n  template:\n    metadata:\n      labels:\n        component: localingress\n    spec:\n      volumes:\n      - name: localingress-config\n        configMap:\n          name: localingress-config\n      containers:\n      - name: ingress\n        image: docker.io/library/haproxy:2.2.4\n        volumeMounts:\n        - name: localingress-config\n          mountPath: /usr/local/etc/haproxy/\n        ports:\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 80\n          hostPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ingress\" has cpu request 0"
  },
  {
    "id": "5789",
    "manifest_path": "data/manifests/the_stack_sample/sample_2082.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: localingress-ds\n  namespace: localingress\n  labels:\n    component: localingress\nspec:\n  selector:\n    matchLabels:\n      component: localingress\n  template:\n    metadata:\n      labels:\n        component: localingress\n    spec:\n      volumes:\n      - name: localingress-config\n        configMap:\n          name: localingress-config\n      containers:\n      - name: ingress\n        image: docker.io/library/haproxy:2.2.4\n        volumeMounts:\n        - name: localingress-config\n          mountPath: /usr/local/etc/haproxy/\n        ports:\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 80\n          hostPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ingress\" has memory limit 0"
  },
  {
    "id": "5790",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "5791",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rook-direct-mount\" does not have a read-only root file system"
  },
  {
    "id": "5792",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"rook-direct-mount\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5793",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"rook-direct-mount\" is privileged"
  },
  {
    "id": "5794",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rook-direct-mount\" is not set to runAsNonRoot"
  },
  {
    "id": "5795",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"rook-direct-mount\""
  },
  {
    "id": "5796",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rook-direct-mount\" has cpu request 0"
  },
  {
    "id": "5797",
    "manifest_path": "data/manifests/the_stack_sample/sample_2083.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:master\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rook-direct-mount\" has memory limit 0"
  },
  {
    "id": "5798",
    "manifest_path": "data/manifests/the_stack_sample/sample_2084.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: portworx-service\n  namespace: kube-test\n  labels:\n    name: portworx\nspec:\n  selector:\n    name: portworx\n  type: ClusterIP\n  ports:\n  - name: px-api\n    protocol: TCP\n    port: 9001\n    targetPort: 10001\n  - name: px-kvdb\n    protocol: TCP\n    port: 9019\n    targetPort: 10019\n  - name: px-sdk\n    protocol: TCP\n    port: 9020\n    targetPort: 10020\n  - name: px-rest-gateway\n    protocol: TCP\n    port: 9021\n    targetPort: 10021\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:portworx])"
  },
  {
    "id": "5799",
    "manifest_path": "data/manifests/the_stack_sample/sample_2086.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: node-app-service\nspec:\n  selector:\n    app: node-app\n  type: NodePort\n  ports:\n  - protocol: TCP\n    port: 5000\n    targetPort: 3000\n    nodePort: 31110\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:node-app])"
  },
  {
    "id": "5800",
    "manifest_path": "data/manifests/the_stack_sample/sample_2087.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: node-4\nspec:\n  ports:\n  - name: '6688'\n    port: 6688\n    targetPort: 6688\n  - name: '6064'\n    port: 6064\n    targetPort: 6060\n",
    "policy_id": "dangling-service",
    "violation_text": "service has no selector specified"
  },
  {
    "id": "5801",
    "manifest_path": "data/manifests/the_stack_sample/sample_2088.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-stress-normal\n  labels:\n    app: cassandra-stress\nspec:\n  volumes:\n  - name: cassandra-stress-profile-volume\n    configMap:\n      name: cassandra-stress-normal\n  securityContext:\n    fsGroup: 1\n    runAsNonRoot: true\n    runAsUser: 1006\n    supplementalGroups:\n    - 1\n  containers:\n  - name: cassie1-cassandra-stress\n    image: cassandra\n    imagePullPolicy: IfNotPresent\n    securityContext:\n      capabilities:\n        add:\n        - IPC_LOCK\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - cassandra-stress 'user profile=/opt/cassandra-stress/normal_stress.yaml ops(insert=10,query_by_sub_id=8)\n      duration=120m cl=one -node cassandra-demo -mode native cql3 user=bench password=monbench\n      -rate threads=30 -pop seq=0..100M -tokenrange -graph file=/tmp/stress-normal.html'\n      && echo END && while true ; do sleep 60; done\n    resources:\n      limits:\n        cpu: '3'\n        memory: 8Gi\n      requests:\n        cpu: '3'\n        memory: 8Gi\n    volumeMounts:\n    - name: cassandra-stress-profile-volume\n      mountPath: /opt/cassandra-stress\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"cassie1-cassandra-stress\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "5802",
    "manifest_path": "data/manifests/the_stack_sample/sample_2088.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-stress-normal\n  labels:\n    app: cassandra-stress\nspec:\n  volumes:\n  - name: cassandra-stress-profile-volume\n    configMap:\n      name: cassandra-stress-normal\n  securityContext:\n    fsGroup: 1\n    runAsNonRoot: true\n    runAsUser: 1006\n    supplementalGroups:\n    - 1\n  containers:\n  - name: cassie1-cassandra-stress\n    image: cassandra\n    imagePullPolicy: IfNotPresent\n    securityContext:\n      capabilities:\n        add:\n        - IPC_LOCK\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - cassandra-stress 'user profile=/opt/cassandra-stress/normal_stress.yaml ops(insert=10,query_by_sub_id=8)\n      duration=120m cl=one -node cassandra-demo -mode native cql3 user=bench password=monbench\n      -rate threads=30 -pop seq=0..100M -tokenrange -graph file=/tmp/stress-normal.html'\n      && echo END && while true ; do sleep 60; done\n    resources:\n      limits:\n        cpu: '3'\n        memory: 8Gi\n      requests:\n        cpu: '3'\n        memory: 8Gi\n    volumeMounts:\n    - name: cassandra-stress-profile-volume\n      mountPath: /opt/cassandra-stress\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cassie1-cassandra-stress\" is using an invalid container image, \"cassandra\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5803",
    "manifest_path": "data/manifests/the_stack_sample/sample_2088.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-stress-normal\n  labels:\n    app: cassandra-stress\nspec:\n  volumes:\n  - name: cassandra-stress-profile-volume\n    configMap:\n      name: cassandra-stress-normal\n  securityContext:\n    fsGroup: 1\n    runAsNonRoot: true\n    runAsUser: 1006\n    supplementalGroups:\n    - 1\n  containers:\n  - name: cassie1-cassandra-stress\n    image: cassandra\n    imagePullPolicy: IfNotPresent\n    securityContext:\n      capabilities:\n        add:\n        - IPC_LOCK\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - cassandra-stress 'user profile=/opt/cassandra-stress/normal_stress.yaml ops(insert=10,query_by_sub_id=8)\n      duration=120m cl=one -node cassandra-demo -mode native cql3 user=bench password=monbench\n      -rate threads=30 -pop seq=0..100M -tokenrange -graph file=/tmp/stress-normal.html'\n      && echo END && while true ; do sleep 60; done\n    resources:\n      limits:\n        cpu: '3'\n        memory: 8Gi\n      requests:\n        cpu: '3'\n        memory: 8Gi\n    volumeMounts:\n    - name: cassandra-stress-profile-volume\n      mountPath: /opt/cassandra-stress\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cassie1-cassandra-stress\" does not have a read-only root file system"
  },
  {
    "id": "5804",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable AGENTS_JWT_SECRET in container \"imicros-users\" found"
  },
  {
    "id": "5805",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable JWT_SECRET in container \"imicros-users\" found"
  },
  {
    "id": "5806",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"imicros-users\" is using an invalid container image, \"al66/imicros-backend:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5807",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"imicros-users\" does not have a read-only root file system"
  },
  {
    "id": "5808",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"imicros-users\" is not set to runAsNonRoot"
  },
  {
    "id": "5809",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"imicros-users\" has cpu request 0"
  },
  {
    "id": "5810",
    "manifest_path": "data/manifests/the_stack_sample/sample_2089.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: imicros-users-deployment\nspec:\n  selector:\n    matchLabels:\n      app: imicros-users\n  template:\n    metadata:\n      labels:\n        app: imicros-users\n        version: '0.1'\n    spec:\n      containers:\n      - name: imicros-users\n        image: al66/imicros-backend:latest\n        imagePullPolicy: Always\n        env:\n        - name: SERVICES\n          value: master, user, groups, agents, acl\n        - name: MASTER_TOKEN\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: service-token\n              key: token\n        - name: TRANSPORTER_NATS\n          value: nats://nats:4222\n        - name: CASSANDRA_CONTACTPOINTS\n          value: cassandra\n        - name: CASSANDRA_DATACENTER\n          value: datacenter1\n        - name: CASSANDRA_KEYSPACE_KEYS\n          value: imicros_keys\n        - name: CASSANDRA_PORT\n          value: '9042'\n        - name: CASSANDRA_USER\n          value: cassandra\n        - name: CASSANDRA_PASSWORD\n          value: cassandra\n        - name: NEO4J_URI\n          value: bolt://neo4j:7687\n        - name: NEO4J_USER\n          value: neo4j\n        - name: NEO4J_PASSWORD\n          value: neo4j\n        - name: JWT_SECRET\n          value: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\n        - name: AGENTS_JWT_SECRET\n          value: my JWT secret - should be set as a Kubernetes secret\n        - name: EVENT_OWNER_ID\n          value: 096f1dff-681a-4746-b5cd-2228f69630c7\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"imicros-users\" has memory limit 0"
  },
  {
    "id": "5811",
    "manifest_path": "data/manifests/the_stack_sample/sample_2090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: test\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 1\n            memory: 500Mi\n          limits:\n            cpu: 2\n            memory: 1024Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5812",
    "manifest_path": "data/manifests/the_stack_sample/sample_2090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: test\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 1\n            memory: 500Mi\n          limits:\n            cpu: 2\n            memory: 1024Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5813",
    "manifest_path": "data/manifests/the_stack_sample/sample_2090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: test\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 1\n            memory: 500Mi\n          limits:\n            cpu: 2\n            memory: 1024Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5814",
    "manifest_path": "data/manifests/the_stack_sample/sample_2092.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: user\n  template:\n    metadata:\n      labels:\n        app: user\n    spec:\n      containers:\n      - name: user\n        image: p3fightclub.azurecr.io/user\n        ports:\n        - containerPort: 5001\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"user\" is using an invalid container image, \"p3fightclub.azurecr.io/user\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5815",
    "manifest_path": "data/manifests/the_stack_sample/sample_2092.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: user\n  template:\n    metadata:\n      labels:\n        app: user\n    spec:\n      containers:\n      - name: user\n        image: p3fightclub.azurecr.io/user\n        ports:\n        - containerPort: 5001\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"user\" does not have a read-only root file system"
  },
  {
    "id": "5816",
    "manifest_path": "data/manifests/the_stack_sample/sample_2092.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: user\n  template:\n    metadata:\n      labels:\n        app: user\n    spec:\n      containers:\n      - name: user\n        image: p3fightclub.azurecr.io/user\n        ports:\n        - containerPort: 5001\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"user\" is not set to runAsNonRoot"
  },
  {
    "id": "5817",
    "manifest_path": "data/manifests/the_stack_sample/sample_2092.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: user\n  template:\n    metadata:\n      labels:\n        app: user\n    spec:\n      containers:\n      - name: user\n        image: p3fightclub.azurecr.io/user\n        ports:\n        - containerPort: 5001\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"user\" has cpu request 0"
  },
  {
    "id": "5818",
    "manifest_path": "data/manifests/the_stack_sample/sample_2092.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: user\n  template:\n    metadata:\n      labels:\n        app: user\n    spec:\n      containers:\n      - name: user\n        image: p3fightclub.azurecr.io/user\n        ports:\n        - containerPort: 5001\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"user\" has memory limit 0"
  },
  {
    "id": "5819",
    "manifest_path": "data/manifests/the_stack_sample/sample_2094.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5820",
    "manifest_path": "data/manifests/the_stack_sample/sample_2094.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5821",
    "manifest_path": "data/manifests/the_stack_sample/sample_2094.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5822",
    "manifest_path": "data/manifests/the_stack_sample/sample_2094.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5823",
    "manifest_path": "data/manifests/the_stack_sample/sample_2094.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5824",
    "manifest_path": "data/manifests/the_stack_sample/sample_2095.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tickets-celery\n  labels:\n    app: tickets-celery\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tickets-celery\n  template:\n    metadata:\n      labels:\n        name: tickets-celery\n    spec:\n      containers:\n      - name: tickets-celery\n        image: ckreuzberger/tickets:1.0\n        imagePullPolicy: Always\n        command:\n        - celery\n        args:\n        - -A\n        - tickets\n        - worker\n        - --loglevel=info\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_DB\n          value: tickets\n        - name: DATABASE_URL\n          value: postgres://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@postgres:5432/$(POSTGRES_DB)\n        - name: DJANGO_SETTINGS_MODULE\n          value: tickets.settings.prod\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: django-secret-key\n              key: secret_key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tickets-celery\" does not have a read-only root file system"
  },
  {
    "id": "5825",
    "manifest_path": "data/manifests/the_stack_sample/sample_2095.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tickets-celery\n  labels:\n    app: tickets-celery\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tickets-celery\n  template:\n    metadata:\n      labels:\n        name: tickets-celery\n    spec:\n      containers:\n      - name: tickets-celery\n        image: ckreuzberger/tickets:1.0\n        imagePullPolicy: Always\n        command:\n        - celery\n        args:\n        - -A\n        - tickets\n        - worker\n        - --loglevel=info\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_DB\n          value: tickets\n        - name: DATABASE_URL\n          value: postgres://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@postgres:5432/$(POSTGRES_DB)\n        - name: DJANGO_SETTINGS_MODULE\n          value: tickets.settings.prod\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: django-secret-key\n              key: secret_key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tickets-celery\" is not set to runAsNonRoot"
  },
  {
    "id": "5826",
    "manifest_path": "data/manifests/the_stack_sample/sample_2095.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tickets-celery\n  labels:\n    app: tickets-celery\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tickets-celery\n  template:\n    metadata:\n      labels:\n        name: tickets-celery\n    spec:\n      containers:\n      - name: tickets-celery\n        image: ckreuzberger/tickets:1.0\n        imagePullPolicy: Always\n        command:\n        - celery\n        args:\n        - -A\n        - tickets\n        - worker\n        - --loglevel=info\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_DB\n          value: tickets\n        - name: DATABASE_URL\n          value: postgres://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@postgres:5432/$(POSTGRES_DB)\n        - name: DJANGO_SETTINGS_MODULE\n          value: tickets.settings.prod\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: django-secret-key\n              key: secret_key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tickets-celery\" has cpu request 0"
  },
  {
    "id": "5827",
    "manifest_path": "data/manifests/the_stack_sample/sample_2095.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tickets-celery\n  labels:\n    app: tickets-celery\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tickets-celery\n  template:\n    metadata:\n      labels:\n        name: tickets-celery\n    spec:\n      containers:\n      - name: tickets-celery\n        image: ckreuzberger/tickets:1.0\n        imagePullPolicy: Always\n        command:\n        - celery\n        args:\n        - -A\n        - tickets\n        - worker\n        - --loglevel=info\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_DB\n          value: tickets\n        - name: DATABASE_URL\n          value: postgres://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@postgres:5432/$(POSTGRES_DB)\n        - name: DJANGO_SETTINGS_MODULE\n          value: tickets.settings.prod\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: django-secret-key\n              key: secret_key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tickets-celery\" has memory limit 0"
  },
  {
    "id": "5828",
    "manifest_path": "data/manifests/the_stack_sample/sample_2096.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-ingress-lb\n  labels:\n    app: nginx-ingress-lb\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    name: http\n    targetPort: 80\n  - port: 443\n    name: https\n    targetPort: 443\n  selector:\n    k8s-app: nginx-ingress-controller\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:nginx-ingress-controller])"
  },
  {
    "id": "5829",
    "manifest_path": "data/manifests/the_stack_sample/sample_2097.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        env:\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - all\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n        - name: tmp\n          mountPath: /tmp\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"argocd-repo-server\" is using an invalid container image, \"quay.io/argoproj/argocd:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5830",
    "manifest_path": "data/manifests/the_stack_sample/sample_2097.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        env:\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - all\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n        - name: tmp\n          mountPath: /tmp\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"argocd-repo-server\" has cpu request 0"
  },
  {
    "id": "5831",
    "manifest_path": "data/manifests/the_stack_sample/sample_2097.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: repo-server\n  name: argocd-repo-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-repo-server\n    spec:\n      containers:\n      - name: argocd-repo-server\n        image: quay.io/argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - entrypoint.sh\n        - argocd-repo-server\n        - --redis\n        - $(ARGOCD_REDIS_SERVICE):6379\n        env:\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        ports:\n        - containerPort: 8081\n        - containerPort: 8084\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: 8084\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8084\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - all\n        volumeMounts:\n        - name: ssh-known-hosts\n          mountPath: /app/config/ssh\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: gpg-keys\n          mountPath: /app/config/gpg/source\n        - name: gpg-keyring\n          mountPath: /app/config/gpg/keys\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n        - name: tmp\n          mountPath: /tmp\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n          - weight: 5\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/part-of: argocd\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"argocd-repo-server\" has memory limit 0"
  },
  {
    "id": "5832",
    "manifest_path": "data/manifests/the_stack_sample/sample_2099.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tomcat\n  labels:\n    app: tomcat\nspec:\n  containers:\n  - name: tomcat\n    image: docker635067/test:preethu\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tomcat\" does not have a read-only root file system"
  },
  {
    "id": "5833",
    "manifest_path": "data/manifests/the_stack_sample/sample_2099.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tomcat\n  labels:\n    app: tomcat\nspec:\n  containers:\n  - name: tomcat\n    image: docker635067/test:preethu\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tomcat\" is not set to runAsNonRoot"
  },
  {
    "id": "5834",
    "manifest_path": "data/manifests/the_stack_sample/sample_2099.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tomcat\n  labels:\n    app: tomcat\nspec:\n  containers:\n  - name: tomcat\n    image: docker635067/test:preethu\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tomcat\" has cpu request 0"
  },
  {
    "id": "5835",
    "manifest_path": "data/manifests/the_stack_sample/sample_2099.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tomcat\n  labels:\n    app: tomcat\nspec:\n  containers:\n  - name: tomcat\n    image: docker635067/test:preethu\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tomcat\" has memory limit 0"
  },
  {
    "id": "5836",
    "manifest_path": "data/manifests/the_stack_sample/sample_2100.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20220114-25626d8ef5\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5837",
    "manifest_path": "data/manifests/the_stack_sample/sample_2100.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20220114-25626d8ef5\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "5838",
    "manifest_path": "data/manifests/the_stack_sample/sample_2100.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20220114-25626d8ef5\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "5839",
    "manifest_path": "data/manifests/the_stack_sample/sample_2100.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20220114-25626d8ef5\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "5840",
    "manifest_path": "data/manifests/the_stack_sample/sample_2100.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20220114-25626d8ef5\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "5841",
    "manifest_path": "data/manifests/the_stack_sample/sample_2101.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: grafana\n    chart: grafana\n    heritage: Tiller\n    release: istio\n  name: istio-grafana-post-install-1.1.6\n  namespace: istio-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-grafana\n        chart: grafana\n        heritage: Tiller\n        release: istio\n      name: istio-grafana-post-install\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/grafana/run.sh\n        - /tmp/grafana/custom-resources.yaml\n        image: docker.io/istio/kubectl:1.1.6\n        name: kubectl\n        volumeMounts:\n        - mountPath: /tmp/grafana\n          name: tmp-configmap-grafana\n      serviceAccountName: istio-grafana-post-install-account\n      volumes:\n      - configMap:\n          name: istio-grafana-custom-resources\n        name: tmp-configmap-grafana\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "5842",
    "manifest_path": "data/manifests/the_stack_sample/sample_2101.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: grafana\n    chart: grafana\n    heritage: Tiller\n    release: istio\n  name: istio-grafana-post-install-1.1.6\n  namespace: istio-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-grafana\n        chart: grafana\n        heritage: Tiller\n        release: istio\n      name: istio-grafana-post-install\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/grafana/run.sh\n        - /tmp/grafana/custom-resources.yaml\n        image: docker.io/istio/kubectl:1.1.6\n        name: kubectl\n        volumeMounts:\n        - mountPath: /tmp/grafana\n          name: tmp-configmap-grafana\n      serviceAccountName: istio-grafana-post-install-account\n      volumes:\n      - configMap:\n          name: istio-grafana-custom-resources\n        name: tmp-configmap-grafana\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubectl\" does not have a read-only root file system"
  },
  {
    "id": "5843",
    "manifest_path": "data/manifests/the_stack_sample/sample_2101.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: grafana\n    chart: grafana\n    heritage: Tiller\n    release: istio\n  name: istio-grafana-post-install-1.1.6\n  namespace: istio-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-grafana\n        chart: grafana\n        heritage: Tiller\n        release: istio\n      name: istio-grafana-post-install\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/grafana/run.sh\n        - /tmp/grafana/custom-resources.yaml\n        image: docker.io/istio/kubectl:1.1.6\n        name: kubectl\n        volumeMounts:\n        - mountPath: /tmp/grafana\n          name: tmp-configmap-grafana\n      serviceAccountName: istio-grafana-post-install-account\n      volumes:\n      - configMap:\n          name: istio-grafana-custom-resources\n        name: tmp-configmap-grafana\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"istio-grafana-post-install-account\" not found"
  },
  {
    "id": "5844",
    "manifest_path": "data/manifests/the_stack_sample/sample_2101.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: grafana\n    chart: grafana\n    heritage: Tiller\n    release: istio\n  name: istio-grafana-post-install-1.1.6\n  namespace: istio-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-grafana\n        chart: grafana\n        heritage: Tiller\n        release: istio\n      name: istio-grafana-post-install\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/grafana/run.sh\n        - /tmp/grafana/custom-resources.yaml\n        image: docker.io/istio/kubectl:1.1.6\n        name: kubectl\n        volumeMounts:\n        - mountPath: /tmp/grafana\n          name: tmp-configmap-grafana\n      serviceAccountName: istio-grafana-post-install-account\n      volumes:\n      - configMap:\n          name: istio-grafana-custom-resources\n        name: tmp-configmap-grafana\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubectl\" is not set to runAsNonRoot"
  },
  {
    "id": "5845",
    "manifest_path": "data/manifests/the_stack_sample/sample_2101.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: grafana\n    chart: grafana\n    heritage: Tiller\n    release: istio\n  name: istio-grafana-post-install-1.1.6\n  namespace: istio-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-grafana\n        chart: grafana\n        heritage: Tiller\n        release: istio\n      name: istio-grafana-post-install\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/grafana/run.sh\n        - /tmp/grafana/custom-resources.yaml\n        image: docker.io/istio/kubectl:1.1.6\n        name: kubectl\n        volumeMounts:\n        - mountPath: /tmp/grafana\n          name: tmp-configmap-grafana\n      serviceAccountName: istio-grafana-post-install-account\n      volumes:\n      - configMap:\n          name: istio-grafana-custom-resources\n        name: tmp-configmap-grafana\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubectl\" has cpu request 0"
  },
  {
    "id": "5846",
    "manifest_path": "data/manifests/the_stack_sample/sample_2101.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: grafana\n    chart: grafana\n    heritage: Tiller\n    release: istio\n  name: istio-grafana-post-install-1.1.6\n  namespace: istio-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-grafana\n        chart: grafana\n        heritage: Tiller\n        release: istio\n      name: istio-grafana-post-install\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/grafana/run.sh\n        - /tmp/grafana/custom-resources.yaml\n        image: docker.io/istio/kubectl:1.1.6\n        name: kubectl\n        volumeMounts:\n        - mountPath: /tmp/grafana\n          name: tmp-configmap-grafana\n      serviceAccountName: istio-grafana-post-install-account\n      volumes:\n      - configMap:\n          name: istio-grafana-custom-resources\n        name: tmp-configmap-grafana\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubectl\" has memory limit 0"
  },
  {
    "id": "5847",
    "manifest_path": "data/manifests/the_stack_sample/sample_2103.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"task-pv-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5848",
    "manifest_path": "data/manifests/the_stack_sample/sample_2103.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"task-pv-container\" does not have a read-only root file system"
  },
  {
    "id": "5849",
    "manifest_path": "data/manifests/the_stack_sample/sample_2103.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"task-pv-container\" is not set to runAsNonRoot"
  },
  {
    "id": "5850",
    "manifest_path": "data/manifests/the_stack_sample/sample_2103.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"task-pv-container\" has cpu request 0"
  },
  {
    "id": "5851",
    "manifest_path": "data/manifests/the_stack_sample/sample_2103.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"task-pv-container\" has memory limit 0"
  },
  {
    "id": "5852",
    "manifest_path": "data/manifests/the_stack_sample/sample_2104.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: pubsub-proxy\n  labels:\n    app: pubsub-proxy\nspec:\n  ports:\n  - protocol: TCP\n    port: 443\n    name: https\n  selector:\n    app: pubsub-proxy\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:pubsub-proxy])"
  },
  {
    "id": "5853",
    "manifest_path": "data/manifests/the_stack_sample/sample_2106.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.openshift.io/serving-cert-secret-name: prometheus-user-workload-thanos-sidecar-tls\n  labels:\n    app.kubernetes.io/component: thanos-sidecar\n    app.kubernetes.io/instance: user-workload\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.33.5\n  name: prometheus-user-workload-thanos-sidecar\n  namespace: openshift-user-workload-monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: thanos-proxy\n    port: 10902\n    targetPort: thanos-proxy\n  selector:\n    app.kubernetes.io/component: prometheus\n    app.kubernetes.io/instance: user-workload\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/part-of: openshift-monitoring\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:prometheus app.kubernetes.io/instance:user-workload app.kubernetes.io/name:prometheus app.kubernetes.io/part-of:openshift-monitoring])"
  },
  {
    "id": "5854",
    "manifest_path": "data/manifests/the_stack_sample/sample_2107.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sensor-gen\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    app: sensor-gen\n  template:\n    metadata:\n      name: sensor-gen\n      labels:\n        app: sensor-gen\n    spec:\n      containers:\n      - name: sensor-gen\n        image: huanphan/sensor-simulator:0.2\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: sensor-config\n          mountPath: /SimulateSensor/config\n      volumes:\n      - name: sensor-config\n        configMap:\n          name: sensor-config\n          items:\n          - key: config.cfg\n            path: config.cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sensor-gen\" does not have a read-only root file system"
  },
  {
    "id": "5855",
    "manifest_path": "data/manifests/the_stack_sample/sample_2107.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sensor-gen\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    app: sensor-gen\n  template:\n    metadata:\n      name: sensor-gen\n      labels:\n        app: sensor-gen\n    spec:\n      containers:\n      - name: sensor-gen\n        image: huanphan/sensor-simulator:0.2\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: sensor-config\n          mountPath: /SimulateSensor/config\n      volumes:\n      - name: sensor-config\n        configMap:\n          name: sensor-config\n          items:\n          - key: config.cfg\n            path: config.cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sensor-gen\" is not set to runAsNonRoot"
  },
  {
    "id": "5856",
    "manifest_path": "data/manifests/the_stack_sample/sample_2107.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sensor-gen\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    app: sensor-gen\n  template:\n    metadata:\n      name: sensor-gen\n      labels:\n        app: sensor-gen\n    spec:\n      containers:\n      - name: sensor-gen\n        image: huanphan/sensor-simulator:0.2\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: sensor-config\n          mountPath: /SimulateSensor/config\n      volumes:\n      - name: sensor-config\n        configMap:\n          name: sensor-config\n          items:\n          - key: config.cfg\n            path: config.cfg\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sensor-gen\" has cpu request 0"
  },
  {
    "id": "5857",
    "manifest_path": "data/manifests/the_stack_sample/sample_2107.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sensor-gen\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    app: sensor-gen\n  template:\n    metadata:\n      name: sensor-gen\n      labels:\n        app: sensor-gen\n    spec:\n      containers:\n      - name: sensor-gen\n        image: huanphan/sensor-simulator:0.2\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: sensor-config\n          mountPath: /SimulateSensor/config\n      volumes:\n      - name: sensor-config\n        configMap:\n          name: sensor-config\n          items:\n          - key: config.cfg\n            path: config.cfg\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sensor-gen\" has memory limit 0"
  },
  {
    "id": "5858",
    "manifest_path": "data/manifests/the_stack_sample/sample_2112.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5859",
    "manifest_path": "data/manifests/the_stack_sample/sample_2112.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5860",
    "manifest_path": "data/manifests/the_stack_sample/sample_2112.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5861",
    "manifest_path": "data/manifests/the_stack_sample/sample_2112.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5862",
    "manifest_path": "data/manifests/the_stack_sample/sample_2112.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5863",
    "manifest_path": "data/manifests/the_stack_sample/sample_2113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: example\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 5 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5864",
    "manifest_path": "data/manifests/the_stack_sample/sample_2113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: example\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5865",
    "manifest_path": "data/manifests/the_stack_sample/sample_2113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: example\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5866",
    "manifest_path": "data/manifests/the_stack_sample/sample_2113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: example\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5867",
    "manifest_path": "data/manifests/the_stack_sample/sample_2113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: example\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5868",
    "manifest_path": "data/manifests/the_stack_sample/sample_2114.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus-service\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  selector:\n    app: prometheus\n  ports:\n  - name: prometheus\n    protocol: TCP\n    port: 9090\n    targetPort: 9090\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:prometheus])"
  },
  {
    "id": "5869",
    "manifest_path": "data/manifests/the_stack_sample/sample_2115.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: transmission\n  labels:\n    app.kubernetes.io/name: transmission\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: transmission\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: transmission\n    spec:\n      containers:\n      - name: transmission\n        image: ghcr.io/k8s-at-home/transmission:v3.00\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n        ports:\n        - name: gui-port\n          protocol: TCP\n          containerPort: 9091\n        - name: tcp-port\n          protocol: TCP\n          containerPort: 51413\n        - name: udp-port\n          protocol: UDP\n          containerPort: 51413\n        volumeMounts:\n        - name: transmission-configs\n          mountPath: /config/settings.json\n          subPath: settings.json\n        - name: transmission-config\n          mountPath: /config\n        - name: transmission-downloads\n          mountPath: /downloads\n        livenessProbe:\n          tcpSocket:\n            port: tcp-port\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          tcpSocket:\n            port: tcp-port\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        env:\n        - name: TZ\n          value: ${CONFIG_TIMEZONE}\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n      volumes:\n      - name: transmission-configs\n        configMap:\n          name: transmission-configs\n      - name: transmission-config\n        persistentVolumeClaim:\n          claimName: transmission-config\n      - name: transmission-downloads\n        persistentVolumeClaim:\n          claimName: shared-downloads\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"transmission\" does not have a read-only root file system"
  },
  {
    "id": "5870",
    "manifest_path": "data/manifests/the_stack_sample/sample_2115.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: transmission\n  labels:\n    app.kubernetes.io/name: transmission\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: transmission\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: transmission\n    spec:\n      containers:\n      - name: transmission\n        image: ghcr.io/k8s-at-home/transmission:v3.00\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n        ports:\n        - name: gui-port\n          protocol: TCP\n          containerPort: 9091\n        - name: tcp-port\n          protocol: TCP\n          containerPort: 51413\n        - name: udp-port\n          protocol: UDP\n          containerPort: 51413\n        volumeMounts:\n        - name: transmission-configs\n          mountPath: /config/settings.json\n          subPath: settings.json\n        - name: transmission-config\n          mountPath: /config\n        - name: transmission-downloads\n          mountPath: /downloads\n        livenessProbe:\n          tcpSocket:\n            port: tcp-port\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          tcpSocket:\n            port: tcp-port\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        env:\n        - name: TZ\n          value: ${CONFIG_TIMEZONE}\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n      volumes:\n      - name: transmission-configs\n        configMap:\n          name: transmission-configs\n      - name: transmission-config\n        persistentVolumeClaim:\n          claimName: transmission-config\n      - name: transmission-downloads\n        persistentVolumeClaim:\n          claimName: shared-downloads\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"transmission\" has memory limit 0"
  },
  {
    "id": "5871",
    "manifest_path": "data/manifests/the_stack_sample/sample_2116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-dns-metrics-bash\n  namespace: kube-system\n  labels:\n    application: kube-dns-metrics-bash\n    version: v0.0.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: kube-dns-metrics-bash\n  template:\n    metadata:\n      labels:\n        application: kube-dns-metrics-bash\n        version: v0.0.4\n    spec:\n      containers:\n      - image: pierone.stups.zalan.do/teapot/kube-dns-metrics-bash:v0.0.5\n        name: kube-dns-metrics-bash\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 25m\n            memory: 100Mi\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-dns-metrics-bash\" does not have a read-only root file system"
  },
  {
    "id": "5872",
    "manifest_path": "data/manifests/the_stack_sample/sample_2116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-dns-metrics-bash\n  namespace: kube-system\n  labels:\n    application: kube-dns-metrics-bash\n    version: v0.0.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: kube-dns-metrics-bash\n  template:\n    metadata:\n      labels:\n        application: kube-dns-metrics-bash\n        version: v0.0.4\n    spec:\n      containers:\n      - image: pierone.stups.zalan.do/teapot/kube-dns-metrics-bash:v0.0.5\n        name: kube-dns-metrics-bash\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 25m\n            memory: 100Mi\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-dns-metrics-bash\" is not set to runAsNonRoot"
  },
  {
    "id": "5873",
    "manifest_path": "data/manifests/the_stack_sample/sample_2118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        security.alpha.kubernetes.io/sysctls: net.ipv4.tcp_syncookies=0,net.ipv4.ip_local_port_range=10000\n          65535\n        security.alpha.kubernetes.io/unsafe-sysctls: net.core.somaxconn=65535,net.ipv4.tcp_tw_reuse=1,net.ipv4.tcp_fin_timeout=30,net.ipv4.tcp_keepalive_intvl=4,net.ipv4.tcp_keepalive_probes=3,net.ipv4.tcp_keepalive_time=120,net.ipv4.tcp_max_syn_backlog=65535,net.ipv4.tcp_rfc1337=1,net.ipv4.tcp_slow_start_after_idle=0,net.ipv4.tcp_fack=1,net.ipv4.tcp_fwmark_accept=1,net.ipv4.fwmark_reflect=1\n    spec:\n      containers:\n      - name: redis\n        image: slpcat/redis-3.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /var/lib/redis\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"slpcat/redis-3.2\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5874",
    "manifest_path": "data/manifests/the_stack_sample/sample_2118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        security.alpha.kubernetes.io/sysctls: net.ipv4.tcp_syncookies=0,net.ipv4.ip_local_port_range=10000\n          65535\n        security.alpha.kubernetes.io/unsafe-sysctls: net.core.somaxconn=65535,net.ipv4.tcp_tw_reuse=1,net.ipv4.tcp_fin_timeout=30,net.ipv4.tcp_keepalive_intvl=4,net.ipv4.tcp_keepalive_probes=3,net.ipv4.tcp_keepalive_time=120,net.ipv4.tcp_max_syn_backlog=65535,net.ipv4.tcp_rfc1337=1,net.ipv4.tcp_slow_start_after_idle=0,net.ipv4.tcp_fack=1,net.ipv4.tcp_fwmark_accept=1,net.ipv4.fwmark_reflect=1\n    spec:\n      containers:\n      - name: redis\n        image: slpcat/redis-3.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /var/lib/redis\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "5875",
    "manifest_path": "data/manifests/the_stack_sample/sample_2118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        security.alpha.kubernetes.io/sysctls: net.ipv4.tcp_syncookies=0,net.ipv4.ip_local_port_range=10000\n          65535\n        security.alpha.kubernetes.io/unsafe-sysctls: net.core.somaxconn=65535,net.ipv4.tcp_tw_reuse=1,net.ipv4.tcp_fin_timeout=30,net.ipv4.tcp_keepalive_intvl=4,net.ipv4.tcp_keepalive_probes=3,net.ipv4.tcp_keepalive_time=120,net.ipv4.tcp_max_syn_backlog=65535,net.ipv4.tcp_rfc1337=1,net.ipv4.tcp_slow_start_after_idle=0,net.ipv4.tcp_fack=1,net.ipv4.tcp_fwmark_accept=1,net.ipv4.fwmark_reflect=1\n    spec:\n      containers:\n      - name: redis\n        image: slpcat/redis-3.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /var/lib/redis\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "5876",
    "manifest_path": "data/manifests/the_stack_sample/sample_2119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: kuard\n  name: kuard\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      run: kuard\n  template:\n    metadata:\n      labels:\n        run: kuard\n    spec:\n      containers:\n      - image: gcr.io/kuar-demo/kuard-amd64:blue\n        name: kuard\n        resources:\n          limits:\n            cpu: 50m\n            memory: 0.1G\n          requests:\n            cpu: 50m\n            memory: 0.1G\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 10 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5877",
    "manifest_path": "data/manifests/the_stack_sample/sample_2119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: kuard\n  name: kuard\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      run: kuard\n  template:\n    metadata:\n      labels:\n        run: kuard\n    spec:\n      containers:\n      - image: gcr.io/kuar-demo/kuard-amd64:blue\n        name: kuard\n        resources:\n          limits:\n            cpu: 50m\n            memory: 0.1G\n          requests:\n            cpu: 50m\n            memory: 0.1G\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kuard\" does not have a read-only root file system"
  },
  {
    "id": "5878",
    "manifest_path": "data/manifests/the_stack_sample/sample_2119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: kuard\n  name: kuard\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      run: kuard\n  template:\n    metadata:\n      labels:\n        run: kuard\n    spec:\n      containers:\n      - image: gcr.io/kuar-demo/kuard-amd64:blue\n        name: kuard\n        resources:\n          limits:\n            cpu: 50m\n            memory: 0.1G\n          requests:\n            cpu: 50m\n            memory: 0.1G\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kuard\" is not set to runAsNonRoot"
  },
  {
    "id": "5879",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"iap\" is using an invalid container image, \"gcr.io/kubeflow-images-public/ingress-setup:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5880",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"iap\" does not have a read-only root file system"
  },
  {
    "id": "5881",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kf-admin\" not found"
  },
  {
    "id": "5882",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"iap\" is not set to runAsNonRoot"
  },
  {
    "id": "5883",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"iap\" has cpu request 0"
  },
  {
    "id": "5884",
    "manifest_path": "data/manifests/the_stack_sample/sample_2120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    kustomize.component: iap-ingress\n  name: iap-enabler\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      kustomize.component: iap-ingress\n  template:\n    metadata:\n      labels:\n        kustomize.component: iap-ingress\n        service: iap-enabler\n    spec:\n      containers:\n      - command:\n        - bash\n        - /var/envoy-config/setup_backend.sh\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/run/secrets/sa/admin-gcp-sa.json\n        - name: NAMESPACE\n          value: istio-system\n        - name: SERVICE\n          value: istio-ingressgateway\n        - name: INGRESS_NAME\n          value: envoy-ingress\n        - name: ENVOY_ADMIN\n          value: http://localhost:8001\n        - name: USE_ISTIO\n          value: 'true'\n        image: gcr.io/kubeflow-images-public/ingress-setup:latest\n        name: iap\n        volumeMounts:\n        - mountPath: /var/run/secrets/sa\n          name: sa-key\n          readOnly: true\n        - mountPath: /var/envoy-config/\n          name: config-volume\n      serviceAccountName: kf-admin\n      volumes:\n      - name: sa-key\n        secret:\n          secretName: admin-gcp-sa\n      - configMap:\n          name: envoy-config\n        name: config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"iap\" has memory limit 0"
  },
  {
    "id": "5885",
    "manifest_path": "data/manifests/the_stack_sample/sample_2121.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5886",
    "manifest_path": "data/manifests/the_stack_sample/sample_2121.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5887",
    "manifest_path": "data/manifests/the_stack_sample/sample_2121.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5888",
    "manifest_path": "data/manifests/the_stack_sample/sample_2121.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5889",
    "manifest_path": "data/manifests/the_stack_sample/sample_2121.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5890",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"packageserver\" is using an invalid container image, \"OLM_OPERATOR_IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5891",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"socks5-proxy\" is using an invalid container image, \"SOCKS5_PROXY_IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5892",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5893",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"packageserver\" does not have a read-only root file system"
  },
  {
    "id": "5894",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"socks5-proxy\" does not have a read-only root file system"
  },
  {
    "id": "5895",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"packageserver\" is not set to runAsNonRoot"
  },
  {
    "id": "5896",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"socks5-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "5897",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"socks5-proxy\" has cpu request 0"
  },
  {
    "id": "5898",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"packageserver\" has memory limit 0"
  },
  {
    "id": "5899",
    "manifest_path": "data/manifests/the_stack_sample/sample_2124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: packageserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: packageserver\n  template:\n    metadata:\n      labels:\n        app: packageserver\n        hypershift.openshift.io/control-plane-component: packageserver\n    spec:\n      containers:\n      - name: socks5-proxy\n        command:\n        - /usr/bin/konnectivity-socks5-proxy\n        args:\n        - run\n        image: SOCKS5_PROXY_IMAGE\n        env:\n        - name: KUBECONFIG\n          value: /etc/openshift/kubeconfig/kubeconfig\n        ports:\n        - containerPort: 8090\n        volumeMounts:\n        - mountPath: /etc/konnectivity-proxy-tls\n          name: oas-konnectivity-proxy-cert\n          readOnly: true\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - '5443'\n        - --global-namespace\n        - openshift-marketplace\n        - --kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authentication-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        - --authorization-kubeconfig\n        - /etc/openshift/kubeconfig/kubeconfig\n        env:\n        - name: OPERATOR_CONDITION_NAME\n          value: packageserver\n        - name: GRPC_PROXY\n          value: socks5://127.0.0.1:8090\n        - name: NO_PROXY\n          value: kube-apiserver,redhat-operators,certified-operators,community-operators,redhat-marketplace\n        image: OLM_OPERATOR_IMAGE\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: packageserver\n        ports:\n        - containerPort: 5443\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 5443\n            scheme: HTTPS\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmpfs\n        - mountPath: /apiserver.local.config/certificates\n          name: apiservice-cert\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: webhook-cert\n        - mountPath: /etc/openshift/kubeconfig\n          name: kubeconfig\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - emptyDir: {}\n        name: tmpfs\n      - name: apiservice-cert\n        secret:\n          defaultMode: 420\n          items:\n          - key: tls.crt\n            path: apiserver.crt\n          - key: tls.key\n            path: apiserver.key\n          secretName: packageserver-cert\n      - name: webhook-cert\n        secret:\n          defaultMode: 420\n          secretName: packageserver-cert\n      - name: kubeconfig\n        secret:\n          secretName: service-network-admin-kubeconfig\n      - name: oas-konnectivity-proxy-cert\n        secret:\n          secretName: konnectivity-client\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"socks5-proxy\" has memory limit 0"
  },
  {
    "id": "5900",
    "manifest_path": "data/manifests/the_stack_sample/sample_2129.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: pbx\n  namespace: pbx\n  labels:\n    app.kubernetes.io/name: pbx\n    app.kubernetes.io/part-of: pbx\nspec:\n  type: ClusterIP\n  ports:\n  - name: sip-control\n    port: 5060\n    targetPort: 5060\n  - name: sip-data\n    port: 5061\n    targetPort: 5061\n  - name: sip-data-udp\n    port: 5060\n    targetPort: 5061\n    protocol: UDP\n  selector:\n    app.kubernetes.io/name: pbx\n    app.kubernetes.io/part-of: pbx\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:pbx app.kubernetes.io/part-of:pbx])"
  },
  {
    "id": "5901",
    "manifest_path": "data/manifests/the_stack_sample/sample_2131.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - name: hello\n    image: raelga/hello\n    resources:\n      limits:\n        cpu: 50m\n        memory: 0.1G\n      requests:\n        cpu: 50m\n        memory: 0.1G\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hello\" is using an invalid container image, \"raelga/hello\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5902",
    "manifest_path": "data/manifests/the_stack_sample/sample_2131.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - name: hello\n    image: raelga/hello\n    resources:\n      limits:\n        cpu: 50m\n        memory: 0.1G\n      requests:\n        cpu: 50m\n        memory: 0.1G\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "5903",
    "manifest_path": "data/manifests/the_stack_sample/sample_2131.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - name: hello\n    image: raelga/hello\n    resources:\n      limits:\n        cpu: 50m\n        memory: 0.1G\n      requests:\n        cpu: 50m\n        memory: 0.1G\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "5904",
    "manifest_path": "data/manifests/the_stack_sample/sample_2132.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: workshop-303\nspec:\n  containers:\n  - name: first-container\n    image: fedora:29\n    command:\n    - sleep\n    - '36000'\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n    volumeMounts:\n    - name: my-configmap\n      mountPath: /config\n  volumes:\n  - name: my-configmap\n    configMap:\n      name: my-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"first-container\" does not have a read-only root file system"
  },
  {
    "id": "5905",
    "manifest_path": "data/manifests/the_stack_sample/sample_2132.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: workshop-303\nspec:\n  containers:\n  - name: first-container\n    image: fedora:29\n    command:\n    - sleep\n    - '36000'\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n    volumeMounts:\n    - name: my-configmap\n      mountPath: /config\n  volumes:\n  - name: my-configmap\n    configMap:\n      name: my-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"first-container\" is not set to runAsNonRoot"
  },
  {
    "id": "5906",
    "manifest_path": "data/manifests/the_stack_sample/sample_2132.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: workshop-303\nspec:\n  containers:\n  - name: first-container\n    image: fedora:29\n    command:\n    - sleep\n    - '36000'\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n    volumeMounts:\n    - name: my-configmap\n      mountPath: /config\n  volumes:\n  - name: my-configmap\n    configMap:\n      name: my-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"first-container\" has cpu request 0"
  },
  {
    "id": "5907",
    "manifest_path": "data/manifests/the_stack_sample/sample_2132.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: workshop-303\nspec:\n  containers:\n  - name: first-container\n    image: fedora:29\n    command:\n    - sleep\n    - '36000'\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n    volumeMounts:\n    - name: my-configmap\n      mountPath: /config\n  volumes:\n  - name: my-configmap\n    configMap:\n      name: my-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"first-container\" has memory limit 0"
  },
  {
    "id": "5908",
    "manifest_path": "data/manifests/the_stack_sample/sample_2133.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: selenium-node-chrome\n  namespace: selenium\n  labels:\n    app: selenium-node\n    browser: chrome\nspec:\n  selector:\n    matchLabels:\n      app: selenium-node\n      browser: chrome\n  template:\n    metadata:\n      labels:\n        app: selenium-node\n        browser: chrome\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cloud.google.com/gke-preemptible\n                operator: DoesNotExist\n              - key: eks.amazonaws.com/capacityType\n                operator: NotIn\n                values:\n                - SPOT\n              - key: kubernetes.azure.com/scalesetpriority\n                operator: NotIn\n                values:\n                - spot\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - name: selenium-node-chrome\n        image: selenium/node-chrome:90.0\n        ports:\n        - containerPort: 5555\n        - containerPort: 5900\n        - containerPort: 7900\n        env:\n        - name: JAVA_OPTS\n          value: -Xmx512m -Dselenium.LOGGER.level=WARNING\n        - name: SE_OPTS\n          value: ''\n        - name: SE_EVENT_BUS_HOST\n          value: selenium-hub\n        - name: SE_EVENT_BUS_PUBLISH_PORT\n          value: '4442'\n        - name: SE_EVENT_BUS_SUBSCRIBE_PORT\n          value: '4443'\n        - name: VNC_NO_PASSWORD\n          value: '1'\n        readinessProbe:\n          httpGet:\n            path: /status\n            port: 5555\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /status\n            port: 5555\n          initialDelaySeconds: 30\n        resources:\n          limits:\n            cpu: 2\n            memory: 1Gi\n          requests:\n            cpu: 300m\n            memory: 615Mi\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"selenium-node-chrome\" does not have a read-only root file system"
  },
  {
    "id": "5909",
    "manifest_path": "data/manifests/the_stack_sample/sample_2134.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200409-becd20a71\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "5910",
    "manifest_path": "data/manifests/the_stack_sample/sample_2134.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200409-becd20a71\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"crier\" not found"
  },
  {
    "id": "5911",
    "manifest_path": "data/manifests/the_stack_sample/sample_2134.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200409-becd20a71\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "5912",
    "manifest_path": "data/manifests/the_stack_sample/sample_2134.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200409-becd20a71\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "5913",
    "manifest_path": "data/manifests/the_stack_sample/sample_2134.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20200409-becd20a71\n        args:\n        - --github-workers=5\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --slack-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --gcs-workers=1\n        - --kubernetes-gcs-workers=1\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "5914",
    "manifest_path": "data/manifests/the_stack_sample/sample_2135.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube2iam\n  namespace: kube-system\n  labels:\n    application: kube2iam\n    version: 0.10.7\nspec:\n  selector:\n    matchLabels:\n      application: kube2iam\n  template:\n    metadata:\n      labels:\n        application: kube2iam\n        version: 0.10.7\n    spec:\n      serviceAccountName: kube2iam\n      containers:\n      - image: registry.opensource.zalan.do/teapot/kube2iam:0.10.7\n        name: kube2iam\n        args:\n        - --auto-discover-base-arn\n        - --verbose\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - containerPort: 8181\n          hostPort: 8181\n          name: http\n        securityContext:\n          privileged: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8181\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 25m\n            memory: 100Mi\n            ephemeral-storage: 256Mi\n          limits:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "5915",
    "manifest_path": "data/manifests/the_stack_sample/sample_2135.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube2iam\n  namespace: kube-system\n  labels:\n    application: kube2iam\n    version: 0.10.7\nspec:\n  selector:\n    matchLabels:\n      application: kube2iam\n  template:\n    metadata:\n      labels:\n        application: kube2iam\n        version: 0.10.7\n    spec:\n      serviceAccountName: kube2iam\n      containers:\n      - image: registry.opensource.zalan.do/teapot/kube2iam:0.10.7\n        name: kube2iam\n        args:\n        - --auto-discover-base-arn\n        - --verbose\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - containerPort: 8181\n          hostPort: 8181\n          name: http\n        securityContext:\n          privileged: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8181\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 25m\n            memory: 100Mi\n            ephemeral-storage: 256Mi\n          limits:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube2iam\" does not have a read-only root file system"
  },
  {
    "id": "5916",
    "manifest_path": "data/manifests/the_stack_sample/sample_2135.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube2iam\n  namespace: kube-system\n  labels:\n    application: kube2iam\n    version: 0.10.7\nspec:\n  selector:\n    matchLabels:\n      application: kube2iam\n  template:\n    metadata:\n      labels:\n        application: kube2iam\n        version: 0.10.7\n    spec:\n      serviceAccountName: kube2iam\n      containers:\n      - image: registry.opensource.zalan.do/teapot/kube2iam:0.10.7\n        name: kube2iam\n        args:\n        - --auto-discover-base-arn\n        - --verbose\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - containerPort: 8181\n          hostPort: 8181\n          name: http\n        securityContext:\n          privileged: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8181\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 25m\n            memory: 100Mi\n            ephemeral-storage: 256Mi\n          limits:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kube2iam\" not found"
  },
  {
    "id": "5917",
    "manifest_path": "data/manifests/the_stack_sample/sample_2135.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube2iam\n  namespace: kube-system\n  labels:\n    application: kube2iam\n    version: 0.10.7\nspec:\n  selector:\n    matchLabels:\n      application: kube2iam\n  template:\n    metadata:\n      labels:\n        application: kube2iam\n        version: 0.10.7\n    spec:\n      serviceAccountName: kube2iam\n      containers:\n      - image: registry.opensource.zalan.do/teapot/kube2iam:0.10.7\n        name: kube2iam\n        args:\n        - --auto-discover-base-arn\n        - --verbose\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - containerPort: 8181\n          hostPort: 8181\n          name: http\n        securityContext:\n          privileged: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8181\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 25m\n            memory: 100Mi\n            ephemeral-storage: 256Mi\n          limits:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"kube2iam\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5918",
    "manifest_path": "data/manifests/the_stack_sample/sample_2135.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube2iam\n  namespace: kube-system\n  labels:\n    application: kube2iam\n    version: 0.10.7\nspec:\n  selector:\n    matchLabels:\n      application: kube2iam\n  template:\n    metadata:\n      labels:\n        application: kube2iam\n        version: 0.10.7\n    spec:\n      serviceAccountName: kube2iam\n      containers:\n      - image: registry.opensource.zalan.do/teapot/kube2iam:0.10.7\n        name: kube2iam\n        args:\n        - --auto-discover-base-arn\n        - --verbose\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - containerPort: 8181\n          hostPort: 8181\n          name: http\n        securityContext:\n          privileged: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8181\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 25m\n            memory: 100Mi\n            ephemeral-storage: 256Mi\n          limits:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"kube2iam\" is privileged"
  },
  {
    "id": "5919",
    "manifest_path": "data/manifests/the_stack_sample/sample_2135.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube2iam\n  namespace: kube-system\n  labels:\n    application: kube2iam\n    version: 0.10.7\nspec:\n  selector:\n    matchLabels:\n      application: kube2iam\n  template:\n    metadata:\n      labels:\n        application: kube2iam\n        version: 0.10.7\n    spec:\n      serviceAccountName: kube2iam\n      containers:\n      - image: registry.opensource.zalan.do/teapot/kube2iam:0.10.7\n        name: kube2iam\n        args:\n        - --auto-discover-base-arn\n        - --verbose\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - containerPort: 8181\n          hostPort: 8181\n          name: http\n        securityContext:\n          privileged: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8181\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 25m\n            memory: 100Mi\n            ephemeral-storage: 256Mi\n          limits:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube2iam\" is not set to runAsNonRoot"
  },
  {
    "id": "5920",
    "manifest_path": "data/manifests/the_stack_sample/sample_2136.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: web-app\n        image: bmuschko/nodejs-hello-world:1.0.0\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "mismatching-selector",
    "violation_text": "labels in pod spec (map[app:myapp]) do not match labels in selector (&LabelSelector{MatchLabels:map[string]string{app: nodejs,},MatchExpressions:[]LabelSelectorRequirement{},})"
  },
  {
    "id": "5921",
    "manifest_path": "data/manifests/the_stack_sample/sample_2136.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: web-app\n        image: bmuschko/nodejs-hello-world:1.0.0\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5922",
    "manifest_path": "data/manifests/the_stack_sample/sample_2136.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: web-app\n        image: bmuschko/nodejs-hello-world:1.0.0\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-app\" does not have a read-only root file system"
  },
  {
    "id": "5923",
    "manifest_path": "data/manifests/the_stack_sample/sample_2136.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: web-app\n        image: bmuschko/nodejs-hello-world:1.0.0\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-app\" is not set to runAsNonRoot"
  },
  {
    "id": "5924",
    "manifest_path": "data/manifests/the_stack_sample/sample_2136.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: web-app\n        image: bmuschko/nodejs-hello-world:1.0.0\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-app\" has cpu request 0"
  },
  {
    "id": "5925",
    "manifest_path": "data/manifests/the_stack_sample/sample_2136.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: web-app\n        image: bmuschko/nodejs-hello-world:1.0.0\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-app\" has memory limit 0"
  },
  {
    "id": "5926",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "mismatching-selector",
    "violation_text": "labels in pod spec (map[nvidiaheartbeat-node:pod]) do not match labels in selector (&LabelSelector{MatchLabels:map[string]string{name: nvidiaheartbeat,},MatchExpressions:[]LabelSelectorRequirement{},})"
  },
  {
    "id": "5927",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nvidiaheartbeat\" does not have a read-only root file system"
  },
  {
    "id": "5928",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nvidiaheartbeat\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5929",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"nvidiaheartbeat\" is privileged"
  },
  {
    "id": "5930",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nvidiaheartbeat\" is not set to runAsNonRoot"
  },
  {
    "id": "5931",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"nvidiaheartbeat\""
  },
  {
    "id": "5932",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nvidiaheartbeat\" has cpu request 0"
  },
  {
    "id": "5933",
    "manifest_path": "data/manifests/the_stack_sample/sample_2139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidiaheartbeat\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: nvidiaheartbeat\n  template:\n    metadata:\n      name: nvidiaheartbeat\n      labels:\n        nvidiaheartbeat-node: pod\n    spec:\n      containers:\n      - name: nvidiaheartbeat\n        image: nvidia/cuda:8.0\n        command:\n        - bash\n        - -c\n        - bash -c 'while true; do nvidia-smi | grep Tesla | wc -l; sleep 10; done'\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /usr/local/nvidia\n          name: nvidia-driver\n        - mountPath: /dev\n          name: dev\n      volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /opt/nvidia-driver/current\n      - name: dev\n        hostPath:\n          path: /dev\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nvidiaheartbeat\" has memory limit 0"
  },
  {
    "id": "5934",
    "manifest_path": "data/manifests/the_stack_sample/sample_2140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8112\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5935",
    "manifest_path": "data/manifests/the_stack_sample/sample_2140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8112\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5936",
    "manifest_path": "data/manifests/the_stack_sample/sample_2140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8112\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5937",
    "manifest_path": "data/manifests/the_stack_sample/sample_2140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8112\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5938",
    "manifest_path": "data/manifests/the_stack_sample/sample_2140.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8112\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5939",
    "manifest_path": "data/manifests/the_stack_sample/sample_2141.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app: kube-state-metrics\n    spec:\n      containers:\n      - name: kube-state-metrics\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "5940",
    "manifest_path": "data/manifests/the_stack_sample/sample_2141.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app: kube-state-metrics\n    spec:\n      containers:\n      - name: kube-state-metrics\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-state-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "5941",
    "manifest_path": "data/manifests/the_stack_sample/sample_2141.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app: kube-state-metrics\n    spec:\n      containers:\n      - name: kube-state-metrics\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-state-metrics\" has cpu request 0"
  },
  {
    "id": "5942",
    "manifest_path": "data/manifests/the_stack_sample/sample_2141.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app: kube-state-metrics\n    spec:\n      containers:\n      - name: kube-state-metrics\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "5943",
    "manifest_path": "data/manifests/the_stack_sample/sample_2142.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        image: controller:latest\n        name: manager\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: ${K8S_CP_LABEL:=node-role.kubernetes.io/control-plane}\n                operator: Exists\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5944",
    "manifest_path": "data/manifests/the_stack_sample/sample_2142.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        image: controller:latest\n        name: manager\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: ${K8S_CP_LABEL:=node-role.kubernetes.io/control-plane}\n                operator: Exists\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "5945",
    "manifest_path": "data/manifests/the_stack_sample/sample_2142.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        image: controller:latest\n        name: manager\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: ${K8S_CP_LABEL:=node-role.kubernetes.io/control-plane}\n                operator: Exists\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "5946",
    "manifest_path": "data/manifests/the_stack_sample/sample_2142.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        image: controller:latest\n        name: manager\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: ${K8S_CP_LABEL:=node-role.kubernetes.io/control-plane}\n                operator: Exists\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"manager\" has cpu request 0"
  },
  {
    "id": "5947",
    "manifest_path": "data/manifests/the_stack_sample/sample_2142.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        image: controller:latest\n        name: manager\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: ${K8S_CP_LABEL:=node-role.kubernetes.io/control-plane}\n                operator: Exists\n          - weight: 10\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"manager\" has memory limit 0"
  },
  {
    "id": "5948",
    "manifest_path": "data/manifests/the_stack_sample/sample_2143.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: echoheaders-https\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: echoheaders-https\n    spec:\n      containers:\n      - name: echoheaders-https\n        image: gcr.io/google_containers/echoserver:1.10\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5949",
    "manifest_path": "data/manifests/the_stack_sample/sample_2143.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: echoheaders-https\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: echoheaders-https\n    spec:\n      containers:\n      - name: echoheaders-https\n        image: gcr.io/google_containers/echoserver:1.10\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echoheaders-https\" does not have a read-only root file system"
  },
  {
    "id": "5950",
    "manifest_path": "data/manifests/the_stack_sample/sample_2143.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: echoheaders-https\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: echoheaders-https\n    spec:\n      containers:\n      - name: echoheaders-https\n        image: gcr.io/google_containers/echoserver:1.10\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echoheaders-https\" is not set to runAsNonRoot"
  },
  {
    "id": "5951",
    "manifest_path": "data/manifests/the_stack_sample/sample_2143.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: echoheaders-https\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: echoheaders-https\n    spec:\n      containers:\n      - name: echoheaders-https\n        image: gcr.io/google_containers/echoserver:1.10\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"echoheaders-https\" has cpu request 0"
  },
  {
    "id": "5952",
    "manifest_path": "data/manifests/the_stack_sample/sample_2143.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: echoheaders-https\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: echoheaders-https\n    spec:\n      containers:\n      - name: echoheaders-https\n        image: gcr.io/google_containers/echoserver:1.10\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"echoheaders-https\" has memory limit 0"
  },
  {
    "id": "5953",
    "manifest_path": "data/manifests/the_stack_sample/sample_2145.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9533\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5954",
    "manifest_path": "data/manifests/the_stack_sample/sample_2145.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9533\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "5955",
    "manifest_path": "data/manifests/the_stack_sample/sample_2145.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9533\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "5956",
    "manifest_path": "data/manifests/the_stack_sample/sample_2145.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9533\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "5957",
    "manifest_path": "data/manifests/the_stack_sample/sample_2145.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9533\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "5958",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (aqua-sa), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "5959",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aqua-gateway\" does not have a read-only root file system"
  },
  {
    "id": "5960",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"aqua-sa\" not found"
  },
  {
    "id": "5961",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"aqua-gateway\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5962",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"aqua-gateway\" is privileged"
  },
  {
    "id": "5963",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aqua-gateway\" is not set to runAsNonRoot"
  },
  {
    "id": "5964",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"aqua-gateway\" has cpu request 0"
  },
  {
    "id": "5965",
    "manifest_path": "data/manifests/the_stack_sample/sample_2146.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: aqua-gateway\n  name: aqua-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-gateway\n  template:\n    metadata:\n      labels:\n        app: aqua-gateway\n      name: aqua-gateway\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-gateway\n        image: registry.aquasec.com/gateway:6.5\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: AQUA_CONSOLE_SECURE_ADDRESS\n          value: aqua-web:443\n        - name: SCALOCK_GATEWAY_PUBLIC_IP\n          value: aqua-gateway\n        - name: SCALOCK_DBUSER\n          value: postgres\n        - name: SCALOCK_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_DBNAME\n          value: scalock\n        - name: SCALOCK_DBHOST\n          value: aqua-db\n        - name: SCALOCK_DBPORT\n          value: '5432'\n        - name: SCALOCK_AUDIT_DBUSER\n          value: postgres\n        - name: SCALOCK_AUDIT_DBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        - name: SCALOCK_AUDIT_DBNAME\n          value: slk_audit\n        - name: SCALOCK_AUDIT_DBHOST\n          value: aqua-db\n        - name: SCALOCK_AUDIT_DBPORT\n          value: '5432'\n        ports:\n        - containerPort: 3622\n          protocol: TCP\n        - containerPort: 8443\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"aqua-gateway\" has memory limit 0"
  },
  {
    "id": "5966",
    "manifest_path": "data/manifests/the_stack_sample/sample_2150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n        version: v1\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      containers:\n      - name: main\n        image: sitaramiyer/bookstore-demo:loadgenerator-v0.1.2\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "5967",
    "manifest_path": "data/manifests/the_stack_sample/sample_2150.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n        version: v1\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      containers:\n      - name: main\n        image: sitaramiyer/bookstore-demo:loadgenerator-v0.1.2\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "5968",
    "manifest_path": "data/manifests/the_stack_sample/sample_2151.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: import-named-polls-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: import-named-polls\n          image: docker.pkg.github.com/demokratie-live/democracy-development/import-named-polls:0.1.2\n          env:\n          - name: DB_URL\n            valueFrom:\n              configMapKeyRef:\n                name: bio-api-config\n                key: DB_URL\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"import-named-polls\" does not have a read-only root file system"
  },
  {
    "id": "5969",
    "manifest_path": "data/manifests/the_stack_sample/sample_2151.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: import-named-polls-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: import-named-polls\n          image: docker.pkg.github.com/demokratie-live/democracy-development/import-named-polls:0.1.2\n          env:\n          - name: DB_URL\n            valueFrom:\n              configMapKeyRef:\n                name: bio-api-config\n                key: DB_URL\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"import-named-polls\" is not set to runAsNonRoot"
  },
  {
    "id": "5970",
    "manifest_path": "data/manifests/the_stack_sample/sample_2151.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: import-named-polls-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: import-named-polls\n          image: docker.pkg.github.com/demokratie-live/democracy-development/import-named-polls:0.1.2\n          env:\n          - name: DB_URL\n            valueFrom:\n              configMapKeyRef:\n                name: bio-api-config\n                key: DB_URL\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"import-named-polls\" has cpu request 0"
  },
  {
    "id": "5971",
    "manifest_path": "data/manifests/the_stack_sample/sample_2151.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: import-named-polls-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: import-named-polls\n          image: docker.pkg.github.com/demokratie-live/democracy-development/import-named-polls:0.1.2\n          env:\n          - name: DB_URL\n            valueFrom:\n              configMapKeyRef:\n                name: bio-api-config\n                key: DB_URL\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"import-named-polls\" has memory limit 0"
  },
  {
    "id": "5972",
    "manifest_path": "data/manifests/the_stack_sample/sample_2152.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia-deploy\n  namespace: chp16-set1612\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia-deploy\n  template:\n    metadata:\n      name: kubia-deploy\n      labels:\n        app: kubia-deploy\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsNonRoot: true\n      serviceAccountName: foo\n      containers:\n      - image: georgebaptista/kubia\n        name: kubia-deploy\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          runAsNonRoot: true\n          runAsUser: 1000\n        resources:\n          limits:\n            cpu: 200m\n          requests:\n            cpu: 100m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kubia-deploy\" is using an invalid container image, \"georgebaptista/kubia\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5973",
    "manifest_path": "data/manifests/the_stack_sample/sample_2152.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia-deploy\n  namespace: chp16-set1612\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia-deploy\n  template:\n    metadata:\n      name: kubia-deploy\n      labels:\n        app: kubia-deploy\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsNonRoot: true\n      serviceAccountName: foo\n      containers:\n      - image: georgebaptista/kubia\n        name: kubia-deploy\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          runAsNonRoot: true\n          runAsUser: 1000\n        resources:\n          limits:\n            cpu: 200m\n          requests:\n            cpu: 100m\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "5974",
    "manifest_path": "data/manifests/the_stack_sample/sample_2152.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia-deploy\n  namespace: chp16-set1612\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia-deploy\n  template:\n    metadata:\n      name: kubia-deploy\n      labels:\n        app: kubia-deploy\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsNonRoot: true\n      serviceAccountName: foo\n      containers:\n      - image: georgebaptista/kubia\n        name: kubia-deploy\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          runAsNonRoot: true\n          runAsUser: 1000\n        resources:\n          limits:\n            cpu: 200m\n          requests:\n            cpu: 100m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubia-deploy\" does not have a read-only root file system"
  },
  {
    "id": "5975",
    "manifest_path": "data/manifests/the_stack_sample/sample_2152.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia-deploy\n  namespace: chp16-set1612\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia-deploy\n  template:\n    metadata:\n      name: kubia-deploy\n      labels:\n        app: kubia-deploy\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsNonRoot: true\n      serviceAccountName: foo\n      containers:\n      - image: georgebaptista/kubia\n        name: kubia-deploy\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          runAsNonRoot: true\n          runAsUser: 1000\n        resources:\n          limits:\n            cpu: 200m\n          requests:\n            cpu: 100m\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"foo\" not found"
  },
  {
    "id": "5976",
    "manifest_path": "data/manifests/the_stack_sample/sample_2152.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubia-deploy\n  namespace: chp16-set1612\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia-deploy\n  template:\n    metadata:\n      name: kubia-deploy\n      labels:\n        app: kubia-deploy\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsNonRoot: true\n      serviceAccountName: foo\n      containers:\n      - image: georgebaptista/kubia\n        name: kubia-deploy\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          runAsNonRoot: true\n          runAsUser: 1000\n        resources:\n          limits:\n            cpu: 200m\n          requests:\n            cpu: 100m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubia-deploy\" has memory limit 0"
  },
  {
    "id": "5977",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dss\" is using an invalid container image, \"dss-node-image\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "5978",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dind\" does not have a read-only root file system"
  },
  {
    "id": "5979",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dood\" does not have a read-only root file system"
  },
  {
    "id": "5980",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dss\" does not have a read-only root file system"
  },
  {
    "id": "5981",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"dind\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5982",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"dss\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "5983",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"dind\" is privileged"
  },
  {
    "id": "5984",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"dss\" is privileged"
  },
  {
    "id": "5985",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dind\" is not set to runAsNonRoot"
  },
  {
    "id": "5986",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dood\" is not set to runAsNonRoot"
  },
  {
    "id": "5987",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dss\" is not set to runAsNonRoot"
  },
  {
    "id": "5988",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dss\" has cpu request 0"
  },
  {
    "id": "5989",
    "manifest_path": "data/manifests/the_stack_sample/sample_2153.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dss\n  labels:\n    app: dss\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dss\n  template:\n    metadata:\n      labels:\n        app: dss\n    spec:\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: dss\n        image: dss-node-image\n        imagePullPolicy: Always\n        command:\n        - /bin/bash\n        args:\n        - -ci\n        - ./startup.sh\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        - name: DOCKER_HOST\n          value: tcp://localhost:2375\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kubectl-config\n          mountPath: /home/ubuntu/.kube\n        - name: shared-storage\n          mountPath: /share\n        - name: gitlab-registry-cred\n          mountPath: /gitlab-repo/credentials\n      - name: dind\n        image: docker:19.03.9-dind\n        resources:\n          requests:\n            cpu: 250m\n            memory: 250Mi\n          limits:\n            cpu: 250m\n            memory: 250Mi\n        env:\n        - name: DOCKER_TLS_CERTDIR\n          value: ''\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: docker-storage\n          mountPath: /var/lib/docker\n      - name: dood\n        image: docker:19.03.8\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            cpu: 200m\n            memory: 250Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        env:\n        - name: DSS_NODE_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: dss-node-type\n              key: DSS_NODE_TYPE\n        envFrom:\n        - secretRef:\n            name: gitlab-registry-cred\n        volumeMounts:\n        - name: docker-sock\n          mountPath: /var/run\n        - name: shared-storage\n          mountPath: /share\n      volumes:\n      - name: docker-storage\n        emptyDir: {}\n      - name: docker-sock\n        hostPath:\n          path: /var/run\n      - name: shared-storage\n        emptyDir: {}\n      - name: kubectl-config\n        secret:\n          secretName: kubectl-config\n          defaultMode: 256\n      - name: gitlab-registry-cred\n        secret:\n          secretName: gitlab-registry-cred\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dss\" has memory limit 0"
  },
  {
    "id": "5990",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"aci-containers-host\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "5991",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"aci-containers-host\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "5992",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"cnideploy\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "5993",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"opflex-agent\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "5994",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"opflex-server\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "5995",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "host-ipc",
    "violation_text": "resource shares host's IPC namespace (via hostIPC=true)."
  },
  {
    "id": "5996",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "5997",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "5998",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"aci-containers-host\" does not expose port 8090 for the HTTPGet"
  },
  {
    "id": "5999",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aci-containers-host\" does not have a read-only root file system"
  },
  {
    "id": "6000",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cnideploy\" does not have a read-only root file system"
  },
  {
    "id": "6001",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"opflex-agent\" does not have a read-only root file system"
  },
  {
    "id": "6002",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"opflex-server\" does not have a read-only root file system"
  },
  {
    "id": "6003",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"aci-containers-host-agent\" not found"
  },
  {
    "id": "6004",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"aci-containers-host\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "6005",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cnideploy\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "6006",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aci-containers-host\" is not set to runAsNonRoot"
  },
  {
    "id": "6007",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cnideploy\" is not set to runAsNonRoot"
  },
  {
    "id": "6008",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"opflex-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "6009",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"opflex-server\" is not set to runAsNonRoot"
  },
  {
    "id": "6010",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"aci-containers-host\" has cpu request 0"
  },
  {
    "id": "6011",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cnideploy\" has cpu request 0"
  },
  {
    "id": "6012",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"opflex-agent\" has cpu request 0"
  },
  {
    "id": "6013",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"opflex-server\" has cpu request 0"
  },
  {
    "id": "6014",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"aci-containers-host\" has memory limit 0"
  },
  {
    "id": "6015",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cnideploy\" has memory limit 0"
  },
  {
    "id": "6016",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"opflex-agent\" has memory limit 0"
  },
  {
    "id": "6017",
    "manifest_path": "data/manifests/the_stack_sample/sample_2154.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    aci-containers-config-version: dummy\n    network-plugin: aci-containers\n  name: aci-containers-host\n  namespace: aci-containers-system\nspec:\n  selector:\n    matchLabels:\n      name: aci-containers-host\n      network-plugin: aci-containers\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9612'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aci-containers-host\n        network-plugin: aci-containers\n    spec:\n      containers:\n      - env:\n        - name: KUBERNETES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: TENANT\n          value: csrtest\n        - name: NODE_EPG\n          value: aci-containers-nodes\n        - name: OPFLEX_MODE\n          value: overlay\n        - name: DURATION_WAIT_FOR_NETWORK\n          value: '210'\n        image: noirolabs/aci-containers-host:ci_test\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /status\n            port: 8090\n            scheme: HTTP\n          initialDelaySeconds: 120\n          periodSeconds: 60\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: aci-containers-host\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - NET_ADMIN\n            - SYS_PTRACE\n            - NET_RAW\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n        - mountPath: /mnt/cni-conf\n          name: cni-conf\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/aci-containers/\n          name: host-config-volume\n      - env:\n        - name: REBOOT_WITH_OVS\n          value: 'true'\n        - name: SSL_MODE\n          value: disabled\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-agent\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/var\n          name: hostvar\n        - mountPath: /run\n          name: hostrun\n        - mountPath: /usr/local/run\n          name: hostrun\n        - mountPath: /usr/local/etc/opflex-agent-ovs/base-conf.d\n          name: opflex-hostconfig-volume\n        - mountPath: /usr/local/etc/opflex-agent-ovs/conf.d\n          name: opflex-config-volume\n      - args:\n        - /usr/local/bin/launch-opflexserver.sh\n        command:\n        - /bin/sh\n        image: noirolabs/opflex:ci_test\n        imagePullPolicy: Always\n        name: opflex-server\n        ports:\n        - containerPort: 19999\n        - containerPort: 9632\n          name: metrics\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        volumeMounts:\n        - mountPath: /usr/local/etc/opflex-server\n          name: opflex-server-config-volume\n        - mountPath: /usr/local/var\n          name: hostvar\n      initContainers:\n      - image: noirolabs/cnideploy:6.0.0.0.0ef4718\n        imagePullPolicy: Always\n        name: cnideploy\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        volumeMounts:\n        - mountPath: /mnt/cni-bin\n          name: cni-bin\n      serviceAccountName: aci-containers-host-agent\n      volumes:\n      - hostPath:\n          path: /var/lib\n        name: cni-bin\n      - hostPath:\n          path: /etc/kubernetes\n        name: cni-conf\n      - hostPath:\n          path: /var\n        name: hostvar\n      - hostPath:\n          path: /run\n        name: hostrun\n      - configMap:\n          items:\n          - key: host-agent-config\n            path: host-agent.conf\n          name: aci-containers-config\n        name: host-config-volume\n      - emptyDir:\n          medium: Memory\n        name: opflex-hostconfig-volume\n      - configMap:\n          items:\n          - key: opflex-agent-config\n            path: local.conf\n          name: aci-containers-config\n        name: opflex-config-volume\n      - name: opflex-server-config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"opflex-server\" has memory limit 0"
  },
  {
    "id": "6018",
    "manifest_path": "data/manifests/the_stack_sample/sample_2155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5267\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6019",
    "manifest_path": "data/manifests/the_stack_sample/sample_2155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5267\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6020",
    "manifest_path": "data/manifests/the_stack_sample/sample_2155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5267\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6021",
    "manifest_path": "data/manifests/the_stack_sample/sample_2155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5267\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6022",
    "manifest_path": "data/manifests/the_stack_sample/sample_2155.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5267\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6023",
    "manifest_path": "data/manifests/the_stack_sample/sample_2156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: covid19-countries\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 3002\n    targetPort: 3002\n  selector:\n    app: covid19-countries\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:covid19-countries])"
  },
  {
    "id": "6024",
    "manifest_path": "data/manifests/the_stack_sample/sample_2157.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: contour\n  labels:\n    app: contour\nspec:\n  type: NodePort\n  selector:\n    app: contour\n  ports:\n  - protocol: TCP\n    port: 80\n    nodePort: 30081\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:contour])"
  },
  {
    "id": "6025",
    "manifest_path": "data/manifests/the_stack_sample/sample_2159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6026",
    "manifest_path": "data/manifests/the_stack_sample/sample_2159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "6027",
    "manifest_path": "data/manifests/the_stack_sample/sample_2159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6028",
    "manifest_path": "data/manifests/the_stack_sample/sample_2159.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "6029",
    "manifest_path": "data/manifests/the_stack_sample/sample_2161.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: network-dude\nspec:\n  selector:\n    app: network-dude\n  ports:\n  - protocol: UDP\n    port: 1999\n    targetPort: 1999\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:network-dude])"
  },
  {
    "id": "6030",
    "manifest_path": "data/manifests/the_stack_sample/sample_2162.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210204-de2fa22b93\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "6031",
    "manifest_path": "data/manifests/the_stack_sample/sample_2162.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210204-de2fa22b93\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "6032",
    "manifest_path": "data/manifests/the_stack_sample/sample_2162.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210204-de2fa22b93\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "6033",
    "manifest_path": "data/manifests/the_stack_sample/sample_2162.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20210204-de2fa22b93\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "6034",
    "manifest_path": "data/manifests/the_stack_sample/sample_2163.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: receipt-verifier\n  namespace: default\nspec:\n  selector:\n    app: receipt-verifier\n  ports:\n  - port: 3000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:receipt-verifier])"
  },
  {
    "id": "6035",
    "manifest_path": "data/manifests/the_stack_sample/sample_2164.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-nginx-controller-loadbalancer\n  namespace: ingress-nginx\nspec:\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 443\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:ingress-nginx app.kubernetes.io/name:ingress-nginx])"
  },
  {
    "id": "6036",
    "manifest_path": "data/manifests/the_stack_sample/sample_2165.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance\n        args:\n        - gateway\n        - gcs\n        - $(GCP_PROJECT_ID)\n        env:\n        - name: GCP_PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: pipeline-install-config\n              key: gcsProjectId\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable MINIO_SECRET_KEY in container \"minio\" found"
  },
  {
    "id": "6037",
    "manifest_path": "data/manifests/the_stack_sample/sample_2165.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance\n        args:\n        - gateway\n        - gcs\n        - $(GCP_PROJECT_ID)\n        env:\n        - name: GCP_PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: pipeline-install-config\n              key: gcsProjectId\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio\" does not have a read-only root file system"
  },
  {
    "id": "6038",
    "manifest_path": "data/manifests/the_stack_sample/sample_2165.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance\n        args:\n        - gateway\n        - gcs\n        - $(GCP_PROJECT_ID)\n        env:\n        - name: GCP_PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: pipeline-install-config\n              key: gcsProjectId\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio\" is not set to runAsNonRoot"
  },
  {
    "id": "6039",
    "manifest_path": "data/manifests/the_stack_sample/sample_2165.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance\n        args:\n        - gateway\n        - gcs\n        - $(GCP_PROJECT_ID)\n        env:\n        - name: GCP_PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: pipeline-install-config\n              key: gcsProjectId\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio\" has cpu request 0"
  },
  {
    "id": "6040",
    "manifest_path": "data/manifests/the_stack_sample/sample_2165.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance\n        args:\n        - gateway\n        - gcs\n        - $(GCP_PROJECT_ID)\n        env:\n        - name: GCP_PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: pipeline-install-config\n              key: gcsProjectId\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio\" has memory limit 0"
  },
  {
    "id": "6041",
    "manifest_path": "data/manifests/the_stack_sample/sample_2166.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: spring-boot-service-two\nspec:\n  ports:\n  - port: 8282\n    targetPort: 8080\n    name: http\n  selector:\n    app: ts-rest-service\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ts-rest-service])"
  },
  {
    "id": "6042",
    "manifest_path": "data/manifests/the_stack_sample/sample_2167.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: authelia-svc\n  namespace: authentication-system\n  labels:\n    app.kubernetes.io/name: authelia\nspec:\n  selector:\n    app.kubernetes.io/name: authelia\n  ports:\n  - name: http\n    protocol: TCP\n    port: 9091\n    targetPort: authelia-port\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:authelia])"
  },
  {
    "id": "6043",
    "manifest_path": "data/manifests/the_stack_sample/sample_2168.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: prod-cron-cancel-booking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-cancel-booking\n          image: 414928843086.dkr.ecr.ap-southeast-1.amazonaws.com/core-api-v2:<VERSION>\n          imagePullPolicy: Always\n          args:\n          - run\n          - start-cron:cancel-booking:prod\n          env:\n          - name: NODE_ENV\n            value: production\n          - name: PORT\n            value: '3000'\n          - name: HOST\n            value: 0.0.0.0\n          - name: DB_DIALECT\n            valueFrom:\n              secretKeyRef:\n                key: DB_DIALECT\n                name: core-api\n          - name: DB_PORT\n            valueFrom:\n              secretKeyRef:\n                key: DB_PORT\n                name: core-api\n          - name: DB_HOST\n            valueFrom:\n              secretKeyRef:\n                key: DB_HOST\n                name: core-api\n          - name: DB_USERNAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_USERNAME\n                name: core-api\n          - name: DB_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                key: DB_PASSWORD\n                name: core-api\n          - name: DB_NAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_NAME\n                name: core-api\n          - name: JWT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: JWT_SECRET\n                name: core-api\n          - name: JWT_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: JWT_EXPIRATION\n                name: core-api\n          - name: DEFAULT_COMMISSION\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_COMMISSION\n                name: core-api\n          - name: TERMINATION_NOTICE_DAYS\n            valueFrom:\n              secretKeyRef:\n                key: TERMINATION_NOTICE_DAYS\n                name: core-api\n          - name: FILTER_BY_STOCK_URL\n            valueFrom:\n              secretKeyRef:\n                name: core-api\n                key: FILTER_BY_STOCK_URL\n          - name: TWILIO_ACCOUNT_SID\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_ACCOUNT_SID\n                name: twilio-secret\n          - name: TWILIO_AUTH_TOKEN\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_AUTH_TOKEN\n                name: twilio-secret\n          - name: TWILIO_SENDER_PHONE\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_SENDER_PHONE\n                name: twilio-secret\n          - name: AWS_S3_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_SECRET_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_REGION\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_REGION\n                name: aws-secret\n          - name: AWS_S3_BUCKET_NAME\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_NAME\n                name: aws-secret\n          - name: AWS_S3_BUCKET_URL\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_URL\n                name: aws-secret\n          - name: DEFAULT_RESIZE_WIDTH\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_RESIZE_WIDTH\n                name: aws-secret\n          - name: SUPPORT_MIME_TYPE\n            valueFrom:\n              secretKeyRef:\n                key: SUPPORT_MIME_TYPE\n                name: aws-secret\n          - name: SENDGRID_API_KEY\n            valueFrom:\n              secretKeyRef:\n                key: SENDGRID_API_KEY\n                name: core-api\n          - name: REFRESH_TOKEN_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: REFRESH_TOKEN_EXPIRATION\n                name: core-api\n          - name: GOOGLE_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_ID\n                name: google-secret\n          - name: GOOGLE_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_SECRET\n                name: google-secret\n          - name: GOOGLE_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CALLBACK_URL\n                name: google-secret\n          - name: CLIENT_BASE_URL\n            valueFrom:\n              secretKeyRef:\n                key: CLIENT_BASE_URL\n                name: google-secret\n          - name: STRIPE_SECRET_KEY\n            valueFrom:\n              secretKeyRef:\n                key: STRIPE_SECRET_KEY\n                name: stripe-secret\n          - name: FACEBOOK_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_ID\n                name: facebook-secret\n          - name: FACEBOOK_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_SECRET\n                name: facebook-secret\n          - name: FACEBOOK_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CALLBACK_URL\n                name: facebook-secret\n          - name: SITE_LINK_LOGIN_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_LOGIN_URL\n                name: sitelink-secret\n          - name: SITE_LINK_UNITS_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_UNITS_URL\n                name: sitelink-secret\n          - name: SITE_LINK_USER\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_USER\n                name: sitelink-secret\n          - name: SITE_LINK_PASS\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_PASS\n                name: sitelink-secret\n          - name: SITE_LINK_CODE\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_CODE\n                name: sitelink-secret\n          - name: SITE_LINK_COMMISSION_PERCENT\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_COMMISSION_PERCENT\n                name: sitelink-secret\n          - name: SITE_LINK_SITES_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_SITES_URL\n                name: sitelink-secret\n          - name: BROWSER_URL\n            valueFrom:\n              secretKeyRef:\n                key: BROWSERLESS_URL\n                name: sitelink-secret\n          - name: GOGOX_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_ID\n          - name: GOGOX_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_SECRET\n          - name: GOGOX_API_URL\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_API_URL\n          - name: YOTPO_API_KEY\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_KEY\n          - name: YOTPO_API_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_SECRET\n          - name: SITE_LINK_USER_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_USER_JWD\n          - name: SITE_LINK_PASS_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_PASS_JWD\n          - name: SITE_LINK_CODE_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_CODE_JWD\n          - name: SITE_LINK_LOCATION_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_LOCATION_JWD\n          - name: RABBITMQ_URL\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: RABBITMQ_URL\n          - name: SND_EXCHANGE\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_EXCHANGE\n          - name: SND_UPDATE_ES_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_ES_KEY\n          - name: SND_UPDATE_STOCK_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_STOCK_KEY\n          - name: CLEVERTAP_ACCOUNT_ID\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_ID\n          - name: CLEVERTAP_ACCOUNT_PASSCODE\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_PASSCODE\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cron-cancel-booking\" does not have a read-only root file system"
  },
  {
    "id": "6044",
    "manifest_path": "data/manifests/the_stack_sample/sample_2168.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: prod-cron-cancel-booking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-cancel-booking\n          image: 414928843086.dkr.ecr.ap-southeast-1.amazonaws.com/core-api-v2:<VERSION>\n          imagePullPolicy: Always\n          args:\n          - run\n          - start-cron:cancel-booking:prod\n          env:\n          - name: NODE_ENV\n            value: production\n          - name: PORT\n            value: '3000'\n          - name: HOST\n            value: 0.0.0.0\n          - name: DB_DIALECT\n            valueFrom:\n              secretKeyRef:\n                key: DB_DIALECT\n                name: core-api\n          - name: DB_PORT\n            valueFrom:\n              secretKeyRef:\n                key: DB_PORT\n                name: core-api\n          - name: DB_HOST\n            valueFrom:\n              secretKeyRef:\n                key: DB_HOST\n                name: core-api\n          - name: DB_USERNAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_USERNAME\n                name: core-api\n          - name: DB_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                key: DB_PASSWORD\n                name: core-api\n          - name: DB_NAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_NAME\n                name: core-api\n          - name: JWT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: JWT_SECRET\n                name: core-api\n          - name: JWT_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: JWT_EXPIRATION\n                name: core-api\n          - name: DEFAULT_COMMISSION\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_COMMISSION\n                name: core-api\n          - name: TERMINATION_NOTICE_DAYS\n            valueFrom:\n              secretKeyRef:\n                key: TERMINATION_NOTICE_DAYS\n                name: core-api\n          - name: FILTER_BY_STOCK_URL\n            valueFrom:\n              secretKeyRef:\n                name: core-api\n                key: FILTER_BY_STOCK_URL\n          - name: TWILIO_ACCOUNT_SID\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_ACCOUNT_SID\n                name: twilio-secret\n          - name: TWILIO_AUTH_TOKEN\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_AUTH_TOKEN\n                name: twilio-secret\n          - name: TWILIO_SENDER_PHONE\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_SENDER_PHONE\n                name: twilio-secret\n          - name: AWS_S3_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_SECRET_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_REGION\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_REGION\n                name: aws-secret\n          - name: AWS_S3_BUCKET_NAME\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_NAME\n                name: aws-secret\n          - name: AWS_S3_BUCKET_URL\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_URL\n                name: aws-secret\n          - name: DEFAULT_RESIZE_WIDTH\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_RESIZE_WIDTH\n                name: aws-secret\n          - name: SUPPORT_MIME_TYPE\n            valueFrom:\n              secretKeyRef:\n                key: SUPPORT_MIME_TYPE\n                name: aws-secret\n          - name: SENDGRID_API_KEY\n            valueFrom:\n              secretKeyRef:\n                key: SENDGRID_API_KEY\n                name: core-api\n          - name: REFRESH_TOKEN_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: REFRESH_TOKEN_EXPIRATION\n                name: core-api\n          - name: GOOGLE_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_ID\n                name: google-secret\n          - name: GOOGLE_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_SECRET\n                name: google-secret\n          - name: GOOGLE_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CALLBACK_URL\n                name: google-secret\n          - name: CLIENT_BASE_URL\n            valueFrom:\n              secretKeyRef:\n                key: CLIENT_BASE_URL\n                name: google-secret\n          - name: STRIPE_SECRET_KEY\n            valueFrom:\n              secretKeyRef:\n                key: STRIPE_SECRET_KEY\n                name: stripe-secret\n          - name: FACEBOOK_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_ID\n                name: facebook-secret\n          - name: FACEBOOK_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_SECRET\n                name: facebook-secret\n          - name: FACEBOOK_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CALLBACK_URL\n                name: facebook-secret\n          - name: SITE_LINK_LOGIN_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_LOGIN_URL\n                name: sitelink-secret\n          - name: SITE_LINK_UNITS_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_UNITS_URL\n                name: sitelink-secret\n          - name: SITE_LINK_USER\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_USER\n                name: sitelink-secret\n          - name: SITE_LINK_PASS\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_PASS\n                name: sitelink-secret\n          - name: SITE_LINK_CODE\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_CODE\n                name: sitelink-secret\n          - name: SITE_LINK_COMMISSION_PERCENT\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_COMMISSION_PERCENT\n                name: sitelink-secret\n          - name: SITE_LINK_SITES_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_SITES_URL\n                name: sitelink-secret\n          - name: BROWSER_URL\n            valueFrom:\n              secretKeyRef:\n                key: BROWSERLESS_URL\n                name: sitelink-secret\n          - name: GOGOX_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_ID\n          - name: GOGOX_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_SECRET\n          - name: GOGOX_API_URL\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_API_URL\n          - name: YOTPO_API_KEY\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_KEY\n          - name: YOTPO_API_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_SECRET\n          - name: SITE_LINK_USER_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_USER_JWD\n          - name: SITE_LINK_PASS_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_PASS_JWD\n          - name: SITE_LINK_CODE_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_CODE_JWD\n          - name: SITE_LINK_LOCATION_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_LOCATION_JWD\n          - name: RABBITMQ_URL\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: RABBITMQ_URL\n          - name: SND_EXCHANGE\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_EXCHANGE\n          - name: SND_UPDATE_ES_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_ES_KEY\n          - name: SND_UPDATE_STOCK_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_STOCK_KEY\n          - name: CLEVERTAP_ACCOUNT_ID\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_ID\n          - name: CLEVERTAP_ACCOUNT_PASSCODE\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_PASSCODE\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cron-cancel-booking\" is not set to runAsNonRoot"
  },
  {
    "id": "6045",
    "manifest_path": "data/manifests/the_stack_sample/sample_2168.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: prod-cron-cancel-booking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-cancel-booking\n          image: 414928843086.dkr.ecr.ap-southeast-1.amazonaws.com/core-api-v2:<VERSION>\n          imagePullPolicy: Always\n          args:\n          - run\n          - start-cron:cancel-booking:prod\n          env:\n          - name: NODE_ENV\n            value: production\n          - name: PORT\n            value: '3000'\n          - name: HOST\n            value: 0.0.0.0\n          - name: DB_DIALECT\n            valueFrom:\n              secretKeyRef:\n                key: DB_DIALECT\n                name: core-api\n          - name: DB_PORT\n            valueFrom:\n              secretKeyRef:\n                key: DB_PORT\n                name: core-api\n          - name: DB_HOST\n            valueFrom:\n              secretKeyRef:\n                key: DB_HOST\n                name: core-api\n          - name: DB_USERNAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_USERNAME\n                name: core-api\n          - name: DB_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                key: DB_PASSWORD\n                name: core-api\n          - name: DB_NAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_NAME\n                name: core-api\n          - name: JWT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: JWT_SECRET\n                name: core-api\n          - name: JWT_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: JWT_EXPIRATION\n                name: core-api\n          - name: DEFAULT_COMMISSION\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_COMMISSION\n                name: core-api\n          - name: TERMINATION_NOTICE_DAYS\n            valueFrom:\n              secretKeyRef:\n                key: TERMINATION_NOTICE_DAYS\n                name: core-api\n          - name: FILTER_BY_STOCK_URL\n            valueFrom:\n              secretKeyRef:\n                name: core-api\n                key: FILTER_BY_STOCK_URL\n          - name: TWILIO_ACCOUNT_SID\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_ACCOUNT_SID\n                name: twilio-secret\n          - name: TWILIO_AUTH_TOKEN\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_AUTH_TOKEN\n                name: twilio-secret\n          - name: TWILIO_SENDER_PHONE\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_SENDER_PHONE\n                name: twilio-secret\n          - name: AWS_S3_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_SECRET_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_REGION\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_REGION\n                name: aws-secret\n          - name: AWS_S3_BUCKET_NAME\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_NAME\n                name: aws-secret\n          - name: AWS_S3_BUCKET_URL\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_URL\n                name: aws-secret\n          - name: DEFAULT_RESIZE_WIDTH\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_RESIZE_WIDTH\n                name: aws-secret\n          - name: SUPPORT_MIME_TYPE\n            valueFrom:\n              secretKeyRef:\n                key: SUPPORT_MIME_TYPE\n                name: aws-secret\n          - name: SENDGRID_API_KEY\n            valueFrom:\n              secretKeyRef:\n                key: SENDGRID_API_KEY\n                name: core-api\n          - name: REFRESH_TOKEN_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: REFRESH_TOKEN_EXPIRATION\n                name: core-api\n          - name: GOOGLE_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_ID\n                name: google-secret\n          - name: GOOGLE_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_SECRET\n                name: google-secret\n          - name: GOOGLE_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CALLBACK_URL\n                name: google-secret\n          - name: CLIENT_BASE_URL\n            valueFrom:\n              secretKeyRef:\n                key: CLIENT_BASE_URL\n                name: google-secret\n          - name: STRIPE_SECRET_KEY\n            valueFrom:\n              secretKeyRef:\n                key: STRIPE_SECRET_KEY\n                name: stripe-secret\n          - name: FACEBOOK_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_ID\n                name: facebook-secret\n          - name: FACEBOOK_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_SECRET\n                name: facebook-secret\n          - name: FACEBOOK_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CALLBACK_URL\n                name: facebook-secret\n          - name: SITE_LINK_LOGIN_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_LOGIN_URL\n                name: sitelink-secret\n          - name: SITE_LINK_UNITS_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_UNITS_URL\n                name: sitelink-secret\n          - name: SITE_LINK_USER\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_USER\n                name: sitelink-secret\n          - name: SITE_LINK_PASS\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_PASS\n                name: sitelink-secret\n          - name: SITE_LINK_CODE\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_CODE\n                name: sitelink-secret\n          - name: SITE_LINK_COMMISSION_PERCENT\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_COMMISSION_PERCENT\n                name: sitelink-secret\n          - name: SITE_LINK_SITES_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_SITES_URL\n                name: sitelink-secret\n          - name: BROWSER_URL\n            valueFrom:\n              secretKeyRef:\n                key: BROWSERLESS_URL\n                name: sitelink-secret\n          - name: GOGOX_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_ID\n          - name: GOGOX_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_SECRET\n          - name: GOGOX_API_URL\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_API_URL\n          - name: YOTPO_API_KEY\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_KEY\n          - name: YOTPO_API_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_SECRET\n          - name: SITE_LINK_USER_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_USER_JWD\n          - name: SITE_LINK_PASS_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_PASS_JWD\n          - name: SITE_LINK_CODE_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_CODE_JWD\n          - name: SITE_LINK_LOCATION_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_LOCATION_JWD\n          - name: RABBITMQ_URL\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: RABBITMQ_URL\n          - name: SND_EXCHANGE\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_EXCHANGE\n          - name: SND_UPDATE_ES_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_ES_KEY\n          - name: SND_UPDATE_STOCK_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_STOCK_KEY\n          - name: CLEVERTAP_ACCOUNT_ID\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_ID\n          - name: CLEVERTAP_ACCOUNT_PASSCODE\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_PASSCODE\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cron-cancel-booking\" has cpu request 0"
  },
  {
    "id": "6046",
    "manifest_path": "data/manifests/the_stack_sample/sample_2168.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: prod-cron-cancel-booking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-cancel-booking\n          image: 414928843086.dkr.ecr.ap-southeast-1.amazonaws.com/core-api-v2:<VERSION>\n          imagePullPolicy: Always\n          args:\n          - run\n          - start-cron:cancel-booking:prod\n          env:\n          - name: NODE_ENV\n            value: production\n          - name: PORT\n            value: '3000'\n          - name: HOST\n            value: 0.0.0.0\n          - name: DB_DIALECT\n            valueFrom:\n              secretKeyRef:\n                key: DB_DIALECT\n                name: core-api\n          - name: DB_PORT\n            valueFrom:\n              secretKeyRef:\n                key: DB_PORT\n                name: core-api\n          - name: DB_HOST\n            valueFrom:\n              secretKeyRef:\n                key: DB_HOST\n                name: core-api\n          - name: DB_USERNAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_USERNAME\n                name: core-api\n          - name: DB_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                key: DB_PASSWORD\n                name: core-api\n          - name: DB_NAME\n            valueFrom:\n              secretKeyRef:\n                key: DB_NAME\n                name: core-api\n          - name: JWT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: JWT_SECRET\n                name: core-api\n          - name: JWT_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: JWT_EXPIRATION\n                name: core-api\n          - name: DEFAULT_COMMISSION\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_COMMISSION\n                name: core-api\n          - name: TERMINATION_NOTICE_DAYS\n            valueFrom:\n              secretKeyRef:\n                key: TERMINATION_NOTICE_DAYS\n                name: core-api\n          - name: FILTER_BY_STOCK_URL\n            valueFrom:\n              secretKeyRef:\n                name: core-api\n                key: FILTER_BY_STOCK_URL\n          - name: TWILIO_ACCOUNT_SID\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_ACCOUNT_SID\n                name: twilio-secret\n          - name: TWILIO_AUTH_TOKEN\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_AUTH_TOKEN\n                name: twilio-secret\n          - name: TWILIO_SENDER_PHONE\n            valueFrom:\n              secretKeyRef:\n                key: TWILIO_SENDER_PHONE\n                name: twilio-secret\n          - name: AWS_S3_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_SECRET_ACCESS_KEY\n                name: aws-secret\n          - name: AWS_S3_REGION\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_REGION\n                name: aws-secret\n          - name: AWS_S3_BUCKET_NAME\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_NAME\n                name: aws-secret\n          - name: AWS_S3_BUCKET_URL\n            valueFrom:\n              secretKeyRef:\n                key: AWS_S3_BUCKET_URL\n                name: aws-secret\n          - name: DEFAULT_RESIZE_WIDTH\n            valueFrom:\n              secretKeyRef:\n                key: DEFAULT_RESIZE_WIDTH\n                name: aws-secret\n          - name: SUPPORT_MIME_TYPE\n            valueFrom:\n              secretKeyRef:\n                key: SUPPORT_MIME_TYPE\n                name: aws-secret\n          - name: SENDGRID_API_KEY\n            valueFrom:\n              secretKeyRef:\n                key: SENDGRID_API_KEY\n                name: core-api\n          - name: REFRESH_TOKEN_EXPIRATION\n            valueFrom:\n              secretKeyRef:\n                key: REFRESH_TOKEN_EXPIRATION\n                name: core-api\n          - name: GOOGLE_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_ID\n                name: google-secret\n          - name: GOOGLE_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CLIENT_SECRET\n                name: google-secret\n          - name: GOOGLE_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: GOOGLE_CALLBACK_URL\n                name: google-secret\n          - name: CLIENT_BASE_URL\n            valueFrom:\n              secretKeyRef:\n                key: CLIENT_BASE_URL\n                name: google-secret\n          - name: STRIPE_SECRET_KEY\n            valueFrom:\n              secretKeyRef:\n                key: STRIPE_SECRET_KEY\n                name: stripe-secret\n          - name: FACEBOOK_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_ID\n                name: facebook-secret\n          - name: FACEBOOK_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CLIENT_SECRET\n                name: facebook-secret\n          - name: FACEBOOK_CALLBACK_URL\n            valueFrom:\n              secretKeyRef:\n                key: FACEBOOK_CALLBACK_URL\n                name: facebook-secret\n          - name: SITE_LINK_LOGIN_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_LOGIN_URL\n                name: sitelink-secret\n          - name: SITE_LINK_UNITS_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_UNITS_URL\n                name: sitelink-secret\n          - name: SITE_LINK_USER\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_USER\n                name: sitelink-secret\n          - name: SITE_LINK_PASS\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_PASS\n                name: sitelink-secret\n          - name: SITE_LINK_CODE\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_CODE\n                name: sitelink-secret\n          - name: SITE_LINK_COMMISSION_PERCENT\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_COMMISSION_PERCENT\n                name: sitelink-secret\n          - name: SITE_LINK_SITES_URL\n            valueFrom:\n              secretKeyRef:\n                key: SITE_LINK_SITES_URL\n                name: sitelink-secret\n          - name: BROWSER_URL\n            valueFrom:\n              secretKeyRef:\n                key: BROWSERLESS_URL\n                name: sitelink-secret\n          - name: GOGOX_CLIENT_ID\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_ID\n          - name: GOGOX_CLIENT_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_CLIENT_SECRET\n          - name: GOGOX_API_URL\n            valueFrom:\n              secretKeyRef:\n                name: gogox-secret\n                key: GOGOX_API_URL\n          - name: YOTPO_API_KEY\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_KEY\n          - name: YOTPO_API_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: yotpo-secret\n                key: YOTPO_API_SECRET\n          - name: SITE_LINK_USER_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_USER_JWD\n          - name: SITE_LINK_PASS_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_PASS_JWD\n          - name: SITE_LINK_CODE_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_CODE_JWD\n          - name: SITE_LINK_LOCATION_JWD\n            valueFrom:\n              secretKeyRef:\n                name: sitelink-secret\n                key: SITE_LINK_LOCATION_JWD\n          - name: RABBITMQ_URL\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: RABBITMQ_URL\n          - name: SND_EXCHANGE\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_EXCHANGE\n          - name: SND_UPDATE_ES_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_ES_KEY\n          - name: SND_UPDATE_STOCK_KEY\n            valueFrom:\n              secretKeyRef:\n                name: site-stock-service\n                key: SND_UPDATE_STOCK_KEY\n          - name: CLEVERTAP_ACCOUNT_ID\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_ID\n          - name: CLEVERTAP_ACCOUNT_PASSCODE\n            valueFrom:\n              secretKeyRef:\n                name: clevertap-secret\n                key: CLEVERTAP_ACCOUNT_PASSCODE\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cron-cancel-booking\" has memory limit 0"
  },
  {
    "id": "6047",
    "manifest_path": "data/manifests/the_stack_sample/sample_2169.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9520\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6048",
    "manifest_path": "data/manifests/the_stack_sample/sample_2169.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9520\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6049",
    "manifest_path": "data/manifests/the_stack_sample/sample_2169.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9520\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6050",
    "manifest_path": "data/manifests/the_stack_sample/sample_2169.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9520\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6051",
    "manifest_path": "data/manifests/the_stack_sample/sample_2169.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9520\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6052",
    "manifest_path": "data/manifests/the_stack_sample/sample_2170.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accountapi-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: accountapi\n  template:\n    metadata:\n      labels:\n        app: accountapi\n    spec:\n      containers:\n      - name: accountapi-container\n        image: gcr.io/staffjoy-prod/accountapi:VERSION\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        env:\n        - name: DEPLOY\n          value: VERSION\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: dsn\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6053",
    "manifest_path": "data/manifests/the_stack_sample/sample_2170.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accountapi-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: accountapi\n  template:\n    metadata:\n      labels:\n        app: accountapi\n    spec:\n      containers:\n      - name: accountapi-container\n        image: gcr.io/staffjoy-prod/accountapi:VERSION\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        env:\n        - name: DEPLOY\n          value: VERSION\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: dsn\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"accountapi-container\" does not have a read-only root file system"
  },
  {
    "id": "6054",
    "manifest_path": "data/manifests/the_stack_sample/sample_2170.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accountapi-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: accountapi\n  template:\n    metadata:\n      labels:\n        app: accountapi\n    spec:\n      containers:\n      - name: accountapi-container\n        image: gcr.io/staffjoy-prod/accountapi:VERSION\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        env:\n        - name: DEPLOY\n          value: VERSION\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: dsn\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"accountapi-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6055",
    "manifest_path": "data/manifests/the_stack_sample/sample_2170.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accountapi-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: accountapi\n  template:\n    metadata:\n      labels:\n        app: accountapi\n    spec:\n      containers:\n      - name: accountapi-container\n        image: gcr.io/staffjoy-prod/accountapi:VERSION\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        env:\n        - name: DEPLOY\n          value: VERSION\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: dsn\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"accountapi-container\" has cpu request 0"
  },
  {
    "id": "6056",
    "manifest_path": "data/manifests/the_stack_sample/sample_2170.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accountapi-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: accountapi\n  template:\n    metadata:\n      labels:\n        app: accountapi\n    spec:\n      containers:\n      - name: accountapi-container\n        image: gcr.io/staffjoy-prod/accountapi:VERSION\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        env:\n        - name: DEPLOY\n          value: VERSION\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: sentry\n              key: dsn\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"accountapi-container\" has memory limit 0"
  },
  {
    "id": "6057",
    "manifest_path": "data/manifests/the_stack_sample/sample_2171.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  namespace: kube-system\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: 'true'\n    spec:\n      containers:\n      - name: smarter-device-manager\n        image: registry.gitlab.com/arm-research/smarter/smarter-device-manager:v1.1.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          mountPath: /dev\n        - name: sys-dir\n          mountPath: /sys\n        - name: config\n          mountPath: /root/config\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n      - name: dev-dir\n        hostPath:\n          path: /dev\n      - name: sys-dir\n        hostPath:\n          path: /sys\n      - name: config\n        configMap:\n          name: smarter-device-manager\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "6058",
    "manifest_path": "data/manifests/the_stack_sample/sample_2171.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  namespace: kube-system\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: 'true'\n    spec:\n      containers:\n      - name: smarter-device-manager\n        image: registry.gitlab.com/arm-research/smarter/smarter-device-manager:v1.1.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          mountPath: /dev\n        - name: sys-dir\n          mountPath: /sys\n        - name: config\n          mountPath: /root/config\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n      - name: dev-dir\n        hostPath:\n          path: /dev\n      - name: sys-dir\n        hostPath:\n          path: /sys\n      - name: config\n        configMap:\n          name: smarter-device-manager\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"smarter-device-manager\" does not have a read-only root file system"
  },
  {
    "id": "6059",
    "manifest_path": "data/manifests/the_stack_sample/sample_2171.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  namespace: kube-system\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: 'true'\n    spec:\n      containers:\n      - name: smarter-device-manager\n        image: registry.gitlab.com/arm-research/smarter/smarter-device-manager:v1.1.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          mountPath: /dev\n        - name: sys-dir\n          mountPath: /sys\n        - name: config\n          mountPath: /root/config\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n      - name: dev-dir\n        hostPath:\n          path: /dev\n      - name: sys-dir\n        hostPath:\n          path: /sys\n      - name: config\n        configMap:\n          name: smarter-device-manager\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"smarter-device-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "6060",
    "manifest_path": "data/manifests/the_stack_sample/sample_2171.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  namespace: kube-system\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: 'true'\n    spec:\n      containers:\n      - name: smarter-device-manager\n        image: registry.gitlab.com/arm-research/smarter/smarter-device-manager:v1.1.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          mountPath: /dev\n        - name: sys-dir\n          mountPath: /sys\n        - name: config\n          mountPath: /root/config\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n      - name: dev-dir\n        hostPath:\n          path: /dev\n      - name: sys-dir\n        hostPath:\n          path: /sys\n      - name: config\n        configMap:\n          name: smarter-device-manager\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"smarter-device-manager\""
  },
  {
    "id": "6061",
    "manifest_path": "data/manifests/the_stack_sample/sample_2171.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  namespace: kube-system\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: 'true'\n    spec:\n      containers:\n      - name: smarter-device-manager\n        image: registry.gitlab.com/arm-research/smarter/smarter-device-manager:v1.1.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          mountPath: /dev\n        - name: sys-dir\n          mountPath: /sys\n        - name: config\n          mountPath: /root/config\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n      - name: dev-dir\n        hostPath:\n          path: /dev\n      - name: sys-dir\n        hostPath:\n          path: /sys\n      - name: config\n        configMap:\n          name: smarter-device-manager\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"smarter-device-manager\""
  },
  {
    "id": "6062",
    "manifest_path": "data/manifests/the_stack_sample/sample_2172.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: siegetest\nspec:\n  replicas: 1\n  selector:\n    name: siegetest\n    version: 1.0.0\n  template:\n    metadata:\n      labels:\n        name: siegetest\n        version: 1.0.0\n    spec:\n      containers:\n      - name: siegetest\n        image: ipedrazas/siegetest:1.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"siegetest\" does not have a read-only root file system"
  },
  {
    "id": "6063",
    "manifest_path": "data/manifests/the_stack_sample/sample_2172.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: siegetest\nspec:\n  replicas: 1\n  selector:\n    name: siegetest\n    version: 1.0.0\n  template:\n    metadata:\n      labels:\n        name: siegetest\n        version: 1.0.0\n    spec:\n      containers:\n      - name: siegetest\n        image: ipedrazas/siegetest:1.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"siegetest\" is not set to runAsNonRoot"
  },
  {
    "id": "6064",
    "manifest_path": "data/manifests/the_stack_sample/sample_2172.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: siegetest\nspec:\n  replicas: 1\n  selector:\n    name: siegetest\n    version: 1.0.0\n  template:\n    metadata:\n      labels:\n        name: siegetest\n        version: 1.0.0\n    spec:\n      containers:\n      - name: siegetest\n        image: ipedrazas/siegetest:1.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"siegetest\" has cpu request 0"
  },
  {
    "id": "6065",
    "manifest_path": "data/manifests/the_stack_sample/sample_2172.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: siegetest\nspec:\n  replicas: 1\n  selector:\n    name: siegetest\n    version: 1.0.0\n  template:\n    metadata:\n      labels:\n        name: siegetest\n        version: 1.0.0\n    spec:\n      containers:\n      - name: siegetest\n        image: ipedrazas/siegetest:1.0.0\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"siegetest\" has memory limit 0"
  },
  {
    "id": "6066",
    "manifest_path": "data/manifests/the_stack_sample/sample_2177.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: acid-minimal-cluster\n  labels:\n    application: spilo\n    version: acid-minimal-cluster\n    spilo-role: master\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: minimal-cluster.acid.staging.db.example.com\n    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '3600'\nspec:\n  type: ClusterIP\n  selector:\n    application: spilo\n    version: acid-minimal-cluster\n  ports:\n  - name: postgres\n    port: 5432\n    protocol: TCP\n    targetPort: 5432\n  - name: tcp-1\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  - name: tcp-2\n    port: 8008\n    protocol: TCP\n    targetPort: 8008\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[application:spilo version:acid-minimal-cluster])"
  },
  {
    "id": "6067",
    "manifest_path": "data/manifests/the_stack_sample/sample_2179.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: jhipstersampleapplication\n  namespace: default\n  labels:\n    app: jhipstersampleapplication\nspec:\n  selector:\n    app: jhipstersampleapplication\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:jhipstersampleapplication])"
  },
  {
    "id": "6068",
    "manifest_path": "data/manifests/the_stack_sample/sample_2180.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-batch\nspec:\n  containers:\n  - name: test-batch\n    image: '{{ test_batch_image.image }}'\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: HAIL_TOKEN_FILE\n      value: /test-jwt/jwt\n    - name: BATCH_URL\n      value: http://batch.{{ default_ns.name }}\n    volumeMounts:\n    - mountPath: /test-jwt\n      readOnly: true\n      name: test-jwt\n  volumes:\n  - name: test-jwt\n    secret:\n      secretName: test-jwt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"test-batch\" is using an invalid container image, \"{{ test_batch_image.image }}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6069",
    "manifest_path": "data/manifests/the_stack_sample/sample_2180.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-batch\nspec:\n  containers:\n  - name: test-batch\n    image: '{{ test_batch_image.image }}'\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: HAIL_TOKEN_FILE\n      value: /test-jwt/jwt\n    - name: BATCH_URL\n      value: http://batch.{{ default_ns.name }}\n    volumeMounts:\n    - mountPath: /test-jwt\n      readOnly: true\n      name: test-jwt\n  volumes:\n  - name: test-jwt\n    secret:\n      secretName: test-jwt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-batch\" does not have a read-only root file system"
  },
  {
    "id": "6070",
    "manifest_path": "data/manifests/the_stack_sample/sample_2180.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-batch\nspec:\n  containers:\n  - name: test-batch\n    image: '{{ test_batch_image.image }}'\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: HAIL_TOKEN_FILE\n      value: /test-jwt/jwt\n    - name: BATCH_URL\n      value: http://batch.{{ default_ns.name }}\n    volumeMounts:\n    - mountPath: /test-jwt\n      readOnly: true\n      name: test-jwt\n  volumes:\n  - name: test-jwt\n    secret:\n      secretName: test-jwt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-batch\" is not set to runAsNonRoot"
  },
  {
    "id": "6071",
    "manifest_path": "data/manifests/the_stack_sample/sample_2180.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-batch\nspec:\n  containers:\n  - name: test-batch\n    image: '{{ test_batch_image.image }}'\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: HAIL_TOKEN_FILE\n      value: /test-jwt/jwt\n    - name: BATCH_URL\n      value: http://batch.{{ default_ns.name }}\n    volumeMounts:\n    - mountPath: /test-jwt\n      readOnly: true\n      name: test-jwt\n  volumes:\n  - name: test-jwt\n    secret:\n      secretName: test-jwt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-batch\" has cpu request 0"
  },
  {
    "id": "6072",
    "manifest_path": "data/manifests/the_stack_sample/sample_2180.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-batch\nspec:\n  containers:\n  - name: test-batch\n    image: '{{ test_batch_image.image }}'\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: HAIL_TOKEN_FILE\n      value: /test-jwt/jwt\n    - name: BATCH_URL\n      value: http://batch.{{ default_ns.name }}\n    volumeMounts:\n    - mountPath: /test-jwt\n      readOnly: true\n      name: test-jwt\n  volumes:\n  - name: test-jwt\n    secret:\n      secretName: test-jwt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-batch\" has memory limit 0"
  },
  {
    "id": "6073",
    "manifest_path": "data/manifests/the_stack_sample/sample_2183.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis\n  labels:\n    app: demo-voting-app\nspec:\n  type: ClusterIP\n  ports:\n  - targetPort: 6379\n    port: 6379\n  selector:\n    app: demo-voting-app\n    name: redis-db\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:demo-voting-app name:redis-db])"
  },
  {
    "id": "6074",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "6075",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "6076",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "6077",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "6078",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "6079",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "6080",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "6081",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "6082",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "6083",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "6084",
    "manifest_path": "data/manifests/the_stack_sample/sample_2184.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-keras-api-connection-v3-8\n    frameworkVersion: tf-nightly\n    mode: connection\n    model: keras-api\n  name: tf-nightly-keras-api-connection-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails  -i aaa_connection\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/keras-api/connection/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-nightly-keras-api-connection-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "6085",
    "manifest_path": "data/manifests/the_stack_sample/sample_2185.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lockvalidation-dpl\n  namespace: kube-lock\n  labels:\n    app: lockvalidation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lockvalidation\n  template:\n    metadata:\n      labels:\n        app: lockvalidation\n    spec:\n      serviceAccountName: lockvalidation-sa\n      containers:\n      - name: lockvalidation\n        image: pkotas/lockvalidation:devel\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /etc/lockvalidation/cert\n          readOnly: true\n      volumes:\n      - name: webhook-certs\n        secret:\n          secretName: lockvalidation-crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lockvalidation\" does not have a read-only root file system"
  },
  {
    "id": "6086",
    "manifest_path": "data/manifests/the_stack_sample/sample_2185.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lockvalidation-dpl\n  namespace: kube-lock\n  labels:\n    app: lockvalidation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lockvalidation\n  template:\n    metadata:\n      labels:\n        app: lockvalidation\n    spec:\n      serviceAccountName: lockvalidation-sa\n      containers:\n      - name: lockvalidation\n        image: pkotas/lockvalidation:devel\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /etc/lockvalidation/cert\n          readOnly: true\n      volumes:\n      - name: webhook-certs\n        secret:\n          secretName: lockvalidation-crt\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lockvalidation-sa\" not found"
  },
  {
    "id": "6087",
    "manifest_path": "data/manifests/the_stack_sample/sample_2185.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lockvalidation-dpl\n  namespace: kube-lock\n  labels:\n    app: lockvalidation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lockvalidation\n  template:\n    metadata:\n      labels:\n        app: lockvalidation\n    spec:\n      serviceAccountName: lockvalidation-sa\n      containers:\n      - name: lockvalidation\n        image: pkotas/lockvalidation:devel\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /etc/lockvalidation/cert\n          readOnly: true\n      volumes:\n      - name: webhook-certs\n        secret:\n          secretName: lockvalidation-crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lockvalidation\" is not set to runAsNonRoot"
  },
  {
    "id": "6088",
    "manifest_path": "data/manifests/the_stack_sample/sample_2185.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lockvalidation-dpl\n  namespace: kube-lock\n  labels:\n    app: lockvalidation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lockvalidation\n  template:\n    metadata:\n      labels:\n        app: lockvalidation\n    spec:\n      serviceAccountName: lockvalidation-sa\n      containers:\n      - name: lockvalidation\n        image: pkotas/lockvalidation:devel\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /etc/lockvalidation/cert\n          readOnly: true\n      volumes:\n      - name: webhook-certs\n        secret:\n          secretName: lockvalidation-crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"lockvalidation\" has cpu request 0"
  },
  {
    "id": "6089",
    "manifest_path": "data/manifests/the_stack_sample/sample_2185.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lockvalidation-dpl\n  namespace: kube-lock\n  labels:\n    app: lockvalidation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lockvalidation\n  template:\n    metadata:\n      labels:\n        app: lockvalidation\n    spec:\n      serviceAccountName: lockvalidation-sa\n      containers:\n      - name: lockvalidation\n        image: pkotas/lockvalidation:devel\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /etc/lockvalidation/cert\n          readOnly: true\n      volumes:\n      - name: webhook-certs\n        secret:\n          secretName: lockvalidation-crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"lockvalidation\" has memory limit 0"
  },
  {
    "id": "6090",
    "manifest_path": "data/manifests/the_stack_sample/sample_2188.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.3.2\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.3.2\n        app.kubernetes.io/version: latest\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: gcr.io/jenkinsxio/jx-pipelines-visualizer:1.3.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.18.134.92.246.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /home/jenkins\n        - name: GIT_SECRET_MOUNT_PATH\n          value: /secrets/git\n        - name: AWS_REGION\n          value: eu-west-2\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable GIT_SECRET_MOUNT_PATH in container \"jx-pipelines-visualizer\" found"
  },
  {
    "id": "6091",
    "manifest_path": "data/manifests/the_stack_sample/sample_2188.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.3.2\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.3.2\n        app.kubernetes.io/version: latest\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: gcr.io/jenkinsxio/jx-pipelines-visualizer:1.3.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.18.134.92.246.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /home/jenkins\n        - name: GIT_SECRET_MOUNT_PATH\n          value: /secrets/git\n        - name: AWS_REGION\n          value: eu-west-2\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jx-pipelines-visualizer\" does not have a read-only root file system"
  },
  {
    "id": "6092",
    "manifest_path": "data/manifests/the_stack_sample/sample_2188.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.3.2\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.3.2\n        app.kubernetes.io/version: latest\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: gcr.io/jenkinsxio/jx-pipelines-visualizer:1.3.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.18.134.92.246.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /home/jenkins\n        - name: GIT_SECRET_MOUNT_PATH\n          value: /secrets/git\n        - name: AWS_REGION\n          value: eu-west-2\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jx-pipelines-visualizer\" not found"
  },
  {
    "id": "6093",
    "manifest_path": "data/manifests/the_stack_sample/sample_2188.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.3.2\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.3.2\n        app.kubernetes.io/version: latest\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: gcr.io/jenkinsxio/jx-pipelines-visualizer:1.3.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - s3://logs-jenkinsx-20210421162444587500000003/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.18.134.92.246.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /home/jenkins\n        - name: GIT_SECRET_MOUNT_PATH\n          value: /secrets/git\n        - name: AWS_REGION\n          value: eu-west-2\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jx-pipelines-visualizer\" is not set to runAsNonRoot"
  },
  {
    "id": "6094",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "6095",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6096",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"azurefile\" does not have a read-only root file system"
  },
  {
    "id": "6097",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "6098",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "6099",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-resizer\" does not have a read-only root file system"
  },
  {
    "id": "6100",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-snapshotter\" does not have a read-only root file system"
  },
  {
    "id": "6101",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "6102",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"csi-azurefile-controller-sa\" not found"
  },
  {
    "id": "6103",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"azurefile\" is not set to runAsNonRoot"
  },
  {
    "id": "6104",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "6105",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "6106",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-resizer\" is not set to runAsNonRoot"
  },
  {
    "id": "6107",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-snapshotter\" is not set to runAsNonRoot"
  },
  {
    "id": "6108",
    "manifest_path": "data/manifests/the_stack_sample/sample_2192.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azurefile-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azurefile-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azurefile-controller\n    spec:\n      serviceAccountName: csi-azurefile-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v2.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --timeout=300s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v2.2.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=120s\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v3.0.3\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.1.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.2.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29612\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azurefile\n        image: mcr.microsoft.com/k8s/csi/azurefile-csi:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29614\n        ports:\n        - containerPort: 29612\n          name: healthz\n          protocol: TCP\n        - containerPort: 29614\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        - mountPath: /var/lib/waagent/ManagedIdentity-Settings\n          readOnly: true\n          name: msi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: Directory\n      - name: msi\n        hostPath:\n          path: /var/lib/waagent/ManagedIdentity-Settings\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "6109",
    "manifest_path": "data/manifests/the_stack_sample/sample_2194.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    control-plane: controller-manager\n  name: controller-manager-metrics-service\n  namespace: system\nspec:\n  ports:\n  - name: https\n    port: 8443\n    targetPort: https\n  selector:\n    control-plane: controller-manager\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[control-plane:controller-manager])"
  },
  {
    "id": "6110",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6111",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"installation-complete\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6112",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api-registry-delete\" does not have a read-only root file system"
  },
  {
    "id": "6113",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"events-broker-delete\" does not have a read-only root file system"
  },
  {
    "id": "6114",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fn-delete\" does not have a read-only root file system"
  },
  {
    "id": "6115",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"installation-complete\" does not have a read-only root file system"
  },
  {
    "id": "6116",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"istio-delete\" does not have a read-only root file system"
  },
  {
    "id": "6117",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kafka-delete\" does not have a read-only root file system"
  },
  {
    "id": "6118",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"observability-tools-delete\" does not have a read-only root file system"
  },
  {
    "id": "6119",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"aura-job\" not found"
  },
  {
    "id": "6120",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"api-registry-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "6121",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"events-broker-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "6122",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fn-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "6123",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"installation-complete\" is not set to runAsNonRoot"
  },
  {
    "id": "6124",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"istio-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "6125",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kafka-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "6126",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"observability-tools-delete\" is not set to runAsNonRoot"
  },
  {
    "id": "6127",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api-registry-delete\" has cpu request 0"
  },
  {
    "id": "6128",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"events-broker-delete\" has cpu request 0"
  },
  {
    "id": "6129",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fn-delete\" has cpu request 0"
  },
  {
    "id": "6130",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"installation-complete\" has cpu request 0"
  },
  {
    "id": "6131",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"istio-delete\" has cpu request 0"
  },
  {
    "id": "6132",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kafka-delete\" has cpu request 0"
  },
  {
    "id": "6133",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"observability-tools-delete\" has cpu request 0"
  },
  {
    "id": "6134",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api-registry-delete\" has memory limit 0"
  },
  {
    "id": "6135",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"events-broker-delete\" has memory limit 0"
  },
  {
    "id": "6136",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fn-delete\" has memory limit 0"
  },
  {
    "id": "6137",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"installation-complete\" has memory limit 0"
  },
  {
    "id": "6138",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"istio-delete\" has memory limit 0"
  },
  {
    "id": "6139",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kafka-delete\" has memory limit 0"
  },
  {
    "id": "6140",
    "manifest_path": "data/manifests/the_stack_sample/sample_2195.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: uninstall-aura-full\n  namespace: aura\nspec:\n  template:\n    spec:\n      serviceAccountName: aura-job\n      initContainers:\n      - name: events-broker-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/events-broker-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: fn-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/fn-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: kafka-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/kafka-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: observability-tools-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/observability-tools-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: api-registry-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/api-registry-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-chart\n        imagePullPolicy: IfNotPresent\n      - name: istio-delete\n        image: phx.ocir.io/oraclepaasmicroservices/aura/istio-install:0.2.1-SNAPSHOT\n        command:\n        - sh\n        - -c\n        - delete-istio\n        imagePullPolicy: IfNotPresent\n      containers:\n      - name: installation-complete\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo \"Helm, Istio, Aura, Kafka, Fn and Events Broker Uninstalled\"\n        imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"observability-tools-delete\" has memory limit 0"
  },
  {
    "id": "6141",
    "manifest_path": "data/manifests/the_stack_sample/sample_2196.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://Kubernetes.kubernetes:6443?inClusterConfig=true&insecure=true\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "6142",
    "manifest_path": "data/manifests/the_stack_sample/sample_2196.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://Kubernetes.kubernetes:6443?inClusterConfig=true&insecure=true\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "6143",
    "manifest_path": "data/manifests/the_stack_sample/sample_2196.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://Kubernetes.kubernetes:6443?inClusterConfig=true&insecure=true\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"heapster\" has cpu request 0"
  },
  {
    "id": "6144",
    "manifest_path": "data/manifests/the_stack_sample/sample_2196.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://Kubernetes.kubernetes:6443?inClusterConfig=true&insecure=true\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"heapster\" has memory limit 0"
  },
  {
    "id": "6145",
    "manifest_path": "data/manifests/the_stack_sample/sample_2197.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: metallb-hcloud-controller\n  labels:\n    app: metallb-hcloud-controller\nspec:\n  ports:\n  - port: 2112\n    protocol: TCP\n    name: metrics-http\n  selector:\n    app: metallb-hcloud-controller\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:metallb-hcloud-controller])"
  },
  {
    "id": "6146",
    "manifest_path": "data/manifests/the_stack_sample/sample_2198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6147",
    "manifest_path": "data/manifests/the_stack_sample/sample_2198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6148",
    "manifest_path": "data/manifests/the_stack_sample/sample_2198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6149",
    "manifest_path": "data/manifests/the_stack_sample/sample_2198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6150",
    "manifest_path": "data/manifests/the_stack_sample/sample_2198.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6151",
    "manifest_path": "data/manifests/the_stack_sample/sample_2199.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      name: nginx\n    spec:\n      containers:\n      - image: nginx:1.15.3\n        name: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "mismatching-selector",
    "violation_text": "labels in pod spec (map[]) do not match labels in selector (&LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[]LabelSelectorRequirement{},})"
  },
  {
    "id": "6152",
    "manifest_path": "data/manifests/the_stack_sample/sample_2199.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      name: nginx\n    spec:\n      containers:\n      - image: nginx:1.15.3\n        name: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6153",
    "manifest_path": "data/manifests/the_stack_sample/sample_2199.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      name: nginx\n    spec:\n      containers:\n      - image: nginx:1.15.3\n        name: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6154",
    "manifest_path": "data/manifests/the_stack_sample/sample_2199.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      name: nginx\n    spec:\n      containers:\n      - image: nginx:1.15.3\n        name: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6155",
    "manifest_path": "data/manifests/the_stack_sample/sample_2199.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      name: nginx\n    spec:\n      containers:\n      - image: nginx:1.15.3\n        name: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6156",
    "manifest_path": "data/manifests/the_stack_sample/sample_2199.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: nginx\n  template:\n    metadata:\n      name: nginx\n    spec:\n      containers:\n      - image: nginx:1.15.3\n        name: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6157",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6158",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"etcd-job\" is using an invalid container image, \"tenstartups/etcdctl\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6159",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"etcd-job\" does not have a read-only root file system"
  },
  {
    "id": "6160",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"etcd-job\" is not set to runAsNonRoot"
  },
  {
    "id": "6161",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"etcd-job\" has cpu request 0"
  },
  {
    "id": "6162",
    "manifest_path": "data/manifests/the_stack_sample/sample_2201.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etcd-job\nspec:\n  template:\n    metadata:\n      name: etcd-job\n    spec:\n      containers:\n      - name: etcd-job\n        image: tenstartups/etcdctl\n        env:\n        - name: ETCDCTL_ENDPOINT\n          value: http://example-etcd-cluster-client-service:2379\n        command:\n        - etcdctl\n        - mkdir\n        - pod-list\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"etcd-job\" has memory limit 0"
  },
  {
    "id": "6163",
    "manifest_path": "data/manifests/the_stack_sample/sample_2203.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: puzyrevyaroslav/hipster_frontend:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6164",
    "manifest_path": "data/manifests/the_stack_sample/sample_2203.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: puzyrevyaroslav/hipster_frontend:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"paymentservice\" does not have a read-only root file system"
  },
  {
    "id": "6165",
    "manifest_path": "data/manifests/the_stack_sample/sample_2203.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: puzyrevyaroslav/hipster_frontend:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"paymentservice\" is not set to runAsNonRoot"
  },
  {
    "id": "6166",
    "manifest_path": "data/manifests/the_stack_sample/sample_2203.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: puzyrevyaroslav/hipster_frontend:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"paymentservice\" has cpu request 0"
  },
  {
    "id": "6167",
    "manifest_path": "data/manifests/the_stack_sample/sample_2203.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: paymentservice\n        image: puzyrevyaroslav/hipster_frontend:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"paymentservice\" has memory limit 0"
  },
  {
    "id": "6168",
    "manifest_path": "data/manifests/the_stack_sample/sample_2204.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-service\n  labels:\n    app: kafka\nspec:\n  ports:\n  - port: 9092\n    name: kafka-port\n    targetPort: 9092\n    nodePort: 30092\n    protocol: TCP\n  selector:\n    app: kafka\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kafka])"
  },
  {
    "id": "6169",
    "manifest_path": "data/manifests/the_stack_sample/sample_2205.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ruler\n  labels:\n    app: ruler\nspec:\n  ports:\n  - port: 80\n    name: http-metrics\n  selector:\n    app: ruler\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ruler])"
  },
  {
    "id": "6170",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6171",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clone-mysql\" does not have a read-only root file system"
  },
  {
    "id": "6172",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-mysql\" does not have a read-only root file system"
  },
  {
    "id": "6173",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql\" does not have a read-only root file system"
  },
  {
    "id": "6174",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"xtrabackup\" does not have a read-only root file system"
  },
  {
    "id": "6175",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clone-mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "6176",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "6177",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "6178",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"xtrabackup\" is not set to runAsNonRoot"
  },
  {
    "id": "6179",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clone-mysql\" has cpu request 0"
  },
  {
    "id": "6180",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-mysql\" has cpu request 0"
  },
  {
    "id": "6181",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clone-mysql\" has memory limit 0"
  },
  {
    "id": "6182",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-mysql\" has memory limit 0"
  },
  {
    "id": "6183",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql\" has memory limit 0"
  },
  {
    "id": "6184",
    "manifest_path": "data/manifests/the_stack_sample/sample_2206.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - -c\n        - \"set -ex\\n# Generate mysql server-id from pod ordinal index.\\n[[ `hostname`\\\n          \\ =~ -([0-9]+)$ ]] || exit 1\\nordinal=${BASH_REMATCH[1]}\\necho [mysqld]\\\n          \\ > /mnt/conf.d/server-id.cnf\\n# Add an offset to avoid reserved server-id=0\\\n          \\ value.\\necho server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n\\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\nif [[ $ordinal\\\n          \\ -eq 0 ]]; then\\n  cp /mnt/config-map/master.cnf /mnt/conf.d/\\nelse\\n \\\n          \\ cp /mnt/config-map/slave.cnf /mnt/conf.d/\\nfi\\n\"\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - -c\n        - 'set -ex\n\n          # Skip the clone if data already exists.\n\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\n          # Skip the clone on master (ordinal index 0).\n\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\n          ordinal=${BASH_REMATCH[1]}\n\n          [[ $ordinal -eq 0 ]] && exit 0\n\n          # Clone data from previous peer.\n\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\n          # Prepare the backup.\n\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n\n          '\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: '1'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command:\n            - mysqladmin\n            - ping\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - mysql\n            - -h\n            - 127.0.0.1\n            - -e\n            - SELECT 1\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - -c\n        - \"set -ex\\ncd /var/lib/mysql\\n\\n# Determine binlog position of cloned data,\\\n          \\ if any.\\nif [[ -f xtrabackup_slave_info ]]; then\\n  # XtraBackup already\\\n          \\ generated a partial \\\"CHANGE MASTER TO\\\" query\\n  # because we're cloning\\\n          \\ from an existing slave.\\n  mv xtrabackup_slave_info change_master_to.sql.in\\n\\\n          \\  # Ignore xtrabackup_binlog_info in this case (it's useless).\\n  rm -f\\\n          \\ xtrabackup_binlog_info\\nelif [[ -f xtrabackup_binlog_info ]]; then\\n \\\n          \\ # We're cloning directly from master. Parse binlog position.\\n  [[ `cat\\\n          \\ xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n  rm\\\n          \\ xtrabackup_binlog_info\\n  echo \\\"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\\\\\n          \\n        MASTER_LOG_POS=${BASH_REMATCH[2]}\\\" > change_master_to.sql.in\\n\\\n          fi\\n\\n# Check if we need to complete a clone by starting replication.\\n\\\n          if [[ -f change_master_to.sql.in ]]; then\\n  echo \\\"Waiting for mysqld to\\\n          \\ be ready (accepting connections)\\\"\\n  until mysql -h 127.0.0.1 -e \\\"SELECT\\\n          \\ 1\\\"; do sleep 1; done\\n\\n  echo \\\"Initializing replication from clone\\\n          \\ position\\\"\\n  # In case of container restart, attempt this at-most-once.\\n\\\n          \\  mv change_master_to.sql.in change_master_to.sql.orig\\n  mysql -h 127.0.0.1\\\n          \\ <<EOF\\n$(<change_master_to.sql.orig),\\n  MASTER_HOST='mysql-0.mysql',\\n\\\n          \\  MASTER_USER='root',\\n  MASTER_PASSWORD='',\\n  MASTER_CONNECT_RETRY=10;\\n\\\n          START SLAVE;\\nEOF\\nfi\\n\\n# Start a server to send backups when requested\\\n          \\ by peers.\\nexec ncat --listen --keep-open --send-only --max-conns=1 3307\\\n          \\ -c \\\\\\n  \\\"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1\\\n          \\ --user=root\\\"\\n\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"xtrabackup\" has memory limit 0"
  },
  {
    "id": "6185",
    "manifest_path": "data/manifests/the_stack_sample/sample_2209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: discord-voice-log-bot-deployment\n  labels:\n    app: discord-voice-log-bot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: discord-voice-log-bot\n  template:\n    metadata:\n      labels:\n        app: discord-voice-log-bot\n    spec:\n      containers:\n      - name: discord-voice-log-bot\n        image: quay.io/satackey/discord-voice-log-bot:latest\n        volumeMounts:\n        - name: bot-config\n          mountPath: /app/configs\n          readOnly: true\n      volumes:\n      - name: bot-config\n        secret:\n          secretName: discord-voice-log-bot-secret\n          items:\n          - key: config\n            path: config.ini\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"discord-voice-log-bot\" is using an invalid container image, \"quay.io/satackey/discord-voice-log-bot:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6186",
    "manifest_path": "data/manifests/the_stack_sample/sample_2209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: discord-voice-log-bot-deployment\n  labels:\n    app: discord-voice-log-bot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: discord-voice-log-bot\n  template:\n    metadata:\n      labels:\n        app: discord-voice-log-bot\n    spec:\n      containers:\n      - name: discord-voice-log-bot\n        image: quay.io/satackey/discord-voice-log-bot:latest\n        volumeMounts:\n        - name: bot-config\n          mountPath: /app/configs\n          readOnly: true\n      volumes:\n      - name: bot-config\n        secret:\n          secretName: discord-voice-log-bot-secret\n          items:\n          - key: config\n            path: config.ini\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"discord-voice-log-bot\" does not have a read-only root file system"
  },
  {
    "id": "6187",
    "manifest_path": "data/manifests/the_stack_sample/sample_2209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: discord-voice-log-bot-deployment\n  labels:\n    app: discord-voice-log-bot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: discord-voice-log-bot\n  template:\n    metadata:\n      labels:\n        app: discord-voice-log-bot\n    spec:\n      containers:\n      - name: discord-voice-log-bot\n        image: quay.io/satackey/discord-voice-log-bot:latest\n        volumeMounts:\n        - name: bot-config\n          mountPath: /app/configs\n          readOnly: true\n      volumes:\n      - name: bot-config\n        secret:\n          secretName: discord-voice-log-bot-secret\n          items:\n          - key: config\n            path: config.ini\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"discord-voice-log-bot\" is not set to runAsNonRoot"
  },
  {
    "id": "6188",
    "manifest_path": "data/manifests/the_stack_sample/sample_2209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: discord-voice-log-bot-deployment\n  labels:\n    app: discord-voice-log-bot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: discord-voice-log-bot\n  template:\n    metadata:\n      labels:\n        app: discord-voice-log-bot\n    spec:\n      containers:\n      - name: discord-voice-log-bot\n        image: quay.io/satackey/discord-voice-log-bot:latest\n        volumeMounts:\n        - name: bot-config\n          mountPath: /app/configs\n          readOnly: true\n      volumes:\n      - name: bot-config\n        secret:\n          secretName: discord-voice-log-bot-secret\n          items:\n          - key: config\n            path: config.ini\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"discord-voice-log-bot\" has cpu request 0"
  },
  {
    "id": "6189",
    "manifest_path": "data/manifests/the_stack_sample/sample_2209.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: discord-voice-log-bot-deployment\n  labels:\n    app: discord-voice-log-bot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: discord-voice-log-bot\n  template:\n    metadata:\n      labels:\n        app: discord-voice-log-bot\n    spec:\n      containers:\n      - name: discord-voice-log-bot\n        image: quay.io/satackey/discord-voice-log-bot:latest\n        volumeMounts:\n        - name: bot-config\n          mountPath: /app/configs\n          readOnly: true\n      volumes:\n      - name: bot-config\n        secret:\n          secretName: discord-voice-log-bot-secret\n          items:\n          - key: config\n            path: config.ini\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"discord-voice-log-bot\" has memory limit 0"
  },
  {
    "id": "6190",
    "manifest_path": "data/manifests/the_stack_sample/sample_2211.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: chrisscullpipelinesjavascriptdocker\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n  selector:\n    app: chrisscullpipelinesjavascriptdocker\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:chrisscullpipelinesjavascriptdocker])"
  },
  {
    "id": "6191",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"manager\""
  },
  {
    "id": "6192",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6193",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "6194",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"manager\" not found"
  },
  {
    "id": "6195",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"manager\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "6196",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"manager\" is privileged"
  },
  {
    "id": "6197",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "6198",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"manager\" has cpu request 0"
  },
  {
    "id": "6199",
    "manifest_path": "data/manifests/the_stack_sample/sample_2213.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        args:\n        - --leader-elect\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --feature-gates=MachinePool=false\n        image: controller:latest\n        name: manager\n        env:\n        - name: NO_PROXY\n          value: 127.0.0.1,localhost\n        ports:\n        - containerPort: 9440\n          name: healthz\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: healthz\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n        volumeMounts:\n        - mountPath: /var/run/docker.sock\n          name: dockersock\n        securityContext:\n          privileged: true\n      serviceAccountName: manager\n      volumes:\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"manager\" has memory limit 0"
  },
  {
    "id": "6200",
    "manifest_path": "data/manifests/the_stack_sample/sample_2214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      name: hello-pod\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello-container\n        image: aswroma3/hello:2021-kube\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6201",
    "manifest_path": "data/manifests/the_stack_sample/sample_2214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      name: hello-pod\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello-container\n        image: aswroma3/hello:2021-kube\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello-container\" does not have a read-only root file system"
  },
  {
    "id": "6202",
    "manifest_path": "data/manifests/the_stack_sample/sample_2214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      name: hello-pod\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello-container\n        image: aswroma3/hello:2021-kube\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6203",
    "manifest_path": "data/manifests/the_stack_sample/sample_2214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      name: hello-pod\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello-container\n        image: aswroma3/hello:2021-kube\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello-container\" has cpu request 0"
  },
  {
    "id": "6204",
    "manifest_path": "data/manifests/the_stack_sample/sample_2214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deploy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      name: hello-pod\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello-container\n        image: aswroma3/hello:2021-kube\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello-container\" has memory limit 0"
  },
  {
    "id": "6205",
    "manifest_path": "data/manifests/the_stack_sample/sample_2215.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opentsdb-read\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: opentsdb-read\n  template:\n    metadata:\n      labels:\n        app: opentsdb-read\n    spec:\n      containers:\n      - name: opentsdb-read\n        image: gcr.io/cloud-solutions-images/opentsdb-bigtable:v2.1\n        ports:\n        - containerPort: 4242\n          protocol: TCP\n        volumeMounts:\n        - name: opentsdb-config\n          mountPath: /opt/opentsdb\n      volumes:\n      - name: opentsdb-config\n        configMap:\n          name: opentsdb-config\n          items:\n          - key: opentsdb.conf\n            path: opentsdb.conf\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6206",
    "manifest_path": "data/manifests/the_stack_sample/sample_2215.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opentsdb-read\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: opentsdb-read\n  template:\n    metadata:\n      labels:\n        app: opentsdb-read\n    spec:\n      containers:\n      - name: opentsdb-read\n        image: gcr.io/cloud-solutions-images/opentsdb-bigtable:v2.1\n        ports:\n        - containerPort: 4242\n          protocol: TCP\n        volumeMounts:\n        - name: opentsdb-config\n          mountPath: /opt/opentsdb\n      volumes:\n      - name: opentsdb-config\n        configMap:\n          name: opentsdb-config\n          items:\n          - key: opentsdb.conf\n            path: opentsdb.conf\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"opentsdb-read\" does not have a read-only root file system"
  },
  {
    "id": "6207",
    "manifest_path": "data/manifests/the_stack_sample/sample_2215.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opentsdb-read\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: opentsdb-read\n  template:\n    metadata:\n      labels:\n        app: opentsdb-read\n    spec:\n      containers:\n      - name: opentsdb-read\n        image: gcr.io/cloud-solutions-images/opentsdb-bigtable:v2.1\n        ports:\n        - containerPort: 4242\n          protocol: TCP\n        volumeMounts:\n        - name: opentsdb-config\n          mountPath: /opt/opentsdb\n      volumes:\n      - name: opentsdb-config\n        configMap:\n          name: opentsdb-config\n          items:\n          - key: opentsdb.conf\n            path: opentsdb.conf\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"opentsdb-read\" is not set to runAsNonRoot"
  },
  {
    "id": "6208",
    "manifest_path": "data/manifests/the_stack_sample/sample_2215.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opentsdb-read\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: opentsdb-read\n  template:\n    metadata:\n      labels:\n        app: opentsdb-read\n    spec:\n      containers:\n      - name: opentsdb-read\n        image: gcr.io/cloud-solutions-images/opentsdb-bigtable:v2.1\n        ports:\n        - containerPort: 4242\n          protocol: TCP\n        volumeMounts:\n        - name: opentsdb-config\n          mountPath: /opt/opentsdb\n      volumes:\n      - name: opentsdb-config\n        configMap:\n          name: opentsdb-config\n          items:\n          - key: opentsdb.conf\n            path: opentsdb.conf\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"opentsdb-read\" has cpu request 0"
  },
  {
    "id": "6209",
    "manifest_path": "data/manifests/the_stack_sample/sample_2215.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opentsdb-read\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: opentsdb-read\n  template:\n    metadata:\n      labels:\n        app: opentsdb-read\n    spec:\n      containers:\n      - name: opentsdb-read\n        image: gcr.io/cloud-solutions-images/opentsdb-bigtable:v2.1\n        ports:\n        - containerPort: 4242\n          protocol: TCP\n        volumeMounts:\n        - name: opentsdb-config\n          mountPath: /opt/opentsdb\n      volumes:\n      - name: opentsdb-config\n        configMap:\n          name: opentsdb-config\n          items:\n          - key: opentsdb.conf\n            path: opentsdb.conf\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"opentsdb-read\" has memory limit 0"
  },
  {
    "id": "6210",
    "manifest_path": "data/manifests/the_stack_sample/sample_2216.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    service: ambassador\n  name: ambassador\n  annotations:\n    getambassador.io/config: '---\n\n      apiVersion: ambassador/v0\n\n      kind:  Mapping\n\n      name:  shopfront_stable\n\n      prefix: /shopfront/\n\n      service: shopfront:8010\n\n      ---\n\n      apiVersion: ambassador/v0\n\n      kind:  Mapping\n\n      name:  shopfront_canary\n\n      prefix: /shopfront/\n\n      weight: 50\n\n      service: shopfront-canary:8010\n\n      '\nspec:\n  type: LoadBalancer\n  ports:\n  - name: ambassador\n    port: 80\n    targetPort: 80\n  selector:\n    service: ambassador\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:ambassador])"
  },
  {
    "id": "6211",
    "manifest_path": "data/manifests/the_stack_sample/sample_2218.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: $SERVICE_NAME-fe\n  labels:\n    app: $SERVICE_NAME-fe\nspec:\n  ports:\n  - port: 80\n    targetPort: 3000\n  selector:\n    app: $SERVICE_NAME-fe\n",
    "policy_id": "dangling-service",
    "violation_text": "service has invalid label selector: values[0][app]: Invalid value: \"$SERVICE_NAME-fe\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "6212",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"machine-controller-manager\" does not expose port 10258 for the HTTPGet"
  },
  {
    "id": "6213",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"machine-controller\" does not have a read-only root file system"
  },
  {
    "id": "6214",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"machine-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "6215",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"machine-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "6216",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"machine-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "6217",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"machine-controller-manager\" has cpu request 0"
  },
  {
    "id": "6218",
    "manifest_path": "data/manifests/the_stack_sample/sample_2220.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: machine-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: machine-controller-manager\n  template:\n    metadata:\n      labels:\n        role: machine-controller-manager\n    spec:\n      containers:\n      - name: machine-controller-manager\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager:v0.39.0\n        imagePullPolicy: Always\n        command:\n        - ./machine-controller-manager\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --control-kubeconfig=inClusterConfig\n        - --safety-up=2\n        - --safety-down=1\n        - --machine-safety-overshooting-period=1m\n        - --v=3\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10258\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n        resources: {}\n      - command:\n        - ./machine-controller\n        - --control-kubeconfig=inClusterConfig\n        - --target-kubeconfig=/var/lib/machine-controller-manager/kubeconfig\n        - --machine-creation-timeout=20m\n        - --machine-drain-timeout=5m\n        - --machine-health-timeout=10m\n        - --machine-safety-orphan-vms-period=30m\n        - --node-conditions=ReadonlyFilesystem,KernelDeadlock,DiskPressure\n        - --v=3\n        image: eu.gcr.io/gardener-project/gardener/machine-controller-manager-provider-gcp:v0.7.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: machine-controller\n        ports:\n        - containerPort: 10259\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 350m\n            memory: 3000Mi\n          requests:\n            cpu: 50m\n            memory: 64Mi\n        volumeMounts:\n        - mountPath: /var/lib/machine-controller-manager\n          name: machine-controller-manager\n          readOnly: true\n      securityContext: {}\n      volumes:\n      - name: machine-controller-manager\n        secret:\n          defaultMode: 420\n          secretName: machine-controller-manager\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"machine-controller-manager\" has memory limit 0"
  },
  {
    "id": "6219",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"menu-backend\" is using an invalid container image, \"scilifelabdatacentre/menu-backend\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6220",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"menu-frontend\" is using an invalid container image, \"scilifelabdatacentre/menu-frontend\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6221",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"menu-backend\" does not have a read-only root file system"
  },
  {
    "id": "6222",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"menu-frontend\" does not have a read-only root file system"
  },
  {
    "id": "6223",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"menu-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "6224",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"menu-frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "6225",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"menu-backend\" has cpu request 0"
  },
  {
    "id": "6226",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"menu-frontend\" has cpu request 0"
  },
  {
    "id": "6227",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"menu-backend\" has memory limit 0"
  },
  {
    "id": "6228",
    "manifest_path": "data/manifests/the_stack_sample/sample_2223.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: menu-deployment\nspec:\n  selector:\n    matchLabels:\n      app: lunch-menu\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: lunch-menu\n    spec:\n      containers:\n      - name: menu-frontend\n        image: scilifelabdatacentre/menu-frontend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        env:\n        - name: API_URL\n          valueFrom:\n            configMapKeyRef:\n              name: lunch-menu-conf\n              key: api-url\n      - name: menu-backend\n        image: scilifelabdatacentre/menu-backend\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"menu-frontend\" has memory limit 0"
  },
  {
    "id": "6229",
    "manifest_path": "data/manifests/the_stack_sample/sample_2225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: gateway\n        name: gateway\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"gateway\" is using an invalid container image, \"gateway\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6230",
    "manifest_path": "data/manifests/the_stack_sample/sample_2225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: gateway\n        name: gateway\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gateway\" does not have a read-only root file system"
  },
  {
    "id": "6231",
    "manifest_path": "data/manifests/the_stack_sample/sample_2225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: gateway\n        name: gateway\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gateway\" is not set to runAsNonRoot"
  },
  {
    "id": "6232",
    "manifest_path": "data/manifests/the_stack_sample/sample_2225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: gateway\n        name: gateway\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gateway\" has cpu request 0"
  },
  {
    "id": "6233",
    "manifest_path": "data/manifests/the_stack_sample/sample_2225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n    spec:\n      containers:\n      - envFrom:\n        - secretRef:\n            name: env\n        imagePullPolicy: Never\n        image: gateway\n        name: gateway\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gateway\" has memory limit 0"
  },
  {
    "id": "6234",
    "manifest_path": "data/manifests/the_stack_sample/sample_2228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1551\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6235",
    "manifest_path": "data/manifests/the_stack_sample/sample_2228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1551\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6236",
    "manifest_path": "data/manifests/the_stack_sample/sample_2228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1551\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6237",
    "manifest_path": "data/manifests/the_stack_sample/sample_2228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1551\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6238",
    "manifest_path": "data/manifests/the_stack_sample/sample_2228.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1551\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6239",
    "manifest_path": "data/manifests/the_stack_sample/sample_2230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sloth\n  namespace: default\n  labels:\n    helm.sh/chart: sloth-0.1.0\n    app.kubernetes.io/managed-by: Helm\n    app: sloth\n    app.kubernetes.io/name: sloth\n    app.kubernetes.io/instance: sloth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sloth\n      app.kubernetes.io/name: sloth\n      app.kubernetes.io/instance: sloth\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: sloth-0.1.0\n        app.kubernetes.io/managed-by: Helm\n        app: sloth\n        app.kubernetes.io/name: sloth\n        app.kubernetes.io/instance: sloth\n      annotations:\n        kubectl.kubernetes.io/default-container: sloth\n    spec:\n      serviceAccountName: sloth\n      containers:\n      - name: sloth\n        image: slok/sloth:v0.6.0\n        args:\n        - kubernetes-controller\n        - --resync-interval=3m\n        - --sli-plugins-path=/plugins\n        ports:\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /plugins/sloth-common-sli-plugins\n        resources:\n          limits:\n            memory: 150Mi\n          requests:\n            cpu: 5m\n            memory: 75Mi\n      - name: git-sync-plugins\n        image: k8s.gcr.io/git-sync/git-sync:v3.3.4\n        args:\n        - --repo=https://github.com/slok/sloth-common-sli-plugins\n        - --branch=main\n        - --wait=30\n        - --webhook-url=http://localhost:8082/-/reload\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /tmp/git\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 5m\n            memory: 50Mi\n      volumes:\n      - name: sloth-common-sli-plugins\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"git-sync-plugins\" does not have a read-only root file system"
  },
  {
    "id": "6240",
    "manifest_path": "data/manifests/the_stack_sample/sample_2230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sloth\n  namespace: default\n  labels:\n    helm.sh/chart: sloth-0.1.0\n    app.kubernetes.io/managed-by: Helm\n    app: sloth\n    app.kubernetes.io/name: sloth\n    app.kubernetes.io/instance: sloth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sloth\n      app.kubernetes.io/name: sloth\n      app.kubernetes.io/instance: sloth\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: sloth-0.1.0\n        app.kubernetes.io/managed-by: Helm\n        app: sloth\n        app.kubernetes.io/name: sloth\n        app.kubernetes.io/instance: sloth\n      annotations:\n        kubectl.kubernetes.io/default-container: sloth\n    spec:\n      serviceAccountName: sloth\n      containers:\n      - name: sloth\n        image: slok/sloth:v0.6.0\n        args:\n        - kubernetes-controller\n        - --resync-interval=3m\n        - --sli-plugins-path=/plugins\n        ports:\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /plugins/sloth-common-sli-plugins\n        resources:\n          limits:\n            memory: 150Mi\n          requests:\n            cpu: 5m\n            memory: 75Mi\n      - name: git-sync-plugins\n        image: k8s.gcr.io/git-sync/git-sync:v3.3.4\n        args:\n        - --repo=https://github.com/slok/sloth-common-sli-plugins\n        - --branch=main\n        - --wait=30\n        - --webhook-url=http://localhost:8082/-/reload\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /tmp/git\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 5m\n            memory: 50Mi\n      volumes:\n      - name: sloth-common-sli-plugins\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sloth\" does not have a read-only root file system"
  },
  {
    "id": "6241",
    "manifest_path": "data/manifests/the_stack_sample/sample_2230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sloth\n  namespace: default\n  labels:\n    helm.sh/chart: sloth-0.1.0\n    app.kubernetes.io/managed-by: Helm\n    app: sloth\n    app.kubernetes.io/name: sloth\n    app.kubernetes.io/instance: sloth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sloth\n      app.kubernetes.io/name: sloth\n      app.kubernetes.io/instance: sloth\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: sloth-0.1.0\n        app.kubernetes.io/managed-by: Helm\n        app: sloth\n        app.kubernetes.io/name: sloth\n        app.kubernetes.io/instance: sloth\n      annotations:\n        kubectl.kubernetes.io/default-container: sloth\n    spec:\n      serviceAccountName: sloth\n      containers:\n      - name: sloth\n        image: slok/sloth:v0.6.0\n        args:\n        - kubernetes-controller\n        - --resync-interval=3m\n        - --sli-plugins-path=/plugins\n        ports:\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /plugins/sloth-common-sli-plugins\n        resources:\n          limits:\n            memory: 150Mi\n          requests:\n            cpu: 5m\n            memory: 75Mi\n      - name: git-sync-plugins\n        image: k8s.gcr.io/git-sync/git-sync:v3.3.4\n        args:\n        - --repo=https://github.com/slok/sloth-common-sli-plugins\n        - --branch=main\n        - --wait=30\n        - --webhook-url=http://localhost:8082/-/reload\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /tmp/git\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 5m\n            memory: 50Mi\n      volumes:\n      - name: sloth-common-sli-plugins\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"sloth\" not found"
  },
  {
    "id": "6242",
    "manifest_path": "data/manifests/the_stack_sample/sample_2230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sloth\n  namespace: default\n  labels:\n    helm.sh/chart: sloth-0.1.0\n    app.kubernetes.io/managed-by: Helm\n    app: sloth\n    app.kubernetes.io/name: sloth\n    app.kubernetes.io/instance: sloth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sloth\n      app.kubernetes.io/name: sloth\n      app.kubernetes.io/instance: sloth\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: sloth-0.1.0\n        app.kubernetes.io/managed-by: Helm\n        app: sloth\n        app.kubernetes.io/name: sloth\n        app.kubernetes.io/instance: sloth\n      annotations:\n        kubectl.kubernetes.io/default-container: sloth\n    spec:\n      serviceAccountName: sloth\n      containers:\n      - name: sloth\n        image: slok/sloth:v0.6.0\n        args:\n        - kubernetes-controller\n        - --resync-interval=3m\n        - --sli-plugins-path=/plugins\n        ports:\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /plugins/sloth-common-sli-plugins\n        resources:\n          limits:\n            memory: 150Mi\n          requests:\n            cpu: 5m\n            memory: 75Mi\n      - name: git-sync-plugins\n        image: k8s.gcr.io/git-sync/git-sync:v3.3.4\n        args:\n        - --repo=https://github.com/slok/sloth-common-sli-plugins\n        - --branch=main\n        - --wait=30\n        - --webhook-url=http://localhost:8082/-/reload\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /tmp/git\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 5m\n            memory: 50Mi\n      volumes:\n      - name: sloth-common-sli-plugins\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"git-sync-plugins\" is not set to runAsNonRoot"
  },
  {
    "id": "6243",
    "manifest_path": "data/manifests/the_stack_sample/sample_2230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sloth\n  namespace: default\n  labels:\n    helm.sh/chart: sloth-0.1.0\n    app.kubernetes.io/managed-by: Helm\n    app: sloth\n    app.kubernetes.io/name: sloth\n    app.kubernetes.io/instance: sloth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sloth\n      app.kubernetes.io/name: sloth\n      app.kubernetes.io/instance: sloth\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: sloth-0.1.0\n        app.kubernetes.io/managed-by: Helm\n        app: sloth\n        app.kubernetes.io/name: sloth\n        app.kubernetes.io/instance: sloth\n      annotations:\n        kubectl.kubernetes.io/default-container: sloth\n    spec:\n      serviceAccountName: sloth\n      containers:\n      - name: sloth\n        image: slok/sloth:v0.6.0\n        args:\n        - kubernetes-controller\n        - --resync-interval=3m\n        - --sli-plugins-path=/plugins\n        ports:\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /plugins/sloth-common-sli-plugins\n        resources:\n          limits:\n            memory: 150Mi\n          requests:\n            cpu: 5m\n            memory: 75Mi\n      - name: git-sync-plugins\n        image: k8s.gcr.io/git-sync/git-sync:v3.3.4\n        args:\n        - --repo=https://github.com/slok/sloth-common-sli-plugins\n        - --branch=main\n        - --wait=30\n        - --webhook-url=http://localhost:8082/-/reload\n        volumeMounts:\n        - name: sloth-common-sli-plugins\n          mountPath: /tmp/git\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 5m\n            memory: 50Mi\n      volumes:\n      - name: sloth-common-sli-plugins\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sloth\" is not set to runAsNonRoot"
  },
  {
    "id": "6244",
    "manifest_path": "data/manifests/the_stack_sample/sample_2232.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: selenium-sessions\n  namespace: selenium\n  labels:\n    app: selenium-sessions\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: selenium-sessions\n  template:\n    metadata:\n      labels:\n        app: selenium-sessions\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cloud.google.com/gke-preemptible\n                operator: DoesNotExist\n              - key: eks.amazonaws.com/capacityType\n                operator: NotIn\n                values:\n                - SPOT\n              - key: kubernetes.azure.com/scalesetpriority\n                operator: NotIn\n                values:\n                - spot\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - name: selenium-sessions\n        image: selenium/sessions:4.0.0\n        ports:\n        - containerPort: 5556\n        env:\n        - name: JAVA_OPTS\n          value: -Xmx512m\n        - name: SE_EVENT_BUS_HOST\n          value: selenium-event-bus\n        - name: SE_EVENT_BUS_PUBLISH_PORT\n          value: '4442'\n        - name: SE_EVENT_BUS_SUBSCRIBE_PORT\n          value: '4443'\n        readinessProbe:\n          tcpSocket:\n            port: 5556\n          initialDelaySeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: 5556\n          initialDelaySeconds: 30\n        resources:\n          limits:\n            cpu: 2\n            memory: 1Gi\n          requests:\n            cpu: 100m\n            memory: 600Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"selenium-sessions\" does not have a read-only root file system"
  },
  {
    "id": "6245",
    "manifest_path": "data/manifests/the_stack_sample/sample_2236.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 64d2b982dc80b5c65271f2b9ff8b94777e4a148eca3fed68a4eebb84f6342f24\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: darrendignam\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 0b85b745d799aa9f82651a8851b657ee809ef8f3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "6246",
    "manifest_path": "data/manifests/the_stack_sample/sample_2236.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 64d2b982dc80b5c65271f2b9ff8b94777e4a148eca3fed68a4eebb84f6342f24\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: darrendignam\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 0b85b745d799aa9f82651a8851b657ee809ef8f3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-foghorn\" not found"
  },
  {
    "id": "6247",
    "manifest_path": "data/manifests/the_stack_sample/sample_2236.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 64d2b982dc80b5c65271f2b9ff8b94777e4a148eca3fed68a4eebb84f6342f24\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: darrendignam\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 0b85b745d799aa9f82651a8851b657ee809ef8f3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "6248",
    "manifest_path": "data/manifests/the_stack_sample/sample_2238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tiger\n  namespace: tiger\nspec:\n  containers:\n  - image: alpine\n    name: main\n    args:\n    - /bin/sh\n    - -c\n    - sleep 60; touch /tmp/healthy; sleep 86400\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 65\n      periodSeconds: 5\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6249",
    "manifest_path": "data/manifests/the_stack_sample/sample_2238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tiger\n  namespace: tiger\nspec:\n  containers:\n  - image: alpine\n    name: main\n    args:\n    - /bin/sh\n    - -c\n    - sleep 60; touch /tmp/healthy; sleep 86400\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 65\n      periodSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "6250",
    "manifest_path": "data/manifests/the_stack_sample/sample_2238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tiger\n  namespace: tiger\nspec:\n  containers:\n  - image: alpine\n    name: main\n    args:\n    - /bin/sh\n    - -c\n    - sleep 60; touch /tmp/healthy; sleep 86400\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 65\n      periodSeconds: 5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "6251",
    "manifest_path": "data/manifests/the_stack_sample/sample_2238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tiger\n  namespace: tiger\nspec:\n  containers:\n  - image: alpine\n    name: main\n    args:\n    - /bin/sh\n    - -c\n    - sleep 60; touch /tmp/healthy; sleep 86400\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 65\n      periodSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "6252",
    "manifest_path": "data/manifests/the_stack_sample/sample_2238.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: tiger\n  namespace: tiger\nspec:\n  containers:\n  - image: alpine\n    name: main\n    args:\n    - /bin/sh\n    - -c\n    - sleep 60; touch /tmp/healthy; sleep 86400\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 65\n      periodSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "6253",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6254",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tileserver-import\" is using an invalid container image, \"greenhalos/tile-server-import:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6255",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tileserver-import\" does not have a read-only root file system"
  },
  {
    "id": "6256",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tileserver-import\" is not set to runAsNonRoot"
  },
  {
    "id": "6257",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tileserver-import\" has cpu request 0"
  },
  {
    "id": "6258",
    "manifest_path": "data/manifests/the_stack_sample/sample_2239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tileserver-import\nspec:\n  template:\n    spec:\n      containers:\n      - image: greenhalos/tile-server-import:latest\n        name: tileserver-import\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tileserver-import\" has memory limit 0"
  },
  {
    "id": "6259",
    "manifest_path": "data/manifests/the_stack_sample/sample_2241.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: esc\n  annotations:\n    version: 1.19.10\n  labels:\n    io.esc.service: web-server\n  name: web-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.esc.service: web-server\n  template:\n    metadata:\n      labels:\n        io.esc.service: web-server\n    spec:\n      containers:\n      - image: eu.gcr.io/zendphp-313619/esc-nginx\n        name: web-server\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n        resources: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web-server\" is using an invalid container image, \"eu.gcr.io/zendphp-313619/esc-nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6260",
    "manifest_path": "data/manifests/the_stack_sample/sample_2241.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: esc\n  annotations:\n    version: 1.19.10\n  labels:\n    io.esc.service: web-server\n  name: web-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.esc.service: web-server\n  template:\n    metadata:\n      labels:\n        io.esc.service: web-server\n    spec:\n      containers:\n      - image: eu.gcr.io/zendphp-313619/esc-nginx\n        name: web-server\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n        resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-server\" does not have a read-only root file system"
  },
  {
    "id": "6261",
    "manifest_path": "data/manifests/the_stack_sample/sample_2241.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: esc\n  annotations:\n    version: 1.19.10\n  labels:\n    io.esc.service: web-server\n  name: web-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.esc.service: web-server\n  template:\n    metadata:\n      labels:\n        io.esc.service: web-server\n    spec:\n      containers:\n      - image: eu.gcr.io/zendphp-313619/esc-nginx\n        name: web-server\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n        resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-server\" is not set to runAsNonRoot"
  },
  {
    "id": "6262",
    "manifest_path": "data/manifests/the_stack_sample/sample_2241.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: esc\n  annotations:\n    version: 1.19.10\n  labels:\n    io.esc.service: web-server\n  name: web-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.esc.service: web-server\n  template:\n    metadata:\n      labels:\n        io.esc.service: web-server\n    spec:\n      containers:\n      - image: eu.gcr.io/zendphp-313619/esc-nginx\n        name: web-server\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n        resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-server\" has cpu request 0"
  },
  {
    "id": "6263",
    "manifest_path": "data/manifests/the_stack_sample/sample_2241.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: esc\n  annotations:\n    version: 1.19.10\n  labels:\n    io.esc.service: web-server\n  name: web-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.esc.service: web-server\n  template:\n    metadata:\n      labels:\n        io.esc.service: web-server\n    spec:\n      containers:\n      - image: eu.gcr.io/zendphp-313619/esc-nginx\n        name: web-server\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n        resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-server\" has memory limit 0"
  },
  {
    "id": "6264",
    "manifest_path": "data/manifests/the_stack_sample/sample_2242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-nuevo\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9.1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6265",
    "manifest_path": "data/manifests/the_stack_sample/sample_2242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-nuevo\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9.1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6266",
    "manifest_path": "data/manifests/the_stack_sample/sample_2242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-nuevo\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9.1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6267",
    "manifest_path": "data/manifests/the_stack_sample/sample_2242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-nuevo\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9.1\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6268",
    "manifest_path": "data/manifests/the_stack_sample/sample_2244.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mysql-backup\n  namespace: mariadb-galera\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - env:\n          - name: MYSQL_ENV_DB_HOST\n            value: mariadb-galera\n          - name: MYSQL_ENV_DB_USER\n            value: root\n          - name: MYSQL_ENV_DB_PASS\n            valueFrom:\n              secretKeyRef:\n                key: mariadb-root-password\n                name: mariadb-galera\n                optional: false\n          image: cube8021/mysql-backups:v0.0.4\n          imagePullPolicy: IfNotPresent\n          name: mysqldump\n          volumeMounts:\n          - name: mysql-backup\n            mountPath: /mysqldump\n        volumes:\n        - name: mysql-backup\n          persistentVolumeClaim:\n            claimName: mysql-backup-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysqldump\" does not have a read-only root file system"
  },
  {
    "id": "6269",
    "manifest_path": "data/manifests/the_stack_sample/sample_2244.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mysql-backup\n  namespace: mariadb-galera\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - env:\n          - name: MYSQL_ENV_DB_HOST\n            value: mariadb-galera\n          - name: MYSQL_ENV_DB_USER\n            value: root\n          - name: MYSQL_ENV_DB_PASS\n            valueFrom:\n              secretKeyRef:\n                key: mariadb-root-password\n                name: mariadb-galera\n                optional: false\n          image: cube8021/mysql-backups:v0.0.4\n          imagePullPolicy: IfNotPresent\n          name: mysqldump\n          volumeMounts:\n          - name: mysql-backup\n            mountPath: /mysqldump\n        volumes:\n        - name: mysql-backup\n          persistentVolumeClaim:\n            claimName: mysql-backup-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysqldump\" is not set to runAsNonRoot"
  },
  {
    "id": "6270",
    "manifest_path": "data/manifests/the_stack_sample/sample_2244.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mysql-backup\n  namespace: mariadb-galera\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - env:\n          - name: MYSQL_ENV_DB_HOST\n            value: mariadb-galera\n          - name: MYSQL_ENV_DB_USER\n            value: root\n          - name: MYSQL_ENV_DB_PASS\n            valueFrom:\n              secretKeyRef:\n                key: mariadb-root-password\n                name: mariadb-galera\n                optional: false\n          image: cube8021/mysql-backups:v0.0.4\n          imagePullPolicy: IfNotPresent\n          name: mysqldump\n          volumeMounts:\n          - name: mysql-backup\n            mountPath: /mysqldump\n        volumes:\n        - name: mysql-backup\n          persistentVolumeClaim:\n            claimName: mysql-backup-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysqldump\" has cpu request 0"
  },
  {
    "id": "6271",
    "manifest_path": "data/manifests/the_stack_sample/sample_2244.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mysql-backup\n  namespace: mariadb-galera\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - env:\n          - name: MYSQL_ENV_DB_HOST\n            value: mariadb-galera\n          - name: MYSQL_ENV_DB_USER\n            value: root\n          - name: MYSQL_ENV_DB_PASS\n            valueFrom:\n              secretKeyRef:\n                key: mariadb-root-password\n                name: mariadb-galera\n                optional: false\n          image: cube8021/mysql-backups:v0.0.4\n          imagePullPolicy: IfNotPresent\n          name: mysqldump\n          volumeMounts:\n          - name: mysql-backup\n            mountPath: /mysqldump\n        volumes:\n        - name: mysql-backup\n          persistentVolumeClaim:\n            claimName: mysql-backup-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysqldump\" has memory limit 0"
  },
  {
    "id": "6272",
    "manifest_path": "data/manifests/the_stack_sample/sample_2246.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  namespace: prow\n  name: ti-community-owners\nspec:\n  selector:\n    app: ti-community-owners\n  ports:\n  - port: 80\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ti-community-owners])"
  },
  {
    "id": "6273",
    "manifest_path": "data/manifests/the_stack_sample/sample_2249.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cra-boilerplate-front\n  labels:\n    app: cra-boilerplate\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: front\n  template:\n    metadata:\n      labels:\n        app: front\n    spec:\n      containers:\n      - name: cra-front\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6274",
    "manifest_path": "data/manifests/the_stack_sample/sample_2249.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cra-boilerplate-front\n  labels:\n    app: cra-boilerplate\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: front\n  template:\n    metadata:\n      labels:\n        app: front\n    spec:\n      containers:\n      - name: cra-front\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cra-front\" does not have a read-only root file system"
  },
  {
    "id": "6275",
    "manifest_path": "data/manifests/the_stack_sample/sample_2249.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cra-boilerplate-front\n  labels:\n    app: cra-boilerplate\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: front\n  template:\n    metadata:\n      labels:\n        app: front\n    spec:\n      containers:\n      - name: cra-front\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cra-front\" is not set to runAsNonRoot"
  },
  {
    "id": "6276",
    "manifest_path": "data/manifests/the_stack_sample/sample_2249.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cra-boilerplate-front\n  labels:\n    app: cra-boilerplate\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: front\n  template:\n    metadata:\n      labels:\n        app: front\n    spec:\n      containers:\n      - name: cra-front\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cra-front\" has cpu request 0"
  },
  {
    "id": "6277",
    "manifest_path": "data/manifests/the_stack_sample/sample_2249.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cra-boilerplate-front\n  labels:\n    app: cra-boilerplate\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: front\n  template:\n    metadata:\n      labels:\n        app: front\n    spec:\n      containers:\n      - name: cra-front\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cra-front\" has memory limit 0"
  },
  {
    "id": "6278",
    "manifest_path": "data/manifests/the_stack_sample/sample_2250.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ceph-mon\n  namespace: ceph\n  labels:\n    app: ceph\n    daemon: mon\nspec:\n  ports:\n  - port: 6789\n    protocol: TCP\n    targetPort: 6789\n  selector:\n    app: ceph\n    daemon: mon\n  clusterIP: None\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ceph daemon:mon])"
  },
  {
    "id": "6279",
    "manifest_path": "data/manifests/the_stack_sample/sample_2251.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gruppenname-frontend\n  namespace: sachs\n  labels:\n    app: gruppenname-frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: gruppenname-frontend\n    spec:\n      containers:\n      - name: gruppenname-frontend\n        image: registry.datexis.com/ksachs/gruppenname-frontend\n        ports:\n        - name: client-port\n          containerPort: 8080\n  selector:\n    matchLabels:\n      app: gruppenname-frontend\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"gruppenname-frontend\" is using an invalid container image, \"registry.datexis.com/ksachs/gruppenname-frontend\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6280",
    "manifest_path": "data/manifests/the_stack_sample/sample_2251.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gruppenname-frontend\n  namespace: sachs\n  labels:\n    app: gruppenname-frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: gruppenname-frontend\n    spec:\n      containers:\n      - name: gruppenname-frontend\n        image: registry.datexis.com/ksachs/gruppenname-frontend\n        ports:\n        - name: client-port\n          containerPort: 8080\n  selector:\n    matchLabels:\n      app: gruppenname-frontend\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gruppenname-frontend\" does not have a read-only root file system"
  },
  {
    "id": "6281",
    "manifest_path": "data/manifests/the_stack_sample/sample_2251.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gruppenname-frontend\n  namespace: sachs\n  labels:\n    app: gruppenname-frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: gruppenname-frontend\n    spec:\n      containers:\n      - name: gruppenname-frontend\n        image: registry.datexis.com/ksachs/gruppenname-frontend\n        ports:\n        - name: client-port\n          containerPort: 8080\n  selector:\n    matchLabels:\n      app: gruppenname-frontend\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gruppenname-frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "6282",
    "manifest_path": "data/manifests/the_stack_sample/sample_2251.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gruppenname-frontend\n  namespace: sachs\n  labels:\n    app: gruppenname-frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: gruppenname-frontend\n    spec:\n      containers:\n      - name: gruppenname-frontend\n        image: registry.datexis.com/ksachs/gruppenname-frontend\n        ports:\n        - name: client-port\n          containerPort: 8080\n  selector:\n    matchLabels:\n      app: gruppenname-frontend\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gruppenname-frontend\" has cpu request 0"
  },
  {
    "id": "6283",
    "manifest_path": "data/manifests/the_stack_sample/sample_2251.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gruppenname-frontend\n  namespace: sachs\n  labels:\n    app: gruppenname-frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: gruppenname-frontend\n    spec:\n      containers:\n      - name: gruppenname-frontend\n        image: registry.datexis.com/ksachs/gruppenname-frontend\n        ports:\n        - name: client-port\n          containerPort: 8080\n  selector:\n    matchLabels:\n      app: gruppenname-frontend\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gruppenname-frontend\" has memory limit 0"
  },
  {
    "id": "6284",
    "manifest_path": "data/manifests/the_stack_sample/sample_2252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.14.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-operator\" does not have a read-only root file system"
  },
  {
    "id": "6285",
    "manifest_path": "data/manifests/the_stack_sample/sample_2252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.14.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jaeger-operator\" not found"
  },
  {
    "id": "6286",
    "manifest_path": "data/manifests/the_stack_sample/sample_2252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.14.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "6287",
    "manifest_path": "data/manifests/the_stack_sample/sample_2252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.14.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-operator\" has cpu request 0"
  },
  {
    "id": "6288",
    "manifest_path": "data/manifests/the_stack_sample/sample_2252.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.14.0\n        ports:\n        - containerPort: 8383\n          name: metrics\n        args:\n        - start\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-operator\" has memory limit 0"
  },
  {
    "id": "6289",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"airbyte-scheduler-container\" is using an invalid container image, \"airbyte/scheduler\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6290",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"airbyte-seed\" is using an invalid container image, \"airbyte/seed\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6291",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"airbyte-scheduler-container\" does not have a read-only root file system"
  },
  {
    "id": "6292",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"airbyte-seed\" does not have a read-only root file system"
  },
  {
    "id": "6293",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"airbyte-admin\" not found"
  },
  {
    "id": "6294",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"airbyte-scheduler-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6295",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"airbyte-seed\" is not set to runAsNonRoot"
  },
  {
    "id": "6296",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"airbyte-scheduler-container\" has cpu request 0"
  },
  {
    "id": "6297",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"airbyte-seed\" has cpu request 0"
  },
  {
    "id": "6298",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"airbyte-scheduler-container\" has memory limit 0"
  },
  {
    "id": "6299",
    "manifest_path": "data/manifests/the_stack_sample/sample_2253.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airbyte-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      airbyte: scheduler\n  template:\n    metadata:\n      labels:\n        airbyte: scheduler\n    spec:\n      serviceAccountName: airbyte-admin\n      containers:\n      - name: airbyte-scheduler-container\n        image: airbyte/scheduler\n        env:\n        - name: AIRBYTE_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AIRBYTE_VERSION\n        - name: CONFIG_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: CONFIG_ROOT\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_URL\n        - name: DATABASE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: DATABASE_USER\n        - name: TRACKING_STRATEGY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TRACKING_STRATEGY\n        - name: WAIT_BEFORE_HOSTS\n          value: '5'\n        - name: WAIT_HOSTS\n          value: airbyte-db-svc:5432\n        - name: WAIT_HOSTS_TIMEOUT\n          value: '45'\n        - name: WORKSPACE_DOCKER_MOUNT\n          value: workspace\n        - name: WORKSPACE_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKSPACE_ROOT\n        - name: WORKER_ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WORKER_ENVIRONMENT\n        - name: LOCAL_ROOT\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: LOCAL_ROOT\n        - name: WEBAPP_URL\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: WEBAPP_URL\n        - name: TEMPORAL_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_HOST\n        - name: TEMPORAL_WORKER_PORTS\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: TEMPORAL_WORKER_PORTS\n        - name: S3_LOG_BUCKET\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET\n        - name: S3_LOG_BUCKET_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: S3_LOG_BUCKET_REGION\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: airbyte-env\n              key: AWS_SECRET_ACCESS_KEY\n        - name: KUBE_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 9000\n        - containerPort: 9001\n        - containerPort: 9002\n        - containerPort: 9003\n        - containerPort: 9004\n        - containerPort: 9005\n        - containerPort: 9006\n        - containerPort: 9007\n        - containerPort: 9008\n        - containerPort: 9009\n        - containerPort: 9010\n        - containerPort: 9011\n        - containerPort: 9012\n        - containerPort: 9013\n        - containerPort: 9014\n        - containerPort: 9015\n        - containerPort: 9016\n        - containerPort: 9017\n        - containerPort: 9018\n        - containerPort: 9019\n        - containerPort: 9020\n        - containerPort: 9021\n        - containerPort: 9022\n        - containerPort: 9023\n        - containerPort: 9024\n        - containerPort: 9025\n        - containerPort: 9026\n        - containerPort: 9027\n        - containerPort: 9028\n        - containerPort: 9029\n        - containerPort: 9030\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      initContainers:\n      - name: airbyte-seed\n        image: airbyte/seed\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - mkdir -p /configs/config && yes n | cp -r -i /app/seed/config /configs\n        volumeMounts:\n        - name: airbyte-volume-configs\n          mountPath: /configs\n        - name: airbyte-volume-workspace\n          mountPath: /workspace\n      volumes:\n      - name: airbyte-volume-workspace\n        persistentVolumeClaim:\n          claimName: airbyte-volume-workspace\n      - name: airbyte-volume-configs\n        persistentVolumeClaim:\n          claimName: airbyte-volume-configs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"airbyte-seed\" has memory limit 0"
  },
  {
    "id": "6300",
    "manifest_path": "data/manifests/the_stack_sample/sample_2257.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    service-label-key: service-label-value\n  name: service-name-loadbalancer\nspec:\n  type: LoadBalancer\n  ports:\n  - name: service-port-name\n    port: 80\n    protocol: TCP\n  selector:\n    deployment-label-key: deployment-label-value#for creating a deployment in kubernetes\n",
    "policy_id": "dangling-service",
    "violation_text": "service has invalid label selector: values[0][deployment-label-key]: Invalid value: \"deployment-label-value#for creating a deployment in kubernetes\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "6301",
    "manifest_path": "data/manifests/the_stack_sample/sample_2258.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slack-post-message\n  labels:\n    app: slack-post-message\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: slack-post-message\n  template:\n    metadata:\n      labels:\n        app: slack-post-message\n    spec:\n      containers:\n      - name: slack-post-message\n        image: gcr.io/k8s-staging-slack-infra/slack-post-message:v20200901-117c06f\n        args:\n        - --config-path=/etc/slack-post-message/config.json\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: PATH_PREFIX\n          value: /infra/post-message\n        volumeMounts:\n        - mountPath: /etc/slack-post-message\n          name: config\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTP\n            port: 8080\n      volumes:\n      - name: config\n        secret:\n          secretName: slack-post-message-config\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6302",
    "manifest_path": "data/manifests/the_stack_sample/sample_2258.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slack-post-message\n  labels:\n    app: slack-post-message\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: slack-post-message\n  template:\n    metadata:\n      labels:\n        app: slack-post-message\n    spec:\n      containers:\n      - name: slack-post-message\n        image: gcr.io/k8s-staging-slack-infra/slack-post-message:v20200901-117c06f\n        args:\n        - --config-path=/etc/slack-post-message/config.json\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: PATH_PREFIX\n          value: /infra/post-message\n        volumeMounts:\n        - mountPath: /etc/slack-post-message\n          name: config\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTP\n            port: 8080\n      volumes:\n      - name: config\n        secret:\n          secretName: slack-post-message-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"slack-post-message\" does not have a read-only root file system"
  },
  {
    "id": "6303",
    "manifest_path": "data/manifests/the_stack_sample/sample_2258.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slack-post-message\n  labels:\n    app: slack-post-message\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: slack-post-message\n  template:\n    metadata:\n      labels:\n        app: slack-post-message\n    spec:\n      containers:\n      - name: slack-post-message\n        image: gcr.io/k8s-staging-slack-infra/slack-post-message:v20200901-117c06f\n        args:\n        - --config-path=/etc/slack-post-message/config.json\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: PATH_PREFIX\n          value: /infra/post-message\n        volumeMounts:\n        - mountPath: /etc/slack-post-message\n          name: config\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTP\n            port: 8080\n      volumes:\n      - name: config\n        secret:\n          secretName: slack-post-message-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"slack-post-message\" is not set to runAsNonRoot"
  },
  {
    "id": "6304",
    "manifest_path": "data/manifests/the_stack_sample/sample_2258.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slack-post-message\n  labels:\n    app: slack-post-message\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: slack-post-message\n  template:\n    metadata:\n      labels:\n        app: slack-post-message\n    spec:\n      containers:\n      - name: slack-post-message\n        image: gcr.io/k8s-staging-slack-infra/slack-post-message:v20200901-117c06f\n        args:\n        - --config-path=/etc/slack-post-message/config.json\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: PATH_PREFIX\n          value: /infra/post-message\n        volumeMounts:\n        - mountPath: /etc/slack-post-message\n          name: config\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTP\n            port: 8080\n      volumes:\n      - name: config\n        secret:\n          secretName: slack-post-message-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"slack-post-message\" has cpu request 0"
  },
  {
    "id": "6305",
    "manifest_path": "data/manifests/the_stack_sample/sample_2258.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slack-post-message\n  labels:\n    app: slack-post-message\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: slack-post-message\n  template:\n    metadata:\n      labels:\n        app: slack-post-message\n    spec:\n      containers:\n      - name: slack-post-message\n        image: gcr.io/k8s-staging-slack-infra/slack-post-message:v20200901-117c06f\n        args:\n        - --config-path=/etc/slack-post-message/config.json\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: PATH_PREFIX\n          value: /infra/post-message\n        volumeMounts:\n        - mountPath: /etc/slack-post-message\n          name: config\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTP\n            port: 8080\n      volumes:\n      - name: config\n        secret:\n          secretName: slack-post-message-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"slack-post-message\" has memory limit 0"
  },
  {
    "id": "6306",
    "manifest_path": "data/manifests/the_stack_sample/sample_2259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: scc-broker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: scc-broker\n  template:\n    metadata:\n      labels:\n        component: scc-broker\n    spec:\n      containers:\n      - name: scc-broker\n        image: socketcluster/scc-broker:v6.0.1\n        ports:\n        - containerPort: 8888\n        env:\n        - name: SCC_STATE_SERVER_HOST\n          value: scc-state\n        - name: SOCKETCLUSTER_WORKERS\n          value: '1'\n        - name: SOCKETCLUSTER_BROKERS\n          value: '1'\n        - name: SCC_INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SCC_BROKER_SERVER_LOG_LEVEL\n          value: '2'\n        livenessProbe:\n          httpGet:\n            path: /health-check\n            port: 8888\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"scc-broker\" does not have a read-only root file system"
  },
  {
    "id": "6307",
    "manifest_path": "data/manifests/the_stack_sample/sample_2259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: scc-broker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: scc-broker\n  template:\n    metadata:\n      labels:\n        component: scc-broker\n    spec:\n      containers:\n      - name: scc-broker\n        image: socketcluster/scc-broker:v6.0.1\n        ports:\n        - containerPort: 8888\n        env:\n        - name: SCC_STATE_SERVER_HOST\n          value: scc-state\n        - name: SOCKETCLUSTER_WORKERS\n          value: '1'\n        - name: SOCKETCLUSTER_BROKERS\n          value: '1'\n        - name: SCC_INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SCC_BROKER_SERVER_LOG_LEVEL\n          value: '2'\n        livenessProbe:\n          httpGet:\n            path: /health-check\n            port: 8888\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"scc-broker\" is not set to runAsNonRoot"
  },
  {
    "id": "6308",
    "manifest_path": "data/manifests/the_stack_sample/sample_2259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: scc-broker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: scc-broker\n  template:\n    metadata:\n      labels:\n        component: scc-broker\n    spec:\n      containers:\n      - name: scc-broker\n        image: socketcluster/scc-broker:v6.0.1\n        ports:\n        - containerPort: 8888\n        env:\n        - name: SCC_STATE_SERVER_HOST\n          value: scc-state\n        - name: SOCKETCLUSTER_WORKERS\n          value: '1'\n        - name: SOCKETCLUSTER_BROKERS\n          value: '1'\n        - name: SCC_INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SCC_BROKER_SERVER_LOG_LEVEL\n          value: '2'\n        livenessProbe:\n          httpGet:\n            path: /health-check\n            port: 8888\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"scc-broker\" has cpu request 0"
  },
  {
    "id": "6309",
    "manifest_path": "data/manifests/the_stack_sample/sample_2259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: scc-broker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: scc-broker\n  template:\n    metadata:\n      labels:\n        component: scc-broker\n    spec:\n      containers:\n      - name: scc-broker\n        image: socketcluster/scc-broker:v6.0.1\n        ports:\n        - containerPort: 8888\n        env:\n        - name: SCC_STATE_SERVER_HOST\n          value: scc-state\n        - name: SOCKETCLUSTER_WORKERS\n          value: '1'\n        - name: SOCKETCLUSTER_BROKERS\n          value: '1'\n        - name: SCC_INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SCC_BROKER_SERVER_LOG_LEVEL\n          value: '2'\n        livenessProbe:\n          httpGet:\n            path: /health-check\n            port: 8888\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"scc-broker\" has memory limit 0"
  },
  {
    "id": "6310",
    "manifest_path": "data/manifests/the_stack_sample/sample_2260.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.kubernetes.io/do-loadbalancer-enable-proxy-protocol: 'true'\n    service.beta.kubernetes.io/do-loadbalancer-hostname: 0.workaround.competitors.club\n  labels:\n    helm.sh/chart: ingress-nginx-2.11.1\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: 0.34.1\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\nspec:\n  type: LoadBalancer\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/component: controller\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:ingress-nginx app.kubernetes.io/name:ingress-nginx])"
  },
  {
    "id": "6311",
    "manifest_path": "data/manifests/the_stack_sample/sample_2261.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210602-c9da972437\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "6312",
    "manifest_path": "data/manifests/the_stack_sample/sample_2261.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210602-c9da972437\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"tide\" not found"
  },
  {
    "id": "6313",
    "manifest_path": "data/manifests/the_stack_sample/sample_2261.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210602-c9da972437\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "6314",
    "manifest_path": "data/manifests/the_stack_sample/sample_2261.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210602-c9da972437\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "6315",
    "manifest_path": "data/manifests/the_stack_sample/sample_2261.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210602-c9da972437\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "6316",
    "manifest_path": "data/manifests/the_stack_sample/sample_2263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: kubesphere\n    component: ks-sample-dev\n  name: ks-sample-dev\n  namespace: kubesphere-sample-dev\nspec:\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: 8089\n    nodePort: 30861\n  selector:\n    app: kubesphere\n    component: ks-sample-dev\n    tier: backend\n  sessionAffinity: None\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kubesphere component:ks-sample-dev tier:backend])"
  },
  {
    "id": "6317",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"od\" is using an invalid container image, \"gcr.io/instructor-partition/od-fasterrcnn\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6318",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 10 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6319",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"od\" does not have a read-only root file system"
  },
  {
    "id": "6320",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"od\" is not set to runAsNonRoot"
  },
  {
    "id": "6321",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"od\" has cpu request 0"
  },
  {
    "id": "6322",
    "manifest_path": "data/manifests/the_stack_sample/sample_2267.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: od-v1\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: od\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: od\n        version: v1\n        env: production\n    spec:\n      containers:\n      - name: od\n        image: gcr.io/instructor-partition/od-fasterrcnn\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"od\" has memory limit 0"
  },
  {
    "id": "6323",
    "manifest_path": "data/manifests/the_stack_sample/sample_2268.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    control-plane: controller-manager\n  name: metrics-service\n  namespace: system\nspec:\n  ports:\n  - name: https\n    port: 8443\n    protocol: TCP\n    targetPort: https\n  selector:\n    control-plane: controller-manager\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[control-plane:controller-manager])"
  },
  {
    "id": "6324",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"service\" is using an invalid container image, \"ghcr.io/drogue-iot/user-auth-service:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6325",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait-for-client-secret\" is using an invalid container image, \"registry.access.redhat.com/ubi8-minimal\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6326",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"service\" does not expose port 9090 for the HTTPGet"
  },
  {
    "id": "6327",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"service\" does not have a read-only root file system"
  },
  {
    "id": "6328",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-for-client-secret\" does not have a read-only root file system"
  },
  {
    "id": "6329",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"service\" does not expose port 9090 for the HTTPGet"
  },
  {
    "id": "6330",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"service\" is not set to runAsNonRoot"
  },
  {
    "id": "6331",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-for-client-secret\" is not set to runAsNonRoot"
  },
  {
    "id": "6332",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"service\" has cpu request 0"
  },
  {
    "id": "6333",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-for-client-secret\" has cpu request 0"
  },
  {
    "id": "6334",
    "manifest_path": "data/manifests/the_stack_sample/sample_2270.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-auth-service\n  annotations:\n    app.openshift.io/connects-to: '[{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"postgres\"}]'\n  labels:\n    app.kubernetes.io/name: user-auth-service\n    app.kubernetes.io/part-of: device-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: user-auth-service\n      app.kubernetes.io/part-of: device-registry\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: user-auth-service\n        app.kubernetes.io/part-of: device-registry\n    spec:\n      initContainers:\n      - name: wait-for-client-secret\n        image: registry.access.redhat.com/ubi8-minimal\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        - \"echo \\\"Waiting for client secret to be populated (/etc/client-secret/CLIENT_SECRET)...\\\"\\\n          \\nwhile test -z \\\"$(cat /etc/client-secret/CLIENT_SECRET)\\\"; do\\n  sleep\\\n          \\ 1\\ndone\\n\"\n        volumeMounts:\n        - mountPath: /etc/client-secret\n          name: client-secret\n          readOnly: true\n      containers:\n      - name: service\n        image: ghcr.io/drogue-iot/user-auth-service:latest\n        imagePullPolicy: Always\n        env:\n        - name: RUST_LOG\n          value: debug\n        - name: BIND_ADDR\n          value: 0.0.0.0:8080\n        - name: HEALTH__BIND_ADDR\n          value: 0.0.0.0:9090\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SERVICE__PG__HOST\n          value: postgres\n        - name: SERVICE__PG__DBNAME\n          valueFrom:\n            configMapKeyRef:\n              name: postgres-config\n              key: databaseName\n        - name: SERVICE__PG__USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.username\n        - name: SERVICE__PG__PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: admin.password\n        - name: OAUTH__SERVICES__CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_ID\n        - name: OAUTH__SERVICES__CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: keycloak-client-secret-services\n              key: CLIENT_SECRET\n        - name: KEYCLOAK__URL\n          value: https://keycloak.$(NAMESPACE).svc.cluster.local.:8443\n        - name: KEYCLOAK__ADMIN_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_USERNAME\n              name: credential-sso\n        - name: KEYCLOAK__ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: ADMIN_PASSWORD\n              name: credential-sso\n        readinessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /readiness\n        livenessProbe:\n          initialDelaySeconds: 2\n          periodSeconds: 1\n          timeoutSeconds: 1\n          failureThreshold: 3\n          httpGet:\n            port: 9090\n            path: /liveness\n        ports:\n        - containerPort: 8080\n          name: api\n          protocol: TCP\n        resources:\n          limits:\n            memory: 128Mi\n      volumes:\n      - name: client-secret\n        secret:\n          secretName: keycloak-client-secret-services\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-for-client-secret\" has memory limit 0"
  },
  {
    "id": "6335",
    "manifest_path": "data/manifests/the_stack_sample/sample_2274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4427\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6336",
    "manifest_path": "data/manifests/the_stack_sample/sample_2274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4427\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6337",
    "manifest_path": "data/manifests/the_stack_sample/sample_2274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4427\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6338",
    "manifest_path": "data/manifests/the_stack_sample/sample_2274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4427\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6339",
    "manifest_path": "data/manifests/the_stack_sample/sample_2274.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4427\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6340",
    "manifest_path": "data/manifests/the_stack_sample/sample_2275.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-keycloak-integration\n  namespace: sso-integration\nspec:\n  template:\n    spec:\n      containers:\n      - name: cluster-keycloak-integration\n        image: quay.io/leoliu2011/cluster-keycloak-integration:v1\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: argocd-configs\n        - secretRef:\n            name: sso-configs\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6341",
    "manifest_path": "data/manifests/the_stack_sample/sample_2275.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-keycloak-integration\n  namespace: sso-integration\nspec:\n  template:\n    spec:\n      containers:\n      - name: cluster-keycloak-integration\n        image: quay.io/leoliu2011/cluster-keycloak-integration:v1\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: argocd-configs\n        - secretRef:\n            name: sso-configs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cluster-keycloak-integration\" does not have a read-only root file system"
  },
  {
    "id": "6342",
    "manifest_path": "data/manifests/the_stack_sample/sample_2275.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-keycloak-integration\n  namespace: sso-integration\nspec:\n  template:\n    spec:\n      containers:\n      - name: cluster-keycloak-integration\n        image: quay.io/leoliu2011/cluster-keycloak-integration:v1\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: argocd-configs\n        - secretRef:\n            name: sso-configs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cluster-keycloak-integration\" is not set to runAsNonRoot"
  },
  {
    "id": "6343",
    "manifest_path": "data/manifests/the_stack_sample/sample_2275.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-keycloak-integration\n  namespace: sso-integration\nspec:\n  template:\n    spec:\n      containers:\n      - name: cluster-keycloak-integration\n        image: quay.io/leoliu2011/cluster-keycloak-integration:v1\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: argocd-configs\n        - secretRef:\n            name: sso-configs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cluster-keycloak-integration\" has cpu request 0"
  },
  {
    "id": "6344",
    "manifest_path": "data/manifests/the_stack_sample/sample_2275.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-keycloak-integration\n  namespace: sso-integration\nspec:\n  template:\n    spec:\n      containers:\n      - name: cluster-keycloak-integration\n        image: quay.io/leoliu2011/cluster-keycloak-integration:v1\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: argocd-configs\n        - secretRef:\n            name: sso-configs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cluster-keycloak-integration\" has memory limit 0"
  },
  {
    "id": "6345",
    "manifest_path": "data/manifests/the_stack_sample/sample_2277.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: willyrjpipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: willyrjpipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: willyrjpipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: willyrjpipelinesjavascriptdocker\n        image: kubecore.azurecr.io/willyrjpipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"willyrjpipelinesjavascriptdocker\" is using an invalid container image, \"kubecore.azurecr.io/willyrjpipelinesjavascriptdocker\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6346",
    "manifest_path": "data/manifests/the_stack_sample/sample_2277.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: willyrjpipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: willyrjpipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: willyrjpipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: willyrjpipelinesjavascriptdocker\n        image: kubecore.azurecr.io/willyrjpipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"willyrjpipelinesjavascriptdocker\" does not have a read-only root file system"
  },
  {
    "id": "6347",
    "manifest_path": "data/manifests/the_stack_sample/sample_2277.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: willyrjpipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: willyrjpipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: willyrjpipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: willyrjpipelinesjavascriptdocker\n        image: kubecore.azurecr.io/willyrjpipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"willyrjpipelinesjavascriptdocker\" is not set to runAsNonRoot"
  },
  {
    "id": "6348",
    "manifest_path": "data/manifests/the_stack_sample/sample_2277.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: willyrjpipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: willyrjpipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: willyrjpipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: willyrjpipelinesjavascriptdocker\n        image: kubecore.azurecr.io/willyrjpipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"willyrjpipelinesjavascriptdocker\" has cpu request 0"
  },
  {
    "id": "6349",
    "manifest_path": "data/manifests/the_stack_sample/sample_2277.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: willyrjpipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: willyrjpipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: willyrjpipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: willyrjpipelinesjavascriptdocker\n        image: kubecore.azurecr.io/willyrjpipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"willyrjpipelinesjavascriptdocker\" has memory limit 0"
  },
  {
    "id": "6350",
    "manifest_path": "data/manifests/the_stack_sample/sample_2279.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20211012-1d1a69807d\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prow-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "6351",
    "manifest_path": "data/manifests/the_stack_sample/sample_2279.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20211012-1d1a69807d\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prow-controller-manager\" not found"
  },
  {
    "id": "6352",
    "manifest_path": "data/manifests/the_stack_sample/sample_2279.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20211012-1d1a69807d\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prow-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "6353",
    "manifest_path": "data/manifests/the_stack_sample/sample_2279.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20211012-1d1a69807d\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prow-controller-manager\" has cpu request 0"
  },
  {
    "id": "6354",
    "manifest_path": "data/manifests/the_stack_sample/sample_2279.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20211012-1d1a69807d\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prow-controller-manager\" has memory limit 0"
  },
  {
    "id": "6355",
    "manifest_path": "data/manifests/the_stack_sample/sample_2282.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5929\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6356",
    "manifest_path": "data/manifests/the_stack_sample/sample_2282.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5929\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6357",
    "manifest_path": "data/manifests/the_stack_sample/sample_2282.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5929\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6358",
    "manifest_path": "data/manifests/the_stack_sample/sample_2282.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5929\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6359",
    "manifest_path": "data/manifests/the_stack_sample/sample_2282.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5929\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6360",
    "manifest_path": "data/manifests/the_stack_sample/sample_2283.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nspec:\n  template:\n    spec:\n      serviceAccountName: agones-controller\n      containers:\n      - name: create-gameserver\n        image: gcr.io/agones-images/crd-client:0.4\n        imagePullPolicy: Always\n        env:\n        - name: GAMESERVER_IMAGE\n          value: gcr.io/agones-images/simple-game-server:0.1\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6361",
    "manifest_path": "data/manifests/the_stack_sample/sample_2283.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nspec:\n  template:\n    spec:\n      serviceAccountName: agones-controller\n      containers:\n      - name: create-gameserver\n        image: gcr.io/agones-images/crd-client:0.4\n        imagePullPolicy: Always\n        env:\n        - name: GAMESERVER_IMAGE\n          value: gcr.io/agones-images/simple-game-server:0.1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"create-gameserver\" does not have a read-only root file system"
  },
  {
    "id": "6362",
    "manifest_path": "data/manifests/the_stack_sample/sample_2283.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nspec:\n  template:\n    spec:\n      serviceAccountName: agones-controller\n      containers:\n      - name: create-gameserver\n        image: gcr.io/agones-images/crd-client:0.4\n        imagePullPolicy: Always\n        env:\n        - name: GAMESERVER_IMAGE\n          value: gcr.io/agones-images/simple-game-server:0.1\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"agones-controller\" not found"
  },
  {
    "id": "6363",
    "manifest_path": "data/manifests/the_stack_sample/sample_2283.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nspec:\n  template:\n    spec:\n      serviceAccountName: agones-controller\n      containers:\n      - name: create-gameserver\n        image: gcr.io/agones-images/crd-client:0.4\n        imagePullPolicy: Always\n        env:\n        - name: GAMESERVER_IMAGE\n          value: gcr.io/agones-images/simple-game-server:0.1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"create-gameserver\" is not set to runAsNonRoot"
  },
  {
    "id": "6364",
    "manifest_path": "data/manifests/the_stack_sample/sample_2283.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nspec:\n  template:\n    spec:\n      serviceAccountName: agones-controller\n      containers:\n      - name: create-gameserver\n        image: gcr.io/agones-images/crd-client:0.4\n        imagePullPolicy: Always\n        env:\n        - name: GAMESERVER_IMAGE\n          value: gcr.io/agones-images/simple-game-server:0.1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"create-gameserver\" has cpu request 0"
  },
  {
    "id": "6365",
    "manifest_path": "data/manifests/the_stack_sample/sample_2283.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nspec:\n  template:\n    spec:\n      serviceAccountName: agones-controller\n      containers:\n      - name: create-gameserver\n        image: gcr.io/agones-images/crd-client:0.4\n        imagePullPolicy: Always\n        env:\n        - name: GAMESERVER_IMAGE\n          value: gcr.io/agones-images/simple-game-server:0.1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"create-gameserver\" has memory limit 0"
  },
  {
    "id": "6366",
    "manifest_path": "data/manifests/the_stack_sample/sample_2285.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-385\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6367",
    "manifest_path": "data/manifests/the_stack_sample/sample_2285.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-385\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6368",
    "manifest_path": "data/manifests/the_stack_sample/sample_2285.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-385\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6369",
    "manifest_path": "data/manifests/the_stack_sample/sample_2285.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-385\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6370",
    "manifest_path": "data/manifests/the_stack_sample/sample_2285.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-385\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6371",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable CLIENT_SECRET in container \"authservice\" found"
  },
  {
    "id": "6372",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"fix-permission\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6373",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"authservice\" does not have a read-only root file system"
  },
  {
    "id": "6374",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fix-permission\" does not have a read-only root file system"
  },
  {
    "id": "6375",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"authservice\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "6376",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"authservice\" is not set to runAsNonRoot"
  },
  {
    "id": "6377",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fix-permission\" is not set to runAsNonRoot"
  },
  {
    "id": "6378",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"authservice\" has cpu request 0"
  },
  {
    "id": "6379",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fix-permission\" has cpu request 0"
  },
  {
    "id": "6380",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"authservice\" has memory limit 0"
  },
  {
    "id": "6381",
    "manifest_path": "data/manifests/the_stack_sample/sample_2287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: authservice\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: authservice\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: authservice\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          value: kubeflow-userid\n        - name: USERID_PREFIX\n          value: ''\n        - name: USERID_CLAIM\n          value: email\n        - name: OIDC_PROVIDER\n          value: http://dex.auth.svc.cluster.local:5556/dex\n        - name: OIDC_AUTH_URL\n          value: /dex/auth\n        - name: OIDC_SCOPES\n          value: profile email groups\n        - name: REDIRECT_URL\n          value: /login/oidc\n        - name: SKIP_AUTH_URI\n          value: /dex\n        - name: PORT\n          value: '8080'\n        - name: CLIENT_ID\n          value: kubeflow-oidc-authservice\n        - name: CLIENT_SECRET\n          value: pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok\n        - name: STORE_PATH\n          value: /var/lib/authservice/data.db\n        image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef\n        imagePullPolicy: Always\n        name: authservice\n        ports:\n        - containerPort: 8080\n          name: http-api\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      initContainers:\n      - args:\n        - chmod -R 777 /var/lib/authservice;\n        command:\n        - sh\n        - -c\n        image: busybox:latest\n        name: fix-permission\n        volumeMounts:\n        - mountPath: /var/lib/authservice\n          name: data\n      securityContext:\n        fsGroup: 111\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: authservice-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fix-permission\" has memory limit 0"
  },
  {
    "id": "6382",
    "manifest_path": "data/manifests/the_stack_sample/sample_2288.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: zipkin\n  name: zipkin\n  namespace: kube-system\nspec:\n  ports:\n  - port: 9411\n    protocol: TCP\n    targetPort: 9411\n  selector:\n    k8s-app: zipkin\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:zipkin])"
  },
  {
    "id": "6383",
    "manifest_path": "data/manifests/the_stack_sample/sample_2291.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: url-shorten-mongodb\nspec:\n  type: ClusterIP\n  ports:\n  - name: '27017'\n    port: 27017\n    targetPort: 27017\n  selector:\n    service: url-shorten-mongodb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:url-shorten-mongodb])"
  },
  {
    "id": "6384",
    "manifest_path": "data/manifests/the_stack_sample/sample_2294.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: rabbitmq\n  namespace: airflow\nspec:\n  selector:\n    app: rabbitmq\n  ports:\n  - port: 5672\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:rabbitmq])"
  },
  {
    "id": "6385",
    "manifest_path": "data/manifests/the_stack_sample/sample_2296.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: gwcloud-auth\n  name: gwcloud-auth\n  namespace: gwcloud\nspec:\n  ports:\n  - port: 8000\n    protocol: TCP\n    targetPort: 8000\n  selector:\n    name: gwcloud-auth\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:gwcloud-auth])"
  },
  {
    "id": "6386",
    "manifest_path": "data/manifests/the_stack_sample/sample_2297.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeder-qualifiers-kprca00053\n  labels:\n    type: cyborg-seeder\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeder-qualifiers-kprca00053\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeder.py qualifiers KPRCA_00053\n      3600 6\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cyborg-seeder-qualifiers-kprca00053\" does not have a read-only root file system"
  },
  {
    "id": "6387",
    "manifest_path": "data/manifests/the_stack_sample/sample_2297.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeder-qualifiers-kprca00053\n  labels:\n    type: cyborg-seeder\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeder-qualifiers-kprca00053\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeder.py qualifiers KPRCA_00053\n      3600 6\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cyborg-seeder-qualifiers-kprca00053\" is not set to runAsNonRoot"
  },
  {
    "id": "6388",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "6389",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"otel-collector\" does not expose port 13133 for the HTTPGet"
  },
  {
    "id": "6390",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"otel-collector\" does not have a read-only root file system"
  },
  {
    "id": "6391",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"default-splunk-otel-collector\" not found"
  },
  {
    "id": "6392",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"otel-collector\" does not expose port 13133 for the HTTPGet"
  },
  {
    "id": "6393",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"otel-collector\" is not set to runAsNonRoot"
  },
  {
    "id": "6394",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"otel-collector\""
  },
  {
    "id": "6395",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/etc\" is mounted on container \"otel-collector\""
  },
  {
    "id": "6396",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"otel-collector\""
  },
  {
    "id": "6397",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"otel-collector\""
  },
  {
    "id": "6398",
    "manifest_path": "data/manifests/the_stack_sample/sample_2300.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app.kubernetes.io/name: splunk-otel-collector\n    helm.sh/chart: splunk-otel-collector-0.39.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: 0.39.0\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.39.0\n    release: default\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 2cd824e416ad60f3693d58ae69703e123c886271e7d5c3599b8118e6be316f59\n        kubectl.kubernetes.io/default-container: otel-collector\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      containers:\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8889\n        ports:\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: otlp-http\n          containerPort: 4318\n          protocol: TCP\n        - name: otlp-http-old\n          containerPort: 55681\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.39.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_observability_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs/dev\n          name: host-dev\n          readOnly: true\n        - mountPath: /hostfs/etc\n          name: host-etc\n          readOnly: true\n        - mountPath: /hostfs/proc\n          name: host-proc\n          readOnly: true\n        - mountPath: /hostfs/run/udev/data\n          name: host-run-udev-data\n          readOnly: true\n        - mountPath: /hostfs/sys\n          name: host-sys\n          readOnly: true\n        - mountPath: /hostfs/var/run/utmp\n          name: host-var-run-utmp\n          readOnly: true\n      volumes:\n      - name: host-dev\n        hostPath:\n          path: /dev\n      - name: host-etc\n        hostPath:\n          path: /etc\n      - name: host-proc\n        hostPath:\n          path: /proc\n      - name: host-run-udev-data\n        hostPath:\n          path: /run/udev/data\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: host-var-run-utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"otel-collector\" has cpu request 0"
  },
  {
    "id": "6399",
    "manifest_path": "data/manifests/the_stack_sample/sample_2307.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: loggenerator\nspec:\n  containers:\n  - name: loggenerator\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - i=0; while true; do echo \"Kubernetes Fundamentals $i\"; i=$((i+1)); sleep 1;\n      done\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"loggenerator\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6400",
    "manifest_path": "data/manifests/the_stack_sample/sample_2307.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: loggenerator\nspec:\n  containers:\n  - name: loggenerator\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - i=0; while true; do echo \"Kubernetes Fundamentals $i\"; i=$((i+1)); sleep 1;\n      done\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"loggenerator\" does not have a read-only root file system"
  },
  {
    "id": "6401",
    "manifest_path": "data/manifests/the_stack_sample/sample_2307.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: loggenerator\nspec:\n  containers:\n  - name: loggenerator\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - i=0; while true; do echo \"Kubernetes Fundamentals $i\"; i=$((i+1)); sleep 1;\n      done\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"loggenerator\" is not set to runAsNonRoot"
  },
  {
    "id": "6402",
    "manifest_path": "data/manifests/the_stack_sample/sample_2307.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: loggenerator\nspec:\n  containers:\n  - name: loggenerator\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - i=0; while true; do echo \"Kubernetes Fundamentals $i\"; i=$((i+1)); sleep 1;\n      done\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"loggenerator\" has cpu request 0"
  },
  {
    "id": "6403",
    "manifest_path": "data/manifests/the_stack_sample/sample_2307.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: loggenerator\nspec:\n  containers:\n  - name: loggenerator\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - i=0; while true; do echo \"Kubernetes Fundamentals $i\"; i=$((i+1)); sleep 1;\n      done\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"loggenerator\" has memory limit 0"
  },
  {
    "id": "6404",
    "manifest_path": "data/manifests/the_stack_sample/sample_2309.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Yo, welcome to Flux!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "6405",
    "manifest_path": "data/manifests/the_stack_sample/sample_2309.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Yo, welcome to Flux!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "6406",
    "manifest_path": "data/manifests/the_stack_sample/sample_2309.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Yo, welcome to Flux!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "6407",
    "manifest_path": "data/manifests/the_stack_sample/sample_2309.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Yo, welcome to Flux!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "6408",
    "manifest_path": "data/manifests/the_stack_sample/sample_2309.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Yo, welcome to Flux!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "6409",
    "manifest_path": "data/manifests/the_stack_sample/sample_2309.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: regex:^3.10.*\n    flux.weave.works/tag.podinfod: semver:~2.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:2.1.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_MESSAGE\n          value: Yo, welcome to Flux!\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "6410",
    "manifest_path": "data/manifests/the_stack_sample/sample_2312.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v5\n  namespace: default\n  labels:\n    k8s-app: heapster\n    version: v5\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v5\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v5\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.15.0\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "6411",
    "manifest_path": "data/manifests/the_stack_sample/sample_2312.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v5\n  namespace: default\n  labels:\n    k8s-app: heapster\n    version: v5\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v5\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v5\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.15.0\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "6412",
    "manifest_path": "data/manifests/the_stack_sample/sample_2312.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v5\n  namespace: default\n  labels:\n    k8s-app: heapster\n    version: v5\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v5\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v5\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.15.0\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 200Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=influxdb:http://monitoring-influxdb:8086\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"heapster\" has cpu request 0"
  },
  {
    "id": "6413",
    "manifest_path": "data/manifests/the_stack_sample/sample_2314.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: my-nginx\n  name: my-nginx\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: my-nginx\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - image: nginx:1.17.5\n        name: my-nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6414",
    "manifest_path": "data/manifests/the_stack_sample/sample_2314.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: my-nginx\n  name: my-nginx\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: my-nginx\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - image: nginx:1.17.5\n        name: my-nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"my-nginx\" does not have a read-only root file system"
  },
  {
    "id": "6415",
    "manifest_path": "data/manifests/the_stack_sample/sample_2314.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: my-nginx\n  name: my-nginx\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: my-nginx\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - image: nginx:1.17.5\n        name: my-nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"my-nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6416",
    "manifest_path": "data/manifests/the_stack_sample/sample_2314.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: my-nginx\n  name: my-nginx\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: my-nginx\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - image: nginx:1.17.5\n        name: my-nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"my-nginx\" has cpu request 0"
  },
  {
    "id": "6417",
    "manifest_path": "data/manifests/the_stack_sample/sample_2314.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: my-nginx\n  name: my-nginx\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      run: my-nginx\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - image: nginx:1.17.5\n        name: my-nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"my-nginx\" has memory limit 0"
  },
  {
    "id": "6418",
    "manifest_path": "data/manifests/the_stack_sample/sample_2316.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: hainan-oa-biz\n    app.kubernetes.io/name: hainan-biz\n    app.kubernetes.io/version: v1\n  name: hainan-oa-biz\n  namespace: hainan-prod\nspec:\n  ports:\n  - name: tcp-8080\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: hainan-oa-biz\n    app.kubernetes.io/name: hainan-biz\n    app.kubernetes.io/version: v1\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:hainan-oa-biz app.kubernetes.io/name:hainan-biz app.kubernetes.io/version:v1])"
  },
  {
    "id": "6419",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql\" does not have a read-only root file system"
  },
  {
    "id": "6420",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql-datagen-pageviews\" does not have a read-only root file system"
  },
  {
    "id": "6421",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql-datagen-users\" does not have a read-only root file system"
  },
  {
    "id": "6422",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql\" is not set to runAsNonRoot"
  },
  {
    "id": "6423",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql-datagen-pageviews\" is not set to runAsNonRoot"
  },
  {
    "id": "6424",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql-datagen-users\" is not set to runAsNonRoot"
  },
  {
    "id": "6425",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql\" has cpu request 0"
  },
  {
    "id": "6426",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql-datagen-pageviews\" has cpu request 0"
  },
  {
    "id": "6427",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql-datagen-users\" has cpu request 0"
  },
  {
    "id": "6428",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql\" has memory limit 0"
  },
  {
    "id": "6429",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql-datagen-pageviews\" has memory limit 0"
  },
  {
    "id": "6430",
    "manifest_path": "data/manifests/the_stack_sample/sample_2318.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql-datagen-users\" has memory limit 0"
  },
  {
    "id": "6431",
    "manifest_path": "data/manifests/the_stack_sample/sample_2319.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: bpftrace\nspec:\n  selector:\n    matchLabels:\n      name: bpftrace\n  template:\n    metadata:\n      labels:\n        name: bpftrace\n    spec:\n      containers:\n      - name: bpftrace\n        image: gcr.io/arroyod-162523/bpftrace:cos-69-10895-123-0\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 600m\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: host-sys\n          mountPath: /sys\n        - name: dev-console\n          mountPath: /dev/console\n      volumes:\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: dev-console\n        hostPath:\n          path: /dev/console\n",
    "policy_id": "host-ipc",
    "violation_text": "resource shares host's IPC namespace (via hostIPC=true)."
  },
  {
    "id": "6432",
    "manifest_path": "data/manifests/the_stack_sample/sample_2319.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: bpftrace\nspec:\n  selector:\n    matchLabels:\n      name: bpftrace\n  template:\n    metadata:\n      labels:\n        name: bpftrace\n    spec:\n      containers:\n      - name: bpftrace\n        image: gcr.io/arroyod-162523/bpftrace:cos-69-10895-123-0\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 600m\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: host-sys\n          mountPath: /sys\n        - name: dev-console\n          mountPath: /dev/console\n      volumes:\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: dev-console\n        hostPath:\n          path: /dev/console\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "6433",
    "manifest_path": "data/manifests/the_stack_sample/sample_2319.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: bpftrace\nspec:\n  selector:\n    matchLabels:\n      name: bpftrace\n  template:\n    metadata:\n      labels:\n        name: bpftrace\n    spec:\n      containers:\n      - name: bpftrace\n        image: gcr.io/arroyod-162523/bpftrace:cos-69-10895-123-0\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 600m\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: host-sys\n          mountPath: /sys\n        - name: dev-console\n          mountPath: /dev/console\n      volumes:\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: dev-console\n        hostPath:\n          path: /dev/console\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "6434",
    "manifest_path": "data/manifests/the_stack_sample/sample_2319.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: bpftrace\nspec:\n  selector:\n    matchLabels:\n      name: bpftrace\n  template:\n    metadata:\n      labels:\n        name: bpftrace\n    spec:\n      containers:\n      - name: bpftrace\n        image: gcr.io/arroyod-162523/bpftrace:cos-69-10895-123-0\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 600m\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: host-sys\n          mountPath: /sys\n        - name: dev-console\n          mountPath: /dev/console\n      volumes:\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: dev-console\n        hostPath:\n          path: /dev/console\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"bpftrace\" does not have a read-only root file system"
  },
  {
    "id": "6435",
    "manifest_path": "data/manifests/the_stack_sample/sample_2319.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: bpftrace\nspec:\n  selector:\n    matchLabels:\n      name: bpftrace\n  template:\n    metadata:\n      labels:\n        name: bpftrace\n    spec:\n      containers:\n      - name: bpftrace\n        image: gcr.io/arroyod-162523/bpftrace:cos-69-10895-123-0\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 600m\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: host-sys\n          mountPath: /sys\n        - name: dev-console\n          mountPath: /dev/console\n      volumes:\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: dev-console\n        hostPath:\n          path: /dev/console\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"bpftrace\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "6436",
    "manifest_path": "data/manifests/the_stack_sample/sample_2319.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: bpftrace\nspec:\n  selector:\n    matchLabels:\n      name: bpftrace\n  template:\n    metadata:\n      labels:\n        name: bpftrace\n    spec:\n      containers:\n      - name: bpftrace\n        image: gcr.io/arroyod-162523/bpftrace:cos-69-10895-123-0\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 600m\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: host-sys\n          mountPath: /sys\n        - name: dev-console\n          mountPath: /dev/console\n      volumes:\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: dev-console\n        hostPath:\n          path: /dev/console\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"bpftrace\" is privileged"
  },
  {
    "id": "6437",
    "manifest_path": "data/manifests/the_stack_sample/sample_2319.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: bpftrace\nspec:\n  selector:\n    matchLabels:\n      name: bpftrace\n  template:\n    metadata:\n      labels:\n        name: bpftrace\n    spec:\n      containers:\n      - name: bpftrace\n        image: gcr.io/arroyod-162523/bpftrace:cos-69-10895-123-0\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 600m\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: host-sys\n          mountPath: /sys\n        - name: dev-console\n          mountPath: /dev/console\n      volumes:\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: dev-console\n        hostPath:\n          path: /dev/console\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"bpftrace\" is not set to runAsNonRoot"
  },
  {
    "id": "6438",
    "manifest_path": "data/manifests/the_stack_sample/sample_2319.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: bpftrace\nspec:\n  selector:\n    matchLabels:\n      name: bpftrace\n  template:\n    metadata:\n      labels:\n        name: bpftrace\n    spec:\n      containers:\n      - name: bpftrace\n        image: gcr.io/arroyod-162523/bpftrace:cos-69-10895-123-0\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 600m\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: host-sys\n          mountPath: /sys\n        - name: dev-console\n          mountPath: /dev/console\n      volumes:\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: dev-console\n        hostPath:\n          path: /dev/console\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"bpftrace\""
  },
  {
    "id": "6439",
    "manifest_path": "data/manifests/the_stack_sample/sample_2319.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: bpftrace\nspec:\n  selector:\n    matchLabels:\n      name: bpftrace\n  template:\n    metadata:\n      labels:\n        name: bpftrace\n    spec:\n      containers:\n      - name: bpftrace\n        image: gcr.io/arroyod-162523/bpftrace:cos-69-10895-123-0\n        command:\n        - tail\n        - -f\n        - /dev/null\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 600m\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: host-sys\n          mountPath: /sys\n        - name: dev-console\n          mountPath: /dev/console\n      volumes:\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: dev-console\n        hostPath:\n          path: /dev/console\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"bpftrace\" has memory limit 0"
  },
  {
    "id": "6440",
    "manifest_path": "data/manifests/the_stack_sample/sample_2322.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  type: ClusterIP\n  selector:\n    app: k3s-cluster-docs\n  ports:\n  - name: http\n    port: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:k3s-cluster-docs])"
  },
  {
    "id": "6441",
    "manifest_path": "data/manifests/the_stack_sample/sample_2323.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: '{{ .Release.Name }}-job'\n  namespace: '{{ .Values.namespace }}'\n  labels:\n    type: pipeline\n    experimentId: '{{ .Values.experimentId }}'\n    sandboxId: '{{ .Values.sandboxId }}'\nspec:\n  template:\n    metadata:\n      name: '{{ .Release.Name }}-server'\n      labels:\n        type: pipeline\n        experimentId: '{{ .Values.experimentId }}'\n        sandboxId: '{{ .Values.sandboxId }}'\n    spec:\n      serviceAccountName: deployment-runner\n      containers:\n      - name: '{{ .Release.Name }}'\n        image: '{{ .Values.image }}'\n        env:\n        - name: CLUSTER_ENV\n          value: '{{ .Values.clusterEnv }}'\n        - name: SANDBOX_ID\n          value: '{{ .Values.sandboxId }}'\n        - name: AWS_ACCOUNT_ID\n          value: '{{ .Values.awsAccountId }}'\n        - name: AWS_DEFAULT_REGION\n          value: '{{ .Values.awsRegion }}'\n        - name: ACTIVITY_ARN\n          value: '{{ .Values.activityArn }}'\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6442",
    "manifest_path": "data/manifests/the_stack_sample/sample_2323.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: '{{ .Release.Name }}-job'\n  namespace: '{{ .Values.namespace }}'\n  labels:\n    type: pipeline\n    experimentId: '{{ .Values.experimentId }}'\n    sandboxId: '{{ .Values.sandboxId }}'\nspec:\n  template:\n    metadata:\n      name: '{{ .Release.Name }}-server'\n      labels:\n        type: pipeline\n        experimentId: '{{ .Values.experimentId }}'\n        sandboxId: '{{ .Values.sandboxId }}'\n    spec:\n      serviceAccountName: deployment-runner\n      containers:\n      - name: '{{ .Release.Name }}'\n        image: '{{ .Values.image }}'\n        env:\n        - name: CLUSTER_ENV\n          value: '{{ .Values.clusterEnv }}'\n        - name: SANDBOX_ID\n          value: '{{ .Values.sandboxId }}'\n        - name: AWS_ACCOUNT_ID\n          value: '{{ .Values.awsAccountId }}'\n        - name: AWS_DEFAULT_REGION\n          value: '{{ .Values.awsRegion }}'\n        - name: ACTIVITY_ARN\n          value: '{{ .Values.activityArn }}'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"{{ .Release.Name }}\" is using an invalid container image, \"{{ .Values.image }}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6443",
    "manifest_path": "data/manifests/the_stack_sample/sample_2323.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: '{{ .Release.Name }}-job'\n  namespace: '{{ .Values.namespace }}'\n  labels:\n    type: pipeline\n    experimentId: '{{ .Values.experimentId }}'\n    sandboxId: '{{ .Values.sandboxId }}'\nspec:\n  template:\n    metadata:\n      name: '{{ .Release.Name }}-server'\n      labels:\n        type: pipeline\n        experimentId: '{{ .Values.experimentId }}'\n        sandboxId: '{{ .Values.sandboxId }}'\n    spec:\n      serviceAccountName: deployment-runner\n      containers:\n      - name: '{{ .Release.Name }}'\n        image: '{{ .Values.image }}'\n        env:\n        - name: CLUSTER_ENV\n          value: '{{ .Values.clusterEnv }}'\n        - name: SANDBOX_ID\n          value: '{{ .Values.sandboxId }}'\n        - name: AWS_ACCOUNT_ID\n          value: '{{ .Values.awsAccountId }}'\n        - name: AWS_DEFAULT_REGION\n          value: '{{ .Values.awsRegion }}'\n        - name: ACTIVITY_ARN\n          value: '{{ .Values.activityArn }}'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"{{ .Release.Name }}\" does not have a read-only root file system"
  },
  {
    "id": "6444",
    "manifest_path": "data/manifests/the_stack_sample/sample_2323.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: '{{ .Release.Name }}-job'\n  namespace: '{{ .Values.namespace }}'\n  labels:\n    type: pipeline\n    experimentId: '{{ .Values.experimentId }}'\n    sandboxId: '{{ .Values.sandboxId }}'\nspec:\n  template:\n    metadata:\n      name: '{{ .Release.Name }}-server'\n      labels:\n        type: pipeline\n        experimentId: '{{ .Values.experimentId }}'\n        sandboxId: '{{ .Values.sandboxId }}'\n    spec:\n      serviceAccountName: deployment-runner\n      containers:\n      - name: '{{ .Release.Name }}'\n        image: '{{ .Values.image }}'\n        env:\n        - name: CLUSTER_ENV\n          value: '{{ .Values.clusterEnv }}'\n        - name: SANDBOX_ID\n          value: '{{ .Values.sandboxId }}'\n        - name: AWS_ACCOUNT_ID\n          value: '{{ .Values.awsAccountId }}'\n        - name: AWS_DEFAULT_REGION\n          value: '{{ .Values.awsRegion }}'\n        - name: ACTIVITY_ARN\n          value: '{{ .Values.activityArn }}'\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"deployment-runner\" not found"
  },
  {
    "id": "6445",
    "manifest_path": "data/manifests/the_stack_sample/sample_2323.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: '{{ .Release.Name }}-job'\n  namespace: '{{ .Values.namespace }}'\n  labels:\n    type: pipeline\n    experimentId: '{{ .Values.experimentId }}'\n    sandboxId: '{{ .Values.sandboxId }}'\nspec:\n  template:\n    metadata:\n      name: '{{ .Release.Name }}-server'\n      labels:\n        type: pipeline\n        experimentId: '{{ .Values.experimentId }}'\n        sandboxId: '{{ .Values.sandboxId }}'\n    spec:\n      serviceAccountName: deployment-runner\n      containers:\n      - name: '{{ .Release.Name }}'\n        image: '{{ .Values.image }}'\n        env:\n        - name: CLUSTER_ENV\n          value: '{{ .Values.clusterEnv }}'\n        - name: SANDBOX_ID\n          value: '{{ .Values.sandboxId }}'\n        - name: AWS_ACCOUNT_ID\n          value: '{{ .Values.awsAccountId }}'\n        - name: AWS_DEFAULT_REGION\n          value: '{{ .Values.awsRegion }}'\n        - name: ACTIVITY_ARN\n          value: '{{ .Values.activityArn }}'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"{{ .Release.Name }}\" is not set to runAsNonRoot"
  },
  {
    "id": "6446",
    "manifest_path": "data/manifests/the_stack_sample/sample_2323.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: '{{ .Release.Name }}-job'\n  namespace: '{{ .Values.namespace }}'\n  labels:\n    type: pipeline\n    experimentId: '{{ .Values.experimentId }}'\n    sandboxId: '{{ .Values.sandboxId }}'\nspec:\n  template:\n    metadata:\n      name: '{{ .Release.Name }}-server'\n      labels:\n        type: pipeline\n        experimentId: '{{ .Values.experimentId }}'\n        sandboxId: '{{ .Values.sandboxId }}'\n    spec:\n      serviceAccountName: deployment-runner\n      containers:\n      - name: '{{ .Release.Name }}'\n        image: '{{ .Values.image }}'\n        env:\n        - name: CLUSTER_ENV\n          value: '{{ .Values.clusterEnv }}'\n        - name: SANDBOX_ID\n          value: '{{ .Values.sandboxId }}'\n        - name: AWS_ACCOUNT_ID\n          value: '{{ .Values.awsAccountId }}'\n        - name: AWS_DEFAULT_REGION\n          value: '{{ .Values.awsRegion }}'\n        - name: ACTIVITY_ARN\n          value: '{{ .Values.activityArn }}'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"{{ .Release.Name }}\" has cpu request 0"
  },
  {
    "id": "6447",
    "manifest_path": "data/manifests/the_stack_sample/sample_2323.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: '{{ .Release.Name }}-job'\n  namespace: '{{ .Values.namespace }}'\n  labels:\n    type: pipeline\n    experimentId: '{{ .Values.experimentId }}'\n    sandboxId: '{{ .Values.sandboxId }}'\nspec:\n  template:\n    metadata:\n      name: '{{ .Release.Name }}-server'\n      labels:\n        type: pipeline\n        experimentId: '{{ .Values.experimentId }}'\n        sandboxId: '{{ .Values.sandboxId }}'\n    spec:\n      serviceAccountName: deployment-runner\n      containers:\n      - name: '{{ .Release.Name }}'\n        image: '{{ .Values.image }}'\n        env:\n        - name: CLUSTER_ENV\n          value: '{{ .Values.clusterEnv }}'\n        - name: SANDBOX_ID\n          value: '{{ .Values.sandboxId }}'\n        - name: AWS_ACCOUNT_ID\n          value: '{{ .Values.awsAccountId }}'\n        - name: AWS_DEFAULT_REGION\n          value: '{{ .Values.awsRegion }}'\n        - name: ACTIVITY_ARN\n          value: '{{ .Values.activityArn }}'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"{{ .Release.Name }}\" has memory limit 0"
  },
  {
    "id": "6448",
    "manifest_path": "data/manifests/the_stack_sample/sample_2325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-website1\nspec:\n  volumes:\n  - name: www\n  containers:\n  - name: website1-container\n    image: nginx\n    volumeMounts:\n    - name: www\n      mountPath: /usr/share/nginx/html/\n  initContainers:\n  - name: clone-from-git-init-container\n    image: alpine\n    command:\n    - sh\n    - -c\n    - apk add --no-cache git && git clone https://github.com/lmammino/sample-web-project.git\n      /www\n    volumeMounts:\n    - name: www\n      mountPath: /www/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"clone-from-git-init-container\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6449",
    "manifest_path": "data/manifests/the_stack_sample/sample_2325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-website1\nspec:\n  volumes:\n  - name: www\n  containers:\n  - name: website1-container\n    image: nginx\n    volumeMounts:\n    - name: www\n      mountPath: /usr/share/nginx/html/\n  initContainers:\n  - name: clone-from-git-init-container\n    image: alpine\n    command:\n    - sh\n    - -c\n    - apk add --no-cache git && git clone https://github.com/lmammino/sample-web-project.git\n      /www\n    volumeMounts:\n    - name: www\n      mountPath: /www/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"website1-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6450",
    "manifest_path": "data/manifests/the_stack_sample/sample_2325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-website1\nspec:\n  volumes:\n  - name: www\n  containers:\n  - name: website1-container\n    image: nginx\n    volumeMounts:\n    - name: www\n      mountPath: /usr/share/nginx/html/\n  initContainers:\n  - name: clone-from-git-init-container\n    image: alpine\n    command:\n    - sh\n    - -c\n    - apk add --no-cache git && git clone https://github.com/lmammino/sample-web-project.git\n      /www\n    volumeMounts:\n    - name: www\n      mountPath: /www/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clone-from-git-init-container\" does not have a read-only root file system"
  },
  {
    "id": "6451",
    "manifest_path": "data/manifests/the_stack_sample/sample_2325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-website1\nspec:\n  volumes:\n  - name: www\n  containers:\n  - name: website1-container\n    image: nginx\n    volumeMounts:\n    - name: www\n      mountPath: /usr/share/nginx/html/\n  initContainers:\n  - name: clone-from-git-init-container\n    image: alpine\n    command:\n    - sh\n    - -c\n    - apk add --no-cache git && git clone https://github.com/lmammino/sample-web-project.git\n      /www\n    volumeMounts:\n    - name: www\n      mountPath: /www/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"website1-container\" does not have a read-only root file system"
  },
  {
    "id": "6452",
    "manifest_path": "data/manifests/the_stack_sample/sample_2325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-website1\nspec:\n  volumes:\n  - name: www\n  containers:\n  - name: website1-container\n    image: nginx\n    volumeMounts:\n    - name: www\n      mountPath: /usr/share/nginx/html/\n  initContainers:\n  - name: clone-from-git-init-container\n    image: alpine\n    command:\n    - sh\n    - -c\n    - apk add --no-cache git && git clone https://github.com/lmammino/sample-web-project.git\n      /www\n    volumeMounts:\n    - name: www\n      mountPath: /www/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clone-from-git-init-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6453",
    "manifest_path": "data/manifests/the_stack_sample/sample_2325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-website1\nspec:\n  volumes:\n  - name: www\n  containers:\n  - name: website1-container\n    image: nginx\n    volumeMounts:\n    - name: www\n      mountPath: /usr/share/nginx/html/\n  initContainers:\n  - name: clone-from-git-init-container\n    image: alpine\n    command:\n    - sh\n    - -c\n    - apk add --no-cache git && git clone https://github.com/lmammino/sample-web-project.git\n      /www\n    volumeMounts:\n    - name: www\n      mountPath: /www/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"website1-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6454",
    "manifest_path": "data/manifests/the_stack_sample/sample_2325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-website1\nspec:\n  volumes:\n  - name: www\n  containers:\n  - name: website1-container\n    image: nginx\n    volumeMounts:\n    - name: www\n      mountPath: /usr/share/nginx/html/\n  initContainers:\n  - name: clone-from-git-init-container\n    image: alpine\n    command:\n    - sh\n    - -c\n    - apk add --no-cache git && git clone https://github.com/lmammino/sample-web-project.git\n      /www\n    volumeMounts:\n    - name: www\n      mountPath: /www/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clone-from-git-init-container\" has cpu request 0"
  },
  {
    "id": "6455",
    "manifest_path": "data/manifests/the_stack_sample/sample_2325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-website1\nspec:\n  volumes:\n  - name: www\n  containers:\n  - name: website1-container\n    image: nginx\n    volumeMounts:\n    - name: www\n      mountPath: /usr/share/nginx/html/\n  initContainers:\n  - name: clone-from-git-init-container\n    image: alpine\n    command:\n    - sh\n    - -c\n    - apk add --no-cache git && git clone https://github.com/lmammino/sample-web-project.git\n      /www\n    volumeMounts:\n    - name: www\n      mountPath: /www/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"website1-container\" has cpu request 0"
  },
  {
    "id": "6456",
    "manifest_path": "data/manifests/the_stack_sample/sample_2325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-website1\nspec:\n  volumes:\n  - name: www\n  containers:\n  - name: website1-container\n    image: nginx\n    volumeMounts:\n    - name: www\n      mountPath: /usr/share/nginx/html/\n  initContainers:\n  - name: clone-from-git-init-container\n    image: alpine\n    command:\n    - sh\n    - -c\n    - apk add --no-cache git && git clone https://github.com/lmammino/sample-web-project.git\n      /www\n    volumeMounts:\n    - name: www\n      mountPath: /www/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clone-from-git-init-container\" has memory limit 0"
  },
  {
    "id": "6457",
    "manifest_path": "data/manifests/the_stack_sample/sample_2325.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-website1\nspec:\n  volumes:\n  - name: www\n  containers:\n  - name: website1-container\n    image: nginx\n    volumeMounts:\n    - name: www\n      mountPath: /usr/share/nginx/html/\n  initContainers:\n  - name: clone-from-git-init-container\n    image: alpine\n    command:\n    - sh\n    - -c\n    - apk add --no-cache git && git clone https://github.com/lmammino/sample-web-project.git\n      /www\n    volumeMounts:\n    - name: www\n      mountPath: /www/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"website1-container\" has memory limit 0"
  },
  {
    "id": "6458",
    "manifest_path": "data/manifests/the_stack_sample/sample_2326.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: application-controller-stateful-set\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: application-controller\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: application-controller\n    spec:\n      containers:\n      - command:\n        - /root/manager\n        env:\n        - name: project\n          value: ''\n        image: gcr.io/kubeflow-images-public/kubernetes-sigs/application:1.0-beta\n        imagePullPolicy: Always\n        name: manager\n      serviceAccountName: application-controller-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "6459",
    "manifest_path": "data/manifests/the_stack_sample/sample_2326.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: application-controller-stateful-set\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: application-controller\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: application-controller\n    spec:\n      containers:\n      - command:\n        - /root/manager\n        env:\n        - name: project\n          value: ''\n        image: gcr.io/kubeflow-images-public/kubernetes-sigs/application:1.0-beta\n        imagePullPolicy: Always\n        name: manager\n      serviceAccountName: application-controller-service-account\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"application-controller-service-account\" not found"
  },
  {
    "id": "6460",
    "manifest_path": "data/manifests/the_stack_sample/sample_2326.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: application-controller-stateful-set\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: application-controller\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: application-controller\n    spec:\n      containers:\n      - command:\n        - /root/manager\n        env:\n        - name: project\n          value: ''\n        image: gcr.io/kubeflow-images-public/kubernetes-sigs/application:1.0-beta\n        imagePullPolicy: Always\n        name: manager\n      serviceAccountName: application-controller-service-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "6461",
    "manifest_path": "data/manifests/the_stack_sample/sample_2326.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: application-controller-stateful-set\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: application-controller\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: application-controller\n    spec:\n      containers:\n      - command:\n        - /root/manager\n        env:\n        - name: project\n          value: ''\n        image: gcr.io/kubeflow-images-public/kubernetes-sigs/application:1.0-beta\n        imagePullPolicy: Always\n        name: manager\n      serviceAccountName: application-controller-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"manager\" has cpu request 0"
  },
  {
    "id": "6462",
    "manifest_path": "data/manifests/the_stack_sample/sample_2326.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: application-controller-stateful-set\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: application-controller\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: application-controller\n    spec:\n      containers:\n      - command:\n        - /root/manager\n        env:\n        - name: project\n          value: ''\n        image: gcr.io/kubeflow-images-public/kubernetes-sigs/application:1.0-beta\n        imagePullPolicy: Always\n        name: manager\n      serviceAccountName: application-controller-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"manager\" has memory limit 0"
  },
  {
    "id": "6463",
    "manifest_path": "data/manifests/the_stack_sample/sample_2331.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-931\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6464",
    "manifest_path": "data/manifests/the_stack_sample/sample_2331.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-931\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6465",
    "manifest_path": "data/manifests/the_stack_sample/sample_2331.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-931\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6466",
    "manifest_path": "data/manifests/the_stack_sample/sample_2331.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-931\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6467",
    "manifest_path": "data/manifests/the_stack_sample/sample_2331.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-931\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6468",
    "manifest_path": "data/manifests/the_stack_sample/sample_2332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    component: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: redis\n  template:\n    metadata:\n      labels:\n        component: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        args:\n        - --appendonly\n        - 'yes'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6469",
    "manifest_path": "data/manifests/the_stack_sample/sample_2332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    component: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: redis\n  template:\n    metadata:\n      labels:\n        component: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        args:\n        - --appendonly\n        - 'yes'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "6470",
    "manifest_path": "data/manifests/the_stack_sample/sample_2332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    component: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: redis\n  template:\n    metadata:\n      labels:\n        component: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        args:\n        - --appendonly\n        - 'yes'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "6471",
    "manifest_path": "data/manifests/the_stack_sample/sample_2332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    component: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: redis\n  template:\n    metadata:\n      labels:\n        component: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        args:\n        - --appendonly\n        - 'yes'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "6472",
    "manifest_path": "data/manifests/the_stack_sample/sample_2332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    component: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: redis\n  template:\n    metadata:\n      labels:\n        component: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        args:\n        - --appendonly\n        - 'yes'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "6473",
    "manifest_path": "data/manifests/the_stack_sample/sample_2333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8820\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6474",
    "manifest_path": "data/manifests/the_stack_sample/sample_2333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8820\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6475",
    "manifest_path": "data/manifests/the_stack_sample/sample_2333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8820\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6476",
    "manifest_path": "data/manifests/the_stack_sample/sample_2333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8820\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6477",
    "manifest_path": "data/manifests/the_stack_sample/sample_2333.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8820\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6478",
    "manifest_path": "data/manifests/the_stack_sample/sample_2336.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-config-operator\n  name: openshift-config-operator\n  labels:\n    app: openshift-config-operator\n  annotations:\n    include.release.openshift.io/self-managed-high-availability: 'true'\n    include.release.openshift.io/single-node-developer: 'true'\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-config-operator\n  template:\n    metadata:\n      name: openshift-config-operator\n      labels:\n        app: openshift-config-operator\n    spec:\n      serviceAccountName: openshift-config-operator\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: config-operator-serving-cert\n          optional: true\n      containers:\n      - name: openshift-config-operator\n        image: quay.io/openshift/origin-cluster-config-operator:v4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cluster-config-operator\n        - operator\n        ports:\n        - containerPort: 8443\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTPS\n            port: 8443\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTPS\n            port: 8443\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-cluster-config-operator:v4.0\n        - name: OPERATOR_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERAND_IMAGE_VERSION\n          value: 0.0.1-snapshot\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"openshift-config-operator\" does not have a read-only root file system"
  },
  {
    "id": "6479",
    "manifest_path": "data/manifests/the_stack_sample/sample_2336.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-config-operator\n  name: openshift-config-operator\n  labels:\n    app: openshift-config-operator\n  annotations:\n    include.release.openshift.io/self-managed-high-availability: 'true'\n    include.release.openshift.io/single-node-developer: 'true'\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-config-operator\n  template:\n    metadata:\n      name: openshift-config-operator\n      labels:\n        app: openshift-config-operator\n    spec:\n      serviceAccountName: openshift-config-operator\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: config-operator-serving-cert\n          optional: true\n      containers:\n      - name: openshift-config-operator\n        image: quay.io/openshift/origin-cluster-config-operator:v4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cluster-config-operator\n        - operator\n        ports:\n        - containerPort: 8443\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTPS\n            port: 8443\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTPS\n            port: 8443\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-cluster-config-operator:v4.0\n        - name: OPERATOR_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERAND_IMAGE_VERSION\n          value: 0.0.1-snapshot\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"openshift-config-operator\" not found"
  },
  {
    "id": "6480",
    "manifest_path": "data/manifests/the_stack_sample/sample_2336.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-config-operator\n  name: openshift-config-operator\n  labels:\n    app: openshift-config-operator\n  annotations:\n    include.release.openshift.io/self-managed-high-availability: 'true'\n    include.release.openshift.io/single-node-developer: 'true'\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-config-operator\n  template:\n    metadata:\n      name: openshift-config-operator\n      labels:\n        app: openshift-config-operator\n    spec:\n      serviceAccountName: openshift-config-operator\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: config-operator-serving-cert\n          optional: true\n      containers:\n      - name: openshift-config-operator\n        image: quay.io/openshift/origin-cluster-config-operator:v4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cluster-config-operator\n        - operator\n        ports:\n        - containerPort: 8443\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTPS\n            port: 8443\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTPS\n            port: 8443\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-cluster-config-operator:v4.0\n        - name: OPERATOR_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERAND_IMAGE_VERSION\n          value: 0.0.1-snapshot\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openshift-config-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "6481",
    "manifest_path": "data/manifests/the_stack_sample/sample_2336.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-config-operator\n  name: openshift-config-operator\n  labels:\n    app: openshift-config-operator\n  annotations:\n    include.release.openshift.io/self-managed-high-availability: 'true'\n    include.release.openshift.io/single-node-developer: 'true'\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-config-operator\n  template:\n    metadata:\n      name: openshift-config-operator\n      labels:\n        app: openshift-config-operator\n    spec:\n      serviceAccountName: openshift-config-operator\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: config-operator-serving-cert\n          optional: true\n      containers:\n      - name: openshift-config-operator\n        image: quay.io/openshift/origin-cluster-config-operator:v4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cluster-config-operator\n        - operator\n        ports:\n        - containerPort: 8443\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTPS\n            port: 8443\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            scheme: HTTPS\n            port: 8443\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        resources:\n          requests:\n            memory: 50Mi\n            cpu: 10m\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-cluster-config-operator:v4.0\n        - name: OPERATOR_IMAGE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERAND_IMAGE_VERSION\n          value: 0.0.1-snapshot\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"openshift-config-operator\" has memory limit 0"
  },
  {
    "id": "6482",
    "manifest_path": "data/manifests/the_stack_sample/sample_2337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: server\n  template:\n    metadata:\n      labels:\n        component: server\n    spec:\n      containers:\n      - name: server\n        image: stephengrider/multi-server\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          value: redis-cluster-ip-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: PGUSER\n          value: postgres\n        - name: PGHOST\n          value: postgres-cluster-ip-service\n        - name: PGPORT\n          value: '5432'\n        - name: PGDATABASE\n          value: postgres\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pgpassword\n              key: PGPASSWORD\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"stephengrider/multi-server\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6483",
    "manifest_path": "data/manifests/the_stack_sample/sample_2337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: server\n  template:\n    metadata:\n      labels:\n        component: server\n    spec:\n      containers:\n      - name: server\n        image: stephengrider/multi-server\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          value: redis-cluster-ip-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: PGUSER\n          value: postgres\n        - name: PGHOST\n          value: postgres-cluster-ip-service\n        - name: PGPORT\n          value: '5432'\n        - name: PGDATABASE\n          value: postgres\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pgpassword\n              key: PGPASSWORD\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6484",
    "manifest_path": "data/manifests/the_stack_sample/sample_2337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: server\n  template:\n    metadata:\n      labels:\n        component: server\n    spec:\n      containers:\n      - name: server\n        image: stephengrider/multi-server\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          value: redis-cluster-ip-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: PGUSER\n          value: postgres\n        - name: PGHOST\n          value: postgres-cluster-ip-service\n        - name: PGPORT\n          value: '5432'\n        - name: PGDATABASE\n          value: postgres\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pgpassword\n              key: PGPASSWORD\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "6485",
    "manifest_path": "data/manifests/the_stack_sample/sample_2337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: server\n  template:\n    metadata:\n      labels:\n        component: server\n    spec:\n      containers:\n      - name: server\n        image: stephengrider/multi-server\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          value: redis-cluster-ip-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: PGUSER\n          value: postgres\n        - name: PGHOST\n          value: postgres-cluster-ip-service\n        - name: PGPORT\n          value: '5432'\n        - name: PGDATABASE\n          value: postgres\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pgpassword\n              key: PGPASSWORD\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "6486",
    "manifest_path": "data/manifests/the_stack_sample/sample_2337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: server\n  template:\n    metadata:\n      labels:\n        component: server\n    spec:\n      containers:\n      - name: server\n        image: stephengrider/multi-server\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          value: redis-cluster-ip-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: PGUSER\n          value: postgres\n        - name: PGHOST\n          value: postgres-cluster-ip-service\n        - name: PGPORT\n          value: '5432'\n        - name: PGDATABASE\n          value: postgres\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pgpassword\n              key: PGPASSWORD\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "6487",
    "manifest_path": "data/manifests/the_stack_sample/sample_2337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: server-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: server\n  template:\n    metadata:\n      labels:\n        component: server\n    spec:\n      containers:\n      - name: server\n        image: stephengrider/multi-server\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          value: redis-cluster-ip-service\n        - name: REDIS_PORT\n          value: '6379'\n        - name: PGUSER\n          value: postgres\n        - name: PGHOST\n          value: postgres-cluster-ip-service\n        - name: PGPORT\n          value: '5432'\n        - name: PGDATABASE\n          value: postgres\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pgpassword\n              key: PGPASSWORD\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "6488",
    "manifest_path": "data/manifests/the_stack_sample/sample_2340.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: celery-worker\n  labels:\n    deployment: celery-worker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: celery-worker\n  template:\n    metadata:\n      labels:\n        pod: celery-worker\n    spec:\n      containers:\n      - name: celery-worker\n        image: backend:1\n        command:\n        - celery\n        - worker\n        - --app=backend.celery_app:app\n        - --loglevel=info\n        env:\n        - name: DJANGO_SETTINGS_MODULE\n          value: backend.settings.minikube\n        - name: SECRET_KEY\n          value: my-secret-key\n        - name: POSTGRES_NAME\n          value: postgres\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable SECRET_KEY in container \"celery-worker\" found"
  },
  {
    "id": "6489",
    "manifest_path": "data/manifests/the_stack_sample/sample_2340.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: celery-worker\n  labels:\n    deployment: celery-worker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: celery-worker\n  template:\n    metadata:\n      labels:\n        pod: celery-worker\n    spec:\n      containers:\n      - name: celery-worker\n        image: backend:1\n        command:\n        - celery\n        - worker\n        - --app=backend.celery_app:app\n        - --loglevel=info\n        env:\n        - name: DJANGO_SETTINGS_MODULE\n          value: backend.settings.minikube\n        - name: SECRET_KEY\n          value: my-secret-key\n        - name: POSTGRES_NAME\n          value: postgres\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"celery-worker\" does not have a read-only root file system"
  },
  {
    "id": "6490",
    "manifest_path": "data/manifests/the_stack_sample/sample_2340.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: celery-worker\n  labels:\n    deployment: celery-worker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: celery-worker\n  template:\n    metadata:\n      labels:\n        pod: celery-worker\n    spec:\n      containers:\n      - name: celery-worker\n        image: backend:1\n        command:\n        - celery\n        - worker\n        - --app=backend.celery_app:app\n        - --loglevel=info\n        env:\n        - name: DJANGO_SETTINGS_MODULE\n          value: backend.settings.minikube\n        - name: SECRET_KEY\n          value: my-secret-key\n        - name: POSTGRES_NAME\n          value: postgres\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"celery-worker\" is not set to runAsNonRoot"
  },
  {
    "id": "6491",
    "manifest_path": "data/manifests/the_stack_sample/sample_2340.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: celery-worker\n  labels:\n    deployment: celery-worker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: celery-worker\n  template:\n    metadata:\n      labels:\n        pod: celery-worker\n    spec:\n      containers:\n      - name: celery-worker\n        image: backend:1\n        command:\n        - celery\n        - worker\n        - --app=backend.celery_app:app\n        - --loglevel=info\n        env:\n        - name: DJANGO_SETTINGS_MODULE\n          value: backend.settings.minikube\n        - name: SECRET_KEY\n          value: my-secret-key\n        - name: POSTGRES_NAME\n          value: postgres\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"celery-worker\" has cpu request 0"
  },
  {
    "id": "6492",
    "manifest_path": "data/manifests/the_stack_sample/sample_2340.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: celery-worker\n  labels:\n    deployment: celery-worker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: celery-worker\n  template:\n    metadata:\n      labels:\n        pod: celery-worker\n    spec:\n      containers:\n      - name: celery-worker\n        image: backend:1\n        command:\n        - celery\n        - worker\n        - --app=backend.celery_app:app\n        - --loglevel=info\n        env:\n        - name: DJANGO_SETTINGS_MODULE\n          value: backend.settings.minikube\n        - name: SECRET_KEY\n          value: my-secret-key\n        - name: POSTGRES_NAME\n          value: postgres\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"celery-worker\" has memory limit 0"
  },
  {
    "id": "6493",
    "manifest_path": "data/manifests/the_stack_sample/sample_2341.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-grpc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: echo-grpc\n  template:\n    metadata:\n      labels:\n        app: echo-grpc\n    spec:\n      containers:\n      - name: echo-grpc\n        image: gcr.io/GOOGLE_CLOUD_PROJECT/echo-grpc\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PORT\n          value: '8081'\n        ports:\n        - containerPort: 8081\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 10\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"echo-grpc\" is using an invalid container image, \"gcr.io/GOOGLE_CLOUD_PROJECT/echo-grpc\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6494",
    "manifest_path": "data/manifests/the_stack_sample/sample_2341.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-grpc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: echo-grpc\n  template:\n    metadata:\n      labels:\n        app: echo-grpc\n    spec:\n      containers:\n      - name: echo-grpc\n        image: gcr.io/GOOGLE_CLOUD_PROJECT/echo-grpc\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PORT\n          value: '8081'\n        ports:\n        - containerPort: 8081\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 10\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6495",
    "manifest_path": "data/manifests/the_stack_sample/sample_2341.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-grpc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: echo-grpc\n  template:\n    metadata:\n      labels:\n        app: echo-grpc\n    spec:\n      containers:\n      - name: echo-grpc\n        image: gcr.io/GOOGLE_CLOUD_PROJECT/echo-grpc\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PORT\n          value: '8081'\n        ports:\n        - containerPort: 8081\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echo-grpc\" does not have a read-only root file system"
  },
  {
    "id": "6496",
    "manifest_path": "data/manifests/the_stack_sample/sample_2341.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-grpc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: echo-grpc\n  template:\n    metadata:\n      labels:\n        app: echo-grpc\n    spec:\n      containers:\n      - name: echo-grpc\n        image: gcr.io/GOOGLE_CLOUD_PROJECT/echo-grpc\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PORT\n          value: '8081'\n        ports:\n        - containerPort: 8081\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echo-grpc\" is not set to runAsNonRoot"
  },
  {
    "id": "6497",
    "manifest_path": "data/manifests/the_stack_sample/sample_2341.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-grpc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: echo-grpc\n  template:\n    metadata:\n      labels:\n        app: echo-grpc\n    spec:\n      containers:\n      - name: echo-grpc\n        image: gcr.io/GOOGLE_CLOUD_PROJECT/echo-grpc\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PORT\n          value: '8081'\n        ports:\n        - containerPort: 8081\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 10\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"echo-grpc\" has cpu request 0"
  },
  {
    "id": "6498",
    "manifest_path": "data/manifests/the_stack_sample/sample_2341.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo-grpc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: echo-grpc\n  template:\n    metadata:\n      labels:\n        app: echo-grpc\n    spec:\n      containers:\n      - name: echo-grpc\n        image: gcr.io/GOOGLE_CLOUD_PROJECT/echo-grpc\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PORT\n          value: '8081'\n        ports:\n        - containerPort: 8081\n        readinessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:8081\n            - -service=Check\n          initialDelaySeconds: 10\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"echo-grpc\" has memory limit 0"
  },
  {
    "id": "6499",
    "manifest_path": "data/manifests/the_stack_sample/sample_2343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: datamaster-pod\n  labels:\n    app: datamaster\nspec:\n  containers:\n  - name: data-master\n    image: alincorodescu/msc-workflows-data-master:latest\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n    env:\n    - name: ASPNETCORE_URLS\n      value: http://+:9377\n    - name: NODE_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.hostIP\n    ports:\n    - containerPort: 9377\n      protocol: TCP\n    - containerPort: 10001\n      protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"data-master\" is using an invalid container image, \"alincorodescu/msc-workflows-data-master:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6500",
    "manifest_path": "data/manifests/the_stack_sample/sample_2343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: datamaster-pod\n  labels:\n    app: datamaster\nspec:\n  containers:\n  - name: data-master\n    image: alincorodescu/msc-workflows-data-master:latest\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n    env:\n    - name: ASPNETCORE_URLS\n      value: http://+:9377\n    - name: NODE_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.hostIP\n    ports:\n    - containerPort: 9377\n      protocol: TCP\n    - containerPort: 10001\n      protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"data-master\" does not have a read-only root file system"
  },
  {
    "id": "6501",
    "manifest_path": "data/manifests/the_stack_sample/sample_2343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: datamaster-pod\n  labels:\n    app: datamaster\nspec:\n  containers:\n  - name: data-master\n    image: alincorodescu/msc-workflows-data-master:latest\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n    env:\n    - name: ASPNETCORE_URLS\n      value: http://+:9377\n    - name: NODE_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.hostIP\n    ports:\n    - containerPort: 9377\n      protocol: TCP\n    - containerPort: 10001\n      protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"data-master\" is not set to runAsNonRoot"
  },
  {
    "id": "6502",
    "manifest_path": "data/manifests/the_stack_sample/sample_2343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: datamaster-pod\n  labels:\n    app: datamaster\nspec:\n  containers:\n  - name: data-master\n    image: alincorodescu/msc-workflows-data-master:latest\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n    env:\n    - name: ASPNETCORE_URLS\n      value: http://+:9377\n    - name: NODE_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.hostIP\n    ports:\n    - containerPort: 9377\n      protocol: TCP\n    - containerPort: 10001\n      protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"data-master\" has cpu request 0"
  },
  {
    "id": "6503",
    "manifest_path": "data/manifests/the_stack_sample/sample_2344.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6504",
    "manifest_path": "data/manifests/the_stack_sample/sample_2344.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6505",
    "manifest_path": "data/manifests/the_stack_sample/sample_2344.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6506",
    "manifest_path": "data/manifests/the_stack_sample/sample_2344.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6507",
    "manifest_path": "data/manifests/the_stack_sample/sample_2345.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shmwrapper\n  namespace: shmwrapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: shmwrapper\n  template:\n    metadata:\n      labels:\n        app: shmwrapper\n    spec:\n      containers:\n      - name: shmwrapper-writer\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && make test_write '\n      - name: shmwrapper-reader\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && sleep 2 && make test_read && make test_clear '\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"shmwrapper-reader\" does not have a read-only root file system"
  },
  {
    "id": "6508",
    "manifest_path": "data/manifests/the_stack_sample/sample_2345.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shmwrapper\n  namespace: shmwrapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: shmwrapper\n  template:\n    metadata:\n      labels:\n        app: shmwrapper\n    spec:\n      containers:\n      - name: shmwrapper-writer\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && make test_write '\n      - name: shmwrapper-reader\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && sleep 2 && make test_read && make test_clear '\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"shmwrapper-writer\" does not have a read-only root file system"
  },
  {
    "id": "6509",
    "manifest_path": "data/manifests/the_stack_sample/sample_2345.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shmwrapper\n  namespace: shmwrapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: shmwrapper\n  template:\n    metadata:\n      labels:\n        app: shmwrapper\n    spec:\n      containers:\n      - name: shmwrapper-writer\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && make test_write '\n      - name: shmwrapper-reader\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && sleep 2 && make test_read && make test_clear '\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"shmwrapper-reader\" is not set to runAsNonRoot"
  },
  {
    "id": "6510",
    "manifest_path": "data/manifests/the_stack_sample/sample_2345.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shmwrapper\n  namespace: shmwrapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: shmwrapper\n  template:\n    metadata:\n      labels:\n        app: shmwrapper\n    spec:\n      containers:\n      - name: shmwrapper-writer\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && make test_write '\n      - name: shmwrapper-reader\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && sleep 2 && make test_read && make test_clear '\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"shmwrapper-writer\" is not set to runAsNonRoot"
  },
  {
    "id": "6511",
    "manifest_path": "data/manifests/the_stack_sample/sample_2345.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shmwrapper\n  namespace: shmwrapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: shmwrapper\n  template:\n    metadata:\n      labels:\n        app: shmwrapper\n    spec:\n      containers:\n      - name: shmwrapper-writer\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && make test_write '\n      - name: shmwrapper-reader\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && sleep 2 && make test_read && make test_clear '\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"shmwrapper-reader\" has cpu request 0"
  },
  {
    "id": "6512",
    "manifest_path": "data/manifests/the_stack_sample/sample_2345.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shmwrapper\n  namespace: shmwrapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: shmwrapper\n  template:\n    metadata:\n      labels:\n        app: shmwrapper\n    spec:\n      containers:\n      - name: shmwrapper-writer\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && make test_write '\n      - name: shmwrapper-reader\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && sleep 2 && make test_read && make test_clear '\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"shmwrapper-writer\" has cpu request 0"
  },
  {
    "id": "6513",
    "manifest_path": "data/manifests/the_stack_sample/sample_2345.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shmwrapper\n  namespace: shmwrapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: shmwrapper\n  template:\n    metadata:\n      labels:\n        app: shmwrapper\n    spec:\n      containers:\n      - name: shmwrapper-writer\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && make test_write '\n      - name: shmwrapper-reader\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && sleep 2 && make test_read && make test_clear '\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"shmwrapper-reader\" has memory limit 0"
  },
  {
    "id": "6514",
    "manifest_path": "data/manifests/the_stack_sample/sample_2345.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shmwrapper\n  namespace: shmwrapper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: shmwrapper\n  template:\n    metadata:\n      labels:\n        app: shmwrapper\n    spec:\n      containers:\n      - name: shmwrapper-writer\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && make test_write '\n      - name: shmwrapper-reader\n        image: alpine:3.12.0\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'apk add --no-cache gcc musl-dev curl make unzip && curl https://github.com/idrissneumann/shmwrapper/archive/master.zip\n          -L -o shmwrapper.zip && unzip shmwrapper.zip && cd shmwrapper-master &&\n          make all_without_test && sleep 2 && make test_read && make test_clear '\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"shmwrapper-writer\" has memory limit 0"
  },
  {
    "id": "6515",
    "manifest_path": "data/manifests/the_stack_sample/sample_2350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: initpod\n  labels:\n    app: initializer\nspec:\n  initContainers:\n  - name: init-ctr\n    image: busybox\n    command:\n    - sh\n    - -c\n    - until nslookup k8sbook; do echo waiting for k8sbook service;\\ sleep 1; done;\n      echo Service found!\n  containers:\n  - name: web-ctr\n    image: nigelpoulton/web-app:1.0\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init-ctr\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6516",
    "manifest_path": "data/manifests/the_stack_sample/sample_2350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: initpod\n  labels:\n    app: initializer\nspec:\n  initContainers:\n  - name: init-ctr\n    image: busybox\n    command:\n    - sh\n    - -c\n    - until nslookup k8sbook; do echo waiting for k8sbook service;\\ sleep 1; done;\n      echo Service found!\n  containers:\n  - name: web-ctr\n    image: nigelpoulton/web-app:1.0\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-ctr\" does not have a read-only root file system"
  },
  {
    "id": "6517",
    "manifest_path": "data/manifests/the_stack_sample/sample_2350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: initpod\n  labels:\n    app: initializer\nspec:\n  initContainers:\n  - name: init-ctr\n    image: busybox\n    command:\n    - sh\n    - -c\n    - until nslookup k8sbook; do echo waiting for k8sbook service;\\ sleep 1; done;\n      echo Service found!\n  containers:\n  - name: web-ctr\n    image: nigelpoulton/web-app:1.0\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-ctr\" does not have a read-only root file system"
  },
  {
    "id": "6518",
    "manifest_path": "data/manifests/the_stack_sample/sample_2350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: initpod\n  labels:\n    app: initializer\nspec:\n  initContainers:\n  - name: init-ctr\n    image: busybox\n    command:\n    - sh\n    - -c\n    - until nslookup k8sbook; do echo waiting for k8sbook service;\\ sleep 1; done;\n      echo Service found!\n  containers:\n  - name: web-ctr\n    image: nigelpoulton/web-app:1.0\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "6519",
    "manifest_path": "data/manifests/the_stack_sample/sample_2350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: initpod\n  labels:\n    app: initializer\nspec:\n  initContainers:\n  - name: init-ctr\n    image: busybox\n    command:\n    - sh\n    - -c\n    - until nslookup k8sbook; do echo waiting for k8sbook service;\\ sleep 1; done;\n      echo Service found!\n  containers:\n  - name: web-ctr\n    image: nigelpoulton/web-app:1.0\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "6520",
    "manifest_path": "data/manifests/the_stack_sample/sample_2350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: initpod\n  labels:\n    app: initializer\nspec:\n  initContainers:\n  - name: init-ctr\n    image: busybox\n    command:\n    - sh\n    - -c\n    - until nslookup k8sbook; do echo waiting for k8sbook service;\\ sleep 1; done;\n      echo Service found!\n  containers:\n  - name: web-ctr\n    image: nigelpoulton/web-app:1.0\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-ctr\" has cpu request 0"
  },
  {
    "id": "6521",
    "manifest_path": "data/manifests/the_stack_sample/sample_2350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: initpod\n  labels:\n    app: initializer\nspec:\n  initContainers:\n  - name: init-ctr\n    image: busybox\n    command:\n    - sh\n    - -c\n    - until nslookup k8sbook; do echo waiting for k8sbook service;\\ sleep 1; done;\n      echo Service found!\n  containers:\n  - name: web-ctr\n    image: nigelpoulton/web-app:1.0\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-ctr\" has cpu request 0"
  },
  {
    "id": "6522",
    "manifest_path": "data/manifests/the_stack_sample/sample_2350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: initpod\n  labels:\n    app: initializer\nspec:\n  initContainers:\n  - name: init-ctr\n    image: busybox\n    command:\n    - sh\n    - -c\n    - until nslookup k8sbook; do echo waiting for k8sbook service;\\ sleep 1; done;\n      echo Service found!\n  containers:\n  - name: web-ctr\n    image: nigelpoulton/web-app:1.0\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-ctr\" has memory limit 0"
  },
  {
    "id": "6523",
    "manifest_path": "data/manifests/the_stack_sample/sample_2350.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: initpod\n  labels:\n    app: initializer\nspec:\n  initContainers:\n  - name: init-ctr\n    image: busybox\n    command:\n    - sh\n    - -c\n    - until nslookup k8sbook; do echo waiting for k8sbook service;\\ sleep 1; done;\n      echo Service found!\n  containers:\n  - name: web-ctr\n    image: nigelpoulton/web-app:1.0\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-ctr\" has memory limit 0"
  },
  {
    "id": "6524",
    "manifest_path": "data/manifests/the_stack_sample/sample_2354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job1\nspec:\n  template:\n    metadata:\n      name: job1\n    spec:\n      containers:\n      - name: job1\n        image: ubuntu:16.04\n        args:\n        - sh\n        - -c\n        - sleep 10000; true\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6525",
    "manifest_path": "data/manifests/the_stack_sample/sample_2354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job1\nspec:\n  template:\n    metadata:\n      name: job1\n    spec:\n      containers:\n      - name: job1\n        image: ubuntu:16.04\n        args:\n        - sh\n        - -c\n        - sleep 10000; true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job1\" does not have a read-only root file system"
  },
  {
    "id": "6526",
    "manifest_path": "data/manifests/the_stack_sample/sample_2354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job1\nspec:\n  template:\n    metadata:\n      name: job1\n    spec:\n      containers:\n      - name: job1\n        image: ubuntu:16.04\n        args:\n        - sh\n        - -c\n        - sleep 10000; true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job1\" is not set to runAsNonRoot"
  },
  {
    "id": "6527",
    "manifest_path": "data/manifests/the_stack_sample/sample_2354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job1\nspec:\n  template:\n    metadata:\n      name: job1\n    spec:\n      containers:\n      - name: job1\n        image: ubuntu:16.04\n        args:\n        - sh\n        - -c\n        - sleep 10000; true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job1\" has cpu request 0"
  },
  {
    "id": "6528",
    "manifest_path": "data/manifests/the_stack_sample/sample_2354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job1\nspec:\n  template:\n    metadata:\n      name: job1\n    spec:\n      containers:\n      - name: job1\n        image: ubuntu:16.04\n        args:\n        - sh\n        - -c\n        - sleep 10000; true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job1\" has memory limit 0"
  },
  {
    "id": "6529",
    "manifest_path": "data/manifests/the_stack_sample/sample_2355.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ms-payment-deploy\n  namespace: shoppingportal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      zone: prod\n      app: payment-svc\n  template:\n    metadata:\n      labels:\n        name: payment-svc\n        version: v1\n        release: stable\n        tier: fe\n        zone: prod\n        managed-by: m2\n        app: payment-svc\n    spec:\n      containers:\n      - name: ms-payment-deploy-ctr\n        image: metamagic/ms-payment-service:1.0\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ms-payment-deploy-ctr\" does not have a read-only root file system"
  },
  {
    "id": "6530",
    "manifest_path": "data/manifests/the_stack_sample/sample_2355.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ms-payment-deploy\n  namespace: shoppingportal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      zone: prod\n      app: payment-svc\n  template:\n    metadata:\n      labels:\n        name: payment-svc\n        version: v1\n        release: stable\n        tier: fe\n        zone: prod\n        managed-by: m2\n        app: payment-svc\n    spec:\n      containers:\n      - name: ms-payment-deploy-ctr\n        image: metamagic/ms-payment-service:1.0\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ms-payment-deploy-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "6531",
    "manifest_path": "data/manifests/the_stack_sample/sample_2355.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ms-payment-deploy\n  namespace: shoppingportal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      zone: prod\n      app: payment-svc\n  template:\n    metadata:\n      labels:\n        name: payment-svc\n        version: v1\n        release: stable\n        tier: fe\n        zone: prod\n        managed-by: m2\n        app: payment-svc\n    spec:\n      containers:\n      - name: ms-payment-deploy-ctr\n        image: metamagic/ms-payment-service:1.0\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ms-payment-deploy-ctr\" has cpu request 0"
  },
  {
    "id": "6532",
    "manifest_path": "data/manifests/the_stack_sample/sample_2355.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ms-payment-deploy\n  namespace: shoppingportal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      zone: prod\n      app: payment-svc\n  template:\n    metadata:\n      labels:\n        name: payment-svc\n        version: v1\n        release: stable\n        tier: fe\n        zone: prod\n        managed-by: m2\n        app: payment-svc\n    spec:\n      containers:\n      - name: ms-payment-deploy-ctr\n        image: metamagic/ms-payment-service:1.0\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: prod\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ms-payment-deploy-ctr\" has memory limit 0"
  },
  {
    "id": "6533",
    "manifest_path": "data/manifests/the_stack_sample/sample_2356.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ngnix-pv-pod\nspec:\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: testvolclaim1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"task-pv-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6534",
    "manifest_path": "data/manifests/the_stack_sample/sample_2356.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ngnix-pv-pod\nspec:\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: testvolclaim1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"task-pv-container\" does not have a read-only root file system"
  },
  {
    "id": "6535",
    "manifest_path": "data/manifests/the_stack_sample/sample_2356.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ngnix-pv-pod\nspec:\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: testvolclaim1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"task-pv-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6536",
    "manifest_path": "data/manifests/the_stack_sample/sample_2356.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ngnix-pv-pod\nspec:\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: testvolclaim1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"task-pv-container\" has cpu request 0"
  },
  {
    "id": "6537",
    "manifest_path": "data/manifests/the_stack_sample/sample_2356.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ngnix-pv-pod\nspec:\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: testvolclaim1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"task-pv-container\" has memory limit 0"
  },
  {
    "id": "6538",
    "manifest_path": "data/manifests/the_stack_sample/sample_2357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myakscluster-b5e2\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myakscluster-b5e2\n  template:\n    metadata:\n      labels:\n        app: myakscluster-b5e2\n    spec:\n      containers:\n      - name: myakscluster-b5e2\n        image: containerregistrypruebas.azurecr.io/myakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"myakscluster-b5e2\" is using an invalid container image, \"containerregistrypruebas.azurecr.io/myakscluster\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6539",
    "manifest_path": "data/manifests/the_stack_sample/sample_2357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myakscluster-b5e2\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myakscluster-b5e2\n  template:\n    metadata:\n      labels:\n        app: myakscluster-b5e2\n    spec:\n      containers:\n      - name: myakscluster-b5e2\n        image: containerregistrypruebas.azurecr.io/myakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6540",
    "manifest_path": "data/manifests/the_stack_sample/sample_2357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myakscluster-b5e2\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myakscluster-b5e2\n  template:\n    metadata:\n      labels:\n        app: myakscluster-b5e2\n    spec:\n      containers:\n      - name: myakscluster-b5e2\n        image: containerregistrypruebas.azurecr.io/myakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"myakscluster-b5e2\" does not have a read-only root file system"
  },
  {
    "id": "6541",
    "manifest_path": "data/manifests/the_stack_sample/sample_2357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myakscluster-b5e2\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myakscluster-b5e2\n  template:\n    metadata:\n      labels:\n        app: myakscluster-b5e2\n    spec:\n      containers:\n      - name: myakscluster-b5e2\n        image: containerregistrypruebas.azurecr.io/myakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"myakscluster-b5e2\" is not set to runAsNonRoot"
  },
  {
    "id": "6542",
    "manifest_path": "data/manifests/the_stack_sample/sample_2357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myakscluster-b5e2\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myakscluster-b5e2\n  template:\n    metadata:\n      labels:\n        app: myakscluster-b5e2\n    spec:\n      containers:\n      - name: myakscluster-b5e2\n        image: containerregistrypruebas.azurecr.io/myakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"myakscluster-b5e2\" has cpu request 0"
  },
  {
    "id": "6543",
    "manifest_path": "data/manifests/the_stack_sample/sample_2357.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myakscluster-b5e2\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myakscluster-b5e2\n  template:\n    metadata:\n      labels:\n        app: myakscluster-b5e2\n    spec:\n      containers:\n      - name: myakscluster-b5e2\n        image: containerregistrypruebas.azurecr.io/myakscluster\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"myakscluster-b5e2\" has memory limit 0"
  },
  {
    "id": "6544",
    "manifest_path": "data/manifests/the_stack_sample/sample_2358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    app: blue\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: blue\n  template:\n    metadata:\n      labels:\n        app: blue\n    spec:\n      containers:\n      - name: blue\n        image: gabrielrufino/devops-capstone\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"blue\" is using an invalid container image, \"gabrielrufino/devops-capstone\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6545",
    "manifest_path": "data/manifests/the_stack_sample/sample_2358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    app: blue\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: blue\n  template:\n    metadata:\n      labels:\n        app: blue\n    spec:\n      containers:\n      - name: blue\n        image: gabrielrufino/devops-capstone\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6546",
    "manifest_path": "data/manifests/the_stack_sample/sample_2358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    app: blue\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: blue\n  template:\n    metadata:\n      labels:\n        app: blue\n    spec:\n      containers:\n      - name: blue\n        image: gabrielrufino/devops-capstone\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"blue\" does not have a read-only root file system"
  },
  {
    "id": "6547",
    "manifest_path": "data/manifests/the_stack_sample/sample_2358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    app: blue\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: blue\n  template:\n    metadata:\n      labels:\n        app: blue\n    spec:\n      containers:\n      - name: blue\n        image: gabrielrufino/devops-capstone\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"blue\" is not set to runAsNonRoot"
  },
  {
    "id": "6548",
    "manifest_path": "data/manifests/the_stack_sample/sample_2358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    app: blue\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: blue\n  template:\n    metadata:\n      labels:\n        app: blue\n    spec:\n      containers:\n      - name: blue\n        image: gabrielrufino/devops-capstone\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"blue\" has cpu request 0"
  },
  {
    "id": "6549",
    "manifest_path": "data/manifests/the_stack_sample/sample_2358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    app: blue\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: blue\n  template:\n    metadata:\n      labels:\n        app: blue\n    spec:\n      containers:\n      - name: blue\n        image: gabrielrufino/devops-capstone\n        ports:\n        - containerPort: 3000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"blue\" has memory limit 0"
  },
  {
    "id": "6550",
    "manifest_path": "data/manifests/the_stack_sample/sample_2359.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: solr\n  name: solr\nspec:\n  ports:\n  - port: 8983\n  selector:\n    name: solr\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:solr])"
  },
  {
    "id": "6551",
    "manifest_path": "data/manifests/the_stack_sample/sample_2360.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: whis-ui\n  labels:\n    app: whis-ui\n    env: dev\nspec:\n  ports:\n  - name: web-frontend\n    protocol: TCP\n    port: 8080\n    targetPort: 8080\n  selector:\n    app: whis-ui\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:whis-ui])"
  },
  {
    "id": "6552",
    "manifest_path": "data/manifests/the_stack_sample/sample_2363.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hook\n  labels:\n    chart: lighthouse-1.0.31\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations: {}\n  namespace: jx\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 8080\n    protocol: TCP\n    name: http\n  selector:\n    app: lighthouse-webhooks\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:lighthouse-webhooks])"
  },
  {
    "id": "6553",
    "manifest_path": "data/manifests/the_stack_sample/sample_2364.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: monitoring\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: docker/default\nspec:\n  selector:\n    matchLabels:\n      p8s-app: cadvisor\n  template:\n    metadata:\n      labels:\n        p8s-app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: k8s.gcr.io/cadvisor:v0.33.0\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 150m\n          limits:\n            cpu: 300m\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: disk\n          mountPath: /dev/disk\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n      - name: disk\n        hostPath:\n          path: /dev/disk\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cadvisor\" does not have a read-only root file system"
  },
  {
    "id": "6554",
    "manifest_path": "data/manifests/the_stack_sample/sample_2364.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: monitoring\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: docker/default\nspec:\n  selector:\n    matchLabels:\n      p8s-app: cadvisor\n  template:\n    metadata:\n      labels:\n        p8s-app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: k8s.gcr.io/cadvisor:v0.33.0\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 150m\n          limits:\n            cpu: 300m\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: disk\n          mountPath: /dev/disk\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n      - name: disk\n        hostPath:\n          path: /dev/disk\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cadvisor\" is not set to runAsNonRoot"
  },
  {
    "id": "6555",
    "manifest_path": "data/manifests/the_stack_sample/sample_2364.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: monitoring\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: docker/default\nspec:\n  selector:\n    matchLabels:\n      p8s-app: cadvisor\n  template:\n    metadata:\n      labels:\n        p8s-app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: k8s.gcr.io/cadvisor:v0.33.0\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 150m\n          limits:\n            cpu: 300m\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: disk\n          mountPath: /dev/disk\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n      - name: disk\n        hostPath:\n          path: /dev/disk\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"cadvisor\""
  },
  {
    "id": "6556",
    "manifest_path": "data/manifests/the_stack_sample/sample_2364.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: monitoring\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: docker/default\nspec:\n  selector:\n    matchLabels:\n      p8s-app: cadvisor\n  template:\n    metadata:\n      labels:\n        p8s-app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: k8s.gcr.io/cadvisor:v0.33.0\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 150m\n          limits:\n            cpu: 300m\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: disk\n          mountPath: /dev/disk\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n      - name: disk\n        hostPath:\n          path: /dev/disk\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"cadvisor\""
  },
  {
    "id": "6557",
    "manifest_path": "data/manifests/the_stack_sample/sample_2364.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: monitoring\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: docker/default\nspec:\n  selector:\n    matchLabels:\n      p8s-app: cadvisor\n  template:\n    metadata:\n      labels:\n        p8s-app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: k8s.gcr.io/cadvisor:v0.33.0\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 150m\n          limits:\n            cpu: 300m\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: disk\n          mountPath: /dev/disk\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n      - name: disk\n        hostPath:\n          path: /dev/disk\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cadvisor\" has memory limit 0"
  },
  {
    "id": "6558",
    "manifest_path": "data/manifests/the_stack_sample/sample_2365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - echo Hello from the debian container > /pod-data/index.html\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"debian-container\" is using an invalid container image, \"debian\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6559",
    "manifest_path": "data/manifests/the_stack_sample/sample_2365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - echo Hello from the debian container > /pod-data/index.html\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6560",
    "manifest_path": "data/manifests/the_stack_sample/sample_2365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - echo Hello from the debian container > /pod-data/index.html\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"debian-container\" does not have a read-only root file system"
  },
  {
    "id": "6561",
    "manifest_path": "data/manifests/the_stack_sample/sample_2365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - echo Hello from the debian container > /pod-data/index.html\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "6562",
    "manifest_path": "data/manifests/the_stack_sample/sample_2365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - echo Hello from the debian container > /pod-data/index.html\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"debian-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6563",
    "manifest_path": "data/manifests/the_stack_sample/sample_2365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - echo Hello from the debian container > /pod-data/index.html\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6564",
    "manifest_path": "data/manifests/the_stack_sample/sample_2365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - echo Hello from the debian container > /pod-data/index.html\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"debian-container\" has cpu request 0"
  },
  {
    "id": "6565",
    "manifest_path": "data/manifests/the_stack_sample/sample_2365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - echo Hello from the debian container > /pod-data/index.html\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "6566",
    "manifest_path": "data/manifests/the_stack_sample/sample_2365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - echo Hello from the debian container > /pod-data/index.html\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"debian-container\" has memory limit 0"
  },
  {
    "id": "6567",
    "manifest_path": "data/manifests/the_stack_sample/sample_2365.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - echo Hello from the debian container > /pod-data/index.html\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "6568",
    "manifest_path": "data/manifests/the_stack_sample/sample_2367.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: zipkin\n  labels:\n    service: zipkin\nspec:\n  ports:\n  - name: standard\n    port: 9411\n  selector:\n    service: zipkin\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:zipkin])"
  },
  {
    "id": "6569",
    "manifest_path": "data/manifests/the_stack_sample/sample_2373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        image: directxman12/k8s-prometheus-adapter-amd64\n        args:\n        - --secure-port=6443\n        - --logtostderr=true\n        - --prometheus-url=http://prometheus:9090/\n        - --metrics-relist-interval=1m\n        - --v=6\n        - --config=/etc/adapter/config.yaml\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /var/run/serving-cert\n          name: volume-serving-cert\n          readOnly: true\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n        - mountPath: /tmp\n          name: tmp-vol\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n      volumes:\n      - name: volume-serving-cert\n        secret:\n          optional: true\n          secretName: cm-adapter-serving-certs\n      - name: config\n        configMap:\n          name: adapter-config\n      - name: tmp-vol\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"custom-metrics-apiserver\" is using an invalid container image, \"directxman12/k8s-prometheus-adapter-amd64\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6570",
    "manifest_path": "data/manifests/the_stack_sample/sample_2373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        image: directxman12/k8s-prometheus-adapter-amd64\n        args:\n        - --secure-port=6443\n        - --logtostderr=true\n        - --prometheus-url=http://prometheus:9090/\n        - --metrics-relist-interval=1m\n        - --v=6\n        - --config=/etc/adapter/config.yaml\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /var/run/serving-cert\n          name: volume-serving-cert\n          readOnly: true\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n        - mountPath: /tmp\n          name: tmp-vol\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n      volumes:\n      - name: volume-serving-cert\n        secret:\n          optional: true\n          secretName: cm-adapter-serving-certs\n      - name: config\n        configMap:\n          name: adapter-config\n      - name: tmp-vol\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"custom-metrics-apiserver\" does not have a read-only root file system"
  },
  {
    "id": "6571",
    "manifest_path": "data/manifests/the_stack_sample/sample_2373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        image: directxman12/k8s-prometheus-adapter-amd64\n        args:\n        - --secure-port=6443\n        - --logtostderr=true\n        - --prometheus-url=http://prometheus:9090/\n        - --metrics-relist-interval=1m\n        - --v=6\n        - --config=/etc/adapter/config.yaml\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /var/run/serving-cert\n          name: volume-serving-cert\n          readOnly: true\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n        - mountPath: /tmp\n          name: tmp-vol\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n      volumes:\n      - name: volume-serving-cert\n        secret:\n          optional: true\n          secretName: cm-adapter-serving-certs\n      - name: config\n        configMap:\n          name: adapter-config\n      - name: tmp-vol\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"custom-metrics-apiserver\" not found"
  },
  {
    "id": "6572",
    "manifest_path": "data/manifests/the_stack_sample/sample_2373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        image: directxman12/k8s-prometheus-adapter-amd64\n        args:\n        - --secure-port=6443\n        - --logtostderr=true\n        - --prometheus-url=http://prometheus:9090/\n        - --metrics-relist-interval=1m\n        - --v=6\n        - --config=/etc/adapter/config.yaml\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /var/run/serving-cert\n          name: volume-serving-cert\n          readOnly: true\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n        - mountPath: /tmp\n          name: tmp-vol\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n      volumes:\n      - name: volume-serving-cert\n        secret:\n          optional: true\n          secretName: cm-adapter-serving-certs\n      - name: config\n        configMap:\n          name: adapter-config\n      - name: tmp-vol\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"custom-metrics-apiserver\" is not set to runAsNonRoot"
  },
  {
    "id": "6573",
    "manifest_path": "data/manifests/the_stack_sample/sample_2373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        image: directxman12/k8s-prometheus-adapter-amd64\n        args:\n        - --secure-port=6443\n        - --logtostderr=true\n        - --prometheus-url=http://prometheus:9090/\n        - --metrics-relist-interval=1m\n        - --v=6\n        - --config=/etc/adapter/config.yaml\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /var/run/serving-cert\n          name: volume-serving-cert\n          readOnly: true\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n        - mountPath: /tmp\n          name: tmp-vol\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n      volumes:\n      - name: volume-serving-cert\n        secret:\n          optional: true\n          secretName: cm-adapter-serving-certs\n      - name: config\n        configMap:\n          name: adapter-config\n      - name: tmp-vol\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"custom-metrics-apiserver\" has cpu request 0"
  },
  {
    "id": "6574",
    "manifest_path": "data/manifests/the_stack_sample/sample_2373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: custom-metrics-apiserver\n  name: custom-metrics-apiserver\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: custom-metrics-apiserver\n  template:\n    metadata:\n      labels:\n        app: custom-metrics-apiserver\n      name: custom-metrics-apiserver\n    spec:\n      serviceAccountName: custom-metrics-apiserver\n      containers:\n      - name: custom-metrics-apiserver\n        image: directxman12/k8s-prometheus-adapter-amd64\n        args:\n        - --secure-port=6443\n        - --logtostderr=true\n        - --prometheus-url=http://prometheus:9090/\n        - --metrics-relist-interval=1m\n        - --v=6\n        - --config=/etc/adapter/config.yaml\n        ports:\n        - containerPort: 6443\n        volumeMounts:\n        - mountPath: /var/run/serving-cert\n          name: volume-serving-cert\n          readOnly: true\n        - mountPath: /etc/adapter/\n          name: config\n          readOnly: true\n        - mountPath: /tmp\n          name: tmp-vol\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n      volumes:\n      - name: volume-serving-cert\n        secret:\n          optional: true\n          secretName: cm-adapter-serving-certs\n      - name: config\n        configMap:\n          name: adapter-config\n      - name: tmp-vol\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"custom-metrics-apiserver\" has memory limit 0"
  },
  {
    "id": "6575",
    "manifest_path": "data/manifests/the_stack_sample/sample_2374.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: coturn\n  name: coturn\nspec:\n  type: NodePort\n  clusterIP: 10.3.0.221\n  ports:\n  - port: 3478\n    nodePort: 30045\n    name: az\n  - port: 25000\n    nodePort: 30000\n    name: a0\n  - port: 25001\n    nodePort: 30001\n    name: a1\n  - port: 25002\n    nodePort: 30002\n    name: a2\n  - port: 25003\n    nodePort: 30003\n    name: a3\n  - port: 25004\n    nodePort: 30004\n    name: a4\n  - port: 25005\n    nodePort: 30005\n    name: a5\n  - port: 25006\n    nodePort: 30006\n    name: a6\n  - port: 25007\n    nodePort: 30007\n    name: a7\n  - port: 25008\n    nodePort: 30008\n    name: a8\n  - port: 25009\n    nodePort: 30009\n    name: a9\n  selector:\n    app: coturn\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:coturn])"
  },
  {
    "id": "6576",
    "manifest_path": "data/manifests/the_stack_sample/sample_2375.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: milvus-e2e\n  namespace: jenkins\nspec:\n  containers:\n  - name: main\n    image: harbor.zilliz.cc/dockerhub/milvusdb/krte:20211213-dcc15e9\n    env:\n    - name: DOCKER_IN_DOCKER_ENABLED\n      value: 'true'\n    - name: DOCKER_VOLUME_DIRECTORY\n      value: /mnt/disk/.docker\n    securityContext:\n      privileged: true\n    args:\n    - cat\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n    volumeMounts:\n    - mountPath: /docker-graph\n      name: docker-graph\n    - mountPath: /var/lib/docker\n      name: docker-root\n    - mountPath: /lib/modules\n      name: modules\n      readOnly: true\n    - mountPath: /sys/fs/cgroup\n      name: cgroup\n    - mountPath: /mnt/disk/.docker\n      name: build-cache\n      subPath: docker-volume\n  - name: pytest\n    image: harbor.zilliz.cc/dockerhub/milvusdb/pytest:20220118-3eb5cc4\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n  volumes:\n  - emptyDir: {}\n    name: docker-graph\n  - emptyDir: {}\n    name: docker-root\n  - hostPath:\n      path: /tmp/krte/cache\n      type: DirectoryOrCreate\n    name: build-cache\n  - hostPath:\n      path: /lib/modules\n      type: Directory\n    name: modules\n  - hostPath:\n      path: /sys/fs/cgroup\n      type: Directory\n    name: cgroup\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "6577",
    "manifest_path": "data/manifests/the_stack_sample/sample_2375.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: milvus-e2e\n  namespace: jenkins\nspec:\n  containers:\n  - name: main\n    image: harbor.zilliz.cc/dockerhub/milvusdb/krte:20211213-dcc15e9\n    env:\n    - name: DOCKER_IN_DOCKER_ENABLED\n      value: 'true'\n    - name: DOCKER_VOLUME_DIRECTORY\n      value: /mnt/disk/.docker\n    securityContext:\n      privileged: true\n    args:\n    - cat\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n    volumeMounts:\n    - mountPath: /docker-graph\n      name: docker-graph\n    - mountPath: /var/lib/docker\n      name: docker-root\n    - mountPath: /lib/modules\n      name: modules\n      readOnly: true\n    - mountPath: /sys/fs/cgroup\n      name: cgroup\n    - mountPath: /mnt/disk/.docker\n      name: build-cache\n      subPath: docker-volume\n  - name: pytest\n    image: harbor.zilliz.cc/dockerhub/milvusdb/pytest:20220118-3eb5cc4\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n  volumes:\n  - emptyDir: {}\n    name: docker-graph\n  - emptyDir: {}\n    name: docker-root\n  - hostPath:\n      path: /tmp/krte/cache\n      type: DirectoryOrCreate\n    name: build-cache\n  - hostPath:\n      path: /lib/modules\n      type: Directory\n    name: modules\n  - hostPath:\n      path: /sys/fs/cgroup\n      type: Directory\n    name: cgroup\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pytest\" does not have a read-only root file system"
  },
  {
    "id": "6578",
    "manifest_path": "data/manifests/the_stack_sample/sample_2375.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: milvus-e2e\n  namespace: jenkins\nspec:\n  containers:\n  - name: main\n    image: harbor.zilliz.cc/dockerhub/milvusdb/krte:20211213-dcc15e9\n    env:\n    - name: DOCKER_IN_DOCKER_ENABLED\n      value: 'true'\n    - name: DOCKER_VOLUME_DIRECTORY\n      value: /mnt/disk/.docker\n    securityContext:\n      privileged: true\n    args:\n    - cat\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n    volumeMounts:\n    - mountPath: /docker-graph\n      name: docker-graph\n    - mountPath: /var/lib/docker\n      name: docker-root\n    - mountPath: /lib/modules\n      name: modules\n      readOnly: true\n    - mountPath: /sys/fs/cgroup\n      name: cgroup\n    - mountPath: /mnt/disk/.docker\n      name: build-cache\n      subPath: docker-volume\n  - name: pytest\n    image: harbor.zilliz.cc/dockerhub/milvusdb/pytest:20220118-3eb5cc4\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n  volumes:\n  - emptyDir: {}\n    name: docker-graph\n  - emptyDir: {}\n    name: docker-root\n  - hostPath:\n      path: /tmp/krte/cache\n      type: DirectoryOrCreate\n    name: build-cache\n  - hostPath:\n      path: /lib/modules\n      type: Directory\n    name: modules\n  - hostPath:\n      path: /sys/fs/cgroup\n      type: Directory\n    name: cgroup\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"main\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "6579",
    "manifest_path": "data/manifests/the_stack_sample/sample_2375.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: milvus-e2e\n  namespace: jenkins\nspec:\n  containers:\n  - name: main\n    image: harbor.zilliz.cc/dockerhub/milvusdb/krte:20211213-dcc15e9\n    env:\n    - name: DOCKER_IN_DOCKER_ENABLED\n      value: 'true'\n    - name: DOCKER_VOLUME_DIRECTORY\n      value: /mnt/disk/.docker\n    securityContext:\n      privileged: true\n    args:\n    - cat\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n    volumeMounts:\n    - mountPath: /docker-graph\n      name: docker-graph\n    - mountPath: /var/lib/docker\n      name: docker-root\n    - mountPath: /lib/modules\n      name: modules\n      readOnly: true\n    - mountPath: /sys/fs/cgroup\n      name: cgroup\n    - mountPath: /mnt/disk/.docker\n      name: build-cache\n      subPath: docker-volume\n  - name: pytest\n    image: harbor.zilliz.cc/dockerhub/milvusdb/pytest:20220118-3eb5cc4\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n  volumes:\n  - emptyDir: {}\n    name: docker-graph\n  - emptyDir: {}\n    name: docker-root\n  - hostPath:\n      path: /tmp/krte/cache\n      type: DirectoryOrCreate\n    name: build-cache\n  - hostPath:\n      path: /lib/modules\n      type: Directory\n    name: modules\n  - hostPath:\n      path: /sys/fs/cgroup\n      type: Directory\n    name: cgroup\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"main\" is privileged"
  },
  {
    "id": "6580",
    "manifest_path": "data/manifests/the_stack_sample/sample_2375.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: milvus-e2e\n  namespace: jenkins\nspec:\n  containers:\n  - name: main\n    image: harbor.zilliz.cc/dockerhub/milvusdb/krte:20211213-dcc15e9\n    env:\n    - name: DOCKER_IN_DOCKER_ENABLED\n      value: 'true'\n    - name: DOCKER_VOLUME_DIRECTORY\n      value: /mnt/disk/.docker\n    securityContext:\n      privileged: true\n    args:\n    - cat\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n    volumeMounts:\n    - mountPath: /docker-graph\n      name: docker-graph\n    - mountPath: /var/lib/docker\n      name: docker-root\n    - mountPath: /lib/modules\n      name: modules\n      readOnly: true\n    - mountPath: /sys/fs/cgroup\n      name: cgroup\n    - mountPath: /mnt/disk/.docker\n      name: build-cache\n      subPath: docker-volume\n  - name: pytest\n    image: harbor.zilliz.cc/dockerhub/milvusdb/pytest:20220118-3eb5cc4\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n  volumes:\n  - emptyDir: {}\n    name: docker-graph\n  - emptyDir: {}\n    name: docker-root\n  - hostPath:\n      path: /tmp/krte/cache\n      type: DirectoryOrCreate\n    name: build-cache\n  - hostPath:\n      path: /lib/modules\n      type: Directory\n    name: modules\n  - hostPath:\n      path: /sys/fs/cgroup\n      type: Directory\n    name: cgroup\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "6581",
    "manifest_path": "data/manifests/the_stack_sample/sample_2375.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: milvus-e2e\n  namespace: jenkins\nspec:\n  containers:\n  - name: main\n    image: harbor.zilliz.cc/dockerhub/milvusdb/krte:20211213-dcc15e9\n    env:\n    - name: DOCKER_IN_DOCKER_ENABLED\n      value: 'true'\n    - name: DOCKER_VOLUME_DIRECTORY\n      value: /mnt/disk/.docker\n    securityContext:\n      privileged: true\n    args:\n    - cat\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n    volumeMounts:\n    - mountPath: /docker-graph\n      name: docker-graph\n    - mountPath: /var/lib/docker\n      name: docker-root\n    - mountPath: /lib/modules\n      name: modules\n      readOnly: true\n    - mountPath: /sys/fs/cgroup\n      name: cgroup\n    - mountPath: /mnt/disk/.docker\n      name: build-cache\n      subPath: docker-volume\n  - name: pytest\n    image: harbor.zilliz.cc/dockerhub/milvusdb/pytest:20220118-3eb5cc4\n    resources:\n      limits:\n        cpu: '6'\n        memory: 12Gi\n      requests:\n        cpu: '0.5'\n        memory: 5Gi\n  volumes:\n  - emptyDir: {}\n    name: docker-graph\n  - emptyDir: {}\n    name: docker-root\n  - hostPath:\n      path: /tmp/krte/cache\n      type: DirectoryOrCreate\n    name: build-cache\n  - hostPath:\n      path: /lib/modules\n      type: Directory\n    name: modules\n  - hostPath:\n      path: /sys/fs/cgroup\n      type: Directory\n    name: cgroup\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pytest\" is not set to runAsNonRoot"
  },
  {
    "id": "6582",
    "manifest_path": "data/manifests/the_stack_sample/sample_2376.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: large-image\nspec:\n  containers:\n  - name: large-image\n    image: nvidia/cuda:11.6.0-devel-ubi8\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"large-image\" does not have a read-only root file system"
  },
  {
    "id": "6583",
    "manifest_path": "data/manifests/the_stack_sample/sample_2376.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: large-image\nspec:\n  containers:\n  - name: large-image\n    image: nvidia/cuda:11.6.0-devel-ubi8\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"large-image\" is not set to runAsNonRoot"
  },
  {
    "id": "6584",
    "manifest_path": "data/manifests/the_stack_sample/sample_2376.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: large-image\nspec:\n  containers:\n  - name: large-image\n    image: nvidia/cuda:11.6.0-devel-ubi8\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"large-image\" has cpu request 0"
  },
  {
    "id": "6585",
    "manifest_path": "data/manifests/the_stack_sample/sample_2376.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: large-image\nspec:\n  containers:\n  - name: large-image\n    image: nvidia/cuda:11.6.0-devel-ubi8\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"large-image\" has memory limit 0"
  },
  {
    "id": "6586",
    "manifest_path": "data/manifests/the_stack_sample/sample_2378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:latest\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n    livenessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"foobar\" is using an invalid container image, \"foo/bar:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6587",
    "manifest_path": "data/manifests/the_stack_sample/sample_2378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:latest\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n    livenessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"foobar\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "6588",
    "manifest_path": "data/manifests/the_stack_sample/sample_2378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:latest\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n    livenessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"foobar\" does not have a read-only root file system"
  },
  {
    "id": "6589",
    "manifest_path": "data/manifests/the_stack_sample/sample_2378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:latest\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n    livenessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"foobar\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "6590",
    "manifest_path": "data/manifests/the_stack_sample/sample_2378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:latest\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n    livenessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"foobar\" is not set to runAsNonRoot"
  },
  {
    "id": "6591",
    "manifest_path": "data/manifests/the_stack_sample/sample_2378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:latest\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n    livenessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"foobar\" has cpu request 0"
  },
  {
    "id": "6592",
    "manifest_path": "data/manifests/the_stack_sample/sample_2378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:latest\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n    livenessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"foobar\" has memory limit 0"
  },
  {
    "id": "6593",
    "manifest_path": "data/manifests/the_stack_sample/sample_2379.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert -c\n    kompose.version: 1.21.0 (992df58d8)\n  labels:\n    io.kompose.service: yc-redis\n  name: yc-redis\nspec:\n  ports:\n  - name: '6379'\n    port: 6379\n    targetPort: 6379\n  selector:\n    io.kompose.service: yc-redis\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[io.kompose.service:yc-redis])"
  },
  {
    "id": "6594",
    "manifest_path": "data/manifests/the_stack_sample/sample_2380.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper\n  namespace: kafka\nspec:\n  ports:\n  - port: 2181\n    name: client\n  selector:\n    app: zookeeper\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:zookeeper])"
  },
  {
    "id": "6595",
    "manifest_path": "data/manifests/the_stack_sample/sample_2383.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3038\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6596",
    "manifest_path": "data/manifests/the_stack_sample/sample_2383.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3038\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6597",
    "manifest_path": "data/manifests/the_stack_sample/sample_2383.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3038\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6598",
    "manifest_path": "data/manifests/the_stack_sample/sample_2383.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3038\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6599",
    "manifest_path": "data/manifests/the_stack_sample/sample_2383.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3038\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6600",
    "manifest_path": "data/manifests/the_stack_sample/sample_2384.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minikube-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: minikube-operator\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        name: minikube-operator\n    spec:\n      serviceAccountName: minikube-operator\n      containers:\n      - name: minikube-operator\n        image: ko://knative.dev/serving-operator/minikube-operator-example/cmd/manager\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: minikube-operator\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving-operator\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        ports:\n        - name: metrics\n          containerPort: 9090\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"minikube-operator\" is using an invalid container image, \"ko://knative.dev/serving-operator/minikube-operator-example/cmd/manager\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6601",
    "manifest_path": "data/manifests/the_stack_sample/sample_2384.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minikube-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: minikube-operator\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        name: minikube-operator\n    spec:\n      serviceAccountName: minikube-operator\n      containers:\n      - name: minikube-operator\n        image: ko://knative.dev/serving-operator/minikube-operator-example/cmd/manager\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: minikube-operator\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving-operator\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        ports:\n        - name: metrics\n          containerPort: 9090\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minikube-operator\" does not have a read-only root file system"
  },
  {
    "id": "6602",
    "manifest_path": "data/manifests/the_stack_sample/sample_2384.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minikube-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: minikube-operator\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        name: minikube-operator\n    spec:\n      serviceAccountName: minikube-operator\n      containers:\n      - name: minikube-operator\n        image: ko://knative.dev/serving-operator/minikube-operator-example/cmd/manager\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: minikube-operator\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving-operator\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        ports:\n        - name: metrics\n          containerPort: 9090\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"minikube-operator\" not found"
  },
  {
    "id": "6603",
    "manifest_path": "data/manifests/the_stack_sample/sample_2384.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minikube-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: minikube-operator\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        name: minikube-operator\n    spec:\n      serviceAccountName: minikube-operator\n      containers:\n      - name: minikube-operator\n        image: ko://knative.dev/serving-operator/minikube-operator-example/cmd/manager\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: minikube-operator\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving-operator\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        ports:\n        - name: metrics\n          containerPort: 9090\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minikube-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "6604",
    "manifest_path": "data/manifests/the_stack_sample/sample_2384.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minikube-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: minikube-operator\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        name: minikube-operator\n    spec:\n      serviceAccountName: minikube-operator\n      containers:\n      - name: minikube-operator\n        image: ko://knative.dev/serving-operator/minikube-operator-example/cmd/manager\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: minikube-operator\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving-operator\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        ports:\n        - name: metrics\n          containerPort: 9090\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minikube-operator\" has cpu request 0"
  },
  {
    "id": "6605",
    "manifest_path": "data/manifests/the_stack_sample/sample_2384.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minikube-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: minikube-operator\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        name: minikube-operator\n    spec:\n      serviceAccountName: minikube-operator\n      containers:\n      - name: minikube-operator\n        image: ko://knative.dev/serving-operator/minikube-operator-example/cmd/manager\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: minikube-operator\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving-operator\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        ports:\n        - name: metrics\n          containerPort: 9090\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minikube-operator\" has memory limit 0"
  },
  {
    "id": "6606",
    "manifest_path": "data/manifests/the_stack_sample/sample_2385.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 0.46.0\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 8080\n    targetPort: http\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: kube-prometheus\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/name:prometheus-operator app.kubernetes.io/part-of:kube-prometheus])"
  },
  {
    "id": "6607",
    "manifest_path": "data/manifests/the_stack_sample/sample_2386.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: '{{{ COMPOSITE_CONTROLLER_NAME }}}'\nspec:\n  selector:\n    application: '{{{ COMPOSITE_CONTROLLER_NAME }}}'\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 8080\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 8443\n",
    "policy_id": "dangling-service",
    "violation_text": "service has invalid label selector: values[0][application]: Invalid value: \"{{{ COMPOSITE_CONTROLLER_NAME }}}\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "6608",
    "manifest_path": "data/manifests/the_stack_sample/sample_2388.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sdx-deliver\n  labels:\n    app: sdx-deliver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sdx-deliver\n  template:\n    metadata:\n      labels:\n        app: sdx-deliver\n    spec:\n      containers:\n      - image: eu.gcr.io/{{ .Values.registry_location }}/sdx-deliver:{{ .Chart.AppVersion\n          }}\n        imagePullPolicy: Always\n        name: sdx-deliver\n        resources:\n          requests:\n            cpu: 250m\n            memory: 512Mi\n          limits:\n            cpu: 250m\n            memory: 512Mi\n        env:\n        - name: PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: project_id\n        - name: LOGGING_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: logging_level\n        - name: DATA_SENSITIVITY\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: data_sensitivity\n        - name: DATA_RECIPIENT\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: data_recipient\n        livenessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 5000\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n          failureThreshold: 3\n      serviceAccountName: sdx-workload-identity\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"sdx-deliver\" does not expose port 5000 for the HTTPGet"
  },
  {
    "id": "6609",
    "manifest_path": "data/manifests/the_stack_sample/sample_2388.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sdx-deliver\n  labels:\n    app: sdx-deliver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sdx-deliver\n  template:\n    metadata:\n      labels:\n        app: sdx-deliver\n    spec:\n      containers:\n      - image: eu.gcr.io/{{ .Values.registry_location }}/sdx-deliver:{{ .Chart.AppVersion\n          }}\n        imagePullPolicy: Always\n        name: sdx-deliver\n        resources:\n          requests:\n            cpu: 250m\n            memory: 512Mi\n          limits:\n            cpu: 250m\n            memory: 512Mi\n        env:\n        - name: PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: project_id\n        - name: LOGGING_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: logging_level\n        - name: DATA_SENSITIVITY\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: data_sensitivity\n        - name: DATA_RECIPIENT\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: data_recipient\n        livenessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 5000\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n          failureThreshold: 3\n      serviceAccountName: sdx-workload-identity\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sdx-deliver\" does not have a read-only root file system"
  },
  {
    "id": "6610",
    "manifest_path": "data/manifests/the_stack_sample/sample_2388.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sdx-deliver\n  labels:\n    app: sdx-deliver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sdx-deliver\n  template:\n    metadata:\n      labels:\n        app: sdx-deliver\n    spec:\n      containers:\n      - image: eu.gcr.io/{{ .Values.registry_location }}/sdx-deliver:{{ .Chart.AppVersion\n          }}\n        imagePullPolicy: Always\n        name: sdx-deliver\n        resources:\n          requests:\n            cpu: 250m\n            memory: 512Mi\n          limits:\n            cpu: 250m\n            memory: 512Mi\n        env:\n        - name: PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: project_id\n        - name: LOGGING_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: logging_level\n        - name: DATA_SENSITIVITY\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: data_sensitivity\n        - name: DATA_RECIPIENT\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: data_recipient\n        livenessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 5000\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n          failureThreshold: 3\n      serviceAccountName: sdx-workload-identity\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"sdx-workload-identity\" not found"
  },
  {
    "id": "6611",
    "manifest_path": "data/manifests/the_stack_sample/sample_2388.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sdx-deliver\n  labels:\n    app: sdx-deliver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sdx-deliver\n  template:\n    metadata:\n      labels:\n        app: sdx-deliver\n    spec:\n      containers:\n      - image: eu.gcr.io/{{ .Values.registry_location }}/sdx-deliver:{{ .Chart.AppVersion\n          }}\n        imagePullPolicy: Always\n        name: sdx-deliver\n        resources:\n          requests:\n            cpu: 250m\n            memory: 512Mi\n          limits:\n            cpu: 250m\n            memory: 512Mi\n        env:\n        - name: PROJECT_ID\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: project_id\n        - name: LOGGING_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: logging_level\n        - name: DATA_SENSITIVITY\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: data_sensitivity\n        - name: DATA_RECIPIENT\n          valueFrom:\n            configMapKeyRef:\n              name: sdx-config\n              key: data_recipient\n        livenessProbe:\n          httpGet:\n            path: /healthcheck\n            port: 5000\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n          failureThreshold: 3\n      serviceAccountName: sdx-workload-identity\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sdx-deliver\" is not set to runAsNonRoot"
  },
  {
    "id": "6612",
    "manifest_path": "data/manifests/the_stack_sample/sample_2389.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2307\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6613",
    "manifest_path": "data/manifests/the_stack_sample/sample_2389.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2307\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6614",
    "manifest_path": "data/manifests/the_stack_sample/sample_2389.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2307\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6615",
    "manifest_path": "data/manifests/the_stack_sample/sample_2389.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2307\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6616",
    "manifest_path": "data/manifests/the_stack_sample/sample_2389.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2307\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6617",
    "manifest_path": "data/manifests/the_stack_sample/sample_2395.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: seldon-core-operator\n    app.kubernetes.io/managed-by: Tiller\n    app.kubernetes.io/name: seldon-core-operator\n    control-plane: seldon-controller-manager\n    controller-tools.k8s.io: '1.0'\n    helm.sh/chart: seldon-core-operator-0.4.1-SNAPSHOT\n  name: webhook-server-service\n  namespace: seldon-system\nspec:\n  ports:\n  - port: 443\n    targetPort: 9876\n  selector:\n    control-plane: seldon-controller-manager\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[control-plane:seldon-controller-manager])"
  },
  {
    "id": "6618",
    "manifest_path": "data/manifests/the_stack_sample/sample_2397.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: forwarding-proxy\n  namespace: snyk-monitor\nspec:\n  selector:\n    matchLabels:\n      app: forwarding-proxy\n  template:\n    metadata:\n      labels:\n        app: forwarding-proxy\n    spec:\n      containers:\n      - name: forwarding-proxy\n        image: docker.io/snyk/runtime-fixtures:tinyproxy\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"forwarding-proxy\" does not have a read-only root file system"
  },
  {
    "id": "6619",
    "manifest_path": "data/manifests/the_stack_sample/sample_2397.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: forwarding-proxy\n  namespace: snyk-monitor\nspec:\n  selector:\n    matchLabels:\n      app: forwarding-proxy\n  template:\n    metadata:\n      labels:\n        app: forwarding-proxy\n    spec:\n      containers:\n      - name: forwarding-proxy\n        image: docker.io/snyk/runtime-fixtures:tinyproxy\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"forwarding-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "6620",
    "manifest_path": "data/manifests/the_stack_sample/sample_2397.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: forwarding-proxy\n  namespace: snyk-monitor\nspec:\n  selector:\n    matchLabels:\n      app: forwarding-proxy\n  template:\n    metadata:\n      labels:\n        app: forwarding-proxy\n    spec:\n      containers:\n      - name: forwarding-proxy\n        image: docker.io/snyk/runtime-fixtures:tinyproxy\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"forwarding-proxy\" has cpu request 0"
  },
  {
    "id": "6621",
    "manifest_path": "data/manifests/the_stack_sample/sample_2401.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: marque\nspec:\n  selector:\n    app: marque\n  ports:\n  - port: 80\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:marque])"
  },
  {
    "id": "6622",
    "manifest_path": "data/manifests/the_stack_sample/sample_2402.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: low-priority\nspec:\n  containers:\n  - name: low-priority\n    image: alpine\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - while true; do echo hello; sleep 10;done\n    resources:\n      requests:\n        memory: 128Mi\n        cpu: 250m\n      limits:\n        memory: 256Mi\n        cpu: 500m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"low-priority\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6623",
    "manifest_path": "data/manifests/the_stack_sample/sample_2402.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: low-priority\nspec:\n  containers:\n  - name: low-priority\n    image: alpine\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - while true; do echo hello; sleep 10;done\n    resources:\n      requests:\n        memory: 128Mi\n        cpu: 250m\n      limits:\n        memory: 256Mi\n        cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"low-priority\" does not have a read-only root file system"
  },
  {
    "id": "6624",
    "manifest_path": "data/manifests/the_stack_sample/sample_2402.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: low-priority\nspec:\n  containers:\n  - name: low-priority\n    image: alpine\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - while true; do echo hello; sleep 10;done\n    resources:\n      requests:\n        memory: 128Mi\n        cpu: 250m\n      limits:\n        memory: 256Mi\n        cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"low-priority\" is not set to runAsNonRoot"
  },
  {
    "id": "6625",
    "manifest_path": "data/manifests/the_stack_sample/sample_2404.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7877\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6626",
    "manifest_path": "data/manifests/the_stack_sample/sample_2404.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7877\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6627",
    "manifest_path": "data/manifests/the_stack_sample/sample_2404.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7877\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6628",
    "manifest_path": "data/manifests/the_stack_sample/sample_2404.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7877\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6629",
    "manifest_path": "data/manifests/the_stack_sample/sample_2404.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7877\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6630",
    "manifest_path": "data/manifests/the_stack_sample/sample_2406.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: slbl7-001\n  namespace: zxcdn\n  labels:\n    pod: slbl7-001\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ' [{ \"name\": \"slbl7-001-macvlan\", \"default-route\":\n      [\"39.137.101.126\"] }]'\nspec:\n  containers:\n  - name: slbl7-001-main\n    image: registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/sbin/init\n    volumeMounts:\n    - name: init-config-volumes\n      mountPath: /home/zte_node/log_exporter/ztetool.ini\n      subPath: ztetool.ini\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    - name: slbl7-config-volumes\n      mountPath: /home/persistence\n    - name: slbl7-nginx-log-volumes\n      mountPath: /home/zxcdn/slbl7/nginx/logs\n    - name: ires-log-volumes\n      mountPath: /home/zxagent/log\n    - name: logexporter-log-volumes\n      mountPath: /home/zte_node/logs/\n    resources:\n      requests:\n        cpu: 2.0\n        memory: 16Gi\n      limits:\n        cpu: 2.0\n        memory: 16Gi\n    securityContext:\n      privileged: true\n  - name: slbl7-001-pg\n    image: registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: ROOTPATH\n      value: /mnt\n    command:\n    - /home/start.sh\n    volumeMounts:\n    - name: pg-root\n      mountPath: /mnt\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    resources:\n      requests:\n        cpu: 1.0\n        memory: 8Gi\n      limits:\n        cpu: 1.0\n        memory: 8Gi\n    securityContext:\n      privileged: true\n  serviceAccount: zxcdn-app\n  volumes:\n  - name: init-config-volumes\n    configMap:\n      name: slbl7-001-config\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: host-time\n    hostPath:\n      path: /etc/localtime\n  - name: host-resolv\n    hostPath:\n      path: /etc/resolv.conf\n  - name: slbl7-config-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/config\n  - name: slbl7-nginx-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/nginx/logs\n  - name: ires-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/ires/logs\n  - name: logexporter-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/logexporter/logs\n  - name: pg-root\n    hostPath:\n      path: /data/ztecdn/slbl7-001/pg/data\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (zxcdn-app), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "6631",
    "manifest_path": "data/manifests/the_stack_sample/sample_2406.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: slbl7-001\n  namespace: zxcdn\n  labels:\n    pod: slbl7-001\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ' [{ \"name\": \"slbl7-001-macvlan\", \"default-route\":\n      [\"39.137.101.126\"] }]'\nspec:\n  containers:\n  - name: slbl7-001-main\n    image: registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/sbin/init\n    volumeMounts:\n    - name: init-config-volumes\n      mountPath: /home/zte_node/log_exporter/ztetool.ini\n      subPath: ztetool.ini\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    - name: slbl7-config-volumes\n      mountPath: /home/persistence\n    - name: slbl7-nginx-log-volumes\n      mountPath: /home/zxcdn/slbl7/nginx/logs\n    - name: ires-log-volumes\n      mountPath: /home/zxagent/log\n    - name: logexporter-log-volumes\n      mountPath: /home/zte_node/logs/\n    resources:\n      requests:\n        cpu: 2.0\n        memory: 16Gi\n      limits:\n        cpu: 2.0\n        memory: 16Gi\n    securityContext:\n      privileged: true\n  - name: slbl7-001-pg\n    image: registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: ROOTPATH\n      value: /mnt\n    command:\n    - /home/start.sh\n    volumeMounts:\n    - name: pg-root\n      mountPath: /mnt\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    resources:\n      requests:\n        cpu: 1.0\n        memory: 8Gi\n      limits:\n        cpu: 1.0\n        memory: 8Gi\n    securityContext:\n      privileged: true\n  serviceAccount: zxcdn-app\n  volumes:\n  - name: init-config-volumes\n    configMap:\n      name: slbl7-001-config\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: host-time\n    hostPath:\n      path: /etc/localtime\n  - name: host-resolv\n    hostPath:\n      path: /etc/resolv.conf\n  - name: slbl7-config-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/config\n  - name: slbl7-nginx-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/nginx/logs\n  - name: ires-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/ires/logs\n  - name: logexporter-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/logexporter/logs\n  - name: pg-root\n    hostPath:\n      path: /data/ztecdn/slbl7-001/pg/data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"slbl7-001-main\" does not have a read-only root file system"
  },
  {
    "id": "6632",
    "manifest_path": "data/manifests/the_stack_sample/sample_2406.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: slbl7-001\n  namespace: zxcdn\n  labels:\n    pod: slbl7-001\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ' [{ \"name\": \"slbl7-001-macvlan\", \"default-route\":\n      [\"39.137.101.126\"] }]'\nspec:\n  containers:\n  - name: slbl7-001-main\n    image: registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/sbin/init\n    volumeMounts:\n    - name: init-config-volumes\n      mountPath: /home/zte_node/log_exporter/ztetool.ini\n      subPath: ztetool.ini\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    - name: slbl7-config-volumes\n      mountPath: /home/persistence\n    - name: slbl7-nginx-log-volumes\n      mountPath: /home/zxcdn/slbl7/nginx/logs\n    - name: ires-log-volumes\n      mountPath: /home/zxagent/log\n    - name: logexporter-log-volumes\n      mountPath: /home/zte_node/logs/\n    resources:\n      requests:\n        cpu: 2.0\n        memory: 16Gi\n      limits:\n        cpu: 2.0\n        memory: 16Gi\n    securityContext:\n      privileged: true\n  - name: slbl7-001-pg\n    image: registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: ROOTPATH\n      value: /mnt\n    command:\n    - /home/start.sh\n    volumeMounts:\n    - name: pg-root\n      mountPath: /mnt\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    resources:\n      requests:\n        cpu: 1.0\n        memory: 8Gi\n      limits:\n        cpu: 1.0\n        memory: 8Gi\n    securityContext:\n      privileged: true\n  serviceAccount: zxcdn-app\n  volumes:\n  - name: init-config-volumes\n    configMap:\n      name: slbl7-001-config\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: host-time\n    hostPath:\n      path: /etc/localtime\n  - name: host-resolv\n    hostPath:\n      path: /etc/resolv.conf\n  - name: slbl7-config-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/config\n  - name: slbl7-nginx-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/nginx/logs\n  - name: ires-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/ires/logs\n  - name: logexporter-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/logexporter/logs\n  - name: pg-root\n    hostPath:\n      path: /data/ztecdn/slbl7-001/pg/data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"slbl7-001-pg\" does not have a read-only root file system"
  },
  {
    "id": "6633",
    "manifest_path": "data/manifests/the_stack_sample/sample_2406.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: slbl7-001\n  namespace: zxcdn\n  labels:\n    pod: slbl7-001\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ' [{ \"name\": \"slbl7-001-macvlan\", \"default-route\":\n      [\"39.137.101.126\"] }]'\nspec:\n  containers:\n  - name: slbl7-001-main\n    image: registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/sbin/init\n    volumeMounts:\n    - name: init-config-volumes\n      mountPath: /home/zte_node/log_exporter/ztetool.ini\n      subPath: ztetool.ini\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    - name: slbl7-config-volumes\n      mountPath: /home/persistence\n    - name: slbl7-nginx-log-volumes\n      mountPath: /home/zxcdn/slbl7/nginx/logs\n    - name: ires-log-volumes\n      mountPath: /home/zxagent/log\n    - name: logexporter-log-volumes\n      mountPath: /home/zte_node/logs/\n    resources:\n      requests:\n        cpu: 2.0\n        memory: 16Gi\n      limits:\n        cpu: 2.0\n        memory: 16Gi\n    securityContext:\n      privileged: true\n  - name: slbl7-001-pg\n    image: registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: ROOTPATH\n      value: /mnt\n    command:\n    - /home/start.sh\n    volumeMounts:\n    - name: pg-root\n      mountPath: /mnt\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    resources:\n      requests:\n        cpu: 1.0\n        memory: 8Gi\n      limits:\n        cpu: 1.0\n        memory: 8Gi\n    securityContext:\n      privileged: true\n  serviceAccount: zxcdn-app\n  volumes:\n  - name: init-config-volumes\n    configMap:\n      name: slbl7-001-config\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: host-time\n    hostPath:\n      path: /etc/localtime\n  - name: host-resolv\n    hostPath:\n      path: /etc/resolv.conf\n  - name: slbl7-config-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/config\n  - name: slbl7-nginx-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/nginx/logs\n  - name: ires-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/ires/logs\n  - name: logexporter-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/logexporter/logs\n  - name: pg-root\n    hostPath:\n      path: /data/ztecdn/slbl7-001/pg/data\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"zxcdn-app\" not found"
  },
  {
    "id": "6634",
    "manifest_path": "data/manifests/the_stack_sample/sample_2406.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: slbl7-001\n  namespace: zxcdn\n  labels:\n    pod: slbl7-001\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ' [{ \"name\": \"slbl7-001-macvlan\", \"default-route\":\n      [\"39.137.101.126\"] }]'\nspec:\n  containers:\n  - name: slbl7-001-main\n    image: registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/sbin/init\n    volumeMounts:\n    - name: init-config-volumes\n      mountPath: /home/zte_node/log_exporter/ztetool.ini\n      subPath: ztetool.ini\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    - name: slbl7-config-volumes\n      mountPath: /home/persistence\n    - name: slbl7-nginx-log-volumes\n      mountPath: /home/zxcdn/slbl7/nginx/logs\n    - name: ires-log-volumes\n      mountPath: /home/zxagent/log\n    - name: logexporter-log-volumes\n      mountPath: /home/zte_node/logs/\n    resources:\n      requests:\n        cpu: 2.0\n        memory: 16Gi\n      limits:\n        cpu: 2.0\n        memory: 16Gi\n    securityContext:\n      privileged: true\n  - name: slbl7-001-pg\n    image: registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: ROOTPATH\n      value: /mnt\n    command:\n    - /home/start.sh\n    volumeMounts:\n    - name: pg-root\n      mountPath: /mnt\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    resources:\n      requests:\n        cpu: 1.0\n        memory: 8Gi\n      limits:\n        cpu: 1.0\n        memory: 8Gi\n    securityContext:\n      privileged: true\n  serviceAccount: zxcdn-app\n  volumes:\n  - name: init-config-volumes\n    configMap:\n      name: slbl7-001-config\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: host-time\n    hostPath:\n      path: /etc/localtime\n  - name: host-resolv\n    hostPath:\n      path: /etc/resolv.conf\n  - name: slbl7-config-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/config\n  - name: slbl7-nginx-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/nginx/logs\n  - name: ires-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/ires/logs\n  - name: logexporter-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/logexporter/logs\n  - name: pg-root\n    hostPath:\n      path: /data/ztecdn/slbl7-001/pg/data\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"slbl7-001-main\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "6635",
    "manifest_path": "data/manifests/the_stack_sample/sample_2406.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: slbl7-001\n  namespace: zxcdn\n  labels:\n    pod: slbl7-001\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ' [{ \"name\": \"slbl7-001-macvlan\", \"default-route\":\n      [\"39.137.101.126\"] }]'\nspec:\n  containers:\n  - name: slbl7-001-main\n    image: registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/sbin/init\n    volumeMounts:\n    - name: init-config-volumes\n      mountPath: /home/zte_node/log_exporter/ztetool.ini\n      subPath: ztetool.ini\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    - name: slbl7-config-volumes\n      mountPath: /home/persistence\n    - name: slbl7-nginx-log-volumes\n      mountPath: /home/zxcdn/slbl7/nginx/logs\n    - name: ires-log-volumes\n      mountPath: /home/zxagent/log\n    - name: logexporter-log-volumes\n      mountPath: /home/zte_node/logs/\n    resources:\n      requests:\n        cpu: 2.0\n        memory: 16Gi\n      limits:\n        cpu: 2.0\n        memory: 16Gi\n    securityContext:\n      privileged: true\n  - name: slbl7-001-pg\n    image: registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: ROOTPATH\n      value: /mnt\n    command:\n    - /home/start.sh\n    volumeMounts:\n    - name: pg-root\n      mountPath: /mnt\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    resources:\n      requests:\n        cpu: 1.0\n        memory: 8Gi\n      limits:\n        cpu: 1.0\n        memory: 8Gi\n    securityContext:\n      privileged: true\n  serviceAccount: zxcdn-app\n  volumes:\n  - name: init-config-volumes\n    configMap:\n      name: slbl7-001-config\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: host-time\n    hostPath:\n      path: /etc/localtime\n  - name: host-resolv\n    hostPath:\n      path: /etc/resolv.conf\n  - name: slbl7-config-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/config\n  - name: slbl7-nginx-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/nginx/logs\n  - name: ires-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/ires/logs\n  - name: logexporter-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/logexporter/logs\n  - name: pg-root\n    hostPath:\n      path: /data/ztecdn/slbl7-001/pg/data\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"slbl7-001-pg\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "6636",
    "manifest_path": "data/manifests/the_stack_sample/sample_2406.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: slbl7-001\n  namespace: zxcdn\n  labels:\n    pod: slbl7-001\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ' [{ \"name\": \"slbl7-001-macvlan\", \"default-route\":\n      [\"39.137.101.126\"] }]'\nspec:\n  containers:\n  - name: slbl7-001-main\n    image: registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/sbin/init\n    volumeMounts:\n    - name: init-config-volumes\n      mountPath: /home/zte_node/log_exporter/ztetool.ini\n      subPath: ztetool.ini\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    - name: slbl7-config-volumes\n      mountPath: /home/persistence\n    - name: slbl7-nginx-log-volumes\n      mountPath: /home/zxcdn/slbl7/nginx/logs\n    - name: ires-log-volumes\n      mountPath: /home/zxagent/log\n    - name: logexporter-log-volumes\n      mountPath: /home/zte_node/logs/\n    resources:\n      requests:\n        cpu: 2.0\n        memory: 16Gi\n      limits:\n        cpu: 2.0\n        memory: 16Gi\n    securityContext:\n      privileged: true\n  - name: slbl7-001-pg\n    image: registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: ROOTPATH\n      value: /mnt\n    command:\n    - /home/start.sh\n    volumeMounts:\n    - name: pg-root\n      mountPath: /mnt\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    resources:\n      requests:\n        cpu: 1.0\n        memory: 8Gi\n      limits:\n        cpu: 1.0\n        memory: 8Gi\n    securityContext:\n      privileged: true\n  serviceAccount: zxcdn-app\n  volumes:\n  - name: init-config-volumes\n    configMap:\n      name: slbl7-001-config\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: host-time\n    hostPath:\n      path: /etc/localtime\n  - name: host-resolv\n    hostPath:\n      path: /etc/resolv.conf\n  - name: slbl7-config-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/config\n  - name: slbl7-nginx-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/nginx/logs\n  - name: ires-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/ires/logs\n  - name: logexporter-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/logexporter/logs\n  - name: pg-root\n    hostPath:\n      path: /data/ztecdn/slbl7-001/pg/data\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"slbl7-001-main\" is privileged"
  },
  {
    "id": "6637",
    "manifest_path": "data/manifests/the_stack_sample/sample_2406.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: slbl7-001\n  namespace: zxcdn\n  labels:\n    pod: slbl7-001\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ' [{ \"name\": \"slbl7-001-macvlan\", \"default-route\":\n      [\"39.137.101.126\"] }]'\nspec:\n  containers:\n  - name: slbl7-001-main\n    image: registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/sbin/init\n    volumeMounts:\n    - name: init-config-volumes\n      mountPath: /home/zte_node/log_exporter/ztetool.ini\n      subPath: ztetool.ini\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    - name: slbl7-config-volumes\n      mountPath: /home/persistence\n    - name: slbl7-nginx-log-volumes\n      mountPath: /home/zxcdn/slbl7/nginx/logs\n    - name: ires-log-volumes\n      mountPath: /home/zxagent/log\n    - name: logexporter-log-volumes\n      mountPath: /home/zte_node/logs/\n    resources:\n      requests:\n        cpu: 2.0\n        memory: 16Gi\n      limits:\n        cpu: 2.0\n        memory: 16Gi\n    securityContext:\n      privileged: true\n  - name: slbl7-001-pg\n    image: registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: ROOTPATH\n      value: /mnt\n    command:\n    - /home/start.sh\n    volumeMounts:\n    - name: pg-root\n      mountPath: /mnt\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    resources:\n      requests:\n        cpu: 1.0\n        memory: 8Gi\n      limits:\n        cpu: 1.0\n        memory: 8Gi\n    securityContext:\n      privileged: true\n  serviceAccount: zxcdn-app\n  volumes:\n  - name: init-config-volumes\n    configMap:\n      name: slbl7-001-config\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: host-time\n    hostPath:\n      path: /etc/localtime\n  - name: host-resolv\n    hostPath:\n      path: /etc/resolv.conf\n  - name: slbl7-config-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/config\n  - name: slbl7-nginx-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/nginx/logs\n  - name: ires-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/ires/logs\n  - name: logexporter-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/logexporter/logs\n  - name: pg-root\n    hostPath:\n      path: /data/ztecdn/slbl7-001/pg/data\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"slbl7-001-pg\" is privileged"
  },
  {
    "id": "6638",
    "manifest_path": "data/manifests/the_stack_sample/sample_2406.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: slbl7-001\n  namespace: zxcdn\n  labels:\n    pod: slbl7-001\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ' [{ \"name\": \"slbl7-001-macvlan\", \"default-route\":\n      [\"39.137.101.126\"] }]'\nspec:\n  containers:\n  - name: slbl7-001-main\n    image: registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/sbin/init\n    volumeMounts:\n    - name: init-config-volumes\n      mountPath: /home/zte_node/log_exporter/ztetool.ini\n      subPath: ztetool.ini\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    - name: slbl7-config-volumes\n      mountPath: /home/persistence\n    - name: slbl7-nginx-log-volumes\n      mountPath: /home/zxcdn/slbl7/nginx/logs\n    - name: ires-log-volumes\n      mountPath: /home/zxagent/log\n    - name: logexporter-log-volumes\n      mountPath: /home/zte_node/logs/\n    resources:\n      requests:\n        cpu: 2.0\n        memory: 16Gi\n      limits:\n        cpu: 2.0\n        memory: 16Gi\n    securityContext:\n      privileged: true\n  - name: slbl7-001-pg\n    image: registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: ROOTPATH\n      value: /mnt\n    command:\n    - /home/start.sh\n    volumeMounts:\n    - name: pg-root\n      mountPath: /mnt\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    resources:\n      requests:\n        cpu: 1.0\n        memory: 8Gi\n      limits:\n        cpu: 1.0\n        memory: 8Gi\n    securityContext:\n      privileged: true\n  serviceAccount: zxcdn-app\n  volumes:\n  - name: init-config-volumes\n    configMap:\n      name: slbl7-001-config\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: host-time\n    hostPath:\n      path: /etc/localtime\n  - name: host-resolv\n    hostPath:\n      path: /etc/resolv.conf\n  - name: slbl7-config-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/config\n  - name: slbl7-nginx-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/nginx/logs\n  - name: ires-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/ires/logs\n  - name: logexporter-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/logexporter/logs\n  - name: pg-root\n    hostPath:\n      path: /data/ztecdn/slbl7-001/pg/data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"slbl7-001-main\" is not set to runAsNonRoot"
  },
  {
    "id": "6639",
    "manifest_path": "data/manifests/the_stack_sample/sample_2406.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: slbl7-001\n  namespace: zxcdn\n  labels:\n    pod: slbl7-001\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ' [{ \"name\": \"slbl7-001-macvlan\", \"default-route\":\n      [\"39.137.101.126\"] }]'\nspec:\n  containers:\n  - name: slbl7-001-main\n    image: registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/sbin/init\n    volumeMounts:\n    - name: init-config-volumes\n      mountPath: /home/zte_node/log_exporter/ztetool.ini\n      subPath: ztetool.ini\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    - name: slbl7-config-volumes\n      mountPath: /home/persistence\n    - name: slbl7-nginx-log-volumes\n      mountPath: /home/zxcdn/slbl7/nginx/logs\n    - name: ires-log-volumes\n      mountPath: /home/zxagent/log\n    - name: logexporter-log-volumes\n      mountPath: /home/zte_node/logs/\n    resources:\n      requests:\n        cpu: 2.0\n        memory: 16Gi\n      limits:\n        cpu: 2.0\n        memory: 16Gi\n    securityContext:\n      privileged: true\n  - name: slbl7-001-pg\n    image: registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: ROOTPATH\n      value: /mnt\n    command:\n    - /home/start.sh\n    volumeMounts:\n    - name: pg-root\n      mountPath: /mnt\n    - name: host-time\n      mountPath: /etc/localtime\n    - name: host-resolv\n      mountPath: /etc/resolv.conf\n    resources:\n      requests:\n        cpu: 1.0\n        memory: 8Gi\n      limits:\n        cpu: 1.0\n        memory: 8Gi\n    securityContext:\n      privileged: true\n  serviceAccount: zxcdn-app\n  volumes:\n  - name: init-config-volumes\n    configMap:\n      name: slbl7-001-config\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: host-time\n    hostPath:\n      path: /etc/localtime\n  - name: host-resolv\n    hostPath:\n      path: /etc/resolv.conf\n  - name: slbl7-config-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/config\n  - name: slbl7-nginx-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/nginx/logs\n  - name: ires-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/ires/logs\n  - name: logexporter-log-volumes\n    hostPath:\n      path: /data/ztecdn/slbl7-001/logexporter/logs\n  - name: pg-root\n    hostPath:\n      path: /data/ztecdn/slbl7-001/pg/data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"slbl7-001-pg\" is not set to runAsNonRoot"
  },
  {
    "id": "6640",
    "manifest_path": "data/manifests/the_stack_sample/sample_2409.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      containers:\n      - name: jobmanager\n        image: ghcr.io/cau-se/theodolite-uc4-flink:latest\n        env:\n        - name: KAFKA_BOOTSTRAP_SERVERS\n          value: theodolite-cp-kafka:9092\n        - name: SCHEMA_REGISTRY_URL\n          value: http://theodolite-cp-schema-registry:8081\n        - name: COMMIT_INTERVAL_MS\n          value: '100'\n        - name: CHECKPOINTING\n          value: 'false'\n        - name: PARALLELISM\n          value: '1'\n        - name: FLINK_STATE_BACKEND\n          value: rocksdb\n        - name: JOB_MANAGER_RPC_ADDRESS\n          value: flink-jobmanager\n        - name: FLINK_PROPERTIES\n          value: 'blob.server.port: 6124\n\n            jobmanager.rpc.port: 6123\n\n            taskmanager.rpc.port: 6122\n\n            queryable-state.proxy.ports: 6125\n\n            jobmanager.memory.process.size: 4Gb\n\n            taskmanager.memory.process.size: 4Gb\n\n            #parallelism.default: 1 #TODO\n\n            '\n        resources:\n          limits:\n            memory: 4Gi\n            cpu: 1000m\n        args:\n        - standalone-job\n        - --job-classname\n        - theodolite.uc4.application.AggregationServiceFlinkJob\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob-server\n        - containerPort: 8081\n          name: webui\n        - containerPort: 9249\n          name: metrics\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume-rw\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 9999\n      initContainers:\n      - name: init-jobmanager\n        image: busybox:1.28\n        command:\n        - cp\n        - -a\n        - /flink-config/.\n        - /flink-config-rw/\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /flink-config/\n        - name: flink-config-volume-rw\n          mountPath: /flink-config-rw/\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j-console.properties\n            path: log4j-console.properties\n      - name: flink-config-volume-rw\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"jobmanager\" is using an invalid container image, \"ghcr.io/cau-se/theodolite-uc4-flink:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6641",
    "manifest_path": "data/manifests/the_stack_sample/sample_2409.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      containers:\n      - name: jobmanager\n        image: ghcr.io/cau-se/theodolite-uc4-flink:latest\n        env:\n        - name: KAFKA_BOOTSTRAP_SERVERS\n          value: theodolite-cp-kafka:9092\n        - name: SCHEMA_REGISTRY_URL\n          value: http://theodolite-cp-schema-registry:8081\n        - name: COMMIT_INTERVAL_MS\n          value: '100'\n        - name: CHECKPOINTING\n          value: 'false'\n        - name: PARALLELISM\n          value: '1'\n        - name: FLINK_STATE_BACKEND\n          value: rocksdb\n        - name: JOB_MANAGER_RPC_ADDRESS\n          value: flink-jobmanager\n        - name: FLINK_PROPERTIES\n          value: 'blob.server.port: 6124\n\n            jobmanager.rpc.port: 6123\n\n            taskmanager.rpc.port: 6122\n\n            queryable-state.proxy.ports: 6125\n\n            jobmanager.memory.process.size: 4Gb\n\n            taskmanager.memory.process.size: 4Gb\n\n            #parallelism.default: 1 #TODO\n\n            '\n        resources:\n          limits:\n            memory: 4Gi\n            cpu: 1000m\n        args:\n        - standalone-job\n        - --job-classname\n        - theodolite.uc4.application.AggregationServiceFlinkJob\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob-server\n        - containerPort: 8081\n          name: webui\n        - containerPort: 9249\n          name: metrics\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume-rw\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 9999\n      initContainers:\n      - name: init-jobmanager\n        image: busybox:1.28\n        command:\n        - cp\n        - -a\n        - /flink-config/.\n        - /flink-config-rw/\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /flink-config/\n        - name: flink-config-volume-rw\n          mountPath: /flink-config-rw/\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j-console.properties\n            path: log4j-console.properties\n      - name: flink-config-volume-rw\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-jobmanager\" does not have a read-only root file system"
  },
  {
    "id": "6642",
    "manifest_path": "data/manifests/the_stack_sample/sample_2409.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      containers:\n      - name: jobmanager\n        image: ghcr.io/cau-se/theodolite-uc4-flink:latest\n        env:\n        - name: KAFKA_BOOTSTRAP_SERVERS\n          value: theodolite-cp-kafka:9092\n        - name: SCHEMA_REGISTRY_URL\n          value: http://theodolite-cp-schema-registry:8081\n        - name: COMMIT_INTERVAL_MS\n          value: '100'\n        - name: CHECKPOINTING\n          value: 'false'\n        - name: PARALLELISM\n          value: '1'\n        - name: FLINK_STATE_BACKEND\n          value: rocksdb\n        - name: JOB_MANAGER_RPC_ADDRESS\n          value: flink-jobmanager\n        - name: FLINK_PROPERTIES\n          value: 'blob.server.port: 6124\n\n            jobmanager.rpc.port: 6123\n\n            taskmanager.rpc.port: 6122\n\n            queryable-state.proxy.ports: 6125\n\n            jobmanager.memory.process.size: 4Gb\n\n            taskmanager.memory.process.size: 4Gb\n\n            #parallelism.default: 1 #TODO\n\n            '\n        resources:\n          limits:\n            memory: 4Gi\n            cpu: 1000m\n        args:\n        - standalone-job\n        - --job-classname\n        - theodolite.uc4.application.AggregationServiceFlinkJob\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob-server\n        - containerPort: 8081\n          name: webui\n        - containerPort: 9249\n          name: metrics\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume-rw\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 9999\n      initContainers:\n      - name: init-jobmanager\n        image: busybox:1.28\n        command:\n        - cp\n        - -a\n        - /flink-config/.\n        - /flink-config-rw/\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /flink-config/\n        - name: flink-config-volume-rw\n          mountPath: /flink-config-rw/\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j-console.properties\n            path: log4j-console.properties\n      - name: flink-config-volume-rw\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jobmanager\" does not have a read-only root file system"
  },
  {
    "id": "6643",
    "manifest_path": "data/manifests/the_stack_sample/sample_2409.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      containers:\n      - name: jobmanager\n        image: ghcr.io/cau-se/theodolite-uc4-flink:latest\n        env:\n        - name: KAFKA_BOOTSTRAP_SERVERS\n          value: theodolite-cp-kafka:9092\n        - name: SCHEMA_REGISTRY_URL\n          value: http://theodolite-cp-schema-registry:8081\n        - name: COMMIT_INTERVAL_MS\n          value: '100'\n        - name: CHECKPOINTING\n          value: 'false'\n        - name: PARALLELISM\n          value: '1'\n        - name: FLINK_STATE_BACKEND\n          value: rocksdb\n        - name: JOB_MANAGER_RPC_ADDRESS\n          value: flink-jobmanager\n        - name: FLINK_PROPERTIES\n          value: 'blob.server.port: 6124\n\n            jobmanager.rpc.port: 6123\n\n            taskmanager.rpc.port: 6122\n\n            queryable-state.proxy.ports: 6125\n\n            jobmanager.memory.process.size: 4Gb\n\n            taskmanager.memory.process.size: 4Gb\n\n            #parallelism.default: 1 #TODO\n\n            '\n        resources:\n          limits:\n            memory: 4Gi\n            cpu: 1000m\n        args:\n        - standalone-job\n        - --job-classname\n        - theodolite.uc4.application.AggregationServiceFlinkJob\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob-server\n        - containerPort: 8081\n          name: webui\n        - containerPort: 9249\n          name: metrics\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume-rw\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 9999\n      initContainers:\n      - name: init-jobmanager\n        image: busybox:1.28\n        command:\n        - cp\n        - -a\n        - /flink-config/.\n        - /flink-config-rw/\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /flink-config/\n        - name: flink-config-volume-rw\n          mountPath: /flink-config-rw/\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j-console.properties\n            path: log4j-console.properties\n      - name: flink-config-volume-rw\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-jobmanager\" is not set to runAsNonRoot"
  },
  {
    "id": "6644",
    "manifest_path": "data/manifests/the_stack_sample/sample_2409.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      containers:\n      - name: jobmanager\n        image: ghcr.io/cau-se/theodolite-uc4-flink:latest\n        env:\n        - name: KAFKA_BOOTSTRAP_SERVERS\n          value: theodolite-cp-kafka:9092\n        - name: SCHEMA_REGISTRY_URL\n          value: http://theodolite-cp-schema-registry:8081\n        - name: COMMIT_INTERVAL_MS\n          value: '100'\n        - name: CHECKPOINTING\n          value: 'false'\n        - name: PARALLELISM\n          value: '1'\n        - name: FLINK_STATE_BACKEND\n          value: rocksdb\n        - name: JOB_MANAGER_RPC_ADDRESS\n          value: flink-jobmanager\n        - name: FLINK_PROPERTIES\n          value: 'blob.server.port: 6124\n\n            jobmanager.rpc.port: 6123\n\n            taskmanager.rpc.port: 6122\n\n            queryable-state.proxy.ports: 6125\n\n            jobmanager.memory.process.size: 4Gb\n\n            taskmanager.memory.process.size: 4Gb\n\n            #parallelism.default: 1 #TODO\n\n            '\n        resources:\n          limits:\n            memory: 4Gi\n            cpu: 1000m\n        args:\n        - standalone-job\n        - --job-classname\n        - theodolite.uc4.application.AggregationServiceFlinkJob\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob-server\n        - containerPort: 8081\n          name: webui\n        - containerPort: 9249\n          name: metrics\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume-rw\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 9999\n      initContainers:\n      - name: init-jobmanager\n        image: busybox:1.28\n        command:\n        - cp\n        - -a\n        - /flink-config/.\n        - /flink-config-rw/\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /flink-config/\n        - name: flink-config-volume-rw\n          mountPath: /flink-config-rw/\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j-console.properties\n            path: log4j-console.properties\n      - name: flink-config-volume-rw\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-jobmanager\" has cpu request 0"
  },
  {
    "id": "6645",
    "manifest_path": "data/manifests/the_stack_sample/sample_2409.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      containers:\n      - name: jobmanager\n        image: ghcr.io/cau-se/theodolite-uc4-flink:latest\n        env:\n        - name: KAFKA_BOOTSTRAP_SERVERS\n          value: theodolite-cp-kafka:9092\n        - name: SCHEMA_REGISTRY_URL\n          value: http://theodolite-cp-schema-registry:8081\n        - name: COMMIT_INTERVAL_MS\n          value: '100'\n        - name: CHECKPOINTING\n          value: 'false'\n        - name: PARALLELISM\n          value: '1'\n        - name: FLINK_STATE_BACKEND\n          value: rocksdb\n        - name: JOB_MANAGER_RPC_ADDRESS\n          value: flink-jobmanager\n        - name: FLINK_PROPERTIES\n          value: 'blob.server.port: 6124\n\n            jobmanager.rpc.port: 6123\n\n            taskmanager.rpc.port: 6122\n\n            queryable-state.proxy.ports: 6125\n\n            jobmanager.memory.process.size: 4Gb\n\n            taskmanager.memory.process.size: 4Gb\n\n            #parallelism.default: 1 #TODO\n\n            '\n        resources:\n          limits:\n            memory: 4Gi\n            cpu: 1000m\n        args:\n        - standalone-job\n        - --job-classname\n        - theodolite.uc4.application.AggregationServiceFlinkJob\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob-server\n        - containerPort: 8081\n          name: webui\n        - containerPort: 9249\n          name: metrics\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume-rw\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 9999\n      initContainers:\n      - name: init-jobmanager\n        image: busybox:1.28\n        command:\n        - cp\n        - -a\n        - /flink-config/.\n        - /flink-config-rw/\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /flink-config/\n        - name: flink-config-volume-rw\n          mountPath: /flink-config-rw/\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j-console.properties\n            path: log4j-console.properties\n      - name: flink-config-volume-rw\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jobmanager\" has cpu request 0"
  },
  {
    "id": "6646",
    "manifest_path": "data/manifests/the_stack_sample/sample_2409.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flink-jobmanager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flink\n      component: jobmanager\n  template:\n    metadata:\n      labels:\n        app: flink\n        component: jobmanager\n    spec:\n      containers:\n      - name: jobmanager\n        image: ghcr.io/cau-se/theodolite-uc4-flink:latest\n        env:\n        - name: KAFKA_BOOTSTRAP_SERVERS\n          value: theodolite-cp-kafka:9092\n        - name: SCHEMA_REGISTRY_URL\n          value: http://theodolite-cp-schema-registry:8081\n        - name: COMMIT_INTERVAL_MS\n          value: '100'\n        - name: CHECKPOINTING\n          value: 'false'\n        - name: PARALLELISM\n          value: '1'\n        - name: FLINK_STATE_BACKEND\n          value: rocksdb\n        - name: JOB_MANAGER_RPC_ADDRESS\n          value: flink-jobmanager\n        - name: FLINK_PROPERTIES\n          value: 'blob.server.port: 6124\n\n            jobmanager.rpc.port: 6123\n\n            taskmanager.rpc.port: 6122\n\n            queryable-state.proxy.ports: 6125\n\n            jobmanager.memory.process.size: 4Gb\n\n            taskmanager.memory.process.size: 4Gb\n\n            #parallelism.default: 1 #TODO\n\n            '\n        resources:\n          limits:\n            memory: 4Gi\n            cpu: 1000m\n        args:\n        - standalone-job\n        - --job-classname\n        - theodolite.uc4.application.AggregationServiceFlinkJob\n        ports:\n        - containerPort: 6123\n          name: rpc\n        - containerPort: 6124\n          name: blob-server\n        - containerPort: 8081\n          name: webui\n        - containerPort: 9249\n          name: metrics\n        livenessProbe:\n          tcpSocket:\n            port: 6123\n          initialDelaySeconds: 30\n          periodSeconds: 60\n        volumeMounts:\n        - name: flink-config-volume-rw\n          mountPath: /opt/flink/conf\n        securityContext:\n          runAsUser: 9999\n      initContainers:\n      - name: init-jobmanager\n        image: busybox:1.28\n        command:\n        - cp\n        - -a\n        - /flink-config/.\n        - /flink-config-rw/\n        volumeMounts:\n        - name: flink-config-volume\n          mountPath: /flink-config/\n        - name: flink-config-volume-rw\n          mountPath: /flink-config-rw/\n      volumes:\n      - name: flink-config-volume\n        configMap:\n          name: flink-config\n          items:\n          - key: flink-conf.yaml\n            path: flink-conf.yaml\n          - key: log4j-console.properties\n            path: log4j-console.properties\n      - name: flink-config-volume-rw\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-jobmanager\" has memory limit 0"
  },
  {
    "id": "6647",
    "manifest_path": "data/manifests/the_stack_sample/sample_2412.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: monitoring-kube-prometheus-coredns\n  labels:\n    app: kube-prometheus-stack-coredns\n    jobLabel: coredns\n    chart: kube-prometheus-stack-10.3.3\n    release: monitoring\n    heritage: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 9153\n    protocol: TCP\n    targetPort: 9153\n  selector:\n    k8s-app: kube-dns\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kube-dns])"
  },
  {
    "id": "6648",
    "manifest_path": "data/manifests/the_stack_sample/sample_2413.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: annotated-pod\n  annotations:\n    commit: 866a8dc\n    author: Benjamin Muschko\n    branch: bm/bugfix\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6649",
    "manifest_path": "data/manifests/the_stack_sample/sample_2413.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: annotated-pod\n  annotations:\n    commit: 866a8dc\n    author: Benjamin Muschko\n    branch: bm/bugfix\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6650",
    "manifest_path": "data/manifests/the_stack_sample/sample_2413.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: annotated-pod\n  annotations:\n    commit: 866a8dc\n    author: Benjamin Muschko\n    branch: bm/bugfix\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6651",
    "manifest_path": "data/manifests/the_stack_sample/sample_2413.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: annotated-pod\n  annotations:\n    commit: 866a8dc\n    author: Benjamin Muschko\n    branch: bm/bugfix\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6652",
    "manifest_path": "data/manifests/the_stack_sample/sample_2413.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: annotated-pod\n  annotations:\n    commit: 866a8dc\n    author: Benjamin Muschko\n    branch: bm/bugfix\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6653",
    "manifest_path": "data/manifests/the_stack_sample/sample_2414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sukhwanpipelinesjavascriptdocker222\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sukhwanpipelinesjavascriptdocker222\n  template:\n    metadata:\n      labels:\n        app: sukhwanpipelinesjavascriptdocker222\n    spec:\n      containers:\n      - name: sukhwanpipelinesjavascriptdocker222\n        image: myakscr01.azurecr.io/sukhwanpipelinesjavascriptdocker222\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sukhwanpipelinesjavascriptdocker222\" is using an invalid container image, \"myakscr01.azurecr.io/sukhwanpipelinesjavascriptdocker222\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6654",
    "manifest_path": "data/manifests/the_stack_sample/sample_2414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sukhwanpipelinesjavascriptdocker222\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sukhwanpipelinesjavascriptdocker222\n  template:\n    metadata:\n      labels:\n        app: sukhwanpipelinesjavascriptdocker222\n    spec:\n      containers:\n      - name: sukhwanpipelinesjavascriptdocker222\n        image: myakscr01.azurecr.io/sukhwanpipelinesjavascriptdocker222\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sukhwanpipelinesjavascriptdocker222\" does not have a read-only root file system"
  },
  {
    "id": "6655",
    "manifest_path": "data/manifests/the_stack_sample/sample_2414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sukhwanpipelinesjavascriptdocker222\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sukhwanpipelinesjavascriptdocker222\n  template:\n    metadata:\n      labels:\n        app: sukhwanpipelinesjavascriptdocker222\n    spec:\n      containers:\n      - name: sukhwanpipelinesjavascriptdocker222\n        image: myakscr01.azurecr.io/sukhwanpipelinesjavascriptdocker222\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sukhwanpipelinesjavascriptdocker222\" is not set to runAsNonRoot"
  },
  {
    "id": "6656",
    "manifest_path": "data/manifests/the_stack_sample/sample_2414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sukhwanpipelinesjavascriptdocker222\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sukhwanpipelinesjavascriptdocker222\n  template:\n    metadata:\n      labels:\n        app: sukhwanpipelinesjavascriptdocker222\n    spec:\n      containers:\n      - name: sukhwanpipelinesjavascriptdocker222\n        image: myakscr01.azurecr.io/sukhwanpipelinesjavascriptdocker222\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sukhwanpipelinesjavascriptdocker222\" has cpu request 0"
  },
  {
    "id": "6657",
    "manifest_path": "data/manifests/the_stack_sample/sample_2414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sukhwanpipelinesjavascriptdocker222\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sukhwanpipelinesjavascriptdocker222\n  template:\n    metadata:\n      labels:\n        app: sukhwanpipelinesjavascriptdocker222\n    spec:\n      containers:\n      - name: sukhwanpipelinesjavascriptdocker222\n        image: myakscr01.azurecr.io/sukhwanpipelinesjavascriptdocker222\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sukhwanpipelinesjavascriptdocker222\" has memory limit 0"
  },
  {
    "id": "6658",
    "manifest_path": "data/manifests/the_stack_sample/sample_2415.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: large-container-daemonset\n  namespace: default\n  labels:\n    app: large-container-daemonset\nspec:\n  selector:\n    matchLabels:\n      app: large-container-daemonset\n  template:\n    metadata:\n      labels:\n        app: large-container-daemonset\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/os\n                operator: In\n                values:\n                - linux\n      containers:\n      - name: large-container\n        image: mcr.microsoft.com/oss/azcu/go-dev:v1.29.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do sleep 1000; done\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"large-container\" does not have a read-only root file system"
  },
  {
    "id": "6659",
    "manifest_path": "data/manifests/the_stack_sample/sample_2415.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: large-container-daemonset\n  namespace: default\n  labels:\n    app: large-container-daemonset\nspec:\n  selector:\n    matchLabels:\n      app: large-container-daemonset\n  template:\n    metadata:\n      labels:\n        app: large-container-daemonset\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/os\n                operator: In\n                values:\n                - linux\n      containers:\n      - name: large-container\n        image: mcr.microsoft.com/oss/azcu/go-dev:v1.29.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do sleep 1000; done\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"large-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6660",
    "manifest_path": "data/manifests/the_stack_sample/sample_2415.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: large-container-daemonset\n  namespace: default\n  labels:\n    app: large-container-daemonset\nspec:\n  selector:\n    matchLabels:\n      app: large-container-daemonset\n  template:\n    metadata:\n      labels:\n        app: large-container-daemonset\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/os\n                operator: In\n                values:\n                - linux\n      containers:\n      - name: large-container\n        image: mcr.microsoft.com/oss/azcu/go-dev:v1.29.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do sleep 1000; done\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"large-container\" has cpu request 0"
  },
  {
    "id": "6661",
    "manifest_path": "data/manifests/the_stack_sample/sample_2415.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: large-container-daemonset\n  namespace: default\n  labels:\n    app: large-container-daemonset\nspec:\n  selector:\n    matchLabels:\n      app: large-container-daemonset\n  template:\n    metadata:\n      labels:\n        app: large-container-daemonset\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/os\n                operator: In\n                values:\n                - linux\n      containers:\n      - name: large-container\n        image: mcr.microsoft.com/oss/azcu/go-dev:v1.29.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - while true; do sleep 1000; done\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"large-container\" has memory limit 0"
  },
  {
    "id": "6662",
    "manifest_path": "data/manifests/the_stack_sample/sample_2416.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20190927-a9e239ad8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "6663",
    "manifest_path": "data/manifests/the_stack_sample/sample_2416.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20190927-a9e239ad8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "6664",
    "manifest_path": "data/manifests/the_stack_sample/sample_2416.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20190927-a9e239ad8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "6665",
    "manifest_path": "data/manifests/the_stack_sample/sample_2416.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20190927-a9e239ad8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "6666",
    "manifest_path": "data/manifests/the_stack_sample/sample_2417.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: golang-test\n  labels:\n    app: golang-test\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: golang-test\n  template:\n    metadata:\n      labels:\n        app: golang-test\n    spec:\n      containers:\n      - name: golang-test\n        image: rootduck/golang-test:1.0.3\n        envFrom:\n        - configMapRef:\n            name: style2-configmap\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6667",
    "manifest_path": "data/manifests/the_stack_sample/sample_2417.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: golang-test\n  labels:\n    app: golang-test\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: golang-test\n  template:\n    metadata:\n      labels:\n        app: golang-test\n    spec:\n      containers:\n      - name: golang-test\n        image: rootduck/golang-test:1.0.3\n        envFrom:\n        - configMapRef:\n            name: style2-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"golang-test\" does not have a read-only root file system"
  },
  {
    "id": "6668",
    "manifest_path": "data/manifests/the_stack_sample/sample_2417.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: golang-test\n  labels:\n    app: golang-test\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: golang-test\n  template:\n    metadata:\n      labels:\n        app: golang-test\n    spec:\n      containers:\n      - name: golang-test\n        image: rootduck/golang-test:1.0.3\n        envFrom:\n        - configMapRef:\n            name: style2-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"golang-test\" is not set to runAsNonRoot"
  },
  {
    "id": "6669",
    "manifest_path": "data/manifests/the_stack_sample/sample_2417.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: golang-test\n  labels:\n    app: golang-test\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: golang-test\n  template:\n    metadata:\n      labels:\n        app: golang-test\n    spec:\n      containers:\n      - name: golang-test\n        image: rootduck/golang-test:1.0.3\n        envFrom:\n        - configMapRef:\n            name: style2-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"golang-test\" has cpu request 0"
  },
  {
    "id": "6670",
    "manifest_path": "data/manifests/the_stack_sample/sample_2417.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: golang-test\n  labels:\n    app: golang-test\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: golang-test\n  template:\n    metadata:\n      labels:\n        app: golang-test\n    spec:\n      containers:\n      - name: golang-test\n        image: rootduck/golang-test:1.0.3\n        envFrom:\n        - configMapRef:\n            name: style2-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"golang-test\" has memory limit 0"
  },
  {
    "id": "6671",
    "manifest_path": "data/manifests/the_stack_sample/sample_2418.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1841\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6672",
    "manifest_path": "data/manifests/the_stack_sample/sample_2418.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1841\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6673",
    "manifest_path": "data/manifests/the_stack_sample/sample_2418.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1841\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6674",
    "manifest_path": "data/manifests/the_stack_sample/sample_2418.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1841\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6675",
    "manifest_path": "data/manifests/the_stack_sample/sample_2418.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1841\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6676",
    "manifest_path": "data/manifests/the_stack_sample/sample_2419.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: catalogue-db\n  labels:\n    app: catalogue-db\n  namespace: sock-shop\nspec:\n  ports:\n  - port: 3306\n    targetPort: 3306\n  selector:\n    app: catalogue-db\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:catalogue-db])"
  },
  {
    "id": "6677",
    "manifest_path": "data/manifests/the_stack_sample/sample_2421.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\n  namespace: postgres\nspec:\n  selector:\n    matchLabels:\n      app: postgres\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12.4-alpine\n        env:\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          value: password\n        - name: PGDATA\n          value: /var/lib/postgresql/data\n        ports:\n        - containerPort: 5432\n          name: tcp\n          protocol: TCP\n        volumeMounts:\n        - name: postgres-pvc\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "6678",
    "manifest_path": "data/manifests/the_stack_sample/sample_2421.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\n  namespace: postgres\nspec:\n  selector:\n    matchLabels:\n      app: postgres\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12.4-alpine\n        env:\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          value: password\n        - name: PGDATA\n          value: /var/lib/postgresql/data\n        ports:\n        - containerPort: 5432\n          name: tcp\n          protocol: TCP\n        volumeMounts:\n        - name: postgres-pvc\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "6679",
    "manifest_path": "data/manifests/the_stack_sample/sample_2421.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\n  namespace: postgres\nspec:\n  selector:\n    matchLabels:\n      app: postgres\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12.4-alpine\n        env:\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          value: password\n        - name: PGDATA\n          value: /var/lib/postgresql/data\n        ports:\n        - containerPort: 5432\n          name: tcp\n          protocol: TCP\n        volumeMounts:\n        - name: postgres-pvc\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "6680",
    "manifest_path": "data/manifests/the_stack_sample/sample_2421.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\n  namespace: postgres\nspec:\n  selector:\n    matchLabels:\n      app: postgres\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12.4-alpine\n        env:\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          value: password\n        - name: PGDATA\n          value: /var/lib/postgresql/data\n        ports:\n        - containerPort: 5432\n          name: tcp\n          protocol: TCP\n        volumeMounts:\n        - name: postgres-pvc\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "6681",
    "manifest_path": "data/manifests/the_stack_sample/sample_2424.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-svc\n  labels:\n    app: frontend-svc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend-svc\n  template:\n    metadata:\n      labels:\n        app: frontend-svc\n    spec:\n      containers:\n      - name: udagram-image-frontend\n        image: fgaviria/udagram-image-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            memory: 10Mi\n            cpu: '0.1'\n          limits:\n            memory: 128Mi\n            cpu: '0.5'\n        env:\n        - name: FORCE_DEPLOY\n          value: '1'\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 3\n          periodSeconds: 60\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 3\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"udagram-image-frontend\" is using an invalid container image, \"fgaviria/udagram-image-frontend:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6682",
    "manifest_path": "data/manifests/the_stack_sample/sample_2424.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-svc\n  labels:\n    app: frontend-svc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend-svc\n  template:\n    metadata:\n      labels:\n        app: frontend-svc\n    spec:\n      containers:\n      - name: udagram-image-frontend\n        image: fgaviria/udagram-image-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            memory: 10Mi\n            cpu: '0.1'\n          limits:\n            memory: 128Mi\n            cpu: '0.5'\n        env:\n        - name: FORCE_DEPLOY\n          value: '1'\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 3\n          periodSeconds: 60\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"udagram-image-frontend\" does not have a read-only root file system"
  },
  {
    "id": "6683",
    "manifest_path": "data/manifests/the_stack_sample/sample_2424.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-svc\n  labels:\n    app: frontend-svc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend-svc\n  template:\n    metadata:\n      labels:\n        app: frontend-svc\n    spec:\n      containers:\n      - name: udagram-image-frontend\n        image: fgaviria/udagram-image-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            memory: 10Mi\n            cpu: '0.1'\n          limits:\n            memory: 128Mi\n            cpu: '0.5'\n        env:\n        - name: FORCE_DEPLOY\n          value: '1'\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 3\n          periodSeconds: 60\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"udagram-image-frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "6684",
    "manifest_path": "data/manifests/the_stack_sample/sample_2425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: alb-ingress-controller\n  name: alb-ingress-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alb-ingress-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alb-ingress-controller\n    spec:\n      containers:\n      - name: alb-ingress-controller\n        args:\n        - --ingress-class=alb\n        - --cluster-name=robotshop-git\n        image: docker.io/amazon/aws-alb-ingress-controller:v1.1.2\n      serviceAccountName: alb-ingress-controller\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"alb-ingress-controller\" does not have a read-only root file system"
  },
  {
    "id": "6685",
    "manifest_path": "data/manifests/the_stack_sample/sample_2425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: alb-ingress-controller\n  name: alb-ingress-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alb-ingress-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alb-ingress-controller\n    spec:\n      containers:\n      - name: alb-ingress-controller\n        args:\n        - --ingress-class=alb\n        - --cluster-name=robotshop-git\n        image: docker.io/amazon/aws-alb-ingress-controller:v1.1.2\n      serviceAccountName: alb-ingress-controller\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"alb-ingress-controller\" not found"
  },
  {
    "id": "6686",
    "manifest_path": "data/manifests/the_stack_sample/sample_2425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: alb-ingress-controller\n  name: alb-ingress-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alb-ingress-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alb-ingress-controller\n    spec:\n      containers:\n      - name: alb-ingress-controller\n        args:\n        - --ingress-class=alb\n        - --cluster-name=robotshop-git\n        image: docker.io/amazon/aws-alb-ingress-controller:v1.1.2\n      serviceAccountName: alb-ingress-controller\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"alb-ingress-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "6687",
    "manifest_path": "data/manifests/the_stack_sample/sample_2425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: alb-ingress-controller\n  name: alb-ingress-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alb-ingress-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alb-ingress-controller\n    spec:\n      containers:\n      - name: alb-ingress-controller\n        args:\n        - --ingress-class=alb\n        - --cluster-name=robotshop-git\n        image: docker.io/amazon/aws-alb-ingress-controller:v1.1.2\n      serviceAccountName: alb-ingress-controller\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"alb-ingress-controller\" has cpu request 0"
  },
  {
    "id": "6688",
    "manifest_path": "data/manifests/the_stack_sample/sample_2425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: alb-ingress-controller\n  name: alb-ingress-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alb-ingress-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alb-ingress-controller\n    spec:\n      containers:\n      - name: alb-ingress-controller\n        args:\n        - --ingress-class=alb\n        - --cluster-name=robotshop-git\n        image: docker.io/amazon/aws-alb-ingress-controller:v1.1.2\n      serviceAccountName: alb-ingress-controller\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"alb-ingress-controller\" has memory limit 0"
  },
  {
    "id": "6689",
    "manifest_path": "data/manifests/the_stack_sample/sample_2427.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-service-hive-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-service-hive\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-service-hive\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-service-hive\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-service-hive\n        image: zhangrong1027/linkis:linkis-mdm-service-hive-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22003\n        livenessProbe:\n          tcpSocket:\n            port: 22003\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22003'\n        volumeMounts:\n        - name: linkis-mdm-service-hive-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-service-hive/logs\n      volumes:\n      - name: linkis-mdm-service-hive-config\n        configMap:\n          name: linkis-mdm-service-hive-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"linkis-mdm-service-hive\" does not have a read-only root file system"
  },
  {
    "id": "6690",
    "manifest_path": "data/manifests/the_stack_sample/sample_2427.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-service-hive-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-service-hive\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-service-hive\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-service-hive\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-service-hive\n        image: zhangrong1027/linkis:linkis-mdm-service-hive-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22003\n        livenessProbe:\n          tcpSocket:\n            port: 22003\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22003'\n        volumeMounts:\n        - name: linkis-mdm-service-hive-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-service-hive/logs\n      volumes:\n      - name: linkis-mdm-service-hive-config\n        configMap:\n          name: linkis-mdm-service-hive-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"linkis-mdm-service-hive\" is not set to runAsNonRoot"
  },
  {
    "id": "6691",
    "manifest_path": "data/manifests/the_stack_sample/sample_2427.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-service-hive-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-service-hive\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-service-hive\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-service-hive\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-service-hive\n        image: zhangrong1027/linkis:linkis-mdm-service-hive-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22003\n        livenessProbe:\n          tcpSocket:\n            port: 22003\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22003'\n        volumeMounts:\n        - name: linkis-mdm-service-hive-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-service-hive/logs\n      volumes:\n      - name: linkis-mdm-service-hive-config\n        configMap:\n          name: linkis-mdm-service-hive-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"linkis-mdm-service-hive\" has cpu request 0"
  },
  {
    "id": "6692",
    "manifest_path": "data/manifests/the_stack_sample/sample_2427.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-service-hive-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-service-hive\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-service-hive\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-service-hive\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-service-hive\n        image: zhangrong1027/linkis:linkis-mdm-service-hive-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22003\n        livenessProbe:\n          tcpSocket:\n            port: 22003\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22003'\n        volumeMounts:\n        - name: linkis-mdm-service-hive-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-service-hive/logs\n      volumes:\n      - name: linkis-mdm-service-hive-config\n        configMap:\n          name: linkis-mdm-service-hive-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"linkis-mdm-service-hive\" has memory limit 0"
  },
  {
    "id": "6693",
    "manifest_path": "data/manifests/the_stack_sample/sample_2429.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-0.0.52\n    app.kubernetes.io/version: 0.0.52\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\nspec:\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n  selector:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:jx-pipelines-visualizer app.kubernetes.io/name:jx-pipelines-visualizer])"
  },
  {
    "id": "6694",
    "manifest_path": "data/manifests/the_stack_sample/sample_2433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: comment-deployment\n  labels:\n    app: reddit\n    component: comment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: comment\n  template:\n    metadata:\n      name: comment-pod\n      labels:\n        app: reddit\n        component: comment\n    spec:\n      containers:\n      - image: nightdiverru/comment\n        name: comment\n        env:\n        - name: COMMENT_DATABASE_HOST\n          value: comment-db\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"comment\" is using an invalid container image, \"nightdiverru/comment\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6695",
    "manifest_path": "data/manifests/the_stack_sample/sample_2433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: comment-deployment\n  labels:\n    app: reddit\n    component: comment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: comment\n  template:\n    metadata:\n      name: comment-pod\n      labels:\n        app: reddit\n        component: comment\n    spec:\n      containers:\n      - image: nightdiverru/comment\n        name: comment\n        env:\n        - name: COMMENT_DATABASE_HOST\n          value: comment-db\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6696",
    "manifest_path": "data/manifests/the_stack_sample/sample_2433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: comment-deployment\n  labels:\n    app: reddit\n    component: comment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: comment\n  template:\n    metadata:\n      name: comment-pod\n      labels:\n        app: reddit\n        component: comment\n    spec:\n      containers:\n      - image: nightdiverru/comment\n        name: comment\n        env:\n        - name: COMMENT_DATABASE_HOST\n          value: comment-db\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"comment\" does not have a read-only root file system"
  },
  {
    "id": "6697",
    "manifest_path": "data/manifests/the_stack_sample/sample_2433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: comment-deployment\n  labels:\n    app: reddit\n    component: comment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: comment\n  template:\n    metadata:\n      name: comment-pod\n      labels:\n        app: reddit\n        component: comment\n    spec:\n      containers:\n      - image: nightdiverru/comment\n        name: comment\n        env:\n        - name: COMMENT_DATABASE_HOST\n          value: comment-db\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"comment\" is not set to runAsNonRoot"
  },
  {
    "id": "6698",
    "manifest_path": "data/manifests/the_stack_sample/sample_2433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: comment-deployment\n  labels:\n    app: reddit\n    component: comment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: comment\n  template:\n    metadata:\n      name: comment-pod\n      labels:\n        app: reddit\n        component: comment\n    spec:\n      containers:\n      - image: nightdiverru/comment\n        name: comment\n        env:\n        - name: COMMENT_DATABASE_HOST\n          value: comment-db\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"comment\" has cpu request 0"
  },
  {
    "id": "6699",
    "manifest_path": "data/manifests/the_stack_sample/sample_2433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: comment-deployment\n  labels:\n    app: reddit\n    component: comment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: comment\n  template:\n    metadata:\n      name: comment-pod\n      labels:\n        app: reddit\n        component: comment\n    spec:\n      containers:\n      - image: nightdiverru/comment\n        name: comment\n        env:\n        - name: COMMENT_DATABASE_HOST\n          value: comment-db\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"comment\" has memory limit 0"
  },
  {
    "id": "6700",
    "manifest_path": "data/manifests/the_stack_sample/sample_2434.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: podinfo\n  namespace: test\n  labels:\n    app: podinfo\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 9898\n    targetPort: 9898\n    protocol: TCP\n  selector:\n    app: podinfo\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:podinfo])"
  },
  {
    "id": "6701",
    "manifest_path": "data/manifests/the_stack_sample/sample_2435.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: vote\n  name: vote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vote\n  template:\n    metadata:\n      labels:\n        app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:v1\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vote\" does not have a read-only root file system"
  },
  {
    "id": "6702",
    "manifest_path": "data/manifests/the_stack_sample/sample_2435.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: vote\n  name: vote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vote\n  template:\n    metadata:\n      labels:\n        app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:v1\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vote\" is not set to runAsNonRoot"
  },
  {
    "id": "6703",
    "manifest_path": "data/manifests/the_stack_sample/sample_2435.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: vote\n  name: vote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vote\n  template:\n    metadata:\n      labels:\n        app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:v1\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vote\" has cpu request 0"
  },
  {
    "id": "6704",
    "manifest_path": "data/manifests/the_stack_sample/sample_2435.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: vote\n  name: vote\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vote\n  template:\n    metadata:\n      labels:\n        app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:v1\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vote\" has memory limit 0"
  },
  {
    "id": "6705",
    "manifest_path": "data/manifests/the_stack_sample/sample_2437.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rsocket-requester\n  labels:\n    app: rsocket-requester\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: rsocket-requester\n      labels:\n        app: rsocket-requester\n    spec:\n      containers:\n      - name: rsocket-requester\n        image: rsocket-requester:latest\n        ports:\n        - containerPort: 8180\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: RSOCKET_BROKERS\n          value: tcp://rsocket-broker.rsocket.svc.cluster.local:9999\n  selector:\n    matchLabels:\n      app: rsocket-requester\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rsocket-requester\" is using an invalid container image, \"rsocket-requester:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6706",
    "manifest_path": "data/manifests/the_stack_sample/sample_2437.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rsocket-requester\n  labels:\n    app: rsocket-requester\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: rsocket-requester\n      labels:\n        app: rsocket-requester\n    spec:\n      containers:\n      - name: rsocket-requester\n        image: rsocket-requester:latest\n        ports:\n        - containerPort: 8180\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: RSOCKET_BROKERS\n          value: tcp://rsocket-broker.rsocket.svc.cluster.local:9999\n  selector:\n    matchLabels:\n      app: rsocket-requester\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rsocket-requester\" does not have a read-only root file system"
  },
  {
    "id": "6707",
    "manifest_path": "data/manifests/the_stack_sample/sample_2437.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rsocket-requester\n  labels:\n    app: rsocket-requester\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: rsocket-requester\n      labels:\n        app: rsocket-requester\n    spec:\n      containers:\n      - name: rsocket-requester\n        image: rsocket-requester:latest\n        ports:\n        - containerPort: 8180\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: RSOCKET_BROKERS\n          value: tcp://rsocket-broker.rsocket.svc.cluster.local:9999\n  selector:\n    matchLabels:\n      app: rsocket-requester\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rsocket-requester\" is not set to runAsNonRoot"
  },
  {
    "id": "6708",
    "manifest_path": "data/manifests/the_stack_sample/sample_2437.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rsocket-requester\n  labels:\n    app: rsocket-requester\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: rsocket-requester\n      labels:\n        app: rsocket-requester\n    spec:\n      containers:\n      - name: rsocket-requester\n        image: rsocket-requester:latest\n        ports:\n        - containerPort: 8180\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: RSOCKET_BROKERS\n          value: tcp://rsocket-broker.rsocket.svc.cluster.local:9999\n  selector:\n    matchLabels:\n      app: rsocket-requester\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rsocket-requester\" has cpu request 0"
  },
  {
    "id": "6709",
    "manifest_path": "data/manifests/the_stack_sample/sample_2437.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rsocket-requester\n  labels:\n    app: rsocket-requester\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: rsocket-requester\n      labels:\n        app: rsocket-requester\n    spec:\n      containers:\n      - name: rsocket-requester\n        image: rsocket-requester:latest\n        ports:\n        - containerPort: 8180\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: RSOCKET_BROKERS\n          value: tcp://rsocket-broker.rsocket.svc.cluster.local:9999\n  selector:\n    matchLabels:\n      app: rsocket-requester\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rsocket-requester\" has memory limit 0"
  },
  {
    "id": "6710",
    "manifest_path": "data/manifests/the_stack_sample/sample_2438.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: diguage-manual\nspec:\n  containers:\n  - name: diguage-kubia\n    image: diguage/kubia\n    ports:\n    - containerPort: 8080\n      protocol: TCP\n    resources:\n      limits:\n        cpu: 500m\n      requests:\n        cpu: 500m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"diguage-kubia\" is using an invalid container image, \"diguage/kubia\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6711",
    "manifest_path": "data/manifests/the_stack_sample/sample_2438.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: diguage-manual\nspec:\n  containers:\n  - name: diguage-kubia\n    image: diguage/kubia\n    ports:\n    - containerPort: 8080\n      protocol: TCP\n    resources:\n      limits:\n        cpu: 500m\n      requests:\n        cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"diguage-kubia\" does not have a read-only root file system"
  },
  {
    "id": "6712",
    "manifest_path": "data/manifests/the_stack_sample/sample_2438.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: diguage-manual\nspec:\n  containers:\n  - name: diguage-kubia\n    image: diguage/kubia\n    ports:\n    - containerPort: 8080\n      protocol: TCP\n    resources:\n      limits:\n        cpu: 500m\n      requests:\n        cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"diguage-kubia\" is not set to runAsNonRoot"
  },
  {
    "id": "6713",
    "manifest_path": "data/manifests/the_stack_sample/sample_2438.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: diguage-manual\nspec:\n  containers:\n  - name: diguage-kubia\n    image: diguage/kubia\n    ports:\n    - containerPort: 8080\n      protocol: TCP\n    resources:\n      limits:\n        cpu: 500m\n      requests:\n        cpu: 500m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"diguage-kubia\" has memory limit 0"
  },
  {
    "id": "6714",
    "manifest_path": "data/manifests/the_stack_sample/sample_2439.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: queue-master\n  labels:\n    name: queue-master\n    app: sock-shop\n  annotations:\n    litmuschaos.io/chaos: 'true'\n  namespace: sock-shop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: queue-master\n      app: sock-shop\n  template:\n    metadata:\n      labels:\n        name: queue-master\n        app: sock-shop\n    spec:\n      containers:\n      - name: queue-master\n        image: weaveworksdemos/queue-master:0.3.1\n        env:\n        - name: ZIPKIN\n          value: zipkin.jaeger.svc.cluster.local\n        - name: JAVA_OPTS\n          value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC\n            -Djava.security.egd=file:/dev/urandom\n        resources:\n          limits:\n            cpu: 100m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 400Mi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 180\n          periodSeconds: 3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"queue-master\" does not have a read-only root file system"
  },
  {
    "id": "6715",
    "manifest_path": "data/manifests/the_stack_sample/sample_2439.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: queue-master\n  labels:\n    name: queue-master\n    app: sock-shop\n  annotations:\n    litmuschaos.io/chaos: 'true'\n  namespace: sock-shop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: queue-master\n      app: sock-shop\n  template:\n    metadata:\n      labels:\n        name: queue-master\n        app: sock-shop\n    spec:\n      containers:\n      - name: queue-master\n        image: weaveworksdemos/queue-master:0.3.1\n        env:\n        - name: ZIPKIN\n          value: zipkin.jaeger.svc.cluster.local\n        - name: JAVA_OPTS\n          value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC\n            -Djava.security.egd=file:/dev/urandom\n        resources:\n          limits:\n            cpu: 100m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 400Mi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 180\n          periodSeconds: 3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"queue-master\" is not set to runAsNonRoot"
  },
  {
    "id": "6716",
    "manifest_path": "data/manifests/the_stack_sample/sample_2442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: nvidia-device-plugin-daemonset\n  name: nvidia-device-plugin-daemonset\n  namespace: gpu-operator-resources\n  annotations:\n    openshift.io/scc: hostmount-anyuid\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-device-plugin-daemonset\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        app: nvidia-device-plugin-daemonset\n    spec:\n      serviceAccount: nvidia-device-plugin\n      containers:\n      - image: FILLED BY THE OPERATOR\n        name: nvidia-device-plugin-ctr\n        securityContext:\n          privileged: true\n        env:\n        - name: NVIDIA_VISIBLE_DEVICES\n          value: all\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (nvidia-device-plugin), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "6717",
    "manifest_path": "data/manifests/the_stack_sample/sample_2442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: nvidia-device-plugin-daemonset\n  name: nvidia-device-plugin-daemonset\n  namespace: gpu-operator-resources\n  annotations:\n    openshift.io/scc: hostmount-anyuid\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-device-plugin-daemonset\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        app: nvidia-device-plugin-daemonset\n    spec:\n      serviceAccount: nvidia-device-plugin\n      containers:\n      - image: FILLED BY THE OPERATOR\n        name: nvidia-device-plugin-ctr\n        securityContext:\n          privileged: true\n        env:\n        - name: NVIDIA_VISIBLE_DEVICES\n          value: all\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nvidia-device-plugin-ctr\" is using an invalid container image, \"FILLED BY THE OPERATOR\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6718",
    "manifest_path": "data/manifests/the_stack_sample/sample_2442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: nvidia-device-plugin-daemonset\n  name: nvidia-device-plugin-daemonset\n  namespace: gpu-operator-resources\n  annotations:\n    openshift.io/scc: hostmount-anyuid\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-device-plugin-daemonset\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        app: nvidia-device-plugin-daemonset\n    spec:\n      serviceAccount: nvidia-device-plugin\n      containers:\n      - image: FILLED BY THE OPERATOR\n        name: nvidia-device-plugin-ctr\n        securityContext:\n          privileged: true\n        env:\n        - name: NVIDIA_VISIBLE_DEVICES\n          value: all\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nvidia-device-plugin-ctr\" does not have a read-only root file system"
  },
  {
    "id": "6719",
    "manifest_path": "data/manifests/the_stack_sample/sample_2442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: nvidia-device-plugin-daemonset\n  name: nvidia-device-plugin-daemonset\n  namespace: gpu-operator-resources\n  annotations:\n    openshift.io/scc: hostmount-anyuid\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-device-plugin-daemonset\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        app: nvidia-device-plugin-daemonset\n    spec:\n      serviceAccount: nvidia-device-plugin\n      containers:\n      - image: FILLED BY THE OPERATOR\n        name: nvidia-device-plugin-ctr\n        securityContext:\n          privileged: true\n        env:\n        - name: NVIDIA_VISIBLE_DEVICES\n          value: all\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"nvidia-device-plugin\" not found"
  },
  {
    "id": "6720",
    "manifest_path": "data/manifests/the_stack_sample/sample_2442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: nvidia-device-plugin-daemonset\n  name: nvidia-device-plugin-daemonset\n  namespace: gpu-operator-resources\n  annotations:\n    openshift.io/scc: hostmount-anyuid\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-device-plugin-daemonset\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        app: nvidia-device-plugin-daemonset\n    spec:\n      serviceAccount: nvidia-device-plugin\n      containers:\n      - image: FILLED BY THE OPERATOR\n        name: nvidia-device-plugin-ctr\n        securityContext:\n          privileged: true\n        env:\n        - name: NVIDIA_VISIBLE_DEVICES\n          value: all\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nvidia-device-plugin-ctr\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "6721",
    "manifest_path": "data/manifests/the_stack_sample/sample_2442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: nvidia-device-plugin-daemonset\n  name: nvidia-device-plugin-daemonset\n  namespace: gpu-operator-resources\n  annotations:\n    openshift.io/scc: hostmount-anyuid\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-device-plugin-daemonset\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        app: nvidia-device-plugin-daemonset\n    spec:\n      serviceAccount: nvidia-device-plugin\n      containers:\n      - image: FILLED BY THE OPERATOR\n        name: nvidia-device-plugin-ctr\n        securityContext:\n          privileged: true\n        env:\n        - name: NVIDIA_VISIBLE_DEVICES\n          value: all\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"nvidia-device-plugin-ctr\" is privileged"
  },
  {
    "id": "6722",
    "manifest_path": "data/manifests/the_stack_sample/sample_2442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: nvidia-device-plugin-daemonset\n  name: nvidia-device-plugin-daemonset\n  namespace: gpu-operator-resources\n  annotations:\n    openshift.io/scc: hostmount-anyuid\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-device-plugin-daemonset\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        app: nvidia-device-plugin-daemonset\n    spec:\n      serviceAccount: nvidia-device-plugin\n      containers:\n      - image: FILLED BY THE OPERATOR\n        name: nvidia-device-plugin-ctr\n        securityContext:\n          privileged: true\n        env:\n        - name: NVIDIA_VISIBLE_DEVICES\n          value: all\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nvidia-device-plugin-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "6723",
    "manifest_path": "data/manifests/the_stack_sample/sample_2442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: nvidia-device-plugin-daemonset\n  name: nvidia-device-plugin-daemonset\n  namespace: gpu-operator-resources\n  annotations:\n    openshift.io/scc: hostmount-anyuid\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-device-plugin-daemonset\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        app: nvidia-device-plugin-daemonset\n    spec:\n      serviceAccount: nvidia-device-plugin\n      containers:\n      - image: FILLED BY THE OPERATOR\n        name: nvidia-device-plugin-ctr\n        securityContext:\n          privileged: true\n        env:\n        - name: NVIDIA_VISIBLE_DEVICES\n          value: all\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nvidia-device-plugin-ctr\" has cpu request 0"
  },
  {
    "id": "6724",
    "manifest_path": "data/manifests/the_stack_sample/sample_2442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: nvidia-device-plugin-daemonset\n  name: nvidia-device-plugin-daemonset\n  namespace: gpu-operator-resources\n  annotations:\n    openshift.io/scc: hostmount-anyuid\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-device-plugin-daemonset\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        app: nvidia-device-plugin-daemonset\n    spec:\n      serviceAccount: nvidia-device-plugin\n      containers:\n      - image: FILLED BY THE OPERATOR\n        name: nvidia-device-plugin-ctr\n        securityContext:\n          privileged: true\n        env:\n        - name: NVIDIA_VISIBLE_DEVICES\n          value: all\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nvidia-device-plugin-ctr\" has memory limit 0"
  }
]