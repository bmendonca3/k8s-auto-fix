[
  {
    "id": "665",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"frr-metrics\" has memory limit 0"
  },
  {
    "id": "666",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"reloader\" has memory limit 0"
  },
  {
    "id": "667",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"speaker\" has memory limit 0"
  },
  {
    "id": "669",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/024_deployment_release-name-metallb-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metallb-controller\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nspec:\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: release-name-metallb-controller\n      terminationGracePeriodSeconds: 0\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: controller\n        image: quay.io/metallb/controller:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        - --tls-min-version=VersionTLS12\n        env:\n        - name: METALLB_ML_SECRET_NAME\n          value: release-name-metallb-memberlist\n        - name: METALLB_DEPLOYMENT\n          value: release-name-metallb-controller\n        - name: METALLB_BGP_TYPE\n          value: frr\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - containerPort: 9443\n          name: webhook-server\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: metallb-webhook-cert\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-metallb-controller\" not found"
  },
  {
    "id": "670",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/024_deployment_release-name-metallb-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metallb-controller\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nspec:\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: release-name-metallb-controller\n      terminationGracePeriodSeconds: 0\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: controller\n        image: quay.io/metallb/controller:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        - --tls-min-version=VersionTLS12\n        env:\n        - name: METALLB_ML_SECRET_NAME\n          value: release-name-metallb-memberlist\n        - name: METALLB_DEPLOYMENT\n          value: release-name-metallb-controller\n        - name: METALLB_BGP_TYPE\n          value: frr\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - containerPort: 9443\n          name: webhook-server\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: metallb-webhook-cert\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"controller\" has cpu request 0"
  },
  {
    "id": "671",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/024_deployment_release-name-metallb-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metallb-controller\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nspec:\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: release-name-metallb-controller\n      terminationGracePeriodSeconds: 0\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: controller\n        image: quay.io/metallb/controller:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        - --tls-min-version=VersionTLS12\n        env:\n        - name: METALLB_ML_SECRET_NAME\n          value: release-name-metallb-memberlist\n        - name: METALLB_DEPLOYMENT\n          value: release-name-metallb-controller\n        - name: METALLB_BGP_TYPE\n          value: frr\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - containerPort: 9443\n          name: webhook-server\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: metallb-webhook-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "672",
    "manifest_path": "data/manifests/artifacthub/metrics-server/metrics-server/007_service_release-name-metrics-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-metrics-server\n  namespace: default\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.8.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n    appProtocol: https\n  selector:\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:metrics-server])"
  },
  {
    "id": "673",
    "manifest_path": "data/manifests/artifacthub/metrics-server/metrics-server/008_deployment_release-name-metrics-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metrics-server\n  namespace: default\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.8.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metrics-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metrics-server\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: release-name-metrics-server\n      priorityClassName: system-cluster-critical\n      containers:\n      - name: metrics-server\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --secure-port=10250\n        - --cert-dir=/tmp\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /livez\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /readyz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-metrics-server\" not found"
  },
  {
    "id": "674",
    "manifest_path": "data/manifests/artifacthub/metrics-server/metrics-server/008_deployment_release-name-metrics-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metrics-server\n  namespace: default\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.8.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metrics-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metrics-server\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: release-name-metrics-server\n      priorityClassName: system-cluster-critical\n      containers:\n      - name: metrics-server\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --secure-port=10250\n        - --cert-dir=/tmp\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /livez\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /readyz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"metrics-server\" has memory limit 0"
  },
  {
    "id": "675",
    "manifest_path": "data/manifests/artifacthub/nextcloud/nextcloud/002_service_release-name-nextcloud.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8080\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:app app.kubernetes.io/instance:release-name app.kubernetes.io/name:nextcloud])"
  },
  {
    "id": "676",
    "manifest_path": "data/manifests/artifacthub/nextcloud/nextcloud/003_deployment_release-name-nextcloud.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources: {}\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nextcloud\" does not have a read-only root file system"
  },
  {
    "id": "677",
    "manifest_path": "data/manifests/artifacthub/nextcloud/nextcloud/003_deployment_release-name-nextcloud.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources: {}\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nextcloud\" is not set to runAsNonRoot"
  },
  {
    "id": "678",
    "manifest_path": "data/manifests/artifacthub/nextcloud/nextcloud/003_deployment_release-name-nextcloud.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources: {}\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nextcloud\" has cpu request 0"
  },
  {
    "id": "679",
    "manifest_path": "data/manifests/artifacthub/nextcloud/nextcloud/003_deployment_release-name-nextcloud.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources: {}\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nextcloud\" has memory limit 0"
  },
  {
    "id": "680",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nfs-subdir-external-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "681",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-nfs-subdir-external-provisioner\" not found"
  },
  {
    "id": "682",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nfs-subdir-external-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "683",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nfs-subdir-external-provisioner\" has cpu request 0"
  },
  {
    "id": "684",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nfs-subdir-external-provisioner\" has memory limit 0"
  },
  {
    "id": "685",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/008_service_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n    nodePort: null\n  - port: 443\n    targetPort: 443\n    protocol: TCP\n    name: https\n    nodePort: null\n  selector:\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:nginx-ingress])"
  },
  {
    "id": "686",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/009_deployment_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress\" does not have a read-only root file system"
  },
  {
    "id": "687",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/009_deployment_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-nginx-ingress\" not found"
  },
  {
    "id": "688",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/009_deployment_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress\" has memory limit 0"
  },
  {
    "id": "689",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/004_service_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: http\n    protocol: TCP\n    appProtocol: http\n    name: http\n  - port: 44180\n    protocol: TCP\n    appProtocol: http\n    targetPort: metrics\n    name: metrics\n  selector:\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:oauth2-proxy])"
  },
  {
    "id": "690",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/005_deployment_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-oauth2-proxy\" not found"
  },
  {
    "id": "691",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/005_deployment_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"oauth2-proxy\" has cpu request 0"
  },
  {
    "id": "692",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/005_deployment_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"oauth2-proxy\" has memory limit 0"
  },
  {
    "id": "693",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/050_service_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  type: ClusterIP\n  ports:\n  - name: http-web\n    port: 80\n    protocol: TCP\n    targetPort: grafana\n  selector:\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:grafana])"
  },
  {
    "id": "694",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/051_service_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\n  annotations: null\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    protocol: TCP\n    port: 8080\n    targetPort: http\n  selector:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kube-state-metrics])"
  },
  {
    "id": "695",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/052_service_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\n    jobLabel: node-exporter\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9100\n    targetPort: 9100\n    protocol: TCP\n    name: http-metrics\n  selector:\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:prometheus-node-exporter])"
  },
  {
    "id": "696",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/053_service_release-name-kube-promethe-alertmanager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-alertmanager\n  namespace: default\n  labels:\n    app: kube-prometheus-stack-alertmanager\n    self-monitor: 'true'\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\nspec:\n  ports:\n  - name: http-web\n    port: 9093\n    targetPort: 9093\n    protocol: TCP\n  - name: reloader-web\n    appProtocol: http\n    port: 8080\n    targetPort: reloader-web\n  selector:\n    app.kubernetes.io/name: alertmanager\n    alertmanager: release-name-kube-promethe-alertmanager\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[alertmanager:release-name-kube-promethe-alertmanager app.kubernetes.io/name:alertmanager])"
  },
  {
    "id": "697",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/054_service_release-name-kube-promethe-coredns.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-coredns\n  labels:\n    app: kube-prometheus-stack-coredns\n    jobLabel: coredns\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 9153\n    protocol: TCP\n    targetPort: 9153\n  selector:\n    k8s-app: kube-dns\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kube-dns])"
  },
  {
    "id": "698",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/055_service_release-name-kube-promethe-kube-controller-manager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-controller-manager\n  labels:\n    app: kube-prometheus-stack-kube-controller-manager\n    jobLabel: kube-controller-manager\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 10257\n    protocol: TCP\n    targetPort: 10257\n  selector:\n    component: kube-controller-manager\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:kube-controller-manager])"
  },
  {
    "id": "699",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/056_service_release-name-kube-promethe-kube-etcd.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-etcd\n  labels:\n    app: kube-prometheus-stack-kube-etcd\n    jobLabel: kube-etcd\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 2381\n    protocol: TCP\n    targetPort: 2381\n  selector:\n    component: etcd\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:etcd])"
  },
  {
    "id": "700",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/057_service_release-name-kube-promethe-kube-proxy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-proxy\n  labels:\n    app: kube-prometheus-stack-kube-proxy\n    jobLabel: kube-proxy\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 10249\n    protocol: TCP\n    targetPort: 10249\n  selector:\n    k8s-app: kube-proxy\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kube-proxy])"
  },
  {
    "id": "701",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/058_service_release-name-kube-promethe-kube-scheduler.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-scheduler\n  labels:\n    app: kube-prometheus-stack-kube-scheduler\n    jobLabel: kube-scheduler\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 10259\n    protocol: TCP\n    targetPort: 10259\n  selector:\n    component: kube-scheduler\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:kube-scheduler])"
  },
  {
    "id": "702",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/059_service_release-name-kube-promethe-operator.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  ports:\n  - name: https\n    port: 443\n    targetPort: https\n  selector:\n    app: kube-prometheus-stack-operator\n    release: release-name\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kube-prometheus-stack-operator release:release-name])"
  },
  {
    "id": "703",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/060_service_release-name-kube-promethe-prometheus.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-prometheus\n  namespace: default\n  labels:\n    app: kube-prometheus-stack-prometheus\n    self-monitor: 'true'\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\nspec:\n  ports:\n  - name: http-web\n    port: 9090\n    targetPort: 9090\n  - name: reloader-web\n    appProtocol: http\n    port: 8080\n    targetPort: reloader-web\n  publishNotReadyAddresses: false\n  selector:\n    app.kubernetes.io/name: prometheus\n    operator.prometheus.io/name: release-name-kube-promethe-prometheus\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:prometheus operator.prometheus.io/name:release-name-kube-promethe-prometheus])"
  },
  {
    "id": "704",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "705",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "706",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-prometheus-node-exporter\" not found"
  },
  {
    "id": "707",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"node-exporter\""
  },
  {
    "id": "708",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"node-exporter\""
  },
  {
    "id": "709",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"node-exporter\""
  },
  {
    "id": "710",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "711",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "712",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "713",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana-sc-dashboard\" does not have a read-only root file system"
  },
  {
    "id": "714",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana-sc-datasources\" does not have a read-only root file system"
  },
  {
    "id": "715",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-grafana\" not found"
  },
  {
    "id": "716",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana\" has cpu request 0"
  },
  {
    "id": "717",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana-sc-dashboard\" has cpu request 0"
  },
  {
    "id": "718",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana-sc-datasources\" has cpu request 0"
  },
  {
    "id": "719",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana\" has memory limit 0"
  },
  {
    "id": "720",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana-sc-dashboard\" has memory limit 0"
  },
  {
    "id": "721",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana-sc-datasources\" has memory limit 0"
  },
  {
    "id": "722",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/063_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n        release: release-name\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kube-state-metrics\" not found"
  },
  {
    "id": "724",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/063_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n        release: release-name\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-state-metrics\" has cpu request 0"
  },
  {
    "id": "725",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/063_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n        release: release-name\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "726",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/064_deployment_release-name-kube-promethe-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kube-prometheus-stack-operator\n      release: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app: kube-prometheus-stack-operator\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator\n    spec:\n      containers:\n      - name: kube-prometheus-stack\n        image: quay.io/prometheus-operator/prometheus-operator:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --kubelet-service=kube-system/release-name-kube-promethe-kubelet\n        - --kubelet-endpoints=true\n        - --kubelet-endpointslice=false\n        - --localhost=127.0.0.1\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        - --config-reloader-cpu-request=0\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-request=0\n        - --config-reloader-memory-limit=0\n        - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.2\n        - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\n        - --web.enable-tls=true\n        - --web.cert-file=/cert/cert\n        - --web.key-file=/cert/key\n        - --web.listen-address=:10250\n        - --web.tls-min-version=VersionTLS13\n        ports:\n        - containerPort: 10250\n          name: https\n        env:\n        - name: GOGC\n          value: '30'\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: tls-secret\n          mountPath: /cert\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: tls-secret\n        secret:\n          defaultMode: 420\n          secretName: release-name-kube-promethe-admission\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: release-name-kube-promethe-operator\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 30\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kube-promethe-operator\" not found"
  },
  {
    "id": "727",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/064_deployment_release-name-kube-promethe-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kube-prometheus-stack-operator\n      release: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app: kube-prometheus-stack-operator\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator\n    spec:\n      containers:\n      - name: kube-prometheus-stack\n        image: quay.io/prometheus-operator/prometheus-operator:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --kubelet-service=kube-system/release-name-kube-promethe-kubelet\n        - --kubelet-endpoints=true\n        - --kubelet-endpointslice=false\n        - --localhost=127.0.0.1\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        - --config-reloader-cpu-request=0\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-request=0\n        - --config-reloader-memory-limit=0\n        - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.2\n        - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\n        - --web.enable-tls=true\n        - --web.cert-file=/cert/cert\n        - --web.key-file=/cert/key\n        - --web.listen-address=:10250\n        - --web.tls-min-version=VersionTLS13\n        ports:\n        - containerPort: 10250\n          name: https\n        env:\n        - name: GOGC\n          value: '30'\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: tls-secret\n          mountPath: /cert\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: tls-secret\n        secret:\n          defaultMode: 420\n          secretName: release-name-kube-promethe-admission\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: release-name-kube-promethe-operator\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 30\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-prometheus-stack\" has cpu request 0"
  },
  {
    "id": "728",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/064_deployment_release-name-kube-promethe-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kube-prometheus-stack-operator\n      release: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app: kube-prometheus-stack-operator\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator\n    spec:\n      containers:\n      - name: kube-prometheus-stack\n        image: quay.io/prometheus-operator/prometheus-operator:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --kubelet-service=kube-system/release-name-kube-promethe-kubelet\n        - --kubelet-endpoints=true\n        - --kubelet-endpointslice=false\n        - --localhost=127.0.0.1\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        - --config-reloader-cpu-request=0\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-request=0\n        - --config-reloader-memory-limit=0\n        - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.2\n        - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\n        - --web.enable-tls=true\n        - --web.cert-file=/cert/cert\n        - --web.key-file=/cert/key\n        - --web.listen-address=:10250\n        - --web.tls-min-version=VersionTLS13\n        ports:\n        - containerPort: 10250\n          name: https\n        env:\n        - name: GOGC\n          value: '30'\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: tls-secret\n          mountPath: /cert\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: tls-secret\n        secret:\n          defaultMode: 420\n          secretName: release-name-kube-promethe-admission\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: release-name-kube-promethe-operator\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 30\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-prometheus-stack\" has memory limit 0"
  },
  {
    "id": "729",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-test\" does not have a read-only root file system"
  },
  {
    "id": "730",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-grafana-test\" not found"
  },
  {
    "id": "731",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"release-name-test\" is not set to runAsNonRoot"
  },
  {
    "id": "732",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-test\" has cpu request 0"
  },
  {
    "id": "733",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-test\" has memory limit 0"
  },
  {
    "id": "734",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/125_job_release-name-kube-promethe-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kube-promethe-admission\" not found"
  },
  {
    "id": "735",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/125_job_release-name-kube-promethe-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"create\" has cpu request 0"
  },
  {
    "id": "736",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/125_job_release-name-kube-promethe-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"create\" has memory limit 0"
  },
  {
    "id": "737",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/126_job_release-name-kube-promethe-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kube-promethe-admission\" not found"
  },
  {
    "id": "738",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/126_job_release-name-kube-promethe-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"patch\" has cpu request 0"
  },
  {
    "id": "739",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/126_job_release-name-kube-promethe-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"patch\" has memory limit 0"
  },
  {
    "id": "740",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/013_service_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9093\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:alertmanager])"
  },
  {
    "id": "741",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/014_service_release-name-alertmanager-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-alertmanager-headless\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - port: 9093\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:alertmanager])"
  },
  {
    "id": "742",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/015_service_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    protocol: TCP\n    port: 8080\n    targetPort: http\n  selector:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kube-state-metrics])"
  },
  {
    "id": "743",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/016_service_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9100\n    targetPort: 9100\n    protocol: TCP\n    name: metrics\n  selector:\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:prometheus-node-exporter])"
  },
  {
    "id": "744",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/017_service_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/probe: pushgateway\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9091\n    targetPort: 9091\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:prometheus-pushgateway])"
  },
  {
    "id": "745",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/018_service_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 9090\n  selector:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:server app.kubernetes.io/instance:release-name app.kubernetes.io/name:prometheus])"
  },
  {
    "id": "746",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "747",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "748",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-prometheus-node-exporter\" not found"
  },
  {
    "id": "749",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"node-exporter\""
  },
  {
    "id": "750",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"node-exporter\""
  },
  {
    "id": "751",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"node-exporter\""
  },
  {
    "id": "752",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "753",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "754",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/020_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kube-state-metrics\" not found"
  },
  {
    "id": "756",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/020_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-state-metrics\" has cpu request 0"
  },
  {
    "id": "757",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/020_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "758",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pushgateway\" does not have a read-only root file system"
  },
  {
    "id": "759",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-prometheus-pushgateway\" not found"
  },
  {
    "id": "760",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pushgateway\" has cpu request 0"
  },
  {
    "id": "761",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pushgateway\" has memory limit 0"
  },
  {
    "id": "762",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-server\" does not have a read-only root file system"
  },
  {
    "id": "763",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-server-configmap-reload\" does not have a read-only root file system"
  },
  {
    "id": "764",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-prometheus-server\" not found"
  },
  {
    "id": "765",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus-server\" has cpu request 0"
  },
  {
    "id": "766",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus-server-configmap-reload\" has cpu request 0"
  },
  {
    "id": "767",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus-server\" has memory limit 0"
  },
  {
    "id": "768",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus-server-configmap-reload\" has memory limit 0"
  },
  {
    "id": "769",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"alertmanager\" does not have a read-only root file system"
  },
  {
    "id": "770",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-alertmanager\" not found"
  },
  {
    "id": "771",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"alertmanager\" has cpu request 0"
  },
  {
    "id": "772",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"alertmanager\" has memory limit 0"
  },
  {
    "id": "773",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/004_service_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:pgadmin4])"
  },
  {
    "id": "774",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/005_deployment_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pgadmin4\" does not have a read-only root file system"
  },
  {
    "id": "775",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/005_deployment_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pgadmin4\" has cpu request 0"
  },
  {
    "id": "776",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/005_deployment_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pgadmin4\" has memory limit 0"
  },
  {
    "id": "777",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n  restartPolicy: Never\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"docker.io/busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "778",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n  restartPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wget\" has cpu request 0"
  },
  {
    "id": "779",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n  restartPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wget\" has memory limit 0"
  },
  {
    "id": "780",
    "manifest_path": "data/manifests/artifacthub/traefik/traefik/004_service_release-name-traefik.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  type: LoadBalancer\n  selector:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n  ports:\n  - port: 80\n    name: web\n    targetPort: web\n    protocol: TCP\n  - port: 443\n    name: websecure\n    targetPort: websecure\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name-default app.kubernetes.io/name:traefik])"
  },
  {
    "id": "781",
    "manifest_path": "data/manifests/artifacthub/traefik/traefik/005_deployment_release-name-traefik.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources: null\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-traefik\" not found"
  },
  {
    "id": "782",
    "manifest_path": "data/manifests/artifacthub/traefik/traefik/005_deployment_release-name-traefik.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources: null\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-traefik\" has cpu request 0"
  },
  {
    "id": "783",
    "manifest_path": "data/manifests/artifacthub/traefik/traefik/005_deployment_release-name-traefik.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources: null\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-traefik\" has memory limit 0"
  },
  {
    "id": "784",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/007_service_release-name-velero.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  type: ClusterIP\n  ports:\n  - name: http-monitoring\n    port: 8085\n    targetPort: http-monitoring\n  selector:\n    name: velero\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:velero name:velero])"
  },
  {
    "id": "785",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"velero\" does not have a read-only root file system"
  },
  {
    "id": "786",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-velero-server\" not found"
  },
  {
    "id": "787",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"velero\" is not set to runAsNonRoot"
  },
  {
    "id": "788",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"velero\" has cpu request 0"
  },
  {
    "id": "789",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"velero\" has memory limit 0"
  },
  {
    "id": "790",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "791",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubectl\" does not have a read-only root file system"
  },
  {
    "id": "792",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"velero\" does not have a read-only root file system"
  },
  {
    "id": "793",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-velero-server-upgrade-crds\" not found"
  },
  {
    "id": "794",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubectl\" is not set to runAsNonRoot"
  },
  {
    "id": "795",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"velero\" is not set to runAsNonRoot"
  },
  {
    "id": "796",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubectl\" has cpu request 0"
  },
  {
    "id": "797",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"velero\" has cpu request 0"
  },
  {
    "id": "798",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubectl\" has memory limit 0"
  },
  {
    "id": "799",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"velero\" has memory limit 0"
  },
  {
    "id": "800",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pulsar-admin\" is using an invalid container image, \"apachepulsar/pulsar:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "801",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pulsar-admin\" does not have a read-only root file system"
  },
  {
    "id": "802",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pulsar-admin\" is not set to runAsNonRoot"
  },
  {
    "id": "803",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pulsar-admin\" has cpu request 0"
  },
  {
    "id": "804",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pulsar-admin\" has memory limit 0"
  },
  {
    "id": "805",
    "manifest_path": "data/manifests/the_stack_sample/sample_0001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: open-api-doc\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app: open-api-doc\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:open-api-doc])"
  },
  {
    "id": "806",
    "manifest_path": "data/manifests/the_stack_sample/sample_0002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: argocd-metrics\nspec:\n  ports:\n  - name: http\n    protocol: TCP\n    port: 8082\n    targetPort: 8082\n  selector:\n    app: argocd-server\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:argocd-server])"
  },
  {
    "id": "807",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"datasets-lib-unb-ca\" is using an invalid container image, \"||DEPLOYMENTIMAGE||\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "808",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"datasets-lib-unb-ca\" does not have a read-only root file system"
  },
  {
    "id": "809",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"datasets-lib-unb-ca\" is not set to runAsNonRoot"
  },
  {
    "id": "810",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"datasets-lib-unb-ca\" has cpu request 0"
  },
  {
    "id": "811",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"datasets-lib-unb-ca\" has memory limit 0"
  },
  {
    "id": "812",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"metrics\" does not have a read-only root file system"
  },
  {
    "id": "813",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "814",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"metrics\" has cpu request 0"
  },
  {
    "id": "815",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"metrics\" has memory limit 0"
  },
  {
    "id": "816",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "817",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"statusreconciler\" not found"
  },
  {
    "id": "818",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "819",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "820",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "821",
    "manifest_path": "data/manifests/the_stack_sample/sample_0009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kubwz0un1-cffc\n  labels:\n    app: kubwz0un1-cffc\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n    targetPort: 8080\n    protocol: TCP\n    name: http\n  selector:\n    app: kubwz0un1-cffc\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kubwz0un1-cffc])"
  },
  {
    "id": "822",
    "manifest_path": "data/manifests/the_stack_sample/sample_0010.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch\n  labels:\n    component: elasticsearch\n    role: data\n  annotations:\n    cloud.google.com/load-balancer-type: Internal\nspec:\n  selector:\n    component: elasticsearch\n    role: data\n  ports:\n  - name: http\n    port: 9200\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:elasticsearch role:data])"
  },
  {
    "id": "823",
    "manifest_path": "data/manifests/the_stack_sample/sample_0012.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    getambassador.io/config: \"---\\napiVersion: ambassador/v0\\nkind:  Mapping\\nname:\\\n      \\ webapp_mapping\\nprefix: /jupyter/\\nservice: jupyter-web-app-service.kubeflow\\n\\\n      add_request_headers:\\n  x-forwarded-prefix: /jupyter\"\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n    run: jupyter-web-app\n  name: jupyter-web-app-service\n  namespace: kubeflow\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 5000\n  selector:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:jupyter-web-app app.kubernetes.io/component:jupyter-web-app app.kubernetes.io/name:jupyter-web-app kustomize.component:jupyter-web-app])"
  },
  {
    "id": "824",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "825",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"healthcheck-ready\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "826",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"holder-vcs-add-profiles\" is using an invalid container image, \"alpine:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "827",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "828",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"healthcheck-ready\" does not have a read-only root file system"
  },
  {
    "id": "829",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"holder-vcs-add-profiles\" does not have a read-only root file system"
  },
  {
    "id": "830",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait\" does not have a read-only root file system"
  },
  {
    "id": "831",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"healthcheck-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "832",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"holder-vcs-add-profiles\" is not set to runAsNonRoot"
  },
  {
    "id": "833",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait\" is not set to runAsNonRoot"
  },
  {
    "id": "834",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"healthcheck-ready\" has cpu request 0"
  },
  {
    "id": "835",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"holder-vcs-add-profiles\" has cpu request 0"
  },
  {
    "id": "836",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait\" has cpu request 0"
  },
  {
    "id": "837",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"healthcheck-ready\" has memory limit 0"
  },
  {
    "id": "838",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"holder-vcs-add-profiles\" has memory limit 0"
  },
  {
    "id": "839",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait\" has memory limit 0"
  },
  {
    "id": "840",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "841",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "842",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "843",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "844",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "845",
    "manifest_path": "data/manifests/the_stack_sample/sample_0015.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pipeline\" does not have a read-only root file system"
  },
  {
    "id": "846",
    "manifest_path": "data/manifests/the_stack_sample/sample_0015.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"prow-pipeline\" not found"
  },
  {
    "id": "847",
    "manifest_path": "data/manifests/the_stack_sample/sample_0015.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pipeline\" is not set to runAsNonRoot"
  },
  {
    "id": "848",
    "manifest_path": "data/manifests/the_stack_sample/sample_0015.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pipeline\" has cpu request 0"
  },
  {
    "id": "849",
    "manifest_path": "data/manifests/the_stack_sample/sample_0015.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pipeline\" has memory limit 0"
  },
  {
    "id": "850",
    "manifest_path": "data/manifests/the_stack_sample/sample_0017.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "851",
    "manifest_path": "data/manifests/the_stack_sample/sample_0017.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "852",
    "manifest_path": "data/manifests/the_stack_sample/sample_0017.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "853",
    "manifest_path": "data/manifests/the_stack_sample/sample_0017.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "854",
    "manifest_path": "data/manifests/the_stack_sample/sample_0017.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "856",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "857",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "858",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"hook\" not found"
  },
  {
    "id": "860",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "861",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "862",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "863",
    "manifest_path": "data/manifests/the_stack_sample/sample_0020.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"test-container\" is using an invalid container image, \"gcr.io/google_containers/test-webserver\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "864",
    "manifest_path": "data/manifests/the_stack_sample/sample_0020.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-container\" does not have a read-only root file system"
  },
  {
    "id": "865",
    "manifest_path": "data/manifests/the_stack_sample/sample_0020.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-container\" is not set to runAsNonRoot"
  },
  {
    "id": "866",
    "manifest_path": "data/manifests/the_stack_sample/sample_0020.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-container\" has cpu request 0"
  },
  {
    "id": "867",
    "manifest_path": "data/manifests/the_stack_sample/sample_0020.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-container\" has memory limit 0"
  },
  {
    "id": "868",
    "manifest_path": "data/manifests/the_stack_sample/sample_0022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"openmcp-snapshot\" does not have a read-only root file system"
  },
  {
    "id": "869",
    "manifest_path": "data/manifests/the_stack_sample/sample_0022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"openmcp-snapshot-sa\" not found"
  },
  {
    "id": "870",
    "manifest_path": "data/manifests/the_stack_sample/sample_0022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openmcp-snapshot\" is not set to runAsNonRoot"
  },
  {
    "id": "871",
    "manifest_path": "data/manifests/the_stack_sample/sample_0022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"openmcp-snapshot\" has cpu request 0"
  },
  {
    "id": "872",
    "manifest_path": "data/manifests/the_stack_sample/sample_0022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"openmcp-snapshot\" has memory limit 0"
  },
  {
    "id": "873",
    "manifest_path": "data/manifests/the_stack_sample/sample_0023.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service-lb\n  labels:\n    app: nginx\nspec:\n  type: LoadBalancer\n  loadBalancerSourceRanges:\n  - 10.30.88.0/24\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: tcp\n  selector:\n    app: nginx\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nginx])"
  },
  {
    "id": "874",
    "manifest_path": "data/manifests/the_stack_sample/sample_0025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "875",
    "manifest_path": "data/manifests/the_stack_sample/sample_0025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "876",
    "manifest_path": "data/manifests/the_stack_sample/sample_0025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "877",
    "manifest_path": "data/manifests/the_stack_sample/sample_0025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "878",
    "manifest_path": "data/manifests/the_stack_sample/sample_0025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "879",
    "manifest_path": "data/manifests/the_stack_sample/sample_0026.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: ws-app01\n  name: vote\n  namespace: attin-studio\nspec:\n  ports:\n  - name: 8080-tcp\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    deploymentconfig: vote\n  sessionAffinity: None\n  type: ClusterIP\nstatus:\n  loadBalancer: {}\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[deploymentconfig:vote])"
  },
  {
    "id": "881",
    "manifest_path": "data/manifests/the_stack_sample/sample_0027.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-ui\n  template:\n    metadata:\n      labels:\n        app: web-ui\n    spec:\n      containers:\n      - name: web-ui\n        image: node:16-alpine\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - npm install --non-interactive && npm start:container\n        workingDir: /usr/src/app/\n        ports:\n        - containerPort: 3000\n        env:\n        - name: PORT\n          value: '3000'\n        - name: ENDPOINT_IAM\n          value: http://iam.example.com\n        - name: ENDPOINT_FLOW\n          value: http://flow-repository.example.com\n        - name: ENDPOINT_COMPONENT\n          value: http://component-repository.example.com\n        - name: ENDPOINT_SECRETS\n          value: http://skm.example.com/api/v1\n        - name: ENDPOINT_DISPATCHER\n          value: http://dispatcher-service.example.com\n        - name: ENDPOINT_METADATA\n          value: http://metadata.example.com/api/v1\n        - name: ENDPOINT_APP_DIRECTORY\n          value: http://app-directory.example.com/api/v1\n        - name: NODE_ENV\n          value: development\n        - name: LOG_LEVEL\n          value: debug\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n        volumeMounts:\n        - name: code\n          mountPath: /usr/src/app\n          subPath: services/web-ui\n        livenessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 300\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        imagePullPolicy: IfNotPresent\n      volumes:\n      - name: code\n        persistentVolumeClaim:\n          claimName: source-volume-claim\n  minReadySeconds: 10\n  revisionHistoryLimit: 2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-ui\" does not have a read-only root file system"
  },
  {
    "id": "882",
    "manifest_path": "data/manifests/the_stack_sample/sample_0027.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-ui\n  template:\n    metadata:\n      labels:\n        app: web-ui\n    spec:\n      containers:\n      - name: web-ui\n        image: node:16-alpine\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - npm install --non-interactive && npm start:container\n        workingDir: /usr/src/app/\n        ports:\n        - containerPort: 3000\n        env:\n        - name: PORT\n          value: '3000'\n        - name: ENDPOINT_IAM\n          value: http://iam.example.com\n        - name: ENDPOINT_FLOW\n          value: http://flow-repository.example.com\n        - name: ENDPOINT_COMPONENT\n          value: http://component-repository.example.com\n        - name: ENDPOINT_SECRETS\n          value: http://skm.example.com/api/v1\n        - name: ENDPOINT_DISPATCHER\n          value: http://dispatcher-service.example.com\n        - name: ENDPOINT_METADATA\n          value: http://metadata.example.com/api/v1\n        - name: ENDPOINT_APP_DIRECTORY\n          value: http://app-directory.example.com/api/v1\n        - name: NODE_ENV\n          value: development\n        - name: LOG_LEVEL\n          value: debug\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n        volumeMounts:\n        - name: code\n          mountPath: /usr/src/app\n          subPath: services/web-ui\n        livenessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 300\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        imagePullPolicy: IfNotPresent\n      volumes:\n      - name: code\n        persistentVolumeClaim:\n          claimName: source-volume-claim\n  minReadySeconds: 10\n  revisionHistoryLimit: 2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-ui\" is not set to runAsNonRoot"
  },
  {
    "id": "883",
    "manifest_path": "data/manifests/the_stack_sample/sample_0027.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-ui\n  template:\n    metadata:\n      labels:\n        app: web-ui\n    spec:\n      containers:\n      - name: web-ui\n        image: node:16-alpine\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - npm install --non-interactive && npm start:container\n        workingDir: /usr/src/app/\n        ports:\n        - containerPort: 3000\n        env:\n        - name: PORT\n          value: '3000'\n        - name: ENDPOINT_IAM\n          value: http://iam.example.com\n        - name: ENDPOINT_FLOW\n          value: http://flow-repository.example.com\n        - name: ENDPOINT_COMPONENT\n          value: http://component-repository.example.com\n        - name: ENDPOINT_SECRETS\n          value: http://skm.example.com/api/v1\n        - name: ENDPOINT_DISPATCHER\n          value: http://dispatcher-service.example.com\n        - name: ENDPOINT_METADATA\n          value: http://metadata.example.com/api/v1\n        - name: ENDPOINT_APP_DIRECTORY\n          value: http://app-directory.example.com/api/v1\n        - name: NODE_ENV\n          value: development\n        - name: LOG_LEVEL\n          value: debug\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n        volumeMounts:\n        - name: code\n          mountPath: /usr/src/app\n          subPath: services/web-ui\n        livenessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 300\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        imagePullPolicy: IfNotPresent\n      volumes:\n      - name: code\n        persistentVolumeClaim:\n          claimName: source-volume-claim\n  minReadySeconds: 10\n  revisionHistoryLimit: 2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-ui\" has cpu request 0"
  },
  {
    "id": "884",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "deprecated-service-account-field",
    "violation_text": "serviceAccount is specified (csi-bos-external-runner), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "885",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"csi-bosplugin\""
  },
  {
    "id": "886",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"csi-bosplugin\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "887",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "888",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-bosplugin\" does not have a read-only root file system"
  },
  {
    "id": "889",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "890",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"csi-bos-external-runner\" not found"
  },
  {
    "id": "891",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"csi-bosplugin\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "892",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"csi-bosplugin\" is privileged"
  },
  {
    "id": "893",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-bosplugin\" is not set to runAsNonRoot"
  },
  {
    "id": "894",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "895",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"csi-bosplugin\""
  },
  {
    "id": "896",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-bosplugin\" has cpu request 0"
  },
  {
    "id": "897",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-driver-registrar\" has cpu request 0"
  },
  {
    "id": "898",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-bosplugin\" has memory limit 0"
  },
  {
    "id": "899",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-driver-registrar\" has memory limit 0"
  },
  {
    "id": "900",
    "manifest_path": "data/manifests/the_stack_sample/sample_0032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"influx\" is using an invalid container image, \"nexclipper/influxdb\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "901",
    "manifest_path": "data/manifests/the_stack_sample/sample_0032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"influx\" does not have a read-only root file system"
  },
  {
    "id": "902",
    "manifest_path": "data/manifests/the_stack_sample/sample_0032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"influx\" is not set to runAsNonRoot"
  },
  {
    "id": "903",
    "manifest_path": "data/manifests/the_stack_sample/sample_0032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"influx\" has cpu request 0"
  },
  {
    "id": "904",
    "manifest_path": "data/manifests/the_stack_sample/sample_0032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"influx\" has memory limit 0"
  },
  {
    "id": "905",
    "manifest_path": "data/manifests/the_stack_sample/sample_0033.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kubernetes-dashboard-test\n  labels:\n    addon: kubernetes-dashboard.addons.k8s.io\n    app: kubernetes-dashboard-test\n    kubernetes.io/cluster-service: 'true'\n    facing: external\nspec:\n  type: LoadBalancer\n  selector:\n    app: kubernetes-dashboard-test\n  ports:\n  - port: 80\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:kubernetes-dashboard-test])"
  },
  {
    "id": "906",
    "manifest_path": "data/manifests/the_stack_sample/sample_0035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"demo-server\" is using an invalid container image, \"docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "907",
    "manifest_path": "data/manifests/the_stack_sample/sample_0035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "policy_id": "no-anti-affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "908",
    "manifest_path": "data/manifests/the_stack_sample/sample_0035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"demo-server\" does not have a read-only root file system"
  },
  {
    "id": "909",
    "manifest_path": "data/manifests/the_stack_sample/sample_0035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"demo-server\" is not set to runAsNonRoot"
  },
  {
    "id": "910",
    "manifest_path": "data/manifests/the_stack_sample/sample_0035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"demo-server\" has cpu request 0"
  },
  {
    "id": "911",
    "manifest_path": "data/manifests/the_stack_sample/sample_0036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n  restartPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"succeeded\" does not have a read-only root file system"
  },
  {
    "id": "912",
    "manifest_path": "data/manifests/the_stack_sample/sample_0036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n  restartPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"succeeded\" is not set to runAsNonRoot"
  },
  {
    "id": "913",
    "manifest_path": "data/manifests/the_stack_sample/sample_0036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n  restartPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"succeeded\" has cpu request 0"
  },
  {
    "id": "914",
    "manifest_path": "data/manifests/the_stack_sample/sample_0036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n  restartPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"succeeded\" has memory limit 0"
  },
  {
    "id": "915",
    "manifest_path": "data/manifests/the_stack_sample/sample_0038.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n  labels:\n    app: nginx-test\n  name: nginx-test\nspec:\n  ports:\n  - name: nginx-test\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx-test\n  type: LoadBalancer\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:nginx-test])"
  },
  {
    "id": "916",
    "manifest_path": "data/manifests/the_stack_sample/sample_0040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kaniko\" does not have a read-only root file system"
  },
  {
    "id": "917",
    "manifest_path": "data/manifests/the_stack_sample/sample_0040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kubeflow-pipelines-container-builder\" not found"
  },
  {
    "id": "918",
    "manifest_path": "data/manifests/the_stack_sample/sample_0040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kaniko\" is not set to runAsNonRoot"
  },
  {
    "id": "919",
    "manifest_path": "data/manifests/the_stack_sample/sample_0040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kaniko\" has cpu request 0"
  },
  {
    "id": "920",
    "manifest_path": "data/manifests/the_stack_sample/sample_0040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kaniko\" has memory limit 0"
  },
  {
    "id": "921",
    "manifest_path": "data/manifests/the_stack_sample/sample_0041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "policy_id": "docker-sock",
    "violation_text": "host system directory \"/var/run/docker.sock\" is mounted on container \"cloudwatch-agent\""
  },
  {
    "id": "922",
    "manifest_path": "data/manifests/the_stack_sample/sample_0041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cloudwatch-agent\" does not have a read-only root file system"
  },
  {
    "id": "923",
    "manifest_path": "data/manifests/the_stack_sample/sample_0041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cloudwatch-agent\" not found"
  },
  {
    "id": "924",
    "manifest_path": "data/manifests/the_stack_sample/sample_0041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cloudwatch-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "925",
    "manifest_path": "data/manifests/the_stack_sample/sample_0041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"cloudwatch-agent\""
  },
  {
    "id": "926",
    "manifest_path": "data/manifests/the_stack_sample/sample_0041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"cloudwatch-agent\""
  },
  {
    "id": "927",
    "manifest_path": "data/manifests/the_stack_sample/sample_0042.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "928",
    "manifest_path": "data/manifests/the_stack_sample/sample_0042.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "929",
    "manifest_path": "data/manifests/the_stack_sample/sample_0042.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "930",
    "manifest_path": "data/manifests/the_stack_sample/sample_0042.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "931",
    "manifest_path": "data/manifests/the_stack_sample/sample_0042.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "932",
    "manifest_path": "data/manifests/the_stack_sample/sample_0043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ekc-operator\nspec:\n  selector:\n    matchLabels:\n      app: ekc-operator\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: ekc-operator\n    spec:\n      serviceAccountName: ekco\n      restartPolicy: Always\n      nodeSelector:\n        node-role.kubernetes.io/master: ''\n      containers:\n      - name: ekc-operator\n        image: replicated/ekco:v0.2.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/ekco\n        - operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: ekco-config\n          mountPath: /etc/ekco\n          readOnly: true\n        - name: certificates-dir\n          mountPath: /etc/kubernetes/pki\n          readOnly: true\n      volumes:\n      - name: ekco-config\n        configMap:\n          name: ekco-config\n      - name: certificates-dir\n        hostPath:\n          path: /etc/kubernetes/pki\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ekc-operator\" does not have a read-only root file system"
  },
  {
    "id": "933",
    "manifest_path": "data/manifests/the_stack_sample/sample_0043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ekc-operator\nspec:\n  selector:\n    matchLabels:\n      app: ekc-operator\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: ekc-operator\n    spec:\n      serviceAccountName: ekco\n      restartPolicy: Always\n      nodeSelector:\n        node-role.kubernetes.io/master: ''\n      containers:\n      - name: ekc-operator\n        image: replicated/ekco:v0.2.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/ekco\n        - operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: ekco-config\n          mountPath: /etc/ekco\n          readOnly: true\n        - name: certificates-dir\n          mountPath: /etc/kubernetes/pki\n          readOnly: true\n      volumes:\n      - name: ekco-config\n        configMap:\n          name: ekco-config\n      - name: certificates-dir\n        hostPath:\n          path: /etc/kubernetes/pki\n          type: Directory\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"ekco\" not found"
  },
  {
    "id": "934",
    "manifest_path": "data/manifests/the_stack_sample/sample_0043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ekc-operator\nspec:\n  selector:\n    matchLabels:\n      app: ekc-operator\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: ekc-operator\n    spec:\n      serviceAccountName: ekco\n      restartPolicy: Always\n      nodeSelector:\n        node-role.kubernetes.io/master: ''\n      containers:\n      - name: ekc-operator\n        image: replicated/ekco:v0.2.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/ekco\n        - operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: ekco-config\n          mountPath: /etc/ekco\n          readOnly: true\n        - name: certificates-dir\n          mountPath: /etc/kubernetes/pki\n          readOnly: true\n      volumes:\n      - name: ekco-config\n        configMap:\n          name: ekco-config\n      - name: certificates-dir\n        hostPath:\n          path: /etc/kubernetes/pki\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ekc-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "935",
    "manifest_path": "data/manifests/the_stack_sample/sample_0045.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    environment: prod\n    region: eu-central-1\n  name: app\n  namespace: apps\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9797'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: app\n    spec:\n      containers:\n      - image: nginx:1.21.4\n        imagePullPolicy: IfNotPresent\n        name: nginx\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "936",
    "manifest_path": "data/manifests/the_stack_sample/sample_0045.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    environment: prod\n    region: eu-central-1\n  name: app\n  namespace: apps\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9797'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: app\n    spec:\n      containers:\n      - image: nginx:1.21.4\n        imagePullPolicy: IfNotPresent\n        name: nginx\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "937",
    "manifest_path": "data/manifests/the_stack_sample/sample_0050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: app\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: app\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:app])"
  },
  {
    "id": "938",
    "manifest_path": "data/manifests/the_stack_sample/sample_0052.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rethinkdb\" does not have a read-only root file system"
  },
  {
    "id": "939",
    "manifest_path": "data/manifests/the_stack_sample/sample_0052.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rethinkdb\" is not set to runAsNonRoot"
  },
  {
    "id": "940",
    "manifest_path": "data/manifests/the_stack_sample/sample_0052.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rethinkdb\" has cpu request 0"
  },
  {
    "id": "941",
    "manifest_path": "data/manifests/the_stack_sample/sample_0052.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rethinkdb\" has memory limit 0"
  },
  {
    "id": "942",
    "manifest_path": "data/manifests/the_stack_sample/sample_0053.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: heapster-v9\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v9\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v9\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v9\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.18.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=gcmautoscaling\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --sink_frequency=2m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "943",
    "manifest_path": "data/manifests/the_stack_sample/sample_0053.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: heapster-v9\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v9\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v9\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v9\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.18.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=gcmautoscaling\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --sink_frequency=2m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "944",
    "manifest_path": "data/manifests/the_stack_sample/sample_0053.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: heapster-v9\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v9\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v9\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v9\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.18.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=gcmautoscaling\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --sink_frequency=2m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"heapster\" has cpu request 0"
  },
  {
    "id": "945",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "946",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"git-clone\" does not have a read-only root file system"
  },
  {
    "id": "947",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job\" does not have a read-only root file system"
  },
  {
    "id": "948",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"jx-boot-job\" not found"
  },
  {
    "id": "949",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"git-clone\" is not set to runAsNonRoot"
  },
  {
    "id": "950",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job\" is not set to runAsNonRoot"
  },
  {
    "id": "951",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"git-clone\" has cpu request 0"
  },
  {
    "id": "952",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job\" has cpu request 0"
  },
  {
    "id": "953",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"git-clone\" has memory limit 0"
  },
  {
    "id": "954",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job\" has memory limit 0"
  },
  {
    "id": "955",
    "manifest_path": "data/manifests/the_stack_sample/sample_0056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"master\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "956",
    "manifest_path": "data/manifests/the_stack_sample/sample_0056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"master\" does not have a read-only root file system"
  },
  {
    "id": "957",
    "manifest_path": "data/manifests/the_stack_sample/sample_0056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"master\" is not set to runAsNonRoot"
  },
  {
    "id": "958",
    "manifest_path": "data/manifests/the_stack_sample/sample_0056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"master\" has cpu request 0"
  },
  {
    "id": "959",
    "manifest_path": "data/manifests/the_stack_sample/sample_0056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"master\" has memory limit 0"
  },
  {
    "id": "960",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "961",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"jornada-rental\" is using an invalid container image, \"wesleywillians/rental_jornada:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "962",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jornada-rental\" does not have a read-only root file system"
  },
  {
    "id": "963",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jornada-rental\" is not set to runAsNonRoot"
  },
  {
    "id": "964",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jornada-rental\" has cpu request 0"
  },
  {
    "id": "965",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jornada-rental\" has memory limit 0"
  },
  {
    "id": "966",
    "manifest_path": "data/manifests/the_stack_sample/sample_0058.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-master-deploy\n  namespace: jenkins\n  labels:\n    app.kubernetes.io/name: jenkins-master\n    app.kubernetes.io/instance: jenkins-master-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: jenkins-master-pod\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins-master\n        app.kubernetes.io/instance: jenkins-master-pod\n    spec:\n      containers:\n      - name: jenkins-master-pod\n        image: jenkins/jenkins:jdk11\n        ports:\n        - containerPort: 8080\n        - containerPort: 50000\n        resources:\n          requests:\n            memory: 1000Mi\n            cpu: 300m\n          limits:\n            memory: 1000Mi\n            cpu: 300m\n        env:\n        - name: JENKINS_JAVA_OPTIONS\n          value: -Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Seoul\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-master-pvc\n      volumes:\n      - name: jenkins-master-pvc\n        persistentVolumeClaim:\n          claimName: jenkins-master-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jenkins-master-pod\" does not have a read-only root file system"
  },
  {
    "id": "967",
    "manifest_path": "data/manifests/the_stack_sample/sample_0058.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-master-deploy\n  namespace: jenkins\n  labels:\n    app.kubernetes.io/name: jenkins-master\n    app.kubernetes.io/instance: jenkins-master-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: jenkins-master-pod\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins-master\n        app.kubernetes.io/instance: jenkins-master-pod\n    spec:\n      containers:\n      - name: jenkins-master-pod\n        image: jenkins/jenkins:jdk11\n        ports:\n        - containerPort: 8080\n        - containerPort: 50000\n        resources:\n          requests:\n            memory: 1000Mi\n            cpu: 300m\n          limits:\n            memory: 1000Mi\n            cpu: 300m\n        env:\n        - name: JENKINS_JAVA_OPTIONS\n          value: -Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Seoul\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-master-pvc\n      volumes:\n      - name: jenkins-master-pvc\n        persistentVolumeClaim:\n          claimName: jenkins-master-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jenkins-master-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "968",
    "manifest_path": "data/manifests/the_stack_sample/sample_0061.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"bpg-parser-container\" does not have a read-only root file system"
  },
  {
    "id": "969",
    "manifest_path": "data/manifests/the_stack_sample/sample_0061.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"bpg-parser-container\" is not set to runAsNonRoot"
  },
  {
    "id": "970",
    "manifest_path": "data/manifests/the_stack_sample/sample_0061.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"bpg-parser-container\" has cpu request 0"
  },
  {
    "id": "971",
    "manifest_path": "data/manifests/the_stack_sample/sample_0061.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"bpg-parser-container\" has memory limit 0"
  },
  {
    "id": "972",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "973",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "974",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "975",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "976",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "977",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "978",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "979",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "980",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rpi3firmware-manager\" is using an invalid container image, \"registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "981",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rpi3firmware-manager\" does not have a read-only root file system"
  },
  {
    "id": "982",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rpi3firmware-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "983",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rpi3firmware-manager\" has cpu request 0"
  },
  {
    "id": "984",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"istio-translate-inspect\" does not have a read-only root file system"
  },
  {
    "id": "985",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"istio-translate-inspect\" is not set to runAsNonRoot"
  },
  {
    "id": "986",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"istio-translate-inspect\" has cpu request 0"
  },
  {
    "id": "987",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"istio-translate-inspect\" has memory limit 0"
  },
  {
    "id": "988",
    "manifest_path": "data/manifests/the_stack_sample/sample_0069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "989",
    "manifest_path": "data/manifests/the_stack_sample/sample_0069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"heapster\" not found"
  },
  {
    "id": "990",
    "manifest_path": "data/manifests/the_stack_sample/sample_0069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  }
]