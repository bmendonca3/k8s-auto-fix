[
  {
    "id": "330",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mount-cgroup\" has memory limit 0"
  },
  {
    "id": "331",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "332",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cilium-envoy\" does not expose port 9878 for the HTTPGet"
  },
  {
    "id": "333",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-envoy\" does not have a read-only root file system"
  },
  {
    "id": "334",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium-envoy\" not found"
  },
  {
    "id": "335",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-envoy\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "336",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cilium-envoy\" does not expose port 9878 for the HTTPGet"
  },
  {
    "id": "337",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-envoy\" is not set to runAsNonRoot"
  },
  {
    "id": "338",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "startup-port",
    "violation_text": "container \"cilium-envoy\" does not expose port 9878 for the HTTPGet"
  },
  {
    "id": "339",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-envoy\" has cpu request 0"
  },
  {
    "id": "340",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-envoy\" has memory limit 0"
  },
  {
    "id": "341",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "342",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cilium-operator\" does not expose port 9234 for the HTTPGet"
  },
  {
    "id": "343",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-operator\" does not have a read-only root file system"
  },
  {
    "id": "344",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium-operator\" not found"
  },
  {
    "id": "345",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cilium-operator\" does not expose port 9234 for the HTTPGet"
  },
  {
    "id": "346",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "347",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-operator\" has cpu request 0"
  },
  {
    "id": "348",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-operator\" has memory limit 0"
  },
  {
    "id": "349",
    "manifest_path": "data/manifests/artifacthub/cluster-autoscaler/cluster-autoscaler/001_poddisruptionbudget_release-name-aws-cluster-autoscaler.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cluster-autoscaler-9.50.1\n  name: release-name-aws-cluster-autoscaler\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: aws-cluster-autoscaler\n  maxUnavailable: 1\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "350",
    "manifest_path": "data/manifests/artifacthub/cluster-autoscaler/cluster-autoscaler/007_service_release-name-aws-cluster-autoscaler.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cluster-autoscaler-9.50.1\n  name: release-name-aws-cluster-autoscaler\n  namespace: default\nspec:\n  ports:\n  - port: 8085\n    protocol: TCP\n    targetPort: 8085\n    name: http\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:aws-cluster-autoscaler])"
  },
  {
    "id": "351",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/018_service_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  type: ClusterIP\n  selector:\n    app: release-name-datadog-cluster-agent\n  ports:\n  - port: 5005\n    name: agentport\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:release-name-datadog-cluster-agent])"
  },
  {
    "id": "352",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/019_service_release-name-datadog-cluster-agent-admission-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog-cluster-agent-admission-controller\n  namespace: default\n  labels:\n    app: release-name-datadog\n    chart: datadog-3.136.1\n    release: release-name\n    heritage: Helm\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  selector:\n    app: release-name-datadog-cluster-agent\n  ports:\n  - port: 443\n    targetPort: 8000\n    name: datadog-webhook\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:release-name-datadog-cluster-agent])"
  },
  {
    "id": "353",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/020_service_release-name-datadog.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog\n  namespace: default\n  labels:\n    app: release-name-datadog\n    chart: datadog-3.136.1\n    release: release-name\n    heritage: Helm\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  selector:\n    app: release-name-datadog\n  ports:\n  - protocol: UDP\n    port: 8125\n    targetPort: 8125\n    name: dogstatsdport\n  - protocol: TCP\n    port: 8126\n    targetPort: 8126\n    name: traceport\n  internalTrafficPolicy: Local\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:release-name-datadog])"
  },
  {
    "id": "354",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cluster-agent\" does not expose port 5556 for the HTTPGet"
  },
  {
    "id": "355",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-volume\" does not have a read-only root file system"
  },
  {
    "id": "356",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-datadog-cluster-agent\" not found"
  },
  {
    "id": "357",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cluster-agent\" does not expose port 5556 for the HTTPGet"
  },
  {
    "id": "358",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cluster-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "359",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-volume\" is not set to runAsNonRoot"
  },
  {
    "id": "360",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "startup-port",
    "violation_text": "container \"cluster-agent\" does not expose port 5556 for the HTTPGet"
  },
  {
    "id": "361",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cluster-agent\" has cpu request 0"
  },
  {
    "id": "362",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-volume\" has cpu request 0"
  },
  {
    "id": "363",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cluster-agent\" has memory limit 0"
  },
  {
    "id": "364",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-volume\" has memory limit 0"
  },
  {
    "id": "365",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/001_poddisruptionbudget_elasticsearch-master-pdb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: elasticsearch-master-pdb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "366",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/004_service_elasticsearch-master.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations: {}\nspec:\n  type: ClusterIP\n  selector:\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  publishNotReadyAddresses: false\n  ports:\n  - name: http\n    protocol: TCP\n    port: 9200\n  - name: transport\n    protocol: TCP\n    port: 9300\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:elasticsearch-master chart:elasticsearch release:release-name])"
  },
  {
    "id": "367",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/005_service_elasticsearch-master-headless.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch-master-headless\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: 'true'\nspec:\n  clusterIP: None\n  publishNotReadyAddresses: true\n  selector:\n    app: elasticsearch-master\n  ports:\n  - name: http\n    port: 9200\n  - name: transport\n    port: 9300\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:elasticsearch-master])"
  },
  {
    "id": "368",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"configure-sysctl\" does not have a read-only root file system"
  },
  {
    "id": "369",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"elasticsearch\" does not have a read-only root file system"
  },
  {
    "id": "370",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"configure-sysctl\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "371",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"configure-sysctl\" is privileged"
  },
  {
    "id": "372",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"configure-sysctl\" is not set to runAsNonRoot"
  },
  {
    "id": "373",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"configure-sysctl\" has cpu request 0"
  },
  {
    "id": "374",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"configure-sysctl\" has memory limit 0"
  },
  {
    "id": "375",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-mvgmy-test\" does not have a read-only root file system"
  },
  {
    "id": "376",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-mvgmy-test\" has cpu request 0"
  },
  {
    "id": "377",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-mvgmy-test\" has memory limit 0"
  },
  {
    "id": "378",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/004_service_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  selector:\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n  ports:\n  - name: http\n    port: 7979\n    targetPort: http\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:external-dns])"
  },
  {
    "id": "379",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-external-dns\" not found"
  },
  {
    "id": "380",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"external-dns\" has cpu request 0"
  },
  {
    "id": "381",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"external-dns\" has memory limit 0"
  },
  {
    "id": "382",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/037_service_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\n    external-secrets.io/component: webhook\nspec:\n  type: ClusterIP\n  ports:\n  - port: 443\n    targetPort: webhook\n    protocol: TCP\n    name: webhook\n  selector:\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:external-secrets-webhook])"
  },
  {
    "id": "383",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"external-secrets-cert-controller\" not found"
  },
  {
    "id": "384",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cert-controller\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "385",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-controller\" has cpu request 0"
  },
  {
    "id": "386",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-controller\" has memory limit 0"
  },
  {
    "id": "387",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-external-secrets\" not found"
  },
  {
    "id": "388",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"external-secrets\" has cpu request 0"
  },
  {
    "id": "389",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"external-secrets\" has memory limit 0"
  },
  {
    "id": "390",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"external-secrets-webhook\" not found"
  },
  {
    "id": "391",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"webhook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "392",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"webhook\" has cpu request 0"
  },
  {
    "id": "393",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"webhook\" has memory limit 0"
  },
  {
    "id": "394",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/005_service_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n  - port: 2020\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:fluent-bit])"
  },
  {
    "id": "395",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluent-bit\" does not have a read-only root file system"
  },
  {
    "id": "396",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-fluent-bit\" not found"
  },
  {
    "id": "397",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fluent-bit\" is not set to runAsNonRoot"
  },
  {
    "id": "398",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fluent-bit\" has cpu request 0"
  },
  {
    "id": "399",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fluent-bit\" has memory limit 0"
  },
  {
    "id": "400",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "401",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wget\" does not have a read-only root file system"
  },
  {
    "id": "402",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wget\" is not set to runAsNonRoot"
  },
  {
    "id": "403",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wget\" has cpu request 0"
  },
  {
    "id": "404",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wget\" has memory limit 0"
  },
  {
    "id": "405",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/004_poddisruptionbudget_release-name-postgresql-ha-pgpool.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql-ha-pgpool\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 4.6.3\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: pgpool\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: pgpool\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "406",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/005_poddisruptionbudget_release-name-postgresql-ha-postgresql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql-ha-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: postgresql\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "407",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/006_poddisruptionbudget_release-name-postgresql-ha-postgresql-witness.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql-ha-postgresql-witness\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\n    role: witness\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: postgresql\n      role: witness\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "408",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/007_poddisruptionbudget_release-name-valkey-cluster.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-valkey-cluster\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: valkey-cluster\n    matchExpressions:\n    - key: job-name\n      operator: NotIn\n      values:\n      - release-name-valkey-cluster-cluster-update\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "409",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/019_service_release-name-postgresql-ha-pgpool.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-ha-pgpool\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 4.6.3\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: pgpool\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: postgresql\n    port: 5432\n    targetPort: postgresql\n    protocol: TCP\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/component: pgpool\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:pgpool app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql-ha])"
  },
  {
    "id": "410",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/020_service_release-name-postgresql-ha-postgresql-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-ha-postgresql-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: false\n  ports:\n  - name: postgresql\n    port: 5432\n    targetPort: postgresql\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/component: postgresql\n    role: data\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:postgresql app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql-ha role:data])"
  },
  {
    "id": "411",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/021_service_release-name-postgresql-ha-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-ha-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\nspec:\n  type: ClusterIP\n  ports:\n  - name: postgresql\n    port: 5432\n    targetPort: postgresql\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/component: postgresql\n    role: data\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:postgresql app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql-ha role:data])"
  },
  {
    "id": "412",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/022_service_release-name-valkey-cluster-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-valkey-cluster-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: tcp-redis\n  - name: tcp-redis-bus\n    port: 16379\n    targetPort: tcp-redis-bus\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: valkey-cluster\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:valkey-cluster])"
  },
  {
    "id": "413",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/023_service_release-name-valkey-cluster.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-valkey-cluster\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: tcp-redis\n    protocol: TCP\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: valkey-cluster\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:valkey-cluster])"
  },
  {
    "id": "414",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/024_service_release-name-gitea-http.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-gitea-http\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations: {}\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: http\n    port: 3000\n    targetPort: null\n  selector:\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:gitea])"
  },
  {
    "id": "415",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/025_service_release-name-gitea-ssh.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-gitea-ssh\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations: {}\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: ssh\n    port: 22\n    targetPort: 2222\n    protocol: TCP\n  selector:\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:gitea])"
  },
  {
    "id": "416",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/026_deployment_release-name-postgresql-ha-pgpool.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-postgresql-ha-pgpool\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 4.6.3\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: pgpool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: pgpool\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql-ha\n        app.kubernetes.io/version: 4.6.3\n        helm.sh/chart: postgresql-ha-16.3.2\n        app.kubernetes.io/component: pgpool\n      annotations: null\n    spec:\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql-ha\n                  app.kubernetes.io/component: pgpool\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-postgresql-ha\n      containers:\n      - name: pgpool\n        image: docker.io/bitnamilegacy/pgpool:4.6.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REPMGR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: PGPOOL_BACKEND_NODES\n          value: 0:release-name-postgresql-ha-postgresql-0.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local:5432,1:release-name-postgresql-ha-postgresql-1.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local:5432,2:release-name-postgresql-ha-postgresql-2.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local:5432,\n        - name: PGPOOL_SR_CHECK_USER\n          value: sr_check_user\n        - name: PGPOOL_SR_CHECK_PASSWORD_FILE\n          value: /opt/bitnami/pgpool/secrets/sr-check-password\n        - name: PGPOOL_SR_CHECK_DATABASE\n          value: postgres\n        - name: PGPOOL_ENABLE_LDAP\n          value: 'no'\n        - name: PGPOOL_POSTGRES_USERNAME\n          value: gitea\n        - name: PGPOOL_POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/pgpool/secrets/pgpool-password\n        - name: PGPOOL_ADMIN_USERNAME\n          value: admin\n        - name: PGPOOL_ADMIN_PASSWORD_FILE\n          value: /opt/bitnami/pgpool/secrets/admin-password\n        - name: PGPOOL_AUTHENTICATION_METHOD\n          value: scram-sha-256\n        - name: PGPOOL_ENABLE_LOAD_BALANCING\n          value: 'yes'\n        - name: PGPOOL_ENABLE_CONNECTION_CACHE\n          value: 'yes'\n        - name: PGPOOL_DISABLE_LOAD_BALANCE_ON_WRITE\n          value: transaction\n        - name: PGPOOL_ENABLE_LOG_CONNECTIONS\n          value: 'no'\n        - name: PGPOOL_ENABLE_LOG_HOSTNAME\n          value: 'yes'\n        - name: PGPOOL_ENABLE_LOG_PCP_PROCESSES\n          value: 'yes'\n        - name: PGPOOL_ENABLE_LOG_PER_NODE_STATEMENT\n          value: 'no'\n        - name: PGPOOL_RESERVED_CONNECTIONS\n          value: '1'\n        - name: PGPOOL_CHILD_LIFE_TIME\n          value: ''\n        - name: PGPOOL_ENABLE_TLS\n          value: 'no'\n        - name: PGPOOL_HEALTH_CHECK_PSQL_TIMEOUT\n          value: '6'\n        envFrom: null\n        ports:\n        - name: postgresql\n          containerPort: 5432\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /opt/bitnami/scripts/pgpool/healthcheck.sh\n        readinessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - bash\n            - -ec\n            - PGPASSWORD=$(< $PGPOOL_POSTGRES_PASSWORD_FILE) psql -U \"gitea\" -d \"gitea\"\n              -h /opt/bitnami/pgpool/tmp -tA -c \"SELECT 1\" >/dev/null\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/etc\n          subPath: app-etc-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/logs\n          subPath: app-logs-dir\n        - name: postgresql-creds\n          subPath: pgpool-password\n          mountPath: /opt/bitnami/pgpool/secrets/pgpool-password\n        - name: pgpool-creds\n          subPath: admin-password\n          mountPath: /opt/bitnami/pgpool/secrets/admin-password\n        - name: pgpool-creds\n          subPath: sr-check-password\n          mountPath: /opt/bitnami/pgpool/secrets/sr-check-password\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-creds\n        secret:\n          secretName: release-name-postgresql-ha-postgresql\n          items:\n          - key: password\n            path: pgpool-password\n      - name: pgpool-creds\n        secret:\n          secretName: release-name-postgresql-ha-pgpool\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-postgresql-ha\" not found"
  },
  {
    "id": "417",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"configure-gitea\" does not have a read-only root file system"
  },
  {
    "id": "418",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gitea\" does not have a read-only root file system"
  },
  {
    "id": "419",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-app-ini\" does not have a read-only root file system"
  },
  {
    "id": "420",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-directories\" does not have a read-only root file system"
  },
  {
    "id": "421",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gitea\" is not set to runAsNonRoot"
  },
  {
    "id": "422",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-app-ini\" is not set to runAsNonRoot"
  },
  {
    "id": "423",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-directories\" is not set to runAsNonRoot"
  },
  {
    "id": "424",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gitea\" has cpu request 0"
  },
  {
    "id": "425",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"configure-gitea\" has memory limit 0"
  },
  {
    "id": "426",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gitea\" has memory limit 0"
  },
  {
    "id": "427",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-app-ini\" has memory limit 0"
  },
  {
    "id": "428",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-directories\" has memory limit 0"
  },
  {
    "id": "429",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/028_statefulset_release-name-postgresql-ha-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql-ha-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\n    role: data\nspec:\n  replicas: 3\n  podManagementPolicy: Parallel\n  serviceName: release-name-postgresql-ha-postgresql-headless\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: postgresql\n      role: data\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql-ha\n        app.kubernetes.io/version: 17.6.0\n        helm.sh/chart: postgresql-ha-16.3.2\n        app.kubernetes.io/component: postgresql\n        role: data\n      annotations: null\n    spec:\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql-ha\n                  app.kubernetes.io/component: postgresql\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-postgresql-ha\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnamilegacy/postgresql-repmgr:17.6.0-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /pre-stop.sh\n              - '25'\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRES_USER\n          value: gitea\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/password\n        - name: POSTGRES_DB\n          value: gitea\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'true'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit, repmgr\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: REPMGR_PORT_NUMBER\n          value: '5432'\n        - name: REPMGR_PRIMARY_PORT\n          value: '5432'\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: REPMGR_UPGRADE_EXTENSION\n          value: 'no'\n        - name: REPMGR_PGHBA_TRUST_ALL\n          value: 'no'\n        - name: REPMGR_MOUNTED_CONF_DIR\n          value: /bitnami/repmgr/conf\n        - name: REPMGR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: REPMGR_PARTNER_NODES\n          value: release-name-postgresql-ha-postgresql-0.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,release-name-postgresql-ha-postgresql-1.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,release-name-postgresql-ha-postgresql-2.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,\n        - name: REPMGR_PRIMARY_HOST\n          value: release-name-postgresql-ha-postgresql-0.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local\n        - name: REPMGR_NODE_NAME\n          value: $(MY_POD_NAME)\n        - name: REPMGR_NODE_NETWORK_NAME\n          value: $(MY_POD_NAME).release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local\n        - name: REPMGR_NODE_TYPE\n          value: data\n        - name: REPMGR_LOG_LEVEL\n          value: NOTICE\n        - name: REPMGR_CONNECT_TIMEOUT\n          value: '5'\n        - name: REPMGR_RECONNECT_ATTEMPTS\n          value: '2'\n        - name: REPMGR_RECONNECT_INTERVAL\n          value: '3'\n        - name: REPMGR_USERNAME\n          value: repmgr\n        - name: REPMGR_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/repmgr-password\n        - name: REPMGR_USE_PASSFILE\n          value: 'true'\n        - name: REPMGR_PASSFILE_PATH\n          value: /opt/bitnami/repmgr/conf/.pgpass\n        - name: REPMGR_DATABASE\n          value: repmgr\n        - name: REPMGR_FENCE_OLD_PRIMARY\n          value: 'no'\n        - name: REPMGR_CHILD_NODES_CHECK_INTERVAL\n          value: '5'\n        - name: REPMGR_CHILD_NODES_CONNECTED_MIN_COUNT\n          value: '1'\n        - name: REPMGR_CHILD_NODES_DISCONNECT_TIMEOUT\n          value: '30'\n        - name: POSTGRESQL_SR_CHECK\n          value: 'yes'\n        - name: POSTGRESQL_SR_CHECK_USERNAME\n          value: sr_check_user\n        - name: POSTGRESQL_SR_CHECK_DATABASE\n          value: postgres\n        - name: POSTGRESQL_SR_CHECK_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/sr-check-password\n        envFrom: null\n        ports:\n        - name: postgresql\n          containerPort: 5432\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /liveness-probe.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /readiness-probe.sh\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/repmgr/conf\n          subPath: repmgr-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/repmgr/tmp\n          subPath: repmgr-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/repmgr/logs\n          subPath: repmgr-logs-dir\n        - name: postgresql-creds\n          mountPath: /opt/bitnami/postgresql/secrets/postgres-password\n          subPath: postgres-password\n        - name: postgresql-creds\n          subPath: password\n          mountPath: /opt/bitnami/postgresql/secrets/password\n        - name: postgresql-creds\n          subPath: repmgr-password\n          mountPath: /opt/bitnami/postgresql/secrets/repmgr-password\n        - name: pgpool-creds\n          subPath: sr-check-password\n          mountPath: /opt/bitnami/postgresql/secrets/sr-check-password\n        - name: data\n          mountPath: /bitnami/postgresql\n        - name: scripts\n          mountPath: /pre-stop.sh\n          subPath: pre-stop.sh\n        - name: scripts\n          mountPath: /liveness-probe.sh\n          subPath: liveness-probe.sh\n        - name: scripts\n          mountPath: /readiness-probe.sh\n          subPath: readiness-probe.sh\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: scripts\n        configMap:\n          name: release-name-postgresql-ha-postgresql-scripts\n          defaultMode: 493\n      - name: postgresql-creds\n        secret:\n          secretName: release-name-postgresql-ha-postgresql\n      - name: pgpool-creds\n        secret:\n          secretName: release-name-postgresql-ha-pgpool\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-postgresql-ha\" not found"
  },
  {
    "id": "430",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/029_statefulset_release-name-valkey-cluster.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-valkey-cluster\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  updateStrategy:\n    rollingUpdate:\n      partition: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: valkey-cluster\n  replicas: 3\n  serviceName: release-name-valkey-cluster-headless\n  podManagementPolicy: Parallel\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: valkey-cluster\n        app.kubernetes.io/version: 8.1.3\n        helm.sh/chart: valkey-cluster-3.0.24\n      annotations:\n        checksum/scripts: b416f9c23ba0f844168db1fda55802c08f6e6158998b5dc543fb3e2827aee988\n        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/config: d260a75cea2840d303318eeb5409b6f6f190e3e221ea66f3e98857402f575fa0\n    spec:\n      hostNetwork: false\n      enableServiceLinks: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-valkey-cluster\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: valkey-cluster\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      containers:\n      - name: release-name-valkey-cluster\n        image: docker.io/bitnamilegacy/valkey-cluster:8.1.3-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - \"# Backwards compatibility change\\nif ! [[ -f /opt/bitnami/valkey/etc/valkey.conf\\\n          \\ ]]; then\\n    echo COPYING FILE\\n    cp  /opt/bitnami/valkey/etc/valkey-default.conf\\\n          \\ /opt/bitnami/valkey/etc/valkey.conf\\nfi\\npod_index=($(echo \\\"$POD_NAME\\\"\\\n          \\ | tr \\\"-\\\" \\\"\\\\n\\\"))\\npod_index=\\\"${pod_index[-1]}\\\"\\nif [[ \\\"$pod_index\\\"\\\n          \\ == \\\"0\\\" ]]; then\\n  export VALKEY_CLUSTER_CREATOR=\\\"yes\\\"\\n  export VALKEY_CLUSTER_REPLICAS=\\\"\\\n          0\\\"\\nfi\\n/opt/bitnami/scripts/valkey-cluster/entrypoint.sh /opt/bitnami/scripts/valkey-cluster/run.sh\\n\"\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VALKEY_NODES\n          value: 'release-name-valkey-cluster-0.release-name-valkey-cluster-headless\n            release-name-valkey-cluster-1.release-name-valkey-cluster-headless release-name-valkey-cluster-2.release-name-valkey-cluster-headless '\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: VALKEY_AOF_ENABLED\n          value: 'yes'\n        - name: VALKEY_TLS_ENABLED\n          value: 'no'\n        - name: VALKEY_PORT_NUMBER\n          value: '6379'\n        ports:\n        - name: tcp-redis\n          containerPort: 6379\n        - name: tcp-redis-bus\n          containerPort: 16379\n        livenessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - sh\n            - -c\n            - /scripts/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - sh\n            - -c\n            - /scripts/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: scripts\n          mountPath: /scripts\n        - name: valkey-data\n          mountPath: /bitnami/valkey/data\n          subPath: null\n        - name: default-config\n          mountPath: /opt/bitnami/valkey/etc/valkey-default.conf\n          subPath: valkey-default.conf\n        - name: empty-dir\n          mountPath: /opt/bitnami/valkey/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/valkey/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/valkey/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: scripts\n        configMap:\n          name: release-name-valkey-cluster-scripts\n          defaultMode: 493\n      - name: default-config\n        configMap:\n          name: release-name-valkey-cluster-default\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - metadata:\n      name: valkey-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: valkey-cluster\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-valkey-cluster\" not found"
  },
  {
    "id": "431",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "432",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wget\" does not have a read-only root file system"
  },
  {
    "id": "433",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wget\" is not set to runAsNonRoot"
  },
  {
    "id": "434",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wget\" has cpu request 0"
  },
  {
    "id": "435",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wget\" has memory limit 0"
  },
  {
    "id": "436",
    "manifest_path": "data/manifests/artifacthub/gitlab/gitlab-runner/002_deployment_release-name-gitlab-runner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitlab-runner\n  namespace: default\n  labels:\n    app: release-name-gitlab-runner\n    chart: gitlab-runner-0.81.0\n    release: release-name\n    heritage: Helm\nspec:\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: release-name-gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: release-name-gitlab-runner\n        chart: gitlab-runner-0.81.0\n        release: release-name\n        heritage: Helm\n      annotations:\n        checksum/configmap: b9a97367f708f909d06f31603187138f92ff2d5db59610719b91b9901db1e075\n    spec:\n      securityContext:\n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: ''\n      containers:\n      - name: release-name-gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v18.4.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /entrypoint\n              - unregister\n              - --all-runners\n        command:\n        - /usr/bin/dumb-init\n        - --\n        - /bin/bash\n        - /configmaps/entrypoint\n        env:\n        - name: CI_SERVER_URL\n          value: null\n        - name: RUNNER_EXECUTOR\n          value: kubernetes\n        - name: REGISTER_LOCKED\n          value: 'true'\n        - name: RUNNER_TAG_LIST\n          value: ''\n        - name: SESSION_SERVER_ADDRESS\n          value: null\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - /configmaps/check-live\n            - '3'\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command:\n            - /usr/bin/pgrep\n            - gitlab.*runner\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: metrics\n          containerPort: 9252\n        volumeMounts:\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources: {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: Memory\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: Memory\n      - name: configmaps\n        configMap:\n          name: release-name-gitlab-runner\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-gitlab-runner\" does not have a read-only root file system"
  },
  {
    "id": "437",
    "manifest_path": "data/manifests/artifacthub/gitlab/gitlab-runner/002_deployment_release-name-gitlab-runner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitlab-runner\n  namespace: default\n  labels:\n    app: release-name-gitlab-runner\n    chart: gitlab-runner-0.81.0\n    release: release-name\n    heritage: Helm\nspec:\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: release-name-gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: release-name-gitlab-runner\n        chart: gitlab-runner-0.81.0\n        release: release-name\n        heritage: Helm\n      annotations:\n        checksum/configmap: b9a97367f708f909d06f31603187138f92ff2d5db59610719b91b9901db1e075\n    spec:\n      securityContext:\n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: ''\n      containers:\n      - name: release-name-gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v18.4.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /entrypoint\n              - unregister\n              - --all-runners\n        command:\n        - /usr/bin/dumb-init\n        - --\n        - /bin/bash\n        - /configmaps/entrypoint\n        env:\n        - name: CI_SERVER_URL\n          value: null\n        - name: RUNNER_EXECUTOR\n          value: kubernetes\n        - name: REGISTER_LOCKED\n          value: 'true'\n        - name: RUNNER_TAG_LIST\n          value: ''\n        - name: SESSION_SERVER_ADDRESS\n          value: null\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - /configmaps/check-live\n            - '3'\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command:\n            - /usr/bin/pgrep\n            - gitlab.*runner\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: metrics\n          containerPort: 9252\n        volumeMounts:\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources: {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: Memory\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: Memory\n      - name: configmaps\n        configMap:\n          name: release-name-gitlab-runner\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-gitlab-runner\" has cpu request 0"
  },
  {
    "id": "438",
    "manifest_path": "data/manifests/artifacthub/gitlab/gitlab-runner/002_deployment_release-name-gitlab-runner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitlab-runner\n  namespace: default\n  labels:\n    app: release-name-gitlab-runner\n    chart: gitlab-runner-0.81.0\n    release: release-name\n    heritage: Helm\nspec:\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: release-name-gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: release-name-gitlab-runner\n        chart: gitlab-runner-0.81.0\n        release: release-name\n        heritage: Helm\n      annotations:\n        checksum/configmap: b9a97367f708f909d06f31603187138f92ff2d5db59610719b91b9901db1e075\n    spec:\n      securityContext:\n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: ''\n      containers:\n      - name: release-name-gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v18.4.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /entrypoint\n              - unregister\n              - --all-runners\n        command:\n        - /usr/bin/dumb-init\n        - --\n        - /bin/bash\n        - /configmaps/entrypoint\n        env:\n        - name: CI_SERVER_URL\n          value: null\n        - name: RUNNER_EXECUTOR\n          value: kubernetes\n        - name: REGISTER_LOCKED\n          value: 'true'\n        - name: RUNNER_TAG_LIST\n          value: ''\n        - name: SESSION_SERVER_ADDRESS\n          value: null\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - /configmaps/check-live\n            - '3'\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command:\n            - /usr/bin/pgrep\n            - gitlab.*runner\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: metrics\n          containerPort: 9252\n        volumeMounts:\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources: {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: Memory\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: Memory\n      - name: configmaps\n        configMap:\n          name: release-name-gitlab-runner\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-gitlab-runner\" has memory limit 0"
  },
  {
    "id": "439",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/008_service_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  type: ClusterIP\n  ports:\n  - name: service\n    port: 80\n    protocol: TCP\n    targetPort: grafana\n  selector:\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:grafana])"
  },
  {
    "id": "440",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "441",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-grafana\" not found"
  },
  {
    "id": "442",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana\" has cpu request 0"
  },
  {
    "id": "443",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana\" has memory limit 0"
  },
  {
    "id": "444",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-test\" does not have a read-only root file system"
  },
  {
    "id": "445",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-grafana-test\" not found"
  },
  {
    "id": "446",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"release-name-test\" is not set to runAsNonRoot"
  },
  {
    "id": "447",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-test\" has cpu request 0"
  },
  {
    "id": "448",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-test\" has memory limit 0"
  },
  {
    "id": "449",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/011_service_release-name-loki-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki-headless\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n    variant: headless\nspec:\n  clusterIP: None\n  ports:\n  - port: 3100\n    protocol: TCP\n    name: http-metrics\n    targetPort: http-metrics\n  selector:\n    app: loki\n    release: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:loki release:release-name])"
  },
  {
    "id": "450",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/012_service_release-name-loki-memberlist.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki-memberlist\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: http\n    port: 7946\n    targetPort: memberlist-port\n    protocol: TCP\n  selector:\n    app: loki\n    release: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:loki release:release-name])"
  },
  {
    "id": "451",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/013_service_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  type: ClusterIP\n  ports:\n  - port: 3100\n    protocol: TCP\n    name: http-metrics\n    targetPort: http-metrics\n  selector:\n    app: loki\n    release: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:loki release:release-name])"
  },
  {
    "id": "452",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-promtail\" not found"
  },
  {
    "id": "453",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"promtail\" is not set to runAsNonRoot"
  },
  {
    "id": "454",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"promtail\" has cpu request 0"
  },
  {
    "id": "455",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"promtail\" has memory limit 0"
  },
  {
    "id": "456",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/015_statefulset_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-loki\" not found"
  },
  {
    "id": "457",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/015_statefulset_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"loki\" has cpu request 0"
  },
  {
    "id": "458",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/015_statefulset_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"loki\" has memory limit 0"
  },
  {
    "id": "459",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test\" does not have a read-only root file system"
  },
  {
    "id": "460",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test\" is not set to runAsNonRoot"
  },
  {
    "id": "461",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test\" has cpu request 0"
  },
  {
    "id": "462",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test\" has memory limit 0"
  },
  {
    "id": "463",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-promtail\" not found"
  },
  {
    "id": "464",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"promtail\" is not set to runAsNonRoot"
  },
  {
    "id": "465",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"promtail\" has cpu request 0"
  },
  {
    "id": "466",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"promtail\" has memory limit 0"
  },
  {
    "id": "467",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/017_service_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-web\n    port: 80\n    targetPort: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: core\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:core release:release-name])"
  },
  {
    "id": "468",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/018_service_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - port: 5432\n  selector:\n    release: release-name\n    app: harbor\n    component: database\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:database release:release-name])"
  },
  {
    "id": "469",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/019_service_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-jobservice\n    port: 80\n    targetPort: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: jobservice\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:jobservice release:release-name])"
  },
  {
    "id": "470",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/020_service_release-name-harbor-portal.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: portal\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:portal release:release-name])"
  },
  {
    "id": "471",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/021_service_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - port: 6379\n  selector:\n    release: release-name\n    app: harbor\n    component: redis\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:redis release:release-name])"
  },
  {
    "id": "472",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/022_service_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-registry\n    port: 5000\n  - name: http-controller\n    port: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: registry\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:registry release:release-name])"
  },
  {
    "id": "473",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/023_service_release-name-harbor-trivy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-trivy\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-trivy\n    protocol: TCP\n    port: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: trivy\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:trivy release:release-name])"
  },
  {
    "id": "474",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/024_deployment_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"core\" does not have a read-only root file system"
  },
  {
    "id": "475",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/024_deployment_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"core\" has cpu request 0"
  },
  {
    "id": "476",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/024_deployment_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"core\" has memory limit 0"
  },
  {
    "id": "477",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/025_deployment_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jobservice\" does not have a read-only root file system"
  },
  {
    "id": "478",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/025_deployment_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jobservice\" has cpu request 0"
  },
  {
    "id": "479",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/025_deployment_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jobservice\" has memory limit 0"
  },
  {
    "id": "480",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/026_deployment_release-name-harbor-portal.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: portal\n    app.kubernetes.io/component: portal\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: portal\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: portal\n        app.kubernetes.io/component: portal\n      annotations:\n        checksum/configmap: c9e04324738b148b4530aa2bd57b606d50600544b949ceef09270ef9c207bf85\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: portal\n        image: goharbor/harbor-portal:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: portal-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n      volumes:\n      - name: portal-config\n        configMap:\n          name: release-name-harbor-portal\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"portal\" does not have a read-only root file system"
  },
  {
    "id": "481",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/026_deployment_release-name-harbor-portal.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: portal\n    app.kubernetes.io/component: portal\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: portal\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: portal\n        app.kubernetes.io/component: portal\n      annotations:\n        checksum/configmap: c9e04324738b148b4530aa2bd57b606d50600544b949ceef09270ef9c207bf85\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: portal\n        image: goharbor/harbor-portal:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: portal-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n      volumes:\n      - name: portal-config\n        configMap:\n          name: release-name-harbor-portal\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"portal\" has cpu request 0"
  },
  {
    "id": "482",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/026_deployment_release-name-harbor-portal.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: portal\n    app.kubernetes.io/component: portal\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: portal\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: portal\n        app.kubernetes.io/component: portal\n      annotations:\n        checksum/configmap: c9e04324738b148b4530aa2bd57b606d50600544b949ceef09270ef9c207bf85\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: portal\n        image: goharbor/harbor-portal:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: portal-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n      volumes:\n      - name: portal-config\n        configMap:\n          name: release-name-harbor-portal\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"portal\" has memory limit 0"
  },
  {
    "id": "483",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"registry\" does not have a read-only root file system"
  },
  {
    "id": "484",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"registryctl\" does not have a read-only root file system"
  },
  {
    "id": "485",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"registry\" has cpu request 0"
  },
  {
    "id": "486",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"registryctl\" has cpu request 0"
  },
  {
    "id": "487",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"registry\" has memory limit 0"
  },
  {
    "id": "488",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"registryctl\" has memory limit 0"
  },
  {
    "id": "489",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"data-permissions-ensurer\" does not have a read-only root file system"
  },
  {
    "id": "490",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"database\" does not have a read-only root file system"
  },
  {
    "id": "491",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"data-permissions-ensurer\" has cpu request 0"
  },
  {
    "id": "492",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"database\" has cpu request 0"
  },
  {
    "id": "493",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"data-permissions-ensurer\" has memory limit 0"
  },
  {
    "id": "494",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"database\" has memory limit 0"
  },
  {
    "id": "495",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/029_statefulset_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"redis\" does not expose port 6379 for the TCPSocket"
  },
  {
    "id": "496",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/029_statefulset_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "497",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/029_statefulset_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"redis\" does not expose port 6379 for the TCPSocket"
  },
  {
    "id": "498",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/029_statefulset_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "499",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/029_statefulset_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "500",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/030_statefulset_release-name-harbor-trivy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-trivy\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: trivy\n    app.kubernetes.io/component: trivy\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-trivy\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: trivy\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: trivy\n        app.kubernetes.io/component: trivy\n      annotations:\n        checksum/secret: 6fbc9db4d37c6bc53260a46776f92e1e0390c1d50cd884e19a8d0a3c73deed8f\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: trivy\n        image: goharbor/trivy-adapter-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: HTTP_PROXY\n          value: ''\n        - name: HTTPS_PROXY\n          value: ''\n        - name: NO_PROXY\n          value: release-name-harbor-core,release-name-harbor-jobservice,release-name-harbor-database,release-name-harbor-registry,release-name-harbor-portal,release-name-harbor-trivy,release-name-harbor-exporter,127.0.0.1,localhost,.local,.internal\n        - name: SCANNER_LOG_LEVEL\n          value: info\n        - name: SCANNER_TRIVY_CACHE_DIR\n          value: /home/scanner/.cache/trivy\n        - name: SCANNER_TRIVY_REPORTS_DIR\n          value: /home/scanner/.cache/reports\n        - name: SCANNER_TRIVY_DEBUG_MODE\n          value: 'false'\n        - name: SCANNER_TRIVY_VULN_TYPE\n          value: os,library\n        - name: SCANNER_TRIVY_TIMEOUT\n          value: 5m0s\n        - name: SCANNER_TRIVY_GITHUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: gitHubToken\n        - name: SCANNER_TRIVY_SEVERITY\n          value: UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL\n        - name: SCANNER_TRIVY_IGNORE_UNFIXED\n          value: 'false'\n        - name: SCANNER_TRIVY_SKIP_UPDATE\n          value: 'false'\n        - name: SCANNER_TRIVY_SKIP_JAVA_DB_UPDATE\n          value: 'false'\n        - name: SCANNER_TRIVY_DB_REPOSITORY\n          value: mirror.gcr.io/aquasec/trivy-db,ghcr.io/aquasecurity/trivy-db\n        - name: SCANNER_TRIVY_JAVA_DB_REPOSITORY\n          value: mirror.gcr.io/aquasec/trivy-java-db,ghcr.io/aquasecurity/trivy-java-db\n        - name: SCANNER_TRIVY_OFFLINE_SCAN\n          value: 'false'\n        - name: SCANNER_TRIVY_SECURITY_CHECKS\n          value: vuln\n        - name: SCANNER_TRIVY_INSECURE\n          value: 'false'\n        - name: SCANNER_API_SERVER_ADDR\n          value: :8080\n        - name: SCANNER_REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: redisURL\n        - name: SCANNER_STORE_REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: redisURL\n        - name: SCANNER_JOB_QUEUE_REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: redisURL\n        ports:\n        - name: api-server\n          containerPort: 8080\n        volumeMounts:\n        - name: data\n          mountPath: /home/scanner/.cache\n          subPath: null\n          readOnly: false\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /probe/healthy\n            port: api-server\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 10\n        readinessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /probe/ready\n            port: api-server\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 1\n            memory: 1Gi\n          requests:\n            cpu: 200m\n            memory: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"trivy\" does not have a read-only root file system"
  },
  {
    "id": "501",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/007_service_release-name-vault-agent-injector-svc.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-vault-agent-injector-svc\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ports:\n  - name: https\n    port: 443\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    component: webhook\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:vault-agent-injector component:webhook])"
  },
  {
    "id": "502",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/008_service_release-name-vault-internal.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-vault-internal\n  namespace: default\n  labels:\n    helm.sh/chart: vault-0.31.0\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    vault-internal: 'true'\n  annotations: null\nspec:\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: http\n    port: 8200\n    targetPort: 8200\n  - name: https-internal\n    port: 8201\n    targetPort: 8201\n  selector:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    component: server\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:vault component:server])"
  },
  {
    "id": "503",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/009_service_release-name-vault.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    helm.sh/chart: vault-0.31.0\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  publishNotReadyAddresses: true\n  ports:\n  - name: http\n    port: 8200\n    targetPort: 8200\n  - name: https-internal\n    port: 8201\n    targetPort: 8201\n  selector:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    component: server\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:vault component:server])"
  },
  {
    "id": "504",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"sidecar-injector\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "505",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sidecar-injector\" does not have a read-only root file system"
  },
  {
    "id": "506",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-vault-agent-injector\" not found"
  },
  {
    "id": "507",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"sidecar-injector\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "508",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "startup-port",
    "violation_text": "container \"sidecar-injector\" does not expose port 8080 for the HTTPGet"
  },
  {
    "id": "509",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sidecar-injector\" has cpu request 0"
  },
  {
    "id": "510",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sidecar-injector\" has memory limit 0"
  },
  {
    "id": "511",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/011_statefulset_release-name-vault.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vault\" does not have a read-only root file system"
  },
  {
    "id": "512",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/011_statefulset_release-name-vault.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-vault\" not found"
  },
  {
    "id": "513",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/011_statefulset_release-name-vault.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vault\" has cpu request 0"
  },
  {
    "id": "514",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/011_statefulset_release-name-vault.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vault\" has memory limit 0"
  },
  {
    "id": "515",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/013_pod_release-name-vault-server-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n  restartPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-server-test\" does not have a read-only root file system"
  },
  {
    "id": "516",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/013_pod_release-name-vault-server-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n  restartPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"release-name-server-test\" is not set to runAsNonRoot"
  },
  {
    "id": "517",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/013_pod_release-name-vault-server-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n  restartPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-server-test\" has cpu request 0"
  },
  {
    "id": "518",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/013_pod_release-name-vault-server-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n  restartPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-server-test\" has memory limit 0"
  },
  {
    "id": "519",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/007_service_release-name-ingress-nginx-controller-admission.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller-admission\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - name: https-webhook\n    port: 443\n    targetPort: webhook\n    appProtocol: https\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:release-name app.kubernetes.io/name:ingress-nginx])"
  },
  {
    "id": "520",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/008_service_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations: null\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  type: LoadBalancer\n  ipFamilyPolicy: SingleStack\n  ipFamilies:\n  - IPv4\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n    appProtocol: http\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n    appProtocol: https\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:release-name app.kubernetes.io/name:ingress-nginx])"
  },
  {
    "id": "521",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/009_deployment_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-ingress-nginx\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"controller\" does not expose port 10254 for the HTTPGet"
  },
  {
    "id": "522",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/009_deployment_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-ingress-nginx\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"controller\" does not have a read-only root file system"
  },
  {
    "id": "523",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/009_deployment_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-ingress-nginx\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-ingress-nginx\" not found"
  },
  {
    "id": "524",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/009_deployment_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-ingress-nginx\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"controller\" does not expose port 10254 for the HTTPGet"
  },
  {
    "id": "525",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/009_deployment_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-ingress-nginx\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "526",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/017_job_release-name-ingress-nginx-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-ingress-nginx-admission\" not found"
  },
  {
    "id": "527",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/017_job_release-name-ingress-nginx-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"create\" has cpu request 0"
  },
  {
    "id": "528",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/017_job_release-name-ingress-nginx-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"create\" has memory limit 0"
  },
  {
    "id": "529",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/018_job_release-name-ingress-nginx-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-ingress-nginx-admission\" not found"
  },
  {
    "id": "530",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/018_job_release-name-ingress-nginx-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"patch\" has cpu request 0"
  },
  {
    "id": "531",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/018_job_release-name-ingress-nginx-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"patch\" has memory limit 0"
  },
  {
    "id": "532",
    "manifest_path": "data/manifests/artifacthub/istio-official/istiod/001_poddisruptionbudget_istiod.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    release: release-name\n    istio: pilot\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: istiod\n      istio: pilot\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "533",
    "manifest_path": "data/manifests/artifacthub/istio-official/istiod/014_service_istiod.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    app: istiod\n    istio: pilot\n    release: release-name\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  ports:\n  - port: 15010\n    name: grpc-xds\n    protocol: TCP\n  - port: 15012\n    name: https-dns\n    protocol: TCP\n  - port: 443\n    name: https-webhook\n    targetPort: 15017\n    protocol: TCP\n  - port: 15014\n    name: http-monitoring\n    protocol: TCP\n  selector:\n    app: istiod\n    istio: pilot\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:istiod istio:pilot])"
  },
  {
    "id": "534",
    "manifest_path": "data/manifests/artifacthub/istio-official/istiod/015_deployment_istiod.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    istio: pilot\n    release: release-name\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: 'false'\n        operator.istio.io/component: Pilot\n        istio: pilot\n        istio.io/dataplane-mode: none\n        app.kubernetes.io/name: istiod\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/part-of: istio\n        app.kubernetes.io/version: 1.27.1\n        helm.sh/chart: istiod-1.27.1\n      annotations:\n        prometheus.io/port: '15014'\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n    spec:\n      tolerations:\n      - key: cni.istio.io/not-ready\n        operator: Exists\n      serviceAccountName: istiod\n      containers:\n      - name: discovery\n        image: docker.io/istio/pilot:1.27.1\n        args:\n        - discovery\n        - --monitoringAddr=:15014\n        - --log_output_level=default:info\n        - --domain\n        - cluster.local\n        - --keepaliveMaxServerConnectionAge\n        - 30m\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: http-debug\n        - containerPort: 15010\n          protocol: TCP\n          name: grpc-xds\n        - containerPort: 15012\n          protocol: TCP\n          name: tls-xds\n        - containerPort: 15017\n          protocol: TCP\n          name: https-webhooks\n        - containerPort: 15014\n          protocol: TCP\n          name: http-monitoring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 3\n          timeoutSeconds: 5\n        env:\n        - name: REVISION\n          value: default\n        - name: PILOT_CERT_PROVIDER\n          value: istiod\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.serviceAccountName\n        - name: KUBECONFIG\n          value: /var/run/secrets/remote/config\n        - name: CA_TRUSTED_NODE_ACCOUNTS\n          value: default/ztunnel\n        - name: PILOT_TRACE_SAMPLING\n          value: '1'\n        - name: PILOT_ENABLE_ANALYSIS\n          value: 'false'\n        - name: CLUSTER_ID\n          value: Kubernetes\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: PLATFORM\n          value: ''\n        resources:\n          requests:\n            cpu: 500m\n            memory: 2048Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: istio-token\n          mountPath: /var/run/secrets/tokens\n          readOnly: true\n        - name: local-certs\n          mountPath: /var/run/secrets/istio-dns\n        - name: cacerts\n          mountPath: /etc/cacerts\n          readOnly: true\n        - name: istio-kubeconfig\n          mountPath: /var/run/secrets/remote\n          readOnly: true\n        - name: istio-csr-dns-cert\n          mountPath: /var/run/secrets/istiod/tls\n          readOnly: true\n        - name: istio-csr-ca-configmap\n          mountPath: /var/run/secrets/istiod/ca\n          readOnly: true\n      volumes:\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              audience: istio-ca\n              expirationSeconds: 43200\n              path: istio-token\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n      - name: istio-csr-dns-cert\n        secret:\n          secretName: istiod-tls\n          optional: true\n      - name: istio-csr-ca-configmap\n        configMap:\n          name: istio-ca-root-cert\n          defaultMode: 420\n          optional: true\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"istiod\" not found"
  },
  {
    "id": "535",
    "manifest_path": "data/manifests/artifacthub/istio-official/istiod/015_deployment_istiod.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    istio: pilot\n    release: release-name\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: 'false'\n        operator.istio.io/component: Pilot\n        istio: pilot\n        istio.io/dataplane-mode: none\n        app.kubernetes.io/name: istiod\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/part-of: istio\n        app.kubernetes.io/version: 1.27.1\n        helm.sh/chart: istiod-1.27.1\n      annotations:\n        prometheus.io/port: '15014'\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n    spec:\n      tolerations:\n      - key: cni.istio.io/not-ready\n        operator: Exists\n      serviceAccountName: istiod\n      containers:\n      - name: discovery\n        image: docker.io/istio/pilot:1.27.1\n        args:\n        - discovery\n        - --monitoringAddr=:15014\n        - --log_output_level=default:info\n        - --domain\n        - cluster.local\n        - --keepaliveMaxServerConnectionAge\n        - 30m\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: http-debug\n        - containerPort: 15010\n          protocol: TCP\n          name: grpc-xds\n        - containerPort: 15012\n          protocol: TCP\n          name: tls-xds\n        - containerPort: 15017\n          protocol: TCP\n          name: https-webhooks\n        - containerPort: 15014\n          protocol: TCP\n          name: http-monitoring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 3\n          timeoutSeconds: 5\n        env:\n        - name: REVISION\n          value: default\n        - name: PILOT_CERT_PROVIDER\n          value: istiod\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.serviceAccountName\n        - name: KUBECONFIG\n          value: /var/run/secrets/remote/config\n        - name: CA_TRUSTED_NODE_ACCOUNTS\n          value: default/ztunnel\n        - name: PILOT_TRACE_SAMPLING\n          value: '1'\n        - name: PILOT_ENABLE_ANALYSIS\n          value: 'false'\n        - name: CLUSTER_ID\n          value: Kubernetes\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: PLATFORM\n          value: ''\n        resources:\n          requests:\n            cpu: 500m\n            memory: 2048Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: istio-token\n          mountPath: /var/run/secrets/tokens\n          readOnly: true\n        - name: local-certs\n          mountPath: /var/run/secrets/istio-dns\n        - name: cacerts\n          mountPath: /etc/cacerts\n          readOnly: true\n        - name: istio-kubeconfig\n          mountPath: /var/run/secrets/remote\n          readOnly: true\n        - name: istio-csr-dns-cert\n          mountPath: /var/run/secrets/istiod/tls\n          readOnly: true\n        - name: istio-csr-ca-configmap\n          mountPath: /var/run/secrets/istiod/ca\n          readOnly: true\n      volumes:\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              audience: istio-ca\n              expirationSeconds: 43200\n              path: istio-token\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n      - name: istio-csr-dns-cert\n        secret:\n          secretName: istiod-tls\n          optional: true\n      - name: istio-csr-ca-configmap\n        configMap:\n          name: istio-ca-root-cert\n          defaultMode: 420\n          optional: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"discovery\" has memory limit 0"
  },
  {
    "id": "536",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/010_service_release-name-jenkins-agent.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-jenkins-agent\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  ports:\n  - port: 50000\n    targetPort: 50000\n    name: agent-listener\n  selector:\n    app.kubernetes.io/component: jenkins-controller\n    app.kubernetes.io/instance: release-name\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:jenkins-controller app.kubernetes.io/instance:release-name])"
  },
  {
    "id": "537",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/011_service_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  ports:\n  - port: 8080\n    name: http\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/component: jenkins-controller\n    app.kubernetes.io/instance: release-name\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:jenkins-controller app.kubernetes.io/instance:release-name])"
  },
  {
    "id": "538",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable SECRETS in container \"jenkins\" found"
  },
  {
    "id": "539",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-jenkins\" not found"
  },
  {
    "id": "540",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"config-reload\" has cpu request 0"
  },
  {
    "id": "541",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"config-reload-init\" has cpu request 0"
  },
  {
    "id": "542",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"config-reload\" has memory limit 0"
  },
  {
    "id": "543",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"config-reload-init\" has memory limit 0"
  },
  {
    "id": "544",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-ui-test\" does not have a read-only root file system"
  },
  {
    "id": "545",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-framework\" does not have a read-only root file system"
  },
  {
    "id": "546",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"release-name-ui-test\" is not set to runAsNonRoot"
  },
  {
    "id": "547",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-framework\" is not set to runAsNonRoot"
  },
  {
    "id": "548",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-ui-test\" has cpu request 0"
  },
  {
    "id": "549",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-framework\" has cpu request 0"
  },
  {
    "id": "550",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-ui-test\" has memory limit 0"
  },
  {
    "id": "551",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-framework\" has memory limit 0"
  },
  {
    "id": "552",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/014_service_release-name-kong-proxy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kong-proxy\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    enable-metrics: 'true'\nspec:\n  type: ClusterIP\n  ports:\n  - name: kong-proxy-tls\n    port: 443\n    targetPort: 8443\n    protocol: TCP\n  selector:\n    app.kubernetes.io/name: kong\n    app.kubernetes.io/component: app\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:app app.kubernetes.io/instance:release-name app.kubernetes.io/name:kong])"
  },
  {
    "id": "553",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/015_service_release-name-kubernetes-dashboard-api.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n    app.kubernetes.io/version: 1.13.0\n    app.kubernetes.io/component: api\n  annotations: null\n  name: release-name-kubernetes-dashboard-api\nspec:\n  type: ClusterIP\n  ports:\n  - name: api\n    port: 8000\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kubernetes-dashboard-api app.kubernetes.io/part-of:kubernetes-dashboard])"
  },
  {
    "id": "554",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/016_service_release-name-kubernetes-dashboard-auth.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-auth\n    app.kubernetes.io/version: 1.3.0\n    app.kubernetes.io/component: auth\n  annotations: null\n  name: release-name-kubernetes-dashboard-auth\nspec:\n  type: ClusterIP\n  ports:\n  - name: auth\n    port: 8000\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-auth\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kubernetes-dashboard-auth app.kubernetes.io/part-of:kubernetes-dashboard])"
  },
  {
    "id": "555",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/017_service_release-name-kubernetes-dashboard-metrics-scraper.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n    app.kubernetes.io/version: 1.2.2\n    app.kubernetes.io/component: metrics-scraper\n  annotations: null\n  name: release-name-kubernetes-dashboard-metrics-scraper\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8000\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kubernetes-dashboard-metrics-scraper app.kubernetes.io/part-of:kubernetes-dashboard])"
  },
  {
    "id": "556",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/018_service_release-name-kubernetes-dashboard-web.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-web\n    app.kubernetes.io/version: 1.7.0\n    app.kubernetes.io/component: web\n  annotations: null\n  name: release-name-kubernetes-dashboard-web\nspec:\n  type: ClusterIP\n  ports:\n  - name: web\n    port: 8000\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-web\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kubernetes-dashboard-web app.kubernetes.io/part-of:kubernetes-dashboard])"
  },
  {
    "id": "557",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kong\" not found"
  },
  {
    "id": "558",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clear-stale-pid\" has cpu request 0"
  },
  {
    "id": "559",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"proxy\" has cpu request 0"
  },
  {
    "id": "560",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clear-stale-pid\" has memory limit 0"
  },
  {
    "id": "561",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"proxy\" has memory limit 0"
  },
  {
    "id": "562",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/020_deployment_release-name-kubernetes-dashboard-api.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n    app.kubernetes.io/version: 1.13.0\n    app.kubernetes.io/component: api\n  annotations: null\n  name: release-name-kubernetes-dashboard-api\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-api\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-api\n        app.kubernetes.io/version: 1.13.0\n        app.kubernetes.io/component: api\n      annotations:\n        checksum/config: 204c1765e58ad5edfbba4d9e2caec1985db487314063c390ec4bb1957d34bc3d\n    spec:\n      containers:\n      - name: kubernetes-dashboard-api\n        image: docker.io/kubernetesui/dashboard-api:1.13.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=default\n        - --metrics-scraper-service-name=release-name-kubernetes-dashboard-metrics-scraper\n        env:\n        - name: CSRF_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kubernetes-dashboard-csrf\n              key: private.key\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          name: api\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-api\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kubernetes-dashboard-api\" not found"
  },
  {
    "id": "563",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/022_deployment_release-name-kubernetes-dashboard-metrics-scraper.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n    app.kubernetes.io/version: 1.2.2\n    app.kubernetes.io/component: metrics-scraper\n  annotations: null\n  name: release-name-kubernetes-dashboard-metrics-scraper\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n        app.kubernetes.io/version: 1.2.2\n        app.kubernetes.io/component: metrics-scraper\n      annotations: null\n    spec:\n      containers:\n      - name: kubernetes-dashboard-metrics-scraper\n        image: docker.io/kubernetesui/dashboard-metrics-scraper:1.2.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8000\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-metrics-scraper\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kubernetes-dashboard-metrics-scraper\" not found"
  },
  {
    "id": "564",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/023_deployment_release-name-kubernetes-dashboard-web.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-web\n    app.kubernetes.io/version: 1.7.0\n    app.kubernetes.io/component: web\n  annotations: null\n  name: release-name-kubernetes-dashboard-web\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-web\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-web\n        app.kubernetes.io/version: 1.7.0\n        app.kubernetes.io/component: web\n      annotations: null\n    spec:\n      containers:\n      - name: kubernetes-dashboard-web\n        image: docker.io/kubernetesui/dashboard-web:1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=default\n        - --settings-config-map-name=release-name-kubernetes-dashboard-web-settings\n        env:\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          name: web\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-web\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kubernetes-dashboard-web\" not found"
  },
  {
    "id": "565",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/055_service_release-name-kyverno-svc.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kyverno-svc\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 443\n    targetPort: https\n    protocol: TCP\n    name: https\n    appProtocol: https\n  selector:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:admission-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "566",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/056_service_release-name-kyverno-svc-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kyverno-svc-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:admission-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "567",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/057_service_kyverno-background-controller-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-background-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:background-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "568",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/058_service_kyverno-cleanup-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-cleanup-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 443\n    targetPort: https\n    protocol: TCP\n    name: https\n    appProtocol: https\n  selector:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:cleanup-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "569",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/059_service_kyverno-cleanup-controller-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-cleanup-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:cleanup-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "570",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/060_service_kyverno-reports-controller-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-reports-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:reports-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "571",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/061_deployment_kyverno-admission-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-admission-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: admission-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: admission-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - admission-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kyverno-pre\n        image: reg.kyverno.io/kyverno/kyvernopre:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --loggingFormat=text\n        - --v=2\n        - --openreportsEnabled=false\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-admission-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:admission-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-admission-controller\n        - name: KYVERNO_SVC\n          value: release-name-kyverno-svc\n      containers:\n      - name: kyverno\n        image: reg.kyverno.io/kyverno/kyverno:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --caSecretName=release-name-kyverno-svc.default.svc.kyverno-tls-ca\n        - --tlsSecretName=release-name-kyverno-svc.default.svc.kyverno-tls-pair\n        - --backgroundServiceAccountName=system:serviceaccount:default:kyverno-background-controller\n        - --reportsServiceAccountName=system:serviceaccount:default:kyverno-reports-controller\n        - --servicePort=443\n        - --webhookServerPort=9443\n        - --resyncPeriod=15m\n        - --crdWatcher=false\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --admissionReports=true\n        - --maxAdmissionReports=1000\n        - --autoUpdateWebhooks=true\n        - --enableConfigMapCaching=true\n        - --controllerRuntimeMetricsAddress=:8080\n        - --enableDeferredLoading=true\n        - --dumpPayload=false\n        - --forceFailurePolicyIgnore=false\n        - --generateValidatingAdmissionPolicy=true\n        - --generateMutatingAdmissionPolicy=false\n        - --dumpPatches=false\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --protectManagedResources=false\n        - --allowInsecureRegistry=false\n        - --registryCredentialHelpers=default,google,amazon,azure,github\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        resources:\n          limits:\n            memory: 384Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics-port\n          protocol: TCP\n        env:\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-admission-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:admission-controller\n        - name: KYVERNO_SVC\n          value: release-name-kyverno-svc\n        - name: TUF_ROOT\n          value: /.sigstore\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-admission-controller\n        startupProbe:\n          failureThreshold: 20\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 2\n          periodSeconds: 6\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "572",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/062_deployment_kyverno-background-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-background-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: background-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: background-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - background-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-background-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/background-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --resyncPeriod=15m\n        - --enableConfigMapCaching=true\n        - --enableDeferredLoading=true\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-background-controller\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-background-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-background-controller\" not found"
  },
  {
    "id": "573",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/063_deployment_kyverno-cleanup-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-cleanup-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cleanup-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cleanup-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - cleanup-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-cleanup-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/cleanup-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --caSecretName=kyverno-cleanup-controller.default.svc.kyverno-tls-ca\n        - --tlsSecretName=kyverno-cleanup-controller.default.svc.kyverno-tls-pair\n        - --servicePort=443\n        - --cleanupServerPort=9443\n        - --webhookServerPort=9443\n        - --resyncPeriod=15m\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --enableDeferredLoading=true\n        - --dumpPayload=false\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --protectManagedResources=false\n        - --ttlReconciliationInterval=1m\n        env:\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-cleanup-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-cleanup-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:cleanup-controller\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_SVC\n          value: kyverno-cleanup-controller\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        startupProbe:\n          failureThreshold: 20\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 2\n          periodSeconds: 6\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-cleanup-controller\" not found"
  },
  {
    "id": "574",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/064_deployment_kyverno-reports-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-reports-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: reports-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: reports-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - reports-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-reports-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/reports-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --disableMetrics=false\n        - --openreportsEnabled=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --resyncPeriod=15m\n        - --admissionReports=true\n        - --aggregateReports=true\n        - --policyReports=true\n        - --validatingAdmissionPolicyReports=true\n        - --mutatingAdmissionPolicyReports=false\n        - --backgroundScan=true\n        - --backgroundScanWorkers=2\n        - --backgroundScanInterval=1h\n        - --skipResourceFilters=true\n        - --enableConfigMapCaching=true\n        - --enableDeferredLoading=true\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --allowInsecureRegistry=false\n        - --registryCredentialHelpers=default,google,amazon,azure,github\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-reports-controller\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-reports-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: TUF_ROOT\n          value: /.sigstore\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-reports-controller\" not found"
  },
  {
    "id": "575",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/073_job_release-name-kyverno-migrate-resources.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-migrate-resources\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '200'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: release-name-kyverno-migrate-resources\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: reg.kyverno.io/kyverno/kyverno-cli:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - migrate\n        - --resource\n        - cleanuppolicies.kyverno.io\n        - --resource\n        - clustercleanuppolicies.kyverno.io\n        - --resource\n        - clusterpolicies.kyverno.io\n        - --resource\n        - globalcontextentries.kyverno.io\n        - --resource\n        - policies.kyverno.io\n        - --resource\n        - policyexceptions.kyverno.io\n        - --resource\n        - updaterequests.kyverno.io\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "576",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/073_job_release-name-kyverno-migrate-resources.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-migrate-resources\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '200'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: release-name-kyverno-migrate-resources\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: reg.kyverno.io/kyverno/kyverno-cli:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - migrate\n        - --resource\n        - cleanuppolicies.kyverno.io\n        - --resource\n        - clustercleanuppolicies.kyverno.io\n        - --resource\n        - clusterpolicies.kyverno.io\n        - --resource\n        - globalcontextentries.kyverno.io\n        - --resource\n        - policies.kyverno.io\n        - --resource\n        - policyexceptions.kyverno.io\n        - --resource\n        - updaterequests.kyverno.io\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kyverno-migrate-resources\" not found"
  },
  {
    "id": "577",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/074_job_release-name-kyverno-rm-mutatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-mutatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - mutatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "578",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/074_job_release-name-kyverno-rm-mutatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-mutatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - mutatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "579",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/075_job_release-name-kyverno-rm-validatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-validatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - validatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "580",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/075_job_release-name-kyverno-rm-validatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-validatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - validatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "581",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/076_job_release-name-kyverno-scale-to-zero.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-scale-to-zero\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '90'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - scale\n        - -n\n        - default\n        - deployment\n        - -l\n        - app.kubernetes.io/part-of=release-name-kyverno\n        - --replicas=0\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "582",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/076_job_release-name-kyverno-scale-to-zero.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-scale-to-zero\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '90'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - scale\n        - -n\n        - default\n        - deployment\n        - -l\n        - app.kubernetes.io/part-of=release-name-kyverno\n        - --replicas=0\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "583",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/035_service_longhorn-backend.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-backend\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    app: longhorn-manager\n  ports:\n  - name: manager\n    port: 9500\n    targetPort: manager\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:longhorn-manager])"
  },
  {
    "id": "584",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/036_service_longhorn-frontend.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-frontend\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    app: longhorn-ui\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n    nodePort: null\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:longhorn-ui])"
  },
  {
    "id": "585",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/037_service_longhorn-admission-webhook.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-admission-webhook\n  name: longhorn-admission-webhook\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    longhorn.io/admission-webhook: longhorn-admission-webhook\n  ports:\n  - name: admission-webhook\n    port: 9502\n    targetPort: admission-wh\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[longhorn.io/admission-webhook:longhorn-admission-webhook])"
  },
  {
    "id": "586",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/038_service_longhorn-recovery-backend.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-recovery-backend\n  name: longhorn-recovery-backend\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    longhorn.io/recovery-backend: longhorn-recovery-backend\n  ports:\n  - name: recovery-backend\n    port: 9503\n    targetPort: recov-backend\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[longhorn.io/recovery-backend:longhorn-recovery-backend])"
  },
  {
    "id": "587",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-manager\" does not have a read-only root file system"
  },
  {
    "id": "588",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pre-pull-share-manager-image\" does not have a read-only root file system"
  },
  {
    "id": "589",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "590",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"longhorn-manager\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "591",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"longhorn-manager\" is privileged"
  },
  {
    "id": "592",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "593",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pre-pull-share-manager-image\" is not set to runAsNonRoot"
  },
  {
    "id": "594",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-manager\" has cpu request 0"
  },
  {
    "id": "595",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pre-pull-share-manager-image\" has cpu request 0"
  },
  {
    "id": "596",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-manager\" has memory limit 0"
  },
  {
    "id": "597",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pre-pull-share-manager-image\" has memory limit 0"
  },
  {
    "id": "598",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-driver-deployer\" does not have a read-only root file system"
  },
  {
    "id": "599",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-longhorn-manager\" does not have a read-only root file system"
  },
  {
    "id": "600",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "601",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-driver-deployer\" is not set to runAsNonRoot"
  },
  {
    "id": "602",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-longhorn-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "603",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-driver-deployer\" has cpu request 0"
  },
  {
    "id": "604",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-longhorn-manager\" has cpu request 0"
  },
  {
    "id": "605",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-driver-deployer\" has memory limit 0"
  },
  {
    "id": "606",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-longhorn-manager\" has memory limit 0"
  },
  {
    "id": "607",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/041_deployment_longhorn-ui.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-ui\" does not have a read-only root file system"
  },
  {
    "id": "608",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/041_deployment_longhorn-ui.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-ui-service-account\" not found"
  },
  {
    "id": "609",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/041_deployment_longhorn-ui.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-ui\" is not set to runAsNonRoot"
  },
  {
    "id": "610",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/041_deployment_longhorn-ui.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-ui\" has cpu request 0"
  },
  {
    "id": "611",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/041_deployment_longhorn-ui.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-ui\" has memory limit 0"
  },
  {
    "id": "612",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "613",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-post-upgrade\" does not have a read-only root file system"
  },
  {
    "id": "614",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "615",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-post-upgrade\" is not set to runAsNonRoot"
  },
  {
    "id": "616",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-post-upgrade\" has cpu request 0"
  },
  {
    "id": "617",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-post-upgrade\" has memory limit 0"
  },
  {
    "id": "618",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "619",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-pre-upgrade\" does not have a read-only root file system"
  },
  {
    "id": "620",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "621",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"longhorn-pre-upgrade\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "622",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"longhorn-pre-upgrade\" is privileged"
  },
  {
    "id": "623",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-pre-upgrade\" is not set to runAsNonRoot"
  },
  {
    "id": "624",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-pre-upgrade\" has cpu request 0"
  },
  {
    "id": "625",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-pre-upgrade\" has memory limit 0"
  },
  {
    "id": "626",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "627",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-uninstall\" does not have a read-only root file system"
  },
  {
    "id": "628",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "629",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-uninstall\" is not set to runAsNonRoot"
  },
  {
    "id": "630",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-uninstall\" has cpu request 0"
  },
  {
    "id": "631",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-uninstall\" has memory limit 0"
  },
  {
    "id": "632",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/022_service_metallb-webhook-service.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: metallb-webhook-service\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ports:\n  - port: 443\n    targetPort: 9443\n  selector:\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:release-name app.kubernetes.io/name:metallb])"
  },
  {
    "id": "633",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"frr\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "634",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"frr\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "635",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"speaker\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "636",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable METALLB_ML_SECRET_KEY_PATH in container \"speaker\" found"
  },
  {
    "id": "637",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "638",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"frr\" does not expose port 7473 for the HTTPGet"
  },
  {
    "id": "639",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cp-frr-files\" does not have a read-only root file system"
  },
  {
    "id": "640",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cp-metrics\" does not have a read-only root file system"
  },
  {
    "id": "641",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cp-reloader\" does not have a read-only root file system"
  },
  {
    "id": "642",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frr\" does not have a read-only root file system"
  },
  {
    "id": "643",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frr-metrics\" does not have a read-only root file system"
  },
  {
    "id": "644",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reloader\" does not have a read-only root file system"
  },
  {
    "id": "645",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-metallb-speaker\" not found"
  },
  {
    "id": "646",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"frr\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "647",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cp-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "648",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cp-reloader\" is not set to runAsNonRoot"
  },
  {
    "id": "649",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"frr\" is not set to runAsNonRoot"
  },
  {
    "id": "650",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"frr-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "651",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reloader\" is not set to runAsNonRoot"
  },
  {
    "id": "652",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"speaker\" is not set to runAsNonRoot"
  },
  {
    "id": "653",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "startup-port",
    "violation_text": "container \"frr\" does not expose port 7473 for the HTTPGet"
  },
  {
    "id": "654",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cp-frr-files\" has cpu request 0"
  },
  {
    "id": "655",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cp-metrics\" has cpu request 0"
  },
  {
    "id": "656",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cp-reloader\" has cpu request 0"
  },
  {
    "id": "657",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"frr\" has cpu request 0"
  },
  {
    "id": "658",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"frr-metrics\" has cpu request 0"
  }
]