[
  {
    "id": "00626",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "00627",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "00628",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "00629",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "00630",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "00631",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "00632",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rpi3firmware-manager\" is using an invalid container image, \"registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00633",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rpi3firmware-manager\" does not have a read-only root file system"
  },
  {
    "id": "00634",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rpi3firmware-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "00635",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rpi3firmware-manager\" has cpu request 0"
  },
  {
    "id": "00636",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"istio-translate-inspect\" does not have a read-only root file system"
  },
  {
    "id": "00637",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"istio-translate-inspect\" is not set to runAsNonRoot"
  },
  {
    "id": "00638",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"istio-translate-inspect\" has cpu request 0"
  },
  {
    "id": "00639",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"istio-translate-inspect\" has memory limit 0"
  },
  {
    "id": "00640",
    "manifest_path": "data/manifests/the_stack_sample/sample_0069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "00641",
    "manifest_path": "data/manifests/the_stack_sample/sample_0069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "00642",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "00643",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "00644",
    "manifest_path": "data/manifests/the_stack_sample/sample_0071.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container-aplicacao-loja\" does not have a read-only root file system"
  },
  {
    "id": "00645",
    "manifest_path": "data/manifests/the_stack_sample/sample_0071.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container-aplicacao-loja\" is not set to runAsNonRoot"
  },
  {
    "id": "00646",
    "manifest_path": "data/manifests/the_stack_sample/sample_0071.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container-aplicacao-loja\" has cpu request 0"
  },
  {
    "id": "00647",
    "manifest_path": "data/manifests/the_stack_sample/sample_0071.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container-aplicacao-loja\" has memory limit 0"
  },
  {
    "id": "00648",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mongo\" is using an invalid container image, \"mongo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00649",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "00650",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "00651",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongo\" has cpu request 0"
  },
  {
    "id": "00652",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongo\" has memory limit 0"
  },
  {
    "id": "00653",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"job-04-exec-limit-c\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00654",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job-04-exec-limit-c\" does not have a read-only root file system"
  },
  {
    "id": "00655",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job-04-exec-limit-c\" is not set to runAsNonRoot"
  },
  {
    "id": "00656",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job-04-exec-limit-c\" has cpu request 0"
  },
  {
    "id": "00657",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job-04-exec-limit-c\" has memory limit 0"
  },
  {
    "id": "00658",
    "manifest_path": "data/manifests/the_stack_sample/sample_0082.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00659",
    "manifest_path": "data/manifests/the_stack_sample/sample_0082.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00660",
    "manifest_path": "data/manifests/the_stack_sample/sample_0082.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "00661",
    "manifest_path": "data/manifests/the_stack_sample/sample_0082.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "00662",
    "manifest_path": "data/manifests/the_stack_sample/sample_0082.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "00663",
    "manifest_path": "data/manifests/the_stack_sample/sample_0084.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"order\" does not have a read-only root file system"
  },
  {
    "id": "00664",
    "manifest_path": "data/manifests/the_stack_sample/sample_0084.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"order\" is not set to runAsNonRoot"
  },
  {
    "id": "00665",
    "manifest_path": "data/manifests/the_stack_sample/sample_0084.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"order\" has cpu request 0"
  },
  {
    "id": "00666",
    "manifest_path": "data/manifests/the_stack_sample/sample_0084.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"order\" has memory limit 0"
  },
  {
    "id": "00667",
    "manifest_path": "data/manifests/the_stack_sample/sample_0085.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gateway-controller\" does not have a read-only root file system"
  },
  {
    "id": "00668",
    "manifest_path": "data/manifests/the_stack_sample/sample_0085.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gateway-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "00669",
    "manifest_path": "data/manifests/the_stack_sample/sample_0085.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gateway-controller\" has cpu request 0"
  },
  {
    "id": "00670",
    "manifest_path": "data/manifests/the_stack_sample/sample_0085.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"gateway-controller\" has memory limit 0"
  },
  {
    "id": "00671",
    "manifest_path": "data/manifests/the_stack_sample/sample_0087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"gcr.io/wise-coyote-827/frontend-server\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00672",
    "manifest_path": "data/manifests/the_stack_sample/sample_0087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "00673",
    "manifest_path": "data/manifests/the_stack_sample/sample_0087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "00674",
    "manifest_path": "data/manifests/the_stack_sample/sample_0087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "00675",
    "manifest_path": "data/manifests/the_stack_sample/sample_0087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "00676",
    "manifest_path": "data/manifests/the_stack_sample/sample_0088.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: efaf2824743f262428efac87b577e83419a9d9c1f7600f4815a09c7177935a0a\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: demonccc\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-keeper\" does not have a read-only root file system"
  },
  {
    "id": "00677",
    "manifest_path": "data/manifests/the_stack_sample/sample_0088.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: efaf2824743f262428efac87b577e83419a9d9c1f7600f4815a09c7177935a0a\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: demonccc\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-keeper\" is not set to runAsNonRoot"
  },
  {
    "id": "00678",
    "manifest_path": "data/manifests/the_stack_sample/sample_0093.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"metabase\" does not have a read-only root file system"
  },
  {
    "id": "00679",
    "manifest_path": "data/manifests/the_stack_sample/sample_0093.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"metabase\" is not set to runAsNonRoot"
  },
  {
    "id": "00680",
    "manifest_path": "data/manifests/the_stack_sample/sample_0093.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"metabase\" has cpu request 0"
  },
  {
    "id": "00681",
    "manifest_path": "data/manifests/the_stack_sample/sample_0093.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"metabase\" has memory limit 0"
  },
  {
    "id": "00682",
    "manifest_path": "data/manifests/the_stack_sample/sample_0094.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"netbox\" does not have a read-only root file system"
  },
  {
    "id": "00683",
    "manifest_path": "data/manifests/the_stack_sample/sample_0094.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"netbox\" is not set to runAsNonRoot"
  },
  {
    "id": "00684",
    "manifest_path": "data/manifests/the_stack_sample/sample_0094.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"netbox\" has cpu request 0"
  },
  {
    "id": "00685",
    "manifest_path": "data/manifests/the_stack_sample/sample_0094.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"netbox\" has memory limit 0"
  },
  {
    "id": "00686",
    "manifest_path": "data/manifests/the_stack_sample/sample_0098.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tf-mnist-1gpu\" is using an invalid container image, \"centaurusinfra/tf-onegpu-mnist:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00687",
    "manifest_path": "data/manifests/the_stack_sample/sample_0098.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tf-mnist-1gpu\" does not have a read-only root file system"
  },
  {
    "id": "00688",
    "manifest_path": "data/manifests/the_stack_sample/sample_0098.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tf-mnist-1gpu\" is not set to runAsNonRoot"
  },
  {
    "id": "00689",
    "manifest_path": "data/manifests/the_stack_sample/sample_0098.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tf-mnist-1gpu\" has cpu request 0"
  },
  {
    "id": "00690",
    "manifest_path": "data/manifests/the_stack_sample/sample_0098.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tf-mnist-1gpu\" has memory limit 0"
  },
  {
    "id": "00691",
    "manifest_path": "data/manifests/the_stack_sample/sample_0099.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "00692",
    "manifest_path": "data/manifests/the_stack_sample/sample_0099.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "00693",
    "manifest_path": "data/manifests/the_stack_sample/sample_0099.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongo\" has cpu request 0"
  },
  {
    "id": "00694",
    "manifest_path": "data/manifests/the_stack_sample/sample_0099.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongo\" has memory limit 0"
  },
  {
    "id": "00695",
    "manifest_path": "data/manifests/the_stack_sample/sample_0101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "00696",
    "manifest_path": "data/manifests/the_stack_sample/sample_0101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "00697",
    "manifest_path": "data/manifests/the_stack_sample/sample_0101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "00698",
    "manifest_path": "data/manifests/the_stack_sample/sample_0101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "00699",
    "manifest_path": "data/manifests/the_stack_sample/sample_0102.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cowweb\" does not have a read-only root file system"
  },
  {
    "id": "00700",
    "manifest_path": "data/manifests/the_stack_sample/sample_0102.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cowweb\" is not set to runAsNonRoot"
  },
  {
    "id": "00701",
    "manifest_path": "data/manifests/the_stack_sample/sample_0102.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cowweb\" has cpu request 0"
  },
  {
    "id": "00702",
    "manifest_path": "data/manifests/the_stack_sample/sample_0102.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cowweb\" has memory limit 0"
  },
  {
    "id": "00703",
    "manifest_path": "data/manifests/the_stack_sample/sample_0103.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jenkins-operator\" does not have a read-only root file system"
  },
  {
    "id": "00704",
    "manifest_path": "data/manifests/the_stack_sample/sample_0103.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jenkins-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "00705",
    "manifest_path": "data/manifests/the_stack_sample/sample_0103.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jenkins-operator\" has cpu request 0"
  },
  {
    "id": "00706",
    "manifest_path": "data/manifests/the_stack_sample/sample_0103.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jenkins-operator\" has memory limit 0"
  },
  {
    "id": "00707",
    "manifest_path": "data/manifests/the_stack_sample/sample_0104.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: libri-experimenter\nspec:\n  volumes:\n  - name: data\n    emptyDir: {}\n  containers:\n  - name: libri-experimenter\n    image: daedalus2718/libri-exp:snapshot-949fb64\n    args:\n    - run\n    - --librarians\n    - librarians-0.libri.default.svc.cluster.local:20100,librarians-1.libri.default.svc.cluster.local:20100,librarians-2.libri.default.svc.cluster.local:20100,librarians-3.libri.default.svc.cluster.local:20100,librarians-4.libri.default.svc.cluster.local:20100,librarians-5.libri.default.svc.cluster.local:20100,librarians-6.libri.default.svc.cluster.local:20100,librarians-7.libri.default.svc.cluster.local:20100\n    - --duration\n    - 30m\n    - --numAuthors\n    - '100'\n    - --docsPerDay\n    - '2560'\n    - --contentSizeKBGammaShape\n    - '1.5'\n    - --contentSizeKBGammaRate\n    - '0.00588'\n    - --sharesPerUpload\n    - '2'\n    - --nUploaders\n    - '64'\n    - --nDownloaders\n    - '192'\n    - --profile\n    env:\n    - name: GODEBUG\n      value: netdns=go\n    volumeMounts:\n    - name: data\n      mountPath: /data\n    resources:\n      limits:\n        memory: 2G\n        cpu: 400m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"libri-experimenter\" does not have a read-only root file system"
  },
  {
    "id": "00708",
    "manifest_path": "data/manifests/the_stack_sample/sample_0104.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: libri-experimenter\nspec:\n  volumes:\n  - name: data\n    emptyDir: {}\n  containers:\n  - name: libri-experimenter\n    image: daedalus2718/libri-exp:snapshot-949fb64\n    args:\n    - run\n    - --librarians\n    - librarians-0.libri.default.svc.cluster.local:20100,librarians-1.libri.default.svc.cluster.local:20100,librarians-2.libri.default.svc.cluster.local:20100,librarians-3.libri.default.svc.cluster.local:20100,librarians-4.libri.default.svc.cluster.local:20100,librarians-5.libri.default.svc.cluster.local:20100,librarians-6.libri.default.svc.cluster.local:20100,librarians-7.libri.default.svc.cluster.local:20100\n    - --duration\n    - 30m\n    - --numAuthors\n    - '100'\n    - --docsPerDay\n    - '2560'\n    - --contentSizeKBGammaShape\n    - '1.5'\n    - --contentSizeKBGammaRate\n    - '0.00588'\n    - --sharesPerUpload\n    - '2'\n    - --nUploaders\n    - '64'\n    - --nDownloaders\n    - '192'\n    - --profile\n    env:\n    - name: GODEBUG\n      value: netdns=go\n    volumeMounts:\n    - name: data\n      mountPath: /data\n    resources:\n      limits:\n        memory: 2G\n        cpu: 400m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"libri-experimenter\" is not set to runAsNonRoot"
  },
  {
    "id": "00709",
    "manifest_path": "data/manifests/the_stack_sample/sample_0104.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: libri-experimenter\nspec:\n  volumes:\n  - name: data\n    emptyDir: {}\n  containers:\n  - name: libri-experimenter\n    image: daedalus2718/libri-exp:snapshot-949fb64\n    args:\n    - run\n    - --librarians\n    - librarians-0.libri.default.svc.cluster.local:20100,librarians-1.libri.default.svc.cluster.local:20100,librarians-2.libri.default.svc.cluster.local:20100,librarians-3.libri.default.svc.cluster.local:20100,librarians-4.libri.default.svc.cluster.local:20100,librarians-5.libri.default.svc.cluster.local:20100,librarians-6.libri.default.svc.cluster.local:20100,librarians-7.libri.default.svc.cluster.local:20100\n    - --duration\n    - 30m\n    - --numAuthors\n    - '100'\n    - --docsPerDay\n    - '2560'\n    - --contentSizeKBGammaShape\n    - '1.5'\n    - --contentSizeKBGammaRate\n    - '0.00588'\n    - --sharesPerUpload\n    - '2'\n    - --nUploaders\n    - '64'\n    - --nDownloaders\n    - '192'\n    - --profile\n    env:\n    - name: GODEBUG\n      value: netdns=go\n    volumeMounts:\n    - name: data\n      mountPath: /data\n    resources:\n      limits:\n        memory: 2G\n        cpu: 400m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"libri-experimenter\" has cpu request 0"
  },
  {
    "id": "00710",
    "manifest_path": "data/manifests/the_stack_sample/sample_0105.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgresql-client\" does not have a read-only root file system"
  },
  {
    "id": "00711",
    "manifest_path": "data/manifests/the_stack_sample/sample_0105.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgresql-client\" is not set to runAsNonRoot"
  },
  {
    "id": "00712",
    "manifest_path": "data/manifests/the_stack_sample/sample_0105.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgresql-client\" has cpu request 0"
  },
  {
    "id": "00713",
    "manifest_path": "data/manifests/the_stack_sample/sample_0105.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgresql-client\" has memory limit 0"
  },
  {
    "id": "00714",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "00715",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "00716",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "00717",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "00718",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "00719",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "00720",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "00721",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "00722",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "00723",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "00724",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - /bin/bash\n          - -c\n          - 'set -u\n\n            set -e\n\n            set -x\n\n\n            export PATH=$PATH:/root/google-cloud-sdk/bin\n\n            gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n            cd tf2-api-tests\n\n            pip3 install behave\n\n            behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n            '\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"alert_for_failed_jobs\\\": true\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "00725",
    "manifest_path": "data/manifests/the_stack_sample/sample_0107.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"skipper-demo\" does not have a read-only root file system"
  },
  {
    "id": "00726",
    "manifest_path": "data/manifests/the_stack_sample/sample_0107.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"skipper-demo\" is not set to runAsNonRoot"
  },
  {
    "id": "00727",
    "manifest_path": "data/manifests/the_stack_sample/sample_0107.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"skipper-demo\" has cpu request 0"
  },
  {
    "id": "00728",
    "manifest_path": "data/manifests/the_stack_sample/sample_0107.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"skipper-demo\" has memory limit 0"
  },
  {
    "id": "00729",
    "manifest_path": "data/manifests/the_stack_sample/sample_0109.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00730",
    "manifest_path": "data/manifests/the_stack_sample/sample_0109.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00731",
    "manifest_path": "data/manifests/the_stack_sample/sample_0109.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "00732",
    "manifest_path": "data/manifests/the_stack_sample/sample_0109.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "00733",
    "manifest_path": "data/manifests/the_stack_sample/sample_0109.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "00734",
    "manifest_path": "data/manifests/the_stack_sample/sample_0110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00735",
    "manifest_path": "data/manifests/the_stack_sample/sample_0110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00736",
    "manifest_path": "data/manifests/the_stack_sample/sample_0110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "00737",
    "manifest_path": "data/manifests/the_stack_sample/sample_0110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "00738",
    "manifest_path": "data/manifests/the_stack_sample/sample_0110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "00739",
    "manifest_path": "data/manifests/the_stack_sample/sample_0113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prow-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "00740",
    "manifest_path": "data/manifests/the_stack_sample/sample_0113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prow-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "00741",
    "manifest_path": "data/manifests/the_stack_sample/sample_0113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prow-controller-manager\" has cpu request 0"
  },
  {
    "id": "00742",
    "manifest_path": "data/manifests/the_stack_sample/sample_0113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prow-controller-manager\" has memory limit 0"
  },
  {
    "id": "00743",
    "manifest_path": "data/manifests/the_stack_sample/sample_0116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "00744",
    "manifest_path": "data/manifests/the_stack_sample/sample_0116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "00745",
    "manifest_path": "data/manifests/the_stack_sample/sample_0116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "00746",
    "manifest_path": "data/manifests/the_stack_sample/sample_0116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "00747",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sotr-project\" is using an invalid container image, \"wsosnowski2/sotr\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00748",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sotr-project\" does not have a read-only root file system"
  },
  {
    "id": "00749",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sotr-project\" is not set to runAsNonRoot"
  },
  {
    "id": "00750",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sotr-project\" has cpu request 0"
  },
  {
    "id": "00751",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sotr-project\" has memory limit 0"
  },
  {
    "id": "00752",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"akstest-3225\" is using an invalid container image, \"aaaatiwarishubregistry.azurecr.io/akstest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00753",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"akstest-3225\" does not have a read-only root file system"
  },
  {
    "id": "00754",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"akstest-3225\" is not set to runAsNonRoot"
  },
  {
    "id": "00755",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"akstest-3225\" has cpu request 0"
  },
  {
    "id": "00756",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"akstest-3225\" has memory limit 0"
  },
  {
    "id": "00757",
    "manifest_path": "data/manifests/the_stack_sample/sample_0128.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    app: hello\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello\n        image: gcr.io/google-samples/hello-app:1.0\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        resources:\n          requests:\n            memory: 61Mi\n            cpu: 200m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "00758",
    "manifest_path": "data/manifests/the_stack_sample/sample_0128.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    app: hello\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello\n        image: gcr.io/google-samples/hello-app:1.0\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        resources:\n          requests:\n            memory: 61Mi\n            cpu: 200m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "00759",
    "manifest_path": "data/manifests/the_stack_sample/sample_0131.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.1.41\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: cca6b2f42d7393045e9e749df2be9e7f342f64b4c723a82c3c7c4d0cb91793f6\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 5e6bccde9c3bac58bf04598ba5c0d440ede72b84637d58a98043893d7122417d\n    spec:\n      serviceAccountName: lighthouse-webhooks\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.1.41\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: OHHYUN\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.41\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 974b52c387d1fabee626da3b78536faf4c7848e9\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-webhooks\" does not have a read-only root file system"
  },
  {
    "id": "00760",
    "manifest_path": "data/manifests/the_stack_sample/sample_0131.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.1.41\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: cca6b2f42d7393045e9e749df2be9e7f342f64b4c723a82c3c7c4d0cb91793f6\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 5e6bccde9c3bac58bf04598ba5c0d440ede72b84637d58a98043893d7122417d\n    spec:\n      serviceAccountName: lighthouse-webhooks\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.1.41\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: OHHYUN\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.41\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 974b52c387d1fabee626da3b78536faf4c7848e9\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-webhooks\" is not set to runAsNonRoot"
  },
  {
    "id": "00761",
    "manifest_path": "data/manifests/the_stack_sample/sample_0132.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hive-controllers\n  namespace: hive\n  labels:\n    control-plane: controller-manager\n    controller-tools.k8s.io: '1.0'\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n      controller-tools.k8s.io: '1.0'\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n        controller-tools.k8s.io: '1.0'\n    spec:\n      serviceAccountName: default\n      volumes:\n      - name: kubectl-cache\n        emptyDir: {}\n      containers:\n      - image: registry.svc.ci.openshift.org/openshift/hive-v4.0:hive\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          requests:\n            cpu: 50m\n            memory: 512Mi\n        command:\n        - /opt/services/manager\n        volumeMounts:\n        - name: kubectl-cache\n          mountPath: /var/cache/kubectl\n        env:\n        - name: CLI_CACHE_DIR\n          value: /var/cache/kubectl\n        livenessProbe:\n          httpGet:\n            path: /debug/health\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "00762",
    "manifest_path": "data/manifests/the_stack_sample/sample_0132.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hive-controllers\n  namespace: hive\n  labels:\n    control-plane: controller-manager\n    controller-tools.k8s.io: '1.0'\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n      controller-tools.k8s.io: '1.0'\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n        controller-tools.k8s.io: '1.0'\n    spec:\n      serviceAccountName: default\n      volumes:\n      - name: kubectl-cache\n        emptyDir: {}\n      containers:\n      - image: registry.svc.ci.openshift.org/openshift/hive-v4.0:hive\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          requests:\n            cpu: 50m\n            memory: 512Mi\n        command:\n        - /opt/services/manager\n        volumeMounts:\n        - name: kubectl-cache\n          mountPath: /var/cache/kubectl\n        env:\n        - name: CLI_CACHE_DIR\n          value: /var/cache/kubectl\n        livenessProbe:\n          httpGet:\n            path: /debug/health\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "00763",
    "manifest_path": "data/manifests/the_stack_sample/sample_0132.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hive-controllers\n  namespace: hive\n  labels:\n    control-plane: controller-manager\n    controller-tools.k8s.io: '1.0'\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n      controller-tools.k8s.io: '1.0'\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n        controller-tools.k8s.io: '1.0'\n    spec:\n      serviceAccountName: default\n      volumes:\n      - name: kubectl-cache\n        emptyDir: {}\n      containers:\n      - image: registry.svc.ci.openshift.org/openshift/hive-v4.0:hive\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          requests:\n            cpu: 50m\n            memory: 512Mi\n        command:\n        - /opt/services/manager\n        volumeMounts:\n        - name: kubectl-cache\n          mountPath: /var/cache/kubectl\n        env:\n        - name: CLI_CACHE_DIR\n          value: /var/cache/kubectl\n        livenessProbe:\n          httpGet:\n            path: /debug/health\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 10\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"manager\" has memory limit 0"
  },
  {
    "id": "00764",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"api-database\" is using an invalid container image, \"postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00765",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pgsql-data-permission-fix\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00766",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api-database\" does not have a read-only root file system"
  },
  {
    "id": "00767",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pgsql-data-permission-fix\" does not have a read-only root file system"
  },
  {
    "id": "00768",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"api-database\" is not set to runAsNonRoot"
  },
  {
    "id": "00769",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pgsql-data-permission-fix\" is not set to runAsNonRoot"
  },
  {
    "id": "00770",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api-database\" has cpu request 0"
  },
  {
    "id": "00771",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pgsql-data-permission-fix\" has cpu request 0"
  },
  {
    "id": "00772",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api-database\" has memory limit 0"
  },
  {
    "id": "00773",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pgsql-data-permission-fix\" has memory limit 0"
  },
  {
    "id": "00774",
    "manifest_path": "data/manifests/the_stack_sample/sample_0139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 60461a69ffa09bc8e76a211946cab1220740169ece13144fe59417d2d20f18ab\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard.jx.change.me\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 9e58caa93ba0a07182b4512285c1b1d01a279995\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      affinity: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-tekton-controller\" does not have a read-only root file system"
  },
  {
    "id": "00775",
    "manifest_path": "data/manifests/the_stack_sample/sample_0139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 60461a69ffa09bc8e76a211946cab1220740169ece13144fe59417d2d20f18ab\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard.jx.change.me\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 9e58caa93ba0a07182b4512285c1b1d01a279995\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      affinity: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-tekton-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "00776",
    "manifest_path": "data/manifests/the_stack_sample/sample_0140.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sinker\" does not have a read-only root file system"
  },
  {
    "id": "00777",
    "manifest_path": "data/manifests/the_stack_sample/sample_0140.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sinker\" is not set to runAsNonRoot"
  },
  {
    "id": "00778",
    "manifest_path": "data/manifests/the_stack_sample/sample_0140.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sinker\" has cpu request 0"
  },
  {
    "id": "00779",
    "manifest_path": "data/manifests/the_stack_sample/sample_0140.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sinker\" has memory limit 0"
  },
  {
    "id": "00780",
    "manifest_path": "data/manifests/the_stack_sample/sample_0145.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "00781",
    "manifest_path": "data/manifests/the_stack_sample/sample_0145.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "00782",
    "manifest_path": "data/manifests/the_stack_sample/sample_0145.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "00783",
    "manifest_path": "data/manifests/the_stack_sample/sample_0145.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "00784",
    "manifest_path": "data/manifests/the_stack_sample/sample_0153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "00785",
    "manifest_path": "data/manifests/the_stack_sample/sample_0153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "00786",
    "manifest_path": "data/manifests/the_stack_sample/sample_0153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "00787",
    "manifest_path": "data/manifests/the_stack_sample/sample_0153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "00788",
    "manifest_path": "data/manifests/the_stack_sample/sample_0154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00789",
    "manifest_path": "data/manifests/the_stack_sample/sample_0154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "00790",
    "manifest_path": "data/manifests/the_stack_sample/sample_0154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "00791",
    "manifest_path": "data/manifests/the_stack_sample/sample_0154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "00792",
    "manifest_path": "data/manifests/the_stack_sample/sample_0154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "00793",
    "manifest_path": "data/manifests/the_stack_sample/sample_0155.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "00794",
    "manifest_path": "data/manifests/the_stack_sample/sample_0155.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "00795",
    "manifest_path": "data/manifests/the_stack_sample/sample_0155.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "00796",
    "manifest_path": "data/manifests/the_stack_sample/sample_0155.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "00797",
    "manifest_path": "data/manifests/the_stack_sample/sample_0156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00798",
    "manifest_path": "data/manifests/the_stack_sample/sample_0156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00799",
    "manifest_path": "data/manifests/the_stack_sample/sample_0156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "00800",
    "manifest_path": "data/manifests/the_stack_sample/sample_0156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "00801",
    "manifest_path": "data/manifests/the_stack_sample/sample_0156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "00802",
    "manifest_path": "data/manifests/the_stack_sample/sample_0157.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deis-slugrunner\" does not have a read-only root file system"
  },
  {
    "id": "00803",
    "manifest_path": "data/manifests/the_stack_sample/sample_0157.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deis-slugrunner\" is not set to runAsNonRoot"
  },
  {
    "id": "00804",
    "manifest_path": "data/manifests/the_stack_sample/sample_0157.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deis-slugrunner\" has cpu request 0"
  },
  {
    "id": "00805",
    "manifest_path": "data/manifests/the_stack_sample/sample_0157.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deis-slugrunner\" has memory limit 0"
  },
  {
    "id": "00806",
    "manifest_path": "data/manifests/the_stack_sample/sample_0158.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"go-server\" does not have a read-only root file system"
  },
  {
    "id": "00807",
    "manifest_path": "data/manifests/the_stack_sample/sample_0158.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"go-server\" is not set to runAsNonRoot"
  },
  {
    "id": "00808",
    "manifest_path": "data/manifests/the_stack_sample/sample_0158.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"go-server\" has cpu request 0"
  },
  {
    "id": "00809",
    "manifest_path": "data/manifests/the_stack_sample/sample_0158.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"go-server\" has memory limit 0"
  },
  {
    "id": "00810",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"vamp-cli-ee\" is using an invalid container image, \"vlesierse/vamp-cli-ee:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00811",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"docker\" does not have a read-only root file system"
  },
  {
    "id": "00812",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vamp-cli-ee\" does not have a read-only root file system"
  },
  {
    "id": "00813",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"docker\" is not set to runAsNonRoot"
  },
  {
    "id": "00814",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vamp-cli-ee\" is not set to runAsNonRoot"
  },
  {
    "id": "00815",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"docker\" has cpu request 0"
  },
  {
    "id": "00816",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vamp-cli-ee\" has cpu request 0"
  },
  {
    "id": "00817",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"docker\" has memory limit 0"
  },
  {
    "id": "00818",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vamp-cli-ee\" has memory limit 0"
  },
  {
    "id": "00819",
    "manifest_path": "data/manifests/the_stack_sample/sample_0163.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 96b3634b43df2329d5c73888957b70798af1e33b9bc37b02d5c340ae6962f9cd\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: Sun-ifly\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-keeper\" does not have a read-only root file system"
  },
  {
    "id": "00820",
    "manifest_path": "data/manifests/the_stack_sample/sample_0163.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 96b3634b43df2329d5c73888957b70798af1e33b9bc37b02d5c340ae6962f9cd\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: Sun-ifly\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-keeper\" is not set to runAsNonRoot"
  },
  {
    "id": "00821",
    "manifest_path": "data/manifests/the_stack_sample/sample_0164.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"flask\" does not have a read-only root file system"
  },
  {
    "id": "00822",
    "manifest_path": "data/manifests/the_stack_sample/sample_0164.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"flask\" is not set to runAsNonRoot"
  },
  {
    "id": "00823",
    "manifest_path": "data/manifests/the_stack_sample/sample_0164.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"flask\" has cpu request 0"
  },
  {
    "id": "00824",
    "manifest_path": "data/manifests/the_stack_sample/sample_0164.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"flask\" has memory limit 0"
  },
  {
    "id": "00825",
    "manifest_path": "data/manifests/the_stack_sample/sample_0170.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: mycron-container\n          image: alpine\n          imagePullPolicy: IfNotPresent\n          command:\n          - sh\n          - -c\n          - echo elastic world ; sleep 5\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mycron-container\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00826",
    "manifest_path": "data/manifests/the_stack_sample/sample_0170.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: mycron-container\n          image: alpine\n          imagePullPolicy: IfNotPresent\n          command:\n          - sh\n          - -c\n          - echo elastic world ; sleep 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mycron-container\" does not have a read-only root file system"
  },
  {
    "id": "00827",
    "manifest_path": "data/manifests/the_stack_sample/sample_0170.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: mycron-container\n          image: alpine\n          imagePullPolicy: IfNotPresent\n          command:\n          - sh\n          - -c\n          - echo elastic world ; sleep 5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mycron-container\" is not set to runAsNonRoot"
  },
  {
    "id": "00828",
    "manifest_path": "data/manifests/the_stack_sample/sample_0170.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: mycron-container\n          image: alpine\n          imagePullPolicy: IfNotPresent\n          command:\n          - sh\n          - -c\n          - echo elastic world ; sleep 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mycron-container\" has cpu request 0"
  },
  {
    "id": "00829",
    "manifest_path": "data/manifests/the_stack_sample/sample_0170.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: mycron-container\n          image: alpine\n          imagePullPolicy: IfNotPresent\n          command:\n          - sh\n          - -c\n          - echo elastic world ; sleep 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mycron-container\" has memory limit 0"
  },
  {
    "id": "00830",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ecsdemo-nodejs\" is using an invalid container image, \"brentley/ecsdemo-nodejs:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00831",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ecsdemo-nodejs\" does not have a read-only root file system"
  },
  {
    "id": "00832",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ecsdemo-nodejs\" is not set to runAsNonRoot"
  },
  {
    "id": "00833",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ecsdemo-nodejs\" has cpu request 0"
  },
  {
    "id": "00834",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ecsdemo-nodejs\" has memory limit 0"
  },
  {
    "id": "00835",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"zookeeper\" is using an invalid container image, \"zcguan/storm-cluster:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00836",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"zookeeper\" does not have a read-only root file system"
  },
  {
    "id": "00837",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"zookeeper\" is not set to runAsNonRoot"
  },
  {
    "id": "00838",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"zookeeper\" has cpu request 0"
  },
  {
    "id": "00839",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"zookeeper\" has memory limit 0"
  },
  {
    "id": "00840",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"git\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00841",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"trigger\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00842",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"git\" does not have a read-only root file system"
  },
  {
    "id": "00843",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"trigger\" does not have a read-only root file system"
  },
  {
    "id": "00844",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"git\" is not set to runAsNonRoot"
  },
  {
    "id": "00845",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"trigger\" is not set to runAsNonRoot"
  },
  {
    "id": "00846",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"git\" has cpu request 0"
  },
  {
    "id": "00847",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"trigger\" has cpu request 0"
  },
  {
    "id": "00848",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"git\" has memory limit 0"
  },
  {
    "id": "00849",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"trigger\" has memory limit 0"
  },
  {
    "id": "00850",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"k8s-demo\" is using an invalid container image, \"diyblockchain/k8s-demo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00851",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"k8s-demo\" does not have a read-only root file system"
  },
  {
    "id": "00852",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"k8s-demo\" is not set to runAsNonRoot"
  },
  {
    "id": "00853",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"k8s-demo\" has cpu request 0"
  },
  {
    "id": "00854",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"k8s-demo\" has memory limit 0"
  },
  {
    "id": "00855",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00856",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00857",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "00858",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "00859",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "00860",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"image-seg\" is using an invalid container image, \"centaurusinfra/test1_maskrcnn\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00861",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"image-seg\" does not have a read-only root file system"
  },
  {
    "id": "00862",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"image-seg\" is not set to runAsNonRoot"
  },
  {
    "id": "00863",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"image-seg\" has cpu request 0"
  },
  {
    "id": "00864",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"image-seg\" has memory limit 0"
  },
  {
    "id": "00865",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00866",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00867",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "00868",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "00869",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "00870",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00871",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00872",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "00873",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "00874",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "00875",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "00876",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "00877",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "00878",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sabnzbd\" does not have a read-only root file system"
  },
  {
    "id": "00879",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sabnzbd\" is not set to runAsNonRoot"
  },
  {
    "id": "00880",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sabnzbd\" has cpu request 0"
  },
  {
    "id": "00881",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sabnzbd\" has memory limit 0"
  },
  {
    "id": "00882",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"myapp-container\" is using an invalid container image, \"cewuandy/nextepc-base\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00883",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-network-client\" does not have a read-only root file system"
  },
  {
    "id": "00884",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"myapp-container\" does not have a read-only root file system"
  },
  {
    "id": "00885",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-network-client\" is not set to runAsNonRoot"
  },
  {
    "id": "00886",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"myapp-container\" is not set to runAsNonRoot"
  },
  {
    "id": "00887",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-network-client\" has cpu request 0"
  },
  {
    "id": "00888",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"myapp-container\" has cpu request 0"
  },
  {
    "id": "00889",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-network-client\" has memory limit 0"
  },
  {
    "id": "00890",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"myapp-container\" has memory limit 0"
  },
  {
    "id": "00891",
    "manifest_path": "data/manifests/the_stack_sample/sample_0202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20200710-7fa016752a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "00892",
    "manifest_path": "data/manifests/the_stack_sample/sample_0202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20200710-7fa016752a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "00893",
    "manifest_path": "data/manifests/the_stack_sample/sample_0202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20200710-7fa016752a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "00894",
    "manifest_path": "data/manifests/the_stack_sample/sample_0202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20200710-7fa016752a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "00895",
    "manifest_path": "data/manifests/the_stack_sample/sample_0204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-server-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-server\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-server\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-server\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-server\n        image: zhangrong1027/linkis:linkis-mdm-server-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22001\n        livenessProbe:\n          tcpSocket:\n            port: 22001\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22001'\n        volumeMounts:\n        - name: linkis-mdm-server-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-server/logs\n      volumes:\n      - name: linkis-mdm-server-config\n        configMap:\n          name: linkis-mdm-server-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"linkis-mdm-server\" does not have a read-only root file system"
  },
  {
    "id": "00896",
    "manifest_path": "data/manifests/the_stack_sample/sample_0204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-server-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-server\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-server\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-server\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-server\n        image: zhangrong1027/linkis:linkis-mdm-server-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22001\n        livenessProbe:\n          tcpSocket:\n            port: 22001\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22001'\n        volumeMounts:\n        - name: linkis-mdm-server-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-server/logs\n      volumes:\n      - name: linkis-mdm-server-config\n        configMap:\n          name: linkis-mdm-server-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"linkis-mdm-server\" is not set to runAsNonRoot"
  },
  {
    "id": "00897",
    "manifest_path": "data/manifests/the_stack_sample/sample_0204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-server-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-server\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-server\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-server\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-server\n        image: zhangrong1027/linkis:linkis-mdm-server-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22001\n        livenessProbe:\n          tcpSocket:\n            port: 22001\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22001'\n        volumeMounts:\n        - name: linkis-mdm-server-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-server/logs\n      volumes:\n      - name: linkis-mdm-server-config\n        configMap:\n          name: linkis-mdm-server-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"linkis-mdm-server\" has cpu request 0"
  },
  {
    "id": "00898",
    "manifest_path": "data/manifests/the_stack_sample/sample_0204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-server-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-server\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-server\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-server\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-server\n        image: zhangrong1027/linkis:linkis-mdm-server-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22001\n        livenessProbe:\n          tcpSocket:\n            port: 22001\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22001'\n        volumeMounts:\n        - name: linkis-mdm-server-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-server/logs\n      volumes:\n      - name: linkis-mdm-server-config\n        configMap:\n          name: linkis-mdm-server-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"linkis-mdm-server\" has memory limit 0"
  },
  {
    "id": "00899",
    "manifest_path": "data/manifests/the_stack_sample/sample_0207.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flog\n  labels:\n    app: flog\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flog\n  template:\n    metadata:\n      labels:\n        app: flog\n    spec:\n      containers:\n      - name: flog-load\n        image: scalyr/flog:v1\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 300m\n        command:\n        - /bin/flog\n        args:\n        - --loop\n        - --format\n        - apache_combined\n        - -m\n        - ''\n        - -r\n        - /tmp/flog_telemetry.log\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"flog-load\" does not have a read-only root file system"
  },
  {
    "id": "00900",
    "manifest_path": "data/manifests/the_stack_sample/sample_0207.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flog\n  labels:\n    app: flog\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flog\n  template:\n    metadata:\n      labels:\n        app: flog\n    spec:\n      containers:\n      - name: flog-load\n        image: scalyr/flog:v1\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 300m\n        command:\n        - /bin/flog\n        args:\n        - --loop\n        - --format\n        - apache_combined\n        - -m\n        - ''\n        - -r\n        - /tmp/flog_telemetry.log\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"flog-load\" is not set to runAsNonRoot"
  },
  {
    "id": "00901",
    "manifest_path": "data/manifests/the_stack_sample/sample_0208.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nfs-provisioner\nspec:\n  containers:\n  - name: nfs-provisioner\n    image: quay.io/kubernetes_incubator/nfs-provisioner:v1.0.5\n    ports:\n    - name: nfs\n      containerPort: 2049\n    - name: mountd\n      containerPort: 20048\n    - name: rpcbind\n      containerPort: 111\n    - name: rpcbind-udp\n      containerPort: 111\n      protocol: UDP\n    securityContext:\n      capabilities:\n        add:\n        - DAC_READ_SEARCH\n    args:\n    - -provisioner=example.com/nfs\n    - -grace-period=0\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    imagePullPolicy: IfNotPresent\n    volumeMounts:\n    - name: export-volume\n      mountPath: /export\n  volumes:\n  - name: export-volume\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nfs-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "00902",
    "manifest_path": "data/manifests/the_stack_sample/sample_0208.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nfs-provisioner\nspec:\n  containers:\n  - name: nfs-provisioner\n    image: quay.io/kubernetes_incubator/nfs-provisioner:v1.0.5\n    ports:\n    - name: nfs\n      containerPort: 2049\n    - name: mountd\n      containerPort: 20048\n    - name: rpcbind\n      containerPort: 111\n    - name: rpcbind-udp\n      containerPort: 111\n      protocol: UDP\n    securityContext:\n      capabilities:\n        add:\n        - DAC_READ_SEARCH\n    args:\n    - -provisioner=example.com/nfs\n    - -grace-period=0\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    imagePullPolicy: IfNotPresent\n    volumeMounts:\n    - name: export-volume\n      mountPath: /export\n  volumes:\n  - name: export-volume\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nfs-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "00903",
    "manifest_path": "data/manifests/the_stack_sample/sample_0208.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nfs-provisioner\nspec:\n  containers:\n  - name: nfs-provisioner\n    image: quay.io/kubernetes_incubator/nfs-provisioner:v1.0.5\n    ports:\n    - name: nfs\n      containerPort: 2049\n    - name: mountd\n      containerPort: 20048\n    - name: rpcbind\n      containerPort: 111\n    - name: rpcbind-udp\n      containerPort: 111\n      protocol: UDP\n    securityContext:\n      capabilities:\n        add:\n        - DAC_READ_SEARCH\n    args:\n    - -provisioner=example.com/nfs\n    - -grace-period=0\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    imagePullPolicy: IfNotPresent\n    volumeMounts:\n    - name: export-volume\n      mountPath: /export\n  volumes:\n  - name: export-volume\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nfs-provisioner\" has cpu request 0"
  },
  {
    "id": "00904",
    "manifest_path": "data/manifests/the_stack_sample/sample_0208.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nfs-provisioner\nspec:\n  containers:\n  - name: nfs-provisioner\n    image: quay.io/kubernetes_incubator/nfs-provisioner:v1.0.5\n    ports:\n    - name: nfs\n      containerPort: 2049\n    - name: mountd\n      containerPort: 20048\n    - name: rpcbind\n      containerPort: 111\n    - name: rpcbind-udp\n      containerPort: 111\n      protocol: UDP\n    securityContext:\n      capabilities:\n        add:\n        - DAC_READ_SEARCH\n    args:\n    - -provisioner=example.com/nfs\n    - -grace-period=0\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    imagePullPolicy: IfNotPresent\n    volumeMounts:\n    - name: export-volume\n      mountPath: /export\n  volumes:\n  - name: export-volume\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nfs-provisioner\" has memory limit 0"
  },
  {
    "id": "00905",
    "manifest_path": "data/manifests/the_stack_sample/sample_0210.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00906",
    "manifest_path": "data/manifests/the_stack_sample/sample_0210.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00907",
    "manifest_path": "data/manifests/the_stack_sample/sample_0210.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "00908",
    "manifest_path": "data/manifests/the_stack_sample/sample_0210.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "00909",
    "manifest_path": "data/manifests/the_stack_sample/sample_0210.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "00910",
    "manifest_path": "data/manifests/the_stack_sample/sample_0212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook-server\n  labels:\n    app: webhook-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook-server\n  template:\n    metadata:\n      labels:\n        app: webhook-server\n    spec:\n      containers:\n      - name: server\n        image: quay.io/masood_faisal/webhooks:0.0.1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /etc/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "00911",
    "manifest_path": "data/manifests/the_stack_sample/sample_0212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook-server\n  labels:\n    app: webhook-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook-server\n  template:\n    metadata:\n      labels:\n        app: webhook-server\n    spec:\n      containers:\n      - name: server\n        image: quay.io/masood_faisal/webhooks:0.0.1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /etc/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "00912",
    "manifest_path": "data/manifests/the_stack_sample/sample_0212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook-server\n  labels:\n    app: webhook-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook-server\n  template:\n    metadata:\n      labels:\n        app: webhook-server\n    spec:\n      containers:\n      - name: server\n        image: quay.io/masood_faisal/webhooks:0.0.1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /etc/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "00913",
    "manifest_path": "data/manifests/the_stack_sample/sample_0212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook-server\n  labels:\n    app: webhook-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook-server\n  template:\n    metadata:\n      labels:\n        app: webhook-server\n    spec:\n      containers:\n      - name: server\n        image: quay.io/masood_faisal/webhooks:0.0.1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /etc/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "00914",
    "manifest_path": "data/manifests/the_stack_sample/sample_0214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: get-started-node\n  labels:\n    app: get-started-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: get-started-node\n  template:\n    metadata:\n      labels:\n        app: get-started-node\n    spec:\n      containers:\n      - name: get-started-node\n        image: <REGISTRY>/<NAMESPACE>/myapp:v1.1.0\n        ports:\n        - containerPort: 8080\n        imagePullPolicy: Always\n        env:\n        - name: CLOUDANT_URL\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: url\n              optional: true\n        - name: CLOUDANT_IAM_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: iamApiKey\n              optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"get-started-node\" does not have a read-only root file system"
  },
  {
    "id": "00915",
    "manifest_path": "data/manifests/the_stack_sample/sample_0214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: get-started-node\n  labels:\n    app: get-started-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: get-started-node\n  template:\n    metadata:\n      labels:\n        app: get-started-node\n    spec:\n      containers:\n      - name: get-started-node\n        image: <REGISTRY>/<NAMESPACE>/myapp:v1.1.0\n        ports:\n        - containerPort: 8080\n        imagePullPolicy: Always\n        env:\n        - name: CLOUDANT_URL\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: url\n              optional: true\n        - name: CLOUDANT_IAM_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: iamApiKey\n              optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"get-started-node\" is not set to runAsNonRoot"
  },
  {
    "id": "00916",
    "manifest_path": "data/manifests/the_stack_sample/sample_0214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: get-started-node\n  labels:\n    app: get-started-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: get-started-node\n  template:\n    metadata:\n      labels:\n        app: get-started-node\n    spec:\n      containers:\n      - name: get-started-node\n        image: <REGISTRY>/<NAMESPACE>/myapp:v1.1.0\n        ports:\n        - containerPort: 8080\n        imagePullPolicy: Always\n        env:\n        - name: CLOUDANT_URL\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: url\n              optional: true\n        - name: CLOUDANT_IAM_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: iamApiKey\n              optional: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"get-started-node\" has cpu request 0"
  },
  {
    "id": "00917",
    "manifest_path": "data/manifests/the_stack_sample/sample_0214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: get-started-node\n  labels:\n    app: get-started-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: get-started-node\n  template:\n    metadata:\n      labels:\n        app: get-started-node\n    spec:\n      containers:\n      - name: get-started-node\n        image: <REGISTRY>/<NAMESPACE>/myapp:v1.1.0\n        ports:\n        - containerPort: 8080\n        imagePullPolicy: Always\n        env:\n        - name: CLOUDANT_URL\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: url\n              optional: true\n        - name: CLOUDANT_IAM_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: iamApiKey\n              optional: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"get-started-node\" has memory limit 0"
  },
  {
    "id": "00918",
    "manifest_path": "data/manifests/the_stack_sample/sample_0222.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: quobyte\nspec:\n  containers:\n  - name: quobyte\n    image: kubernetes/pause\n    volumeMounts:\n    - mountPath: /mnt\n      name: quobytevolume\n  volumes:\n  - name: quobytevolume\n    quobyte:\n      registry: registry:7861\n      volume: testVolume\n      readOnly: false\n      user: root\n      group: root\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"quobyte\" is using an invalid container image, \"kubernetes/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00919",
    "manifest_path": "data/manifests/the_stack_sample/sample_0222.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: quobyte\nspec:\n  containers:\n  - name: quobyte\n    image: kubernetes/pause\n    volumeMounts:\n    - mountPath: /mnt\n      name: quobytevolume\n  volumes:\n  - name: quobytevolume\n    quobyte:\n      registry: registry:7861\n      volume: testVolume\n      readOnly: false\n      user: root\n      group: root\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"quobyte\" does not have a read-only root file system"
  },
  {
    "id": "00920",
    "manifest_path": "data/manifests/the_stack_sample/sample_0222.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: quobyte\nspec:\n  containers:\n  - name: quobyte\n    image: kubernetes/pause\n    volumeMounts:\n    - mountPath: /mnt\n      name: quobytevolume\n  volumes:\n  - name: quobytevolume\n    quobyte:\n      registry: registry:7861\n      volume: testVolume\n      readOnly: false\n      user: root\n      group: root\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"quobyte\" is not set to runAsNonRoot"
  },
  {
    "id": "00921",
    "manifest_path": "data/manifests/the_stack_sample/sample_0222.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: quobyte\nspec:\n  containers:\n  - name: quobyte\n    image: kubernetes/pause\n    volumeMounts:\n    - mountPath: /mnt\n      name: quobytevolume\n  volumes:\n  - name: quobytevolume\n    quobyte:\n      registry: registry:7861\n      volume: testVolume\n      readOnly: false\n      user: root\n      group: root\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"quobyte\" has cpu request 0"
  },
  {
    "id": "00922",
    "manifest_path": "data/manifests/the_stack_sample/sample_0222.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: quobyte\nspec:\n  containers:\n  - name: quobyte\n    image: kubernetes/pause\n    volumeMounts:\n    - mountPath: /mnt\n      name: quobytevolume\n  volumes:\n  - name: quobytevolume\n    quobyte:\n      registry: registry:7861\n      volume: testVolume\n      readOnly: false\n      user: root\n      group: root\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"quobyte\" has memory limit 0"
  },
  {
    "id": "00923",
    "manifest_path": "data/manifests/the_stack_sample/sample_0223.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-unbherbarium-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-unbherbarium-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: unbherbarium-lib-unb-ca\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cron-unbherbarium-lib-unb-ca\" is using an invalid container image, \"||DEPLOYMENTIMAGE||\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00924",
    "manifest_path": "data/manifests/the_stack_sample/sample_0223.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-unbherbarium-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-unbherbarium-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: unbherbarium-lib-unb-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cron-unbherbarium-lib-unb-ca\" does not have a read-only root file system"
  },
  {
    "id": "00925",
    "manifest_path": "data/manifests/the_stack_sample/sample_0223.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-unbherbarium-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-unbherbarium-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: unbherbarium-lib-unb-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cron-unbherbarium-lib-unb-ca\" is not set to runAsNonRoot"
  },
  {
    "id": "00926",
    "manifest_path": "data/manifests/the_stack_sample/sample_0223.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-unbherbarium-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-unbherbarium-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: unbherbarium-lib-unb-ca\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cron-unbherbarium-lib-unb-ca\" has cpu request 0"
  },
  {
    "id": "00927",
    "manifest_path": "data/manifests/the_stack_sample/sample_0223.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-unbherbarium-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-unbherbarium-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: unbherbarium-lib-unb-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cron-unbherbarium-lib-unb-ca\" has memory limit 0"
  },
  {
    "id": "00928",
    "manifest_path": "data/manifests/the_stack_sample/sample_0225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200501-e6124e633\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account,node-e2e-project\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"boskos-reaper\" does not have a read-only root file system"
  },
  {
    "id": "00929",
    "manifest_path": "data/manifests/the_stack_sample/sample_0225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200501-e6124e633\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account,node-e2e-project\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"boskos-reaper\" is not set to runAsNonRoot"
  },
  {
    "id": "00930",
    "manifest_path": "data/manifests/the_stack_sample/sample_0225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200501-e6124e633\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account,node-e2e-project\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"boskos-reaper\" has cpu request 0"
  },
  {
    "id": "00931",
    "manifest_path": "data/manifests/the_stack_sample/sample_0225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200501-e6124e633\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account,node-e2e-project\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"boskos-reaper\" has memory limit 0"
  },
  {
    "id": "00932",
    "manifest_path": "data/manifests/the_stack_sample/sample_0229.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v6\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v6\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.16.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --poll_duration=2m\n        - --stats_resolution=1m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "00933",
    "manifest_path": "data/manifests/the_stack_sample/sample_0229.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v6\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v6\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.16.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --poll_duration=2m\n        - --stats_resolution=1m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "00934",
    "manifest_path": "data/manifests/the_stack_sample/sample_0229.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v6\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v6\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.16.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --poll_duration=2m\n        - --stats_resolution=1m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"heapster\" has cpu request 0"
  },
  {
    "id": "00935",
    "manifest_path": "data/manifests/the_stack_sample/sample_0230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: users\n  template:\n    metadata:\n      labels:\n        component: users\n    spec:\n      containers:\n      - name: users\n        image: sbalasubramanian14/users-api\n        ports:\n        - containerPort: 5000\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"users\" is using an invalid container image, \"sbalasubramanian14/users-api\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00936",
    "manifest_path": "data/manifests/the_stack_sample/sample_0230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: users\n  template:\n    metadata:\n      labels:\n        component: users\n    spec:\n      containers:\n      - name: users\n        image: sbalasubramanian14/users-api\n        ports:\n        - containerPort: 5000\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"users\" does not have a read-only root file system"
  },
  {
    "id": "00937",
    "manifest_path": "data/manifests/the_stack_sample/sample_0230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: users\n  template:\n    metadata:\n      labels:\n        component: users\n    spec:\n      containers:\n      - name: users\n        image: sbalasubramanian14/users-api\n        ports:\n        - containerPort: 5000\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"users\" is not set to runAsNonRoot"
  },
  {
    "id": "00938",
    "manifest_path": "data/manifests/the_stack_sample/sample_0230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: users\n  template:\n    metadata:\n      labels:\n        component: users\n    spec:\n      containers:\n      - name: users\n        image: sbalasubramanian14/users-api\n        ports:\n        - containerPort: 5000\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"users\" has cpu request 0"
  },
  {
    "id": "00939",
    "manifest_path": "data/manifests/the_stack_sample/sample_0230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: users\n  template:\n    metadata:\n      labels:\n        component: users\n    spec:\n      containers:\n      - name: users\n        image: sbalasubramanian14/users-api\n        ports:\n        - containerPort: 5000\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"users\" has memory limit 0"
  },
  {
    "id": "00940",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cilium-agent\" is using an invalid container image, \"docker.io/cilium/cilium:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00941",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "00942",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "00943",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "00944",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"clean-cilium-state\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "00945",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cilium-agent\" is privileged"
  },
  {
    "id": "00946",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"clean-cilium-state\" is privileged"
  },
  {
    "id": "00947",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "00948",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "00949",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "00950",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "00951",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "00952",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "00953",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable TLS_SECRET_NAME in container \"proxy\" found"
  },
  {
    "id": "00954",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"proxy\" is using an invalid container image, \"kurl/proxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00955",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"proxy\" does not have a read-only root file system"
  },
  {
    "id": "00956",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "00957",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"proxy\" has cpu request 0"
  },
  {
    "id": "00958",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"proxy\" has memory limit 0"
  },
  {
    "id": "00959",
    "manifest_path": "data/manifests/the_stack_sample/sample_0235.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    app: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: client\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: client\n        image: alpine\n        imagePullPolicy: IfNotPresent\n        command:\n        - bin/sh\n        - -c\n        - sleep 10086\n  selector:\n    matchLabels:\n      app: client\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"client\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00960",
    "manifest_path": "data/manifests/the_stack_sample/sample_0235.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    app: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: client\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: client\n        image: alpine\n        imagePullPolicy: IfNotPresent\n        command:\n        - bin/sh\n        - -c\n        - sleep 10086\n  selector:\n    matchLabels:\n      app: client\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"client\" does not have a read-only root file system"
  },
  {
    "id": "00961",
    "manifest_path": "data/manifests/the_stack_sample/sample_0235.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    app: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: client\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: client\n        image: alpine\n        imagePullPolicy: IfNotPresent\n        command:\n        - bin/sh\n        - -c\n        - sleep 10086\n  selector:\n    matchLabels:\n      app: client\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"client\" is not set to runAsNonRoot"
  },
  {
    "id": "00962",
    "manifest_path": "data/manifests/the_stack_sample/sample_0235.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    app: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: client\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: client\n        image: alpine\n        imagePullPolicy: IfNotPresent\n        command:\n        - bin/sh\n        - -c\n        - sleep 10086\n  selector:\n    matchLabels:\n      app: client\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"client\" has cpu request 0"
  },
  {
    "id": "00963",
    "manifest_path": "data/manifests/the_stack_sample/sample_0235.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    app: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: client\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: client\n        image: alpine\n        imagePullPolicy: IfNotPresent\n        command:\n        - bin/sh\n        - -c\n        - sleep 10086\n  selector:\n    matchLabels:\n      app: client\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"client\" has memory limit 0"
  },
  {
    "id": "00964",
    "manifest_path": "data/manifests/the_stack_sample/sample_0237.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-2\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-2-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mlperf-inference-container\" is using an invalid container image, \"aferikoglou/mlperf-inference:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00965",
    "manifest_path": "data/manifests/the_stack_sample/sample_0237.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-2\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-2-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mlperf-inference-container\" does not have a read-only root file system"
  },
  {
    "id": "00966",
    "manifest_path": "data/manifests/the_stack_sample/sample_0237.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-2\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-2-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mlperf-inference-container\" is not set to runAsNonRoot"
  },
  {
    "id": "00967",
    "manifest_path": "data/manifests/the_stack_sample/sample_0237.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-2\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-2-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mlperf-inference-container\" has cpu request 0"
  },
  {
    "id": "00968",
    "manifest_path": "data/manifests/the_stack_sample/sample_0237.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-2\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-2-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mlperf-inference-container\" has memory limit 0"
  },
  {
    "id": "00969",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloada\" does not have a read-only root file system"
  },
  {
    "id": "00970",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloadb\" does not have a read-only root file system"
  },
  {
    "id": "00971",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloadc\" does not have a read-only root file system"
  },
  {
    "id": "00972",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloadd\" does not have a read-only root file system"
  },
  {
    "id": "00973",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloade\" does not have a read-only root file system"
  },
  {
    "id": "00974",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloadf\" does not have a read-only root file system"
  },
  {
    "id": "00975",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloada\" is not set to runAsNonRoot"
  },
  {
    "id": "00976",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloadb\" is not set to runAsNonRoot"
  },
  {
    "id": "00977",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloadc\" is not set to runAsNonRoot"
  },
  {
    "id": "00978",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloadd\" is not set to runAsNonRoot"
  },
  {
    "id": "00979",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloade\" is not set to runAsNonRoot"
  },
  {
    "id": "00980",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloadf\" is not set to runAsNonRoot"
  },
  {
    "id": "00981",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloada\" has cpu request 0"
  },
  {
    "id": "00982",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloadb\" has cpu request 0"
  },
  {
    "id": "00983",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloadc\" has cpu request 0"
  },
  {
    "id": "00984",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloadd\" has cpu request 0"
  },
  {
    "id": "00985",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloade\" has cpu request 0"
  },
  {
    "id": "00986",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloadf\" has cpu request 0"
  },
  {
    "id": "00987",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloada\" has memory limit 0"
  },
  {
    "id": "00988",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloadb\" has memory limit 0"
  },
  {
    "id": "00989",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloadc\" has memory limit 0"
  },
  {
    "id": "00990",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloadd\" has memory limit 0"
  },
  {
    "id": "00991",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloade\" has memory limit 0"
  },
  {
    "id": "00992",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloadf\" has memory limit 0"
  },
  {
    "id": "00993",
    "manifest_path": "data/manifests/the_stack_sample/sample_0242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-kube-scheduler-operator\n  name: openshift-kube-scheduler-operator\n  labels:\n    app: openshift-kube-scheduler-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-kube-scheduler-operator\n  template:\n    metadata:\n      name: openshift-kube-scheduler-operator\n      labels:\n        app: openshift-kube-scheduler-operator\n    spec:\n      serviceAccountName: openshift-kube-scheduler-operator\n      containers:\n      - name: kube-scheduler-operator-container\n        image: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        imagePullPolicy: Always\n        command:\n        - cluster-kube-scheduler-operator\n        - operator\n        args:\n        - --config=/var/run/configmaps/config/config.yaml\n        - -v=4\n        resources:\n          requests:\n            memory: 50Mi\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-hyperkube:v4.0\n        - name: OPERATOR_IMAGE\n          value: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n      volumes:\n      - name: config\n        configMap:\n          name: openshift-kube-scheduler-operator-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-scheduler-operator-container\" does not have a read-only root file system"
  },
  {
    "id": "00994",
    "manifest_path": "data/manifests/the_stack_sample/sample_0242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-kube-scheduler-operator\n  name: openshift-kube-scheduler-operator\n  labels:\n    app: openshift-kube-scheduler-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-kube-scheduler-operator\n  template:\n    metadata:\n      name: openshift-kube-scheduler-operator\n      labels:\n        app: openshift-kube-scheduler-operator\n    spec:\n      serviceAccountName: openshift-kube-scheduler-operator\n      containers:\n      - name: kube-scheduler-operator-container\n        image: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        imagePullPolicy: Always\n        command:\n        - cluster-kube-scheduler-operator\n        - operator\n        args:\n        - --config=/var/run/configmaps/config/config.yaml\n        - -v=4\n        resources:\n          requests:\n            memory: 50Mi\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-hyperkube:v4.0\n        - name: OPERATOR_IMAGE\n          value: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n      volumes:\n      - name: config\n        configMap:\n          name: openshift-kube-scheduler-operator-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-scheduler-operator-container\" is not set to runAsNonRoot"
  },
  {
    "id": "00995",
    "manifest_path": "data/manifests/the_stack_sample/sample_0242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-kube-scheduler-operator\n  name: openshift-kube-scheduler-operator\n  labels:\n    app: openshift-kube-scheduler-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-kube-scheduler-operator\n  template:\n    metadata:\n      name: openshift-kube-scheduler-operator\n      labels:\n        app: openshift-kube-scheduler-operator\n    spec:\n      serviceAccountName: openshift-kube-scheduler-operator\n      containers:\n      - name: kube-scheduler-operator-container\n        image: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        imagePullPolicy: Always\n        command:\n        - cluster-kube-scheduler-operator\n        - operator\n        args:\n        - --config=/var/run/configmaps/config/config.yaml\n        - -v=4\n        resources:\n          requests:\n            memory: 50Mi\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-hyperkube:v4.0\n        - name: OPERATOR_IMAGE\n          value: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n      volumes:\n      - name: config\n        configMap:\n          name: openshift-kube-scheduler-operator-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-scheduler-operator-container\" has cpu request 0"
  },
  {
    "id": "00996",
    "manifest_path": "data/manifests/the_stack_sample/sample_0242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-kube-scheduler-operator\n  name: openshift-kube-scheduler-operator\n  labels:\n    app: openshift-kube-scheduler-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-kube-scheduler-operator\n  template:\n    metadata:\n      name: openshift-kube-scheduler-operator\n      labels:\n        app: openshift-kube-scheduler-operator\n    spec:\n      serviceAccountName: openshift-kube-scheduler-operator\n      containers:\n      - name: kube-scheduler-operator-container\n        image: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        imagePullPolicy: Always\n        command:\n        - cluster-kube-scheduler-operator\n        - operator\n        args:\n        - --config=/var/run/configmaps/config/config.yaml\n        - -v=4\n        resources:\n          requests:\n            memory: 50Mi\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-hyperkube:v4.0\n        - name: OPERATOR_IMAGE\n          value: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n      volumes:\n      - name: config\n        configMap:\n          name: openshift-kube-scheduler-operator-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-scheduler-operator-container\" has memory limit 0"
  },
  {
    "id": "00997",
    "manifest_path": "data/manifests/the_stack_sample/sample_0243.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-622\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00998",
    "manifest_path": "data/manifests/the_stack_sample/sample_0243.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-622\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00999",
    "manifest_path": "data/manifests/the_stack_sample/sample_0243.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-622\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01000",
    "manifest_path": "data/manifests/the_stack_sample/sample_0243.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-622\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01001",
    "manifest_path": "data/manifests/the_stack_sample/sample_0243.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-622\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01002",
    "manifest_path": "data/manifests/the_stack_sample/sample_0244.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: matkuber-b39d\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: matkuber-b39d\n  template:\n    metadata:\n      labels:\n        app: matkuber-b39d\n    spec:\n      containers:\n      - name: matkuber-b39d\n        image: matacr.azurecr.io/matkuber\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"matkuber-b39d\" is using an invalid container image, \"matacr.azurecr.io/matkuber\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01003",
    "manifest_path": "data/manifests/the_stack_sample/sample_0244.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: matkuber-b39d\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: matkuber-b39d\n  template:\n    metadata:\n      labels:\n        app: matkuber-b39d\n    spec:\n      containers:\n      - name: matkuber-b39d\n        image: matacr.azurecr.io/matkuber\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"matkuber-b39d\" does not have a read-only root file system"
  },
  {
    "id": "01004",
    "manifest_path": "data/manifests/the_stack_sample/sample_0244.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: matkuber-b39d\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: matkuber-b39d\n  template:\n    metadata:\n      labels:\n        app: matkuber-b39d\n    spec:\n      containers:\n      - name: matkuber-b39d\n        image: matacr.azurecr.io/matkuber\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"matkuber-b39d\" is not set to runAsNonRoot"
  },
  {
    "id": "01005",
    "manifest_path": "data/manifests/the_stack_sample/sample_0244.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: matkuber-b39d\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: matkuber-b39d\n  template:\n    metadata:\n      labels:\n        app: matkuber-b39d\n    spec:\n      containers:\n      - name: matkuber-b39d\n        image: matacr.azurecr.io/matkuber\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"matkuber-b39d\" has cpu request 0"
  },
  {
    "id": "01006",
    "manifest_path": "data/manifests/the_stack_sample/sample_0244.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: matkuber-b39d\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: matkuber-b39d\n  template:\n    metadata:\n      labels:\n        app: matkuber-b39d\n    spec:\n      containers:\n      - name: matkuber-b39d\n        image: matacr.azurecr.io/matkuber\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"matkuber-b39d\" has memory limit 0"
  },
  {
    "id": "01007",
    "manifest_path": "data/manifests/the_stack_sample/sample_0250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: dataservice-forecast-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: dataservice-forecast-job\n          image: gwdowner/dataservice:latest\n          command:\n          - npm\n          - run\n          - batchjob:forecast\n          imagePullPolicy: Always\n          envFrom:\n          - secretRef:\n              name: data-service\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dataservice-forecast-job\" is using an invalid container image, \"gwdowner/dataservice:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01008",
    "manifest_path": "data/manifests/the_stack_sample/sample_0250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: dataservice-forecast-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: dataservice-forecast-job\n          image: gwdowner/dataservice:latest\n          command:\n          - npm\n          - run\n          - batchjob:forecast\n          imagePullPolicy: Always\n          envFrom:\n          - secretRef:\n              name: data-service\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dataservice-forecast-job\" does not have a read-only root file system"
  },
  {
    "id": "01009",
    "manifest_path": "data/manifests/the_stack_sample/sample_0250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: dataservice-forecast-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: dataservice-forecast-job\n          image: gwdowner/dataservice:latest\n          command:\n          - npm\n          - run\n          - batchjob:forecast\n          imagePullPolicy: Always\n          envFrom:\n          - secretRef:\n              name: data-service\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dataservice-forecast-job\" is not set to runAsNonRoot"
  },
  {
    "id": "01010",
    "manifest_path": "data/manifests/the_stack_sample/sample_0250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: dataservice-forecast-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: dataservice-forecast-job\n          image: gwdowner/dataservice:latest\n          command:\n          - npm\n          - run\n          - batchjob:forecast\n          imagePullPolicy: Always\n          envFrom:\n          - secretRef:\n              name: data-service\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dataservice-forecast-job\" has cpu request 0"
  },
  {
    "id": "01011",
    "manifest_path": "data/manifests/the_stack_sample/sample_0250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: dataservice-forecast-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: dataservice-forecast-job\n          image: gwdowner/dataservice:latest\n          command:\n          - npm\n          - run\n          - batchjob:forecast\n          imagePullPolicy: Always\n          envFrom:\n          - secretRef:\n              name: data-service\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dataservice-forecast-job\" has memory limit 0"
  },
  {
    "id": "01012",
    "manifest_path": "data/manifests/the_stack_sample/sample_0255.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config-8kcgd8t8th\n        name: config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jupyter-web-app\" does not have a read-only root file system"
  },
  {
    "id": "01013",
    "manifest_path": "data/manifests/the_stack_sample/sample_0255.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config-8kcgd8t8th\n        name: config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jupyter-web-app\" is not set to runAsNonRoot"
  },
  {
    "id": "01014",
    "manifest_path": "data/manifests/the_stack_sample/sample_0255.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config-8kcgd8t8th\n        name: config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jupyter-web-app\" has cpu request 0"
  },
  {
    "id": "01015",
    "manifest_path": "data/manifests/the_stack_sample/sample_0255.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config-8kcgd8t8th\n        name: config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jupyter-web-app\" has memory limit 0"
  },
  {
    "id": "01016",
    "manifest_path": "data/manifests/the_stack_sample/sample_0259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: job-trigger-controller-manager\n  labels:\n    app: prow\n    component: job-trigger-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: job-trigger-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: job-trigger-controller-manager\n    spec:\n      serviceAccount: job-trigger-controller-manager\n      containers:\n      - image: job-trigger-controller-manager:latest\n        name: job-trigger-controller-manager\n        command:\n        - job-trigger-controller-manager\n        args:\n        - --dry-run=false\n        - --namespace=ci\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"job-trigger-controller-manager\" is using an invalid container image, \"job-trigger-controller-manager:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01017",
    "manifest_path": "data/manifests/the_stack_sample/sample_0259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: job-trigger-controller-manager\n  labels:\n    app: prow\n    component: job-trigger-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: job-trigger-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: job-trigger-controller-manager\n    spec:\n      serviceAccount: job-trigger-controller-manager\n      containers:\n      - image: job-trigger-controller-manager:latest\n        name: job-trigger-controller-manager\n        command:\n        - job-trigger-controller-manager\n        args:\n        - --dry-run=false\n        - --namespace=ci\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job-trigger-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "01018",
    "manifest_path": "data/manifests/the_stack_sample/sample_0259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: job-trigger-controller-manager\n  labels:\n    app: prow\n    component: job-trigger-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: job-trigger-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: job-trigger-controller-manager\n    spec:\n      serviceAccount: job-trigger-controller-manager\n      containers:\n      - image: job-trigger-controller-manager:latest\n        name: job-trigger-controller-manager\n        command:\n        - job-trigger-controller-manager\n        args:\n        - --dry-run=false\n        - --namespace=ci\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job-trigger-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "01019",
    "manifest_path": "data/manifests/the_stack_sample/sample_0259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: job-trigger-controller-manager\n  labels:\n    app: prow\n    component: job-trigger-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: job-trigger-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: job-trigger-controller-manager\n    spec:\n      serviceAccount: job-trigger-controller-manager\n      containers:\n      - image: job-trigger-controller-manager:latest\n        name: job-trigger-controller-manager\n        command:\n        - job-trigger-controller-manager\n        args:\n        - --dry-run=false\n        - --namespace=ci\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job-trigger-controller-manager\" has cpu request 0"
  },
  {
    "id": "01020",
    "manifest_path": "data/manifests/the_stack_sample/sample_0259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: job-trigger-controller-manager\n  labels:\n    app: prow\n    component: job-trigger-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: job-trigger-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: job-trigger-controller-manager\n    spec:\n      serviceAccount: job-trigger-controller-manager\n      containers:\n      - image: job-trigger-controller-manager:latest\n        name: job-trigger-controller-manager\n        command:\n        - job-trigger-controller-manager\n        args:\n        - --dry-run=false\n        - --namespace=ci\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job-trigger-controller-manager\" has memory limit 0"
  },
  {
    "id": "01021",
    "manifest_path": "data/manifests/the_stack_sample/sample_0260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: ifinodin/payment:v0.0.1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "01022",
    "manifest_path": "data/manifests/the_stack_sample/sample_0260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: ifinodin/payment:v0.0.1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "01023",
    "manifest_path": "data/manifests/the_stack_sample/sample_0260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: ifinodin/payment:v0.0.1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "01024",
    "manifest_path": "data/manifests/the_stack_sample/sample_0260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: ifinodin/payment:v0.0.1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "01025",
    "manifest_path": "data/manifests/the_stack_sample/sample_0261.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://10.0.0.1\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --service-cluster-ip-range=10.10.0.0/24\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "01026",
    "manifest_path": "data/manifests/the_stack_sample/sample_0261.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://10.0.0.1\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --service-cluster-ip-range=10.10.0.0/24\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "01027",
    "manifest_path": "data/manifests/the_stack_sample/sample_0261.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://10.0.0.1\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --service-cluster-ip-range=10.10.0.0/24\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"heapster\" has cpu request 0"
  },
  {
    "id": "01028",
    "manifest_path": "data/manifests/the_stack_sample/sample_0261.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://10.0.0.1\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --service-cluster-ip-range=10.10.0.0/24\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"heapster\" has memory limit 0"
  },
  {
    "id": "01029",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql\" does not have a read-only root file system"
  },
  {
    "id": "01030",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql-datagen-pageviews\" does not have a read-only root file system"
  },
  {
    "id": "01031",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql-datagen-users\" does not have a read-only root file system"
  },
  {
    "id": "01032",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql\" is not set to runAsNonRoot"
  },
  {
    "id": "01033",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql-datagen-pageviews\" is not set to runAsNonRoot"
  },
  {
    "id": "01034",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql-datagen-users\" is not set to runAsNonRoot"
  },
  {
    "id": "01035",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql\" has cpu request 0"
  },
  {
    "id": "01036",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql-datagen-pageviews\" has cpu request 0"
  },
  {
    "id": "01037",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql-datagen-users\" has cpu request 0"
  },
  {
    "id": "01038",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql\" has memory limit 0"
  },
  {
    "id": "01039",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql-datagen-pageviews\" has memory limit 0"
  },
  {
    "id": "01040",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql-datagen-users\" has memory limit 0"
  },
  {
    "id": "01041",
    "manifest_path": "data/manifests/the_stack_sample/sample_0265.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6084\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01042",
    "manifest_path": "data/manifests/the_stack_sample/sample_0265.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6084\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01043",
    "manifest_path": "data/manifests/the_stack_sample/sample_0265.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6084\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01044",
    "manifest_path": "data/manifests/the_stack_sample/sample_0265.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6084\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01045",
    "manifest_path": "data/manifests/the_stack_sample/sample_0265.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6084\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01046",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "01047",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container2\" does not have a read-only root file system"
  },
  {
    "id": "01048",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container1\" is not set to runAsNonRoot"
  },
  {
    "id": "01049",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container2\" is not set to runAsNonRoot"
  },
  {
    "id": "01050",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "01051",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container2\" has cpu request 0"
  },
  {
    "id": "01052",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "01053",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container2\" has memory limit 0"
  },
  {
    "id": "01054",
    "manifest_path": "data/manifests/the_stack_sample/sample_0268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  name: grafana-deployment\nspec:\n  selector:\n    matchLabels:\n      app: grafana-deployment\n  template:\n    metadata:\n      labels:\n        app: grafana-deployment\n    spec:\n      securityContext:\n        runAsUser: 472\n        fsGroup: 472\n      containers:\n      - name: grafana\n        image: grafana/grafana\n        env:\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          value: admin\n        - name: GF_INSTALL_PLUGINS\n          value: grafana-clock-panel,grafana-piechart-panel,camptocamp-prometheus-alertmanager-datasource,vonage-status-panel,alexanderzobnin-zabbix-app,grafana-worldmap-panel,raintank-worldping-app,agenty-flowcharting-panel\n        ports:\n        - containerPort: 3000\n        volumeMounts:\n        - name: grafana-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: grafana-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"grafana\" is using an invalid container image, \"grafana/grafana\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01055",
    "manifest_path": "data/manifests/the_stack_sample/sample_0268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  name: grafana-deployment\nspec:\n  selector:\n    matchLabels:\n      app: grafana-deployment\n  template:\n    metadata:\n      labels:\n        app: grafana-deployment\n    spec:\n      securityContext:\n        runAsUser: 472\n        fsGroup: 472\n      containers:\n      - name: grafana\n        image: grafana/grafana\n        env:\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          value: admin\n        - name: GF_INSTALL_PLUGINS\n          value: grafana-clock-panel,grafana-piechart-panel,camptocamp-prometheus-alertmanager-datasource,vonage-status-panel,alexanderzobnin-zabbix-app,grafana-worldmap-panel,raintank-worldping-app,agenty-flowcharting-panel\n        ports:\n        - containerPort: 3000\n        volumeMounts:\n        - name: grafana-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: grafana-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "01056",
    "manifest_path": "data/manifests/the_stack_sample/sample_0268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  name: grafana-deployment\nspec:\n  selector:\n    matchLabels:\n      app: grafana-deployment\n  template:\n    metadata:\n      labels:\n        app: grafana-deployment\n    spec:\n      securityContext:\n        runAsUser: 472\n        fsGroup: 472\n      containers:\n      - name: grafana\n        image: grafana/grafana\n        env:\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          value: admin\n        - name: GF_INSTALL_PLUGINS\n          value: grafana-clock-panel,grafana-piechart-panel,camptocamp-prometheus-alertmanager-datasource,vonage-status-panel,alexanderzobnin-zabbix-app,grafana-worldmap-panel,raintank-worldping-app,agenty-flowcharting-panel\n        ports:\n        - containerPort: 3000\n        volumeMounts:\n        - name: grafana-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: grafana-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana\" has cpu request 0"
  },
  {
    "id": "01057",
    "manifest_path": "data/manifests/the_stack_sample/sample_0268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  name: grafana-deployment\nspec:\n  selector:\n    matchLabels:\n      app: grafana-deployment\n  template:\n    metadata:\n      labels:\n        app: grafana-deployment\n    spec:\n      securityContext:\n        runAsUser: 472\n        fsGroup: 472\n      containers:\n      - name: grafana\n        image: grafana/grafana\n        env:\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          value: admin\n        - name: GF_INSTALL_PLUGINS\n          value: grafana-clock-panel,grafana-piechart-panel,camptocamp-prometheus-alertmanager-datasource,vonage-status-panel,alexanderzobnin-zabbix-app,grafana-worldmap-panel,raintank-worldping-app,agenty-flowcharting-panel\n        ports:\n        - containerPort: 3000\n        volumeMounts:\n        - name: grafana-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: grafana-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana\" has memory limit 0"
  },
  {
    "id": "01058",
    "manifest_path": "data/manifests/the_stack_sample/sample_0269.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-taint\nspec:\n  containers:\n  - name: nginx-image\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-image\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01059",
    "manifest_path": "data/manifests/the_stack_sample/sample_0269.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-taint\nspec:\n  containers:\n  - name: nginx-image\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-image\" does not have a read-only root file system"
  },
  {
    "id": "01060",
    "manifest_path": "data/manifests/the_stack_sample/sample_0269.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-taint\nspec:\n  containers:\n  - name: nginx-image\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-image\" is not set to runAsNonRoot"
  },
  {
    "id": "01061",
    "manifest_path": "data/manifests/the_stack_sample/sample_0269.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-taint\nspec:\n  containers:\n  - name: nginx-image\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-image\" has cpu request 0"
  },
  {
    "id": "01062",
    "manifest_path": "data/manifests/the_stack_sample/sample_0269.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-taint\nspec:\n  containers:\n  - name: nginx-image\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-image\" has memory limit 0"
  },
  {
    "id": "01063",
    "manifest_path": "data/manifests/the_stack_sample/sample_0272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: addresses-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: addresses-service\n  template:\n    metadata:\n      labels:\n        app: addresses-service\n    spec:\n      containers:\n      - name: addresses-service\n        image: stokei/addresses-service:latest\n        resources: {}\n        imagePullPolicy: Always\n        env:\n        - name: DB_NAME\n          value: addresses\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-user\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-password\n        - name: DB_OPTIONS\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-options\n        - name: DB_PREFIX\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-prefix\n        - name: DB_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-timeout\n        - name: QUEUE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-host\n        - name: QUEUE_PORT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-port\n        - name: QUEUE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-password\n        - name: QUEUE_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-timeout\n        - name: MICROSERVICE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-secret\n              key: url\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"addresses-service\" is using an invalid container image, \"stokei/addresses-service:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01064",
    "manifest_path": "data/manifests/the_stack_sample/sample_0272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: addresses-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: addresses-service\n  template:\n    metadata:\n      labels:\n        app: addresses-service\n    spec:\n      containers:\n      - name: addresses-service\n        image: stokei/addresses-service:latest\n        resources: {}\n        imagePullPolicy: Always\n        env:\n        - name: DB_NAME\n          value: addresses\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-user\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-password\n        - name: DB_OPTIONS\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-options\n        - name: DB_PREFIX\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-prefix\n        - name: DB_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-timeout\n        - name: QUEUE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-host\n        - name: QUEUE_PORT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-port\n        - name: QUEUE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-password\n        - name: QUEUE_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-timeout\n        - name: MICROSERVICE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-secret\n              key: url\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"addresses-service\" does not have a read-only root file system"
  },
  {
    "id": "01065",
    "manifest_path": "data/manifests/the_stack_sample/sample_0272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: addresses-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: addresses-service\n  template:\n    metadata:\n      labels:\n        app: addresses-service\n    spec:\n      containers:\n      - name: addresses-service\n        image: stokei/addresses-service:latest\n        resources: {}\n        imagePullPolicy: Always\n        env:\n        - name: DB_NAME\n          value: addresses\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-user\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-password\n        - name: DB_OPTIONS\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-options\n        - name: DB_PREFIX\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-prefix\n        - name: DB_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-timeout\n        - name: QUEUE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-host\n        - name: QUEUE_PORT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-port\n        - name: QUEUE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-password\n        - name: QUEUE_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-timeout\n        - name: MICROSERVICE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-secret\n              key: url\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"addresses-service\" is not set to runAsNonRoot"
  },
  {
    "id": "01066",
    "manifest_path": "data/manifests/the_stack_sample/sample_0272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: addresses-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: addresses-service\n  template:\n    metadata:\n      labels:\n        app: addresses-service\n    spec:\n      containers:\n      - name: addresses-service\n        image: stokei/addresses-service:latest\n        resources: {}\n        imagePullPolicy: Always\n        env:\n        - name: DB_NAME\n          value: addresses\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-user\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-password\n        - name: DB_OPTIONS\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-options\n        - name: DB_PREFIX\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-prefix\n        - name: DB_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-timeout\n        - name: QUEUE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-host\n        - name: QUEUE_PORT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-port\n        - name: QUEUE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-password\n        - name: QUEUE_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-timeout\n        - name: MICROSERVICE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-secret\n              key: url\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"addresses-service\" has cpu request 0"
  },
  {
    "id": "01067",
    "manifest_path": "data/manifests/the_stack_sample/sample_0272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: addresses-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: addresses-service\n  template:\n    metadata:\n      labels:\n        app: addresses-service\n    spec:\n      containers:\n      - name: addresses-service\n        image: stokei/addresses-service:latest\n        resources: {}\n        imagePullPolicy: Always\n        env:\n        - name: DB_NAME\n          value: addresses\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-user\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-password\n        - name: DB_OPTIONS\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-options\n        - name: DB_PREFIX\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-prefix\n        - name: DB_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-timeout\n        - name: QUEUE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-host\n        - name: QUEUE_PORT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-port\n        - name: QUEUE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-password\n        - name: QUEUE_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-timeout\n        - name: MICROSERVICE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-secret\n              key: url\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"addresses-service\" has memory limit 0"
  },
  {
    "id": "01068",
    "manifest_path": "data/manifests/the_stack_sample/sample_0274.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: startstopdemo-687f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: startstopdemo-687f\n  template:\n    metadata:\n      labels:\n        app: startstopdemo-687f\n    spec:\n      containers:\n      - name: startstopdemo-687f\n        image: testrg12.azurecr.io/startstopdemo\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"startstopdemo-687f\" is using an invalid container image, \"testrg12.azurecr.io/startstopdemo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01069",
    "manifest_path": "data/manifests/the_stack_sample/sample_0274.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: startstopdemo-687f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: startstopdemo-687f\n  template:\n    metadata:\n      labels:\n        app: startstopdemo-687f\n    spec:\n      containers:\n      - name: startstopdemo-687f\n        image: testrg12.azurecr.io/startstopdemo\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"startstopdemo-687f\" does not have a read-only root file system"
  },
  {
    "id": "01070",
    "manifest_path": "data/manifests/the_stack_sample/sample_0274.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: startstopdemo-687f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: startstopdemo-687f\n  template:\n    metadata:\n      labels:\n        app: startstopdemo-687f\n    spec:\n      containers:\n      - name: startstopdemo-687f\n        image: testrg12.azurecr.io/startstopdemo\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"startstopdemo-687f\" is not set to runAsNonRoot"
  },
  {
    "id": "01071",
    "manifest_path": "data/manifests/the_stack_sample/sample_0274.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: startstopdemo-687f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: startstopdemo-687f\n  template:\n    metadata:\n      labels:\n        app: startstopdemo-687f\n    spec:\n      containers:\n      - name: startstopdemo-687f\n        image: testrg12.azurecr.io/startstopdemo\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"startstopdemo-687f\" has cpu request 0"
  },
  {
    "id": "01072",
    "manifest_path": "data/manifests/the_stack_sample/sample_0274.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: startstopdemo-687f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: startstopdemo-687f\n  template:\n    metadata:\n      labels:\n        app: startstopdemo-687f\n    spec:\n      containers:\n      - name: startstopdemo-687f\n        image: testrg12.azurecr.io/startstopdemo\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"startstopdemo-687f\" has memory limit 0"
  },
  {
    "id": "01073",
    "manifest_path": "data/manifests/the_stack_sample/sample_0275.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200404-591527a41\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config-misc\n          mountPath: /etc/job-config/misc\n          readOnly: true\n        - name: job-config-master\n          mountPath: /etc/job-config/master\n          readOnly: true\n        - name: job-config-3x\n          mountPath: /etc/job-config/3.x\n          readOnly: true\n        - name: job-config-41\n          mountPath: /etc/job-config/4.1\n          readOnly: true\n        - name: job-config-42\n          mountPath: /etc/job-config/4.2\n          readOnly: true\n        - name: job-config-43\n          mountPath: /etc/job-config/4.3\n          readOnly: true\n        - name: job-config-44\n          mountPath: /etc/job-config/4.4\n          readOnly: true\n        - name: job-config-45\n          mountPath: /etc/job-config/4.5\n          readOnly: true\n        - name: job-config-46\n          mountPath: /etc/job-config/4.6\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: oauth\n        secret:\n          secretName: github-credentials-openshift-ci-robot\n      - name: config\n        configMap:\n          name: config\n      - name: job-config-misc\n        configMap:\n          name: job-config-misc\n      - name: job-config-master\n        configMap:\n          name: job-config-master\n      - name: job-config-3x\n        configMap:\n          name: job-config-3.x\n      - name: job-config-41\n        configMap:\n          name: job-config-4.1\n      - name: job-config-42\n        configMap:\n          name: job-config-4.2\n      - name: job-config-43\n        configMap:\n          name: job-config-4.3\n      - name: job-config-44\n        configMap:\n          name: job-config-4.4\n      - name: job-config-45\n        configMap:\n          name: job-config-4.5\n      - name: job-config-46\n        configMap:\n          name: job-config-4.6\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "01074",
    "manifest_path": "data/manifests/the_stack_sample/sample_0275.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200404-591527a41\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config-misc\n          mountPath: /etc/job-config/misc\n          readOnly: true\n        - name: job-config-master\n          mountPath: /etc/job-config/master\n          readOnly: true\n        - name: job-config-3x\n          mountPath: /etc/job-config/3.x\n          readOnly: true\n        - name: job-config-41\n          mountPath: /etc/job-config/4.1\n          readOnly: true\n        - name: job-config-42\n          mountPath: /etc/job-config/4.2\n          readOnly: true\n        - name: job-config-43\n          mountPath: /etc/job-config/4.3\n          readOnly: true\n        - name: job-config-44\n          mountPath: /etc/job-config/4.4\n          readOnly: true\n        - name: job-config-45\n          mountPath: /etc/job-config/4.5\n          readOnly: true\n        - name: job-config-46\n          mountPath: /etc/job-config/4.6\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: oauth\n        secret:\n          secretName: github-credentials-openshift-ci-robot\n      - name: config\n        configMap:\n          name: config\n      - name: job-config-misc\n        configMap:\n          name: job-config-misc\n      - name: job-config-master\n        configMap:\n          name: job-config-master\n      - name: job-config-3x\n        configMap:\n          name: job-config-3.x\n      - name: job-config-41\n        configMap:\n          name: job-config-4.1\n      - name: job-config-42\n        configMap:\n          name: job-config-4.2\n      - name: job-config-43\n        configMap:\n          name: job-config-4.3\n      - name: job-config-44\n        configMap:\n          name: job-config-4.4\n      - name: job-config-45\n        configMap:\n          name: job-config-4.5\n      - name: job-config-46\n        configMap:\n          name: job-config-4.6\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "01075",
    "manifest_path": "data/manifests/the_stack_sample/sample_0275.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200404-591527a41\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config-misc\n          mountPath: /etc/job-config/misc\n          readOnly: true\n        - name: job-config-master\n          mountPath: /etc/job-config/master\n          readOnly: true\n        - name: job-config-3x\n          mountPath: /etc/job-config/3.x\n          readOnly: true\n        - name: job-config-41\n          mountPath: /etc/job-config/4.1\n          readOnly: true\n        - name: job-config-42\n          mountPath: /etc/job-config/4.2\n          readOnly: true\n        - name: job-config-43\n          mountPath: /etc/job-config/4.3\n          readOnly: true\n        - name: job-config-44\n          mountPath: /etc/job-config/4.4\n          readOnly: true\n        - name: job-config-45\n          mountPath: /etc/job-config/4.5\n          readOnly: true\n        - name: job-config-46\n          mountPath: /etc/job-config/4.6\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: oauth\n        secret:\n          secretName: github-credentials-openshift-ci-robot\n      - name: config\n        configMap:\n          name: config\n      - name: job-config-misc\n        configMap:\n          name: job-config-misc\n      - name: job-config-master\n        configMap:\n          name: job-config-master\n      - name: job-config-3x\n        configMap:\n          name: job-config-3.x\n      - name: job-config-41\n        configMap:\n          name: job-config-4.1\n      - name: job-config-42\n        configMap:\n          name: job-config-4.2\n      - name: job-config-43\n        configMap:\n          name: job-config-4.3\n      - name: job-config-44\n        configMap:\n          name: job-config-4.4\n      - name: job-config-45\n        configMap:\n          name: job-config-4.5\n      - name: job-config-46\n        configMap:\n          name: job-config-4.6\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "01076",
    "manifest_path": "data/manifests/the_stack_sample/sample_0278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add:\n        - SYS_ADMIN\n        - CHOWN\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sec-ctx-demo\" does not have a read-only root file system"
  },
  {
    "id": "01077",
    "manifest_path": "data/manifests/the_stack_sample/sample_0278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add:\n        - SYS_ADMIN\n        - CHOWN\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"sec-ctx-demo\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "01078",
    "manifest_path": "data/manifests/the_stack_sample/sample_0278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add:\n        - SYS_ADMIN\n        - CHOWN\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sec-ctx-demo\" is not set to runAsNonRoot"
  },
  {
    "id": "01079",
    "manifest_path": "data/manifests/the_stack_sample/sample_0278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add:\n        - SYS_ADMIN\n        - CHOWN\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sec-ctx-demo\" has cpu request 0"
  },
  {
    "id": "01080",
    "manifest_path": "data/manifests/the_stack_sample/sample_0278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add:\n        - SYS_ADMIN\n        - CHOWN\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sec-ctx-demo\" has memory limit 0"
  },
  {
    "id": "01081",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"director\" is using an invalid container image, \"{{- .Values.openmatch.director.image -}}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01082",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait-nakama-grpc-api\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01083",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait-om-function\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01084",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait-open-match-backend\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01085",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"director\" does not have a read-only root file system"
  },
  {
    "id": "01086",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-nakama-grpc-api\" does not have a read-only root file system"
  },
  {
    "id": "01087",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-om-function\" does not have a read-only root file system"
  },
  {
    "id": "01088",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-open-match-backend\" does not have a read-only root file system"
  },
  {
    "id": "01089",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"director\" is not set to runAsNonRoot"
  },
  {
    "id": "01090",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-nakama-grpc-api\" is not set to runAsNonRoot"
  },
  {
    "id": "01091",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-om-function\" is not set to runAsNonRoot"
  },
  {
    "id": "01092",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-open-match-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "01093",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"director\" has cpu request 0"
  },
  {
    "id": "01094",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-nakama-grpc-api\" has cpu request 0"
  },
  {
    "id": "01095",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-om-function\" has cpu request 0"
  },
  {
    "id": "01096",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-open-match-backend\" has cpu request 0"
  },
  {
    "id": "01097",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"director\" has memory limit 0"
  },
  {
    "id": "01098",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-nakama-grpc-api\" has memory limit 0"
  },
  {
    "id": "01099",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-om-function\" has memory limit 0"
  },
  {
    "id": "01100",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-open-match-backend\" has memory limit 0"
  },
  {
    "id": "01101",
    "manifest_path": "data/manifests/the_stack_sample/sample_0284.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: blossom-reposter-staging\n  name: instagram-producer\n  labels:\n    app: instagram-producer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: instagram-producer\n  template:\n    metadata:\n      labels:\n        app: instagram-producer\n    spec:\n      containers:\n      - name: instagram-producer\n        image: docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: blossom-reposter-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"instagram-producer\" is using an invalid container image, \"docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01102",
    "manifest_path": "data/manifests/the_stack_sample/sample_0284.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: blossom-reposter-staging\n  name: instagram-producer\n  labels:\n    app: instagram-producer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: instagram-producer\n  template:\n    metadata:\n      labels:\n        app: instagram-producer\n    spec:\n      containers:\n      - name: instagram-producer\n        image: docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: blossom-reposter-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"instagram-producer\" does not have a read-only root file system"
  },
  {
    "id": "01103",
    "manifest_path": "data/manifests/the_stack_sample/sample_0284.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: blossom-reposter-staging\n  name: instagram-producer\n  labels:\n    app: instagram-producer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: instagram-producer\n  template:\n    metadata:\n      labels:\n        app: instagram-producer\n    spec:\n      containers:\n      - name: instagram-producer\n        image: docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: blossom-reposter-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"instagram-producer\" is not set to runAsNonRoot"
  },
  {
    "id": "01104",
    "manifest_path": "data/manifests/the_stack_sample/sample_0284.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: blossom-reposter-staging\n  name: instagram-producer\n  labels:\n    app: instagram-producer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: instagram-producer\n  template:\n    metadata:\n      labels:\n        app: instagram-producer\n    spec:\n      containers:\n      - name: instagram-producer\n        image: docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: blossom-reposter-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"instagram-producer\" has cpu request 0"
  },
  {
    "id": "01105",
    "manifest_path": "data/manifests/the_stack_sample/sample_0284.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: blossom-reposter-staging\n  name: instagram-producer\n  labels:\n    app: instagram-producer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: instagram-producer\n  template:\n    metadata:\n      labels:\n        app: instagram-producer\n    spec:\n      containers:\n      - name: instagram-producer\n        image: docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: blossom-reposter-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"instagram-producer\" has memory limit 0"
  },
  {
    "id": "01106",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ttcs-agent\" is using an invalid container image, \"gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01107",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ttcs-agent\" does not have a read-only root file system"
  },
  {
    "id": "01108",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"ttcs-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "01109",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"ttcs-agent\" is privileged"
  },
  {
    "id": "01110",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ttcs-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "01111",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ttcs-agent\" has cpu request 0"
  },
  {
    "id": "01112",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ttcs-agent\" has memory limit 0"
  },
  {
    "id": "01113",
    "manifest_path": "data/manifests/the_stack_sample/sample_0291.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cli\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cli\n  template:\n    metadata:\n      labels:\n        app: cli\n    spec:\n      containers:\n      - name: cli\n        image: target/consensource-cli\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 50m\n            memory: 100Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        command:\n        - bash\n        args:\n        - -c\n        - 'tail -f /dev/null\n\n          '\n        volumeMounts:\n        - name: consensource-keys\n          mountPath: /root/.sawtooth\n          readOnly: true\n      volumes:\n      - name: consensource-keys\n        secret:\n          secretName: cli\n          items:\n          - key: sawtooth-pub-key\n            path: keys/root.pub\n          - key: sawtooth-priv-key\n            path: keys/root.priv\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cli\" is using an invalid container image, \"target/consensource-cli\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01114",
    "manifest_path": "data/manifests/the_stack_sample/sample_0291.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cli\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cli\n  template:\n    metadata:\n      labels:\n        app: cli\n    spec:\n      containers:\n      - name: cli\n        image: target/consensource-cli\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 50m\n            memory: 100Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        command:\n        - bash\n        args:\n        - -c\n        - 'tail -f /dev/null\n\n          '\n        volumeMounts:\n        - name: consensource-keys\n          mountPath: /root/.sawtooth\n          readOnly: true\n      volumes:\n      - name: consensource-keys\n        secret:\n          secretName: cli\n          items:\n          - key: sawtooth-pub-key\n            path: keys/root.pub\n          - key: sawtooth-priv-key\n            path: keys/root.priv\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cli\" does not have a read-only root file system"
  },
  {
    "id": "01115",
    "manifest_path": "data/manifests/the_stack_sample/sample_0291.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cli\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cli\n  template:\n    metadata:\n      labels:\n        app: cli\n    spec:\n      containers:\n      - name: cli\n        image: target/consensource-cli\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 50m\n            memory: 100Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        command:\n        - bash\n        args:\n        - -c\n        - 'tail -f /dev/null\n\n          '\n        volumeMounts:\n        - name: consensource-keys\n          mountPath: /root/.sawtooth\n          readOnly: true\n      volumes:\n      - name: consensource-keys\n        secret:\n          secretName: cli\n          items:\n          - key: sawtooth-pub-key\n            path: keys/root.pub\n          - key: sawtooth-priv-key\n            path: keys/root.priv\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cli\" is not set to runAsNonRoot"
  },
  {
    "id": "01116",
    "manifest_path": "data/manifests/the_stack_sample/sample_0293.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keda-olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: keda-olm-operator\n  template:\n    metadata:\n      labels:\n        name: keda-olm-operator\n    spec:\n      serviceAccountName: keda-olm-operator\n      containers:\n      - name: keda-olm-operator\n        image: ghcr.io/kedacore/keda-olm-operator:main\n        command:\n        - /manager\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 25\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 20\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"keda-olm-operator\" does not have a read-only root file system"
  },
  {
    "id": "01117",
    "manifest_path": "data/manifests/the_stack_sample/sample_0293.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keda-olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: keda-olm-operator\n  template:\n    metadata:\n      labels:\n        name: keda-olm-operator\n    spec:\n      serviceAccountName: keda-olm-operator\n      containers:\n      - name: keda-olm-operator\n        image: ghcr.io/kedacore/keda-olm-operator:main\n        command:\n        - /manager\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 25\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 20\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"keda-olm-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "01118",
    "manifest_path": "data/manifests/the_stack_sample/sample_0297.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-datastore-distributor\n  namespace: keptn-datastore\nspec:\n  selector:\n    matchLabels:\n      run: distributor\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: distributor\n    spec:\n      containers:\n      - name: distributor\n        image: keptn/distributor:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: 32Mi\n            cpu: 50m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: PUBSUB_IMPL\n          value: nats\n        - name: PUBSUB_URL\n          value: nats://keptn-nats-cluster.keptn.svc.cluster.local\n        - name: PUBSUB_TOPIC\n          value: sh.keptn.>\n        - name: PUBSUB_RECIPIENT\n          value: mongodb-datastore\n        - name: PUBSUB_RECIPIENT_PATH\n          value: /event\n      serviceAccountName: keptn-ds-default\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"distributor\" is using an invalid container image, \"keptn/distributor:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01119",
    "manifest_path": "data/manifests/the_stack_sample/sample_0297.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-datastore-distributor\n  namespace: keptn-datastore\nspec:\n  selector:\n    matchLabels:\n      run: distributor\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: distributor\n    spec:\n      containers:\n      - name: distributor\n        image: keptn/distributor:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: 32Mi\n            cpu: 50m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: PUBSUB_IMPL\n          value: nats\n        - name: PUBSUB_URL\n          value: nats://keptn-nats-cluster.keptn.svc.cluster.local\n        - name: PUBSUB_TOPIC\n          value: sh.keptn.>\n        - name: PUBSUB_RECIPIENT\n          value: mongodb-datastore\n        - name: PUBSUB_RECIPIENT_PATH\n          value: /event\n      serviceAccountName: keptn-ds-default\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"distributor\" does not have a read-only root file system"
  },
  {
    "id": "01120",
    "manifest_path": "data/manifests/the_stack_sample/sample_0297.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-datastore-distributor\n  namespace: keptn-datastore\nspec:\n  selector:\n    matchLabels:\n      run: distributor\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: distributor\n    spec:\n      containers:\n      - name: distributor\n        image: keptn/distributor:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: 32Mi\n            cpu: 50m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: PUBSUB_IMPL\n          value: nats\n        - name: PUBSUB_URL\n          value: nats://keptn-nats-cluster.keptn.svc.cluster.local\n        - name: PUBSUB_TOPIC\n          value: sh.keptn.>\n        - name: PUBSUB_RECIPIENT\n          value: mongodb-datastore\n        - name: PUBSUB_RECIPIENT_PATH\n          value: /event\n      serviceAccountName: keptn-ds-default\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"distributor\" is not set to runAsNonRoot"
  },
  {
    "id": "01121",
    "manifest_path": "data/manifests/the_stack_sample/sample_0298.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2881\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01122",
    "manifest_path": "data/manifests/the_stack_sample/sample_0298.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2881\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01123",
    "manifest_path": "data/manifests/the_stack_sample/sample_0298.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2881\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01124",
    "manifest_path": "data/manifests/the_stack_sample/sample_0298.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2881\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01125",
    "manifest_path": "data/manifests/the_stack_sample/sample_0298.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2881\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01126",
    "manifest_path": "data/manifests/the_stack_sample/sample_0301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: theindiangeek/edjx-api:latest\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"api\" is using an invalid container image, \"theindiangeek/edjx-api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01127",
    "manifest_path": "data/manifests/the_stack_sample/sample_0301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: theindiangeek/edjx-api:latest\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api\" does not have a read-only root file system"
  },
  {
    "id": "01128",
    "manifest_path": "data/manifests/the_stack_sample/sample_0301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: theindiangeek/edjx-api:latest\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"api\" is not set to runAsNonRoot"
  },
  {
    "id": "01129",
    "manifest_path": "data/manifests/the_stack_sample/sample_0301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: theindiangeek/edjx-api:latest\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api\" has cpu request 0"
  },
  {
    "id": "01130",
    "manifest_path": "data/manifests/the_stack_sample/sample_0301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: theindiangeek/edjx-api:latest\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api\" has memory limit 0"
  },
  {
    "id": "01131",
    "manifest_path": "data/manifests/the_stack_sample/sample_0303.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-service\n  labels:\n    app: config-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: config-service\n  template:\n    metadata:\n      labels:\n        app: config-service\n    spec:\n      containers:\n      - name: config-service\n        image: polarbookshop/config-service:0.0.1-SNAPSHOT\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8888\n        env:\n        - name: BPL_JVM_THREAD_COUNT\n          value: '50'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"config-service\" does not have a read-only root file system"
  },
  {
    "id": "01132",
    "manifest_path": "data/manifests/the_stack_sample/sample_0303.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-service\n  labels:\n    app: config-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: config-service\n  template:\n    metadata:\n      labels:\n        app: config-service\n    spec:\n      containers:\n      - name: config-service\n        image: polarbookshop/config-service:0.0.1-SNAPSHOT\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8888\n        env:\n        - name: BPL_JVM_THREAD_COUNT\n          value: '50'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"config-service\" is not set to runAsNonRoot"
  },
  {
    "id": "01133",
    "manifest_path": "data/manifests/the_stack_sample/sample_0303.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-service\n  labels:\n    app: config-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: config-service\n  template:\n    metadata:\n      labels:\n        app: config-service\n    spec:\n      containers:\n      - name: config-service\n        image: polarbookshop/config-service:0.0.1-SNAPSHOT\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8888\n        env:\n        - name: BPL_JVM_THREAD_COUNT\n          value: '50'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"config-service\" has cpu request 0"
  },
  {
    "id": "01134",
    "manifest_path": "data/manifests/the_stack_sample/sample_0303.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-service\n  labels:\n    app: config-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: config-service\n  template:\n    metadata:\n      labels:\n        app: config-service\n    spec:\n      containers:\n      - name: config-service\n        image: polarbookshop/config-service:0.0.1-SNAPSHOT\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8888\n        env:\n        - name: BPL_JVM_THREAD_COUNT\n          value: '50'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"config-service\" has memory limit 0"
  },
  {
    "id": "01135",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "01136",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"webinit\" does not have a read-only root file system"
  },
  {
    "id": "01137",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "01138",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"webinit\" is not set to runAsNonRoot"
  },
  {
    "id": "01139",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "01140",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"webinit\" has cpu request 0"
  },
  {
    "id": "01141",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "01142",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"webinit\" has memory limit 0"
  },
  {
    "id": "01143",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"azuredisk\" does not have a read-only root file system"
  },
  {
    "id": "01144",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "01145",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "01146",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-resizer\" does not have a read-only root file system"
  },
  {
    "id": "01147",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-snapshotter\" does not have a read-only root file system"
  },
  {
    "id": "01148",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "01149",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"azuredisk\" is not set to runAsNonRoot"
  },
  {
    "id": "01150",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "01151",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "01152",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-resizer\" is not set to runAsNonRoot"
  },
  {
    "id": "01153",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-snapshotter\" is not set to runAsNonRoot"
  },
  {
    "id": "01154",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "01155",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "01156",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "01157",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "01158",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "01159",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "01160",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "01161",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "01162",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "01163",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "01164",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "01165",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "01166",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable PL_VIZIER_IMAGE_SECRET_FILE in container \"api-server\" found"
  },
  {
    "id": "01167",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable PL_VIZIER_IMAGE_SECRET_PATH in container \"api-server\" found"
  },
  {
    "id": "01168",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"api-server\" is using an invalid container image, \"gcr.io/pl-dev-infra/cloud/api_server_image\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01169",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api-server\" does not have a read-only root file system"
  },
  {
    "id": "01170",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"api-server\" is not set to runAsNonRoot"
  },
  {
    "id": "01171",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api-server\" has cpu request 0"
  },
  {
    "id": "01172",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api-server\" has memory limit 0"
  },
  {
    "id": "01173",
    "manifest_path": "data/manifests/the_stack_sample/sample_0312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mull-frontend\n  namespace: mull\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mull-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mull-frontend\n    spec:\n      containers:\n      - name: mull-frontend\n        image: ritchellegmp/mull-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - name: frontend\n          containerPort: 4200\n          protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mull-frontend\" is using an invalid container image, \"ritchellegmp/mull-frontend:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01174",
    "manifest_path": "data/manifests/the_stack_sample/sample_0312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mull-frontend\n  namespace: mull\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mull-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mull-frontend\n    spec:\n      containers:\n      - name: mull-frontend\n        image: ritchellegmp/mull-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - name: frontend\n          containerPort: 4200\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mull-frontend\" does not have a read-only root file system"
  },
  {
    "id": "01175",
    "manifest_path": "data/manifests/the_stack_sample/sample_0312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mull-frontend\n  namespace: mull\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mull-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mull-frontend\n    spec:\n      containers:\n      - name: mull-frontend\n        image: ritchellegmp/mull-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - name: frontend\n          containerPort: 4200\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mull-frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "01176",
    "manifest_path": "data/manifests/the_stack_sample/sample_0312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mull-frontend\n  namespace: mull\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mull-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mull-frontend\n    spec:\n      containers:\n      - name: mull-frontend\n        image: ritchellegmp/mull-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - name: frontend\n          containerPort: 4200\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mull-frontend\" has cpu request 0"
  },
  {
    "id": "01177",
    "manifest_path": "data/manifests/the_stack_sample/sample_0312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mull-frontend\n  namespace: mull\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mull-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mull-frontend\n    spec:\n      containers:\n      - name: mull-frontend\n        image: ritchellegmp/mull-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - name: frontend\n          containerPort: 4200\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mull-frontend\" has memory limit 0"
  },
  {
    "id": "01178",
    "manifest_path": "data/manifests/the_stack_sample/sample_0315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: replay\n  name: replay\nspec:\n  replicas: 1\n  selector:\n    app: replay\n  template:\n    metadata:\n      labels:\n        app: replay\n    spec:\n      containers:\n      - command:\n        - bash\n        - -c\n        - source /etc/kube-replay/config && node main.js\n        image: paralin/dota-replay:latest\n        imagePullPolicy: Always\n        name: replay\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 10304\n          name: desktop\n        volumeMounts:\n        - mountPath: /etc/kube-replay\n          name: replay-config\n          readOnly: true\n      volumes:\n      - name: replay-config\n        secret:\n          secretName: replay-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"replay\" is using an invalid container image, \"paralin/dota-replay:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01179",
    "manifest_path": "data/manifests/the_stack_sample/sample_0315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: replay\n  name: replay\nspec:\n  replicas: 1\n  selector:\n    app: replay\n  template:\n    metadata:\n      labels:\n        app: replay\n    spec:\n      containers:\n      - command:\n        - bash\n        - -c\n        - source /etc/kube-replay/config && node main.js\n        image: paralin/dota-replay:latest\n        imagePullPolicy: Always\n        name: replay\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 10304\n          name: desktop\n        volumeMounts:\n        - mountPath: /etc/kube-replay\n          name: replay-config\n          readOnly: true\n      volumes:\n      - name: replay-config\n        secret:\n          secretName: replay-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"replay\" does not have a read-only root file system"
  },
  {
    "id": "01180",
    "manifest_path": "data/manifests/the_stack_sample/sample_0315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: replay\n  name: replay\nspec:\n  replicas: 1\n  selector:\n    app: replay\n  template:\n    metadata:\n      labels:\n        app: replay\n    spec:\n      containers:\n      - command:\n        - bash\n        - -c\n        - source /etc/kube-replay/config && node main.js\n        image: paralin/dota-replay:latest\n        imagePullPolicy: Always\n        name: replay\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 10304\n          name: desktop\n        volumeMounts:\n        - mountPath: /etc/kube-replay\n          name: replay-config\n          readOnly: true\n      volumes:\n      - name: replay-config\n        secret:\n          secretName: replay-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"replay\" is not set to runAsNonRoot"
  },
  {
    "id": "01181",
    "manifest_path": "data/manifests/the_stack_sample/sample_0315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: replay\n  name: replay\nspec:\n  replicas: 1\n  selector:\n    app: replay\n  template:\n    metadata:\n      labels:\n        app: replay\n    spec:\n      containers:\n      - command:\n        - bash\n        - -c\n        - source /etc/kube-replay/config && node main.js\n        image: paralin/dota-replay:latest\n        imagePullPolicy: Always\n        name: replay\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 10304\n          name: desktop\n        volumeMounts:\n        - mountPath: /etc/kube-replay\n          name: replay-config\n          readOnly: true\n      volumes:\n      - name: replay-config\n        secret:\n          secretName: replay-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"replay\" has cpu request 0"
  },
  {
    "id": "01182",
    "manifest_path": "data/manifests/the_stack_sample/sample_0315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: replay\n  name: replay\nspec:\n  replicas: 1\n  selector:\n    app: replay\n  template:\n    metadata:\n      labels:\n        app: replay\n    spec:\n      containers:\n      - command:\n        - bash\n        - -c\n        - source /etc/kube-replay/config && node main.js\n        image: paralin/dota-replay:latest\n        imagePullPolicy: Always\n        name: replay\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 10304\n          name: desktop\n        volumeMounts:\n        - mountPath: /etc/kube-replay\n          name: replay-config\n          readOnly: true\n      volumes:\n      - name: replay-config\n        secret:\n          secretName: replay-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"replay\" has memory limit 0"
  },
  {
    "id": "01183",
    "manifest_path": "data/manifests/the_stack_sample/sample_0316.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pdns\n  namespace: default\n  labels:\n    app: pdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pdns\n  template:\n    metadata:\n      labels:\n        app: pdns\n    spec:\n      volumes:\n      - name: service-account\n        secret:\n          secretName: dns-account\n          defaultMode: 256\n      serviceAccountName: pdns\n      containers:\n      - name: service\n        image: tanelmae/private-dns:latest\n        imagePullPolicy: Always\n        args:\n        - -gcp-zone=k8s-dns\n        - -gcp-reverse-zone=k8s-reverse-dns\n        - -gcp-cred=/account/dns.json\n        - -v=4\n        volumeMounts:\n        - name: service-account\n          mountPath: /account\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"service\" is using an invalid container image, \"tanelmae/private-dns:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01184",
    "manifest_path": "data/manifests/the_stack_sample/sample_0316.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pdns\n  namespace: default\n  labels:\n    app: pdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pdns\n  template:\n    metadata:\n      labels:\n        app: pdns\n    spec:\n      volumes:\n      - name: service-account\n        secret:\n          secretName: dns-account\n          defaultMode: 256\n      serviceAccountName: pdns\n      containers:\n      - name: service\n        image: tanelmae/private-dns:latest\n        imagePullPolicy: Always\n        args:\n        - -gcp-zone=k8s-dns\n        - -gcp-reverse-zone=k8s-reverse-dns\n        - -gcp-cred=/account/dns.json\n        - -v=4\n        volumeMounts:\n        - name: service-account\n          mountPath: /account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"service\" does not have a read-only root file system"
  },
  {
    "id": "01185",
    "manifest_path": "data/manifests/the_stack_sample/sample_0316.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pdns\n  namespace: default\n  labels:\n    app: pdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pdns\n  template:\n    metadata:\n      labels:\n        app: pdns\n    spec:\n      volumes:\n      - name: service-account\n        secret:\n          secretName: dns-account\n          defaultMode: 256\n      serviceAccountName: pdns\n      containers:\n      - name: service\n        image: tanelmae/private-dns:latest\n        imagePullPolicy: Always\n        args:\n        - -gcp-zone=k8s-dns\n        - -gcp-reverse-zone=k8s-reverse-dns\n        - -gcp-cred=/account/dns.json\n        - -v=4\n        volumeMounts:\n        - name: service-account\n          mountPath: /account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"service\" is not set to runAsNonRoot"
  },
  {
    "id": "01186",
    "manifest_path": "data/manifests/the_stack_sample/sample_0316.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pdns\n  namespace: default\n  labels:\n    app: pdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pdns\n  template:\n    metadata:\n      labels:\n        app: pdns\n    spec:\n      volumes:\n      - name: service-account\n        secret:\n          secretName: dns-account\n          defaultMode: 256\n      serviceAccountName: pdns\n      containers:\n      - name: service\n        image: tanelmae/private-dns:latest\n        imagePullPolicy: Always\n        args:\n        - -gcp-zone=k8s-dns\n        - -gcp-reverse-zone=k8s-reverse-dns\n        - -gcp-cred=/account/dns.json\n        - -v=4\n        volumeMounts:\n        - name: service-account\n          mountPath: /account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"service\" has cpu request 0"
  },
  {
    "id": "01187",
    "manifest_path": "data/manifests/the_stack_sample/sample_0316.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pdns\n  namespace: default\n  labels:\n    app: pdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pdns\n  template:\n    metadata:\n      labels:\n        app: pdns\n    spec:\n      volumes:\n      - name: service-account\n        secret:\n          secretName: dns-account\n          defaultMode: 256\n      serviceAccountName: pdns\n      containers:\n      - name: service\n        image: tanelmae/private-dns:latest\n        imagePullPolicy: Always\n        args:\n        - -gcp-zone=k8s-dns\n        - -gcp-reverse-zone=k8s-reverse-dns\n        - -gcp-cred=/account/dns.json\n        - -v=4\n        volumeMounts:\n        - name: service-account\n          mountPath: /account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"service\" has memory limit 0"
  },
  {
    "id": "01188",
    "manifest_path": "data/manifests/the_stack_sample/sample_0317.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20220120-e267164240\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "01189",
    "manifest_path": "data/manifests/the_stack_sample/sample_0317.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20220120-e267164240\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "01190",
    "manifest_path": "data/manifests/the_stack_sample/sample_0317.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20220120-e267164240\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "01191",
    "manifest_path": "data/manifests/the_stack_sample/sample_0317.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20220120-e267164240\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "01192",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "01193",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "01194",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "01195",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "01196",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "01197",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "01198",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "01199",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "01200",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "01201",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "01202",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "01203",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "01204",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "01205",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01206",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "01207",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "01208",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cidotnet-pod\" is using an invalid container image, \"markaw/cidotnet-app\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01209",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cidotnet-pod\" does not have a read-only root file system"
  },
  {
    "id": "01210",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cidotnet-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "01211",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cidotnet-pod\" has cpu request 0"
  },
  {
    "id": "01212",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cidotnet-pod\" has memory limit 0"
  },
  {
    "id": "01213",
    "manifest_path": "data/manifests/the_stack_sample/sample_0330.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upstream\nspec:\n  selector:\n    matchLabels:\n      app: upstream\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: upstream\n    spec:\n      containers:\n      - name: upstream\n        image: signalrbenchmark/perf:1.4.4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 1024Mi\n          limits:\n            cpu: 150m\n            memory: 1024Mi\n        volumeMounts:\n        - mountPath: /mnt/perf\n          name: volume\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /mnt/perf/manifest/SignalRUpstream/SignalRUpstream.zip /home ; cd /home\n          ; unzip SignalRUpstream.zip ; exec ./SignalRUpstream\n      volumes:\n      - name: volume\n        azureFile:\n          secretName: azure-secret\n          shareName: perf\n          readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"upstream\" does not have a read-only root file system"
  },
  {
    "id": "01214",
    "manifest_path": "data/manifests/the_stack_sample/sample_0330.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upstream\nspec:\n  selector:\n    matchLabels:\n      app: upstream\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: upstream\n    spec:\n      containers:\n      - name: upstream\n        image: signalrbenchmark/perf:1.4.4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 1024Mi\n          limits:\n            cpu: 150m\n            memory: 1024Mi\n        volumeMounts:\n        - mountPath: /mnt/perf\n          name: volume\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /mnt/perf/manifest/SignalRUpstream/SignalRUpstream.zip /home ; cd /home\n          ; unzip SignalRUpstream.zip ; exec ./SignalRUpstream\n      volumes:\n      - name: volume\n        azureFile:\n          secretName: azure-secret\n          shareName: perf\n          readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"upstream\" is not set to runAsNonRoot"
  },
  {
    "id": "01215",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "01216",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "01217",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "01218",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dhcp-server\" is using an invalid container image, \"xunholy/dhcp-server:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01219",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dhcp-server\" does not have a read-only root file system"
  },
  {
    "id": "01220",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dhcp-server\" is not set to runAsNonRoot"
  },
  {
    "id": "01221",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dhcp-server\" has cpu request 0"
  },
  {
    "id": "01222",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dhcp-server\" has memory limit 0"
  },
  {
    "id": "01223",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"my-cron-job\" does not have a read-only root file system"
  },
  {
    "id": "01224",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"my-cron-job\" is not set to runAsNonRoot"
  },
  {
    "id": "01225",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"my-cron-job\" has cpu request 0"
  },
  {
    "id": "01226",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "01227",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "01228",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "01229",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"webhook\" is using an invalid container image, \"knative.dev/serving/cmd/webhook\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01230",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"webhook\" does not have a read-only root file system"
  },
  {
    "id": "01231",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"webhook\" is not set to runAsNonRoot"
  },
  {
    "id": "01232",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "01233",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "01234",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "01235",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "01236",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "01237",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "01238",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "01239",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "01240",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"npm\" is using an invalid container image, \"hub.opshub.sh/containerops/dependence-nodejs-npm:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01241",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"npm\" does not have a read-only root file system"
  },
  {
    "id": "01242",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"npm\" is not set to runAsNonRoot"
  },
  {
    "id": "01243",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"npm\" has memory limit 0"
  },
  {
    "id": "01244",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01245",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01246",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "01247",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "01248",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "01249",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "01250",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  }
]