[
  {
    "id": "00441",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/064_deployment_release-name-kube-promethe-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-prometheus-stack-operator\n      release: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app: kube-prometheus-stack-operator\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator\n    spec:\n      containers:\n      - name: kube-prometheus-stack\n        image: quay.io/prometheus-operator/prometheus-operator:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --kubelet-service=kube-system/release-name-kube-promethe-kubelet\n        - --kubelet-endpoints=true\n        - --kubelet-endpointslice=false\n        - --localhost=127.0.0.1\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        - --config-reloader-cpu-request=0\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-request=0\n        - --config-reloader-memory-limit=0\n        - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.2\n        - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\n        - --web.enable-tls=true\n        - --web.cert-file=/cert/cert\n        - --web.key-file=/cert/key\n        - --web.listen-address=:10250\n        - --web.tls-min-version=VersionTLS13\n        ports:\n        - containerPort: 10250\n          name: https\n        env:\n        - name: GOGC\n          value: '30'\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: tls-secret\n          mountPath: /cert\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: tls-secret\n        secret:\n          defaultMode: 420\n          secretName: release-name-kube-promethe-admission\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: release-name-kube-promethe-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-prometheus-stack\" has memory limit 0"
  },
  {
    "id": "00442",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-test\" does not have a read-only root file system"
  },
  {
    "id": "00443",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"release-name-test\" is not set to runAsNonRoot"
  },
  {
    "id": "00444",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-test\" has cpu request 0"
  },
  {
    "id": "00445",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-test\" has memory limit 0"
  },
  {
    "id": "00446",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/125_job_release-name-kube-promethe-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"create\" has cpu request 0"
  },
  {
    "id": "00447",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/125_job_release-name-kube-promethe-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"create\" has memory limit 0"
  },
  {
    "id": "00448",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/126_job_release-name-kube-promethe-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"patch\" has cpu request 0"
  },
  {
    "id": "00449",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/126_job_release-name-kube-promethe-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"patch\" has memory limit 0"
  },
  {
    "id": "00450",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "00451",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "00452",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/020_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-state-metrics\" has cpu request 0"
  },
  {
    "id": "00453",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/020_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "00454",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pushgateway\" does not have a read-only root file system"
  },
  {
    "id": "00455",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pushgateway\" has cpu request 0"
  },
  {
    "id": "00456",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pushgateway\" has memory limit 0"
  },
  {
    "id": "00457",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-server\" does not have a read-only root file system"
  },
  {
    "id": "00458",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-server-configmap-reload\" does not have a read-only root file system"
  },
  {
    "id": "00459",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus-server\" has cpu request 0"
  },
  {
    "id": "00460",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus-server-configmap-reload\" has cpu request 0"
  },
  {
    "id": "00461",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus-server\" has memory limit 0"
  },
  {
    "id": "00462",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus-server-configmap-reload\" has memory limit 0"
  },
  {
    "id": "00463",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"alertmanager\" does not have a read-only root file system"
  },
  {
    "id": "00464",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"alertmanager\" has cpu request 0"
  },
  {
    "id": "00465",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"alertmanager\" has memory limit 0"
  },
  {
    "id": "00466",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/005_deployment_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pgadmin4\" does not have a read-only root file system"
  },
  {
    "id": "00467",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/005_deployment_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pgadmin4\" has cpu request 0"
  },
  {
    "id": "00468",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/005_deployment_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pgadmin4\" has memory limit 0"
  },
  {
    "id": "00469",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"docker.io/busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00470",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wget\" has cpu request 0"
  },
  {
    "id": "00471",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wget\" has memory limit 0"
  },
  {
    "id": "00472",
    "manifest_path": "data/manifests/artifacthub/traefik/traefik/005_deployment_release-name-traefik.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-traefik\" has cpu request 0"
  },
  {
    "id": "00473",
    "manifest_path": "data/manifests/artifacthub/traefik/traefik/005_deployment_release-name-traefik.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-traefik\" has memory limit 0"
  },
  {
    "id": "00474",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      serviceAccountName: release-name-velero-server\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"velero\" does not have a read-only root file system"
  },
  {
    "id": "00475",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      serviceAccountName: release-name-velero-server\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"velero\" is not set to runAsNonRoot"
  },
  {
    "id": "00476",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      serviceAccountName: release-name-velero-server\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"velero\" has cpu request 0"
  },
  {
    "id": "00477",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      serviceAccountName: release-name-velero-server\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"velero\" has memory limit 0"
  },
  {
    "id": "00478",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubectl\" does not have a read-only root file system"
  },
  {
    "id": "00479",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"velero\" does not have a read-only root file system"
  },
  {
    "id": "00480",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubectl\" is not set to runAsNonRoot"
  }
]