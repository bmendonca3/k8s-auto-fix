[
  {
    "id": "01201",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "01202",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "01203",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "01204",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "01205",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01206",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "01207",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "01208",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cidotnet-pod\" is using an invalid container image, \"markaw/cidotnet-app\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01209",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cidotnet-pod\" does not have a read-only root file system"
  },
  {
    "id": "01210",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cidotnet-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "01211",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cidotnet-pod\" has cpu request 0"
  },
  {
    "id": "01212",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cidotnet-pod\" has memory limit 0"
  },
  {
    "id": "01213",
    "manifest_path": "data/manifests/the_stack_sample/sample_0330.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upstream\nspec:\n  selector:\n    matchLabels:\n      app: upstream\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: upstream\n    spec:\n      containers:\n      - name: upstream\n        image: signalrbenchmark/perf:1.4.4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 1024Mi\n          limits:\n            cpu: 150m\n            memory: 1024Mi\n        volumeMounts:\n        - mountPath: /mnt/perf\n          name: volume\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /mnt/perf/manifest/SignalRUpstream/SignalRUpstream.zip /home ; cd /home\n          ; unzip SignalRUpstream.zip ; exec ./SignalRUpstream\n      volumes:\n      - name: volume\n        azureFile:\n          secretName: azure-secret\n          shareName: perf\n          readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"upstream\" does not have a read-only root file system"
  },
  {
    "id": "01214",
    "manifest_path": "data/manifests/the_stack_sample/sample_0330.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upstream\nspec:\n  selector:\n    matchLabels:\n      app: upstream\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: upstream\n    spec:\n      containers:\n      - name: upstream\n        image: signalrbenchmark/perf:1.4.4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 1024Mi\n          limits:\n            cpu: 150m\n            memory: 1024Mi\n        volumeMounts:\n        - mountPath: /mnt/perf\n          name: volume\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /mnt/perf/manifest/SignalRUpstream/SignalRUpstream.zip /home ; cd /home\n          ; unzip SignalRUpstream.zip ; exec ./SignalRUpstream\n      volumes:\n      - name: volume\n        azureFile:\n          secretName: azure-secret\n          shareName: perf\n          readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"upstream\" is not set to runAsNonRoot"
  },
  {
    "id": "01215",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "01216",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "01217",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "01218",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dhcp-server\" is using an invalid container image, \"xunholy/dhcp-server:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01219",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dhcp-server\" does not have a read-only root file system"
  },
  {
    "id": "01220",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dhcp-server\" is not set to runAsNonRoot"
  },
  {
    "id": "01221",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dhcp-server\" has cpu request 0"
  },
  {
    "id": "01222",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dhcp-server\" has memory limit 0"
  },
  {
    "id": "01223",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"my-cron-job\" does not have a read-only root file system"
  },
  {
    "id": "01224",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"my-cron-job\" is not set to runAsNonRoot"
  },
  {
    "id": "01225",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"my-cron-job\" has cpu request 0"
  },
  {
    "id": "01226",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "01227",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "01228",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "01229",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"webhook\" is using an invalid container image, \"knative.dev/serving/cmd/webhook\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01230",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"webhook\" does not have a read-only root file system"
  },
  {
    "id": "01231",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"webhook\" is not set to runAsNonRoot"
  },
  {
    "id": "01232",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "01233",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "01234",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "01235",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "01236",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "01237",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "01238",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "01239",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "01240",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"npm\" is using an invalid container image, \"hub.opshub.sh/containerops/dependence-nodejs-npm:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  }
]