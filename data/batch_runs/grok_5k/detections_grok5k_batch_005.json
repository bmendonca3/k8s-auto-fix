[
  {
    "id": "00201",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "00202",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"config\" has memory limit 0"
  },
  {
    "id": "00203",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"install-cni-binaries\" has memory limit 0"
  },
  {
    "id": "00204",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mount-bpf-fs\" has memory limit 0"
  },
  {
    "id": "00205",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mount-cgroup\" has memory limit 0"
  },
  {
    "id": "00206",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-envoy\" does not have a read-only root file system"
  },
  {
    "id": "00207",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-envoy\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "00208",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-envoy\" is not set to runAsNonRoot"
  },
  {
    "id": "00209",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-envoy\" has cpu request 0"
  },
  {
    "id": "00210",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-envoy\" has memory limit 0"
  },
  {
    "id": "00211",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-operator\" does not have a read-only root file system"
  },
  {
    "id": "00212",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "00213",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-operator\" has cpu request 0"
  },
  {
    "id": "00214",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-operator\" has memory limit 0"
  },
  {
    "id": "00215",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-volume\" does not have a read-only root file system"
  },
  {
    "id": "00216",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cluster-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "00217",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-volume\" is not set to runAsNonRoot"
  },
  {
    "id": "00218",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cluster-agent\" has cpu request 0"
  },
  {
    "id": "00219",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-volume\" has cpu request 0"
  },
  {
    "id": "00220",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cluster-agent\" has memory limit 0"
  },
  {
    "id": "00221",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-volume\" has memory limit 0"
  },
  {
    "id": "00222",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"configure-sysctl\" does not have a read-only root file system"
  },
  {
    "id": "00223",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"elasticsearch\" does not have a read-only root file system"
  },
  {
    "id": "00224",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"configure-sysctl\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "00225",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"configure-sysctl\" is privileged"
  },
  {
    "id": "00226",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"configure-sysctl\" is not set to runAsNonRoot"
  },
  {
    "id": "00227",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"configure-sysctl\" has cpu request 0"
  },
  {
    "id": "00228",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"configure-sysctl\" has memory limit 0"
  },
  {
    "id": "00229",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-mvgmy-test\" does not have a read-only root file system"
  },
  {
    "id": "00230",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-mvgmy-test\" has cpu request 0"
  },
  {
    "id": "00231",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-mvgmy-test\" has memory limit 0"
  },
  {
    "id": "00232",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"external-dns\" has cpu request 0"
  },
  {
    "id": "00233",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"external-dns\" has memory limit 0"
  },
  {
    "id": "00234",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-controller\" has cpu request 0"
  },
  {
    "id": "00235",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-controller\" has memory limit 0"
  },
  {
    "id": "00236",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"external-secrets\" has cpu request 0"
  },
  {
    "id": "00237",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"external-secrets\" has memory limit 0"
  },
  {
    "id": "00238",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-webhook\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"webhook\" has cpu request 0"
  },
  {
    "id": "00239",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-webhook\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"webhook\" has memory limit 0"
  },
  {
    "id": "00240",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluent-bit\" does not have a read-only root file system"
  }
]