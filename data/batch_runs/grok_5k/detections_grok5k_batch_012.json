[
  {
    "id": "00481",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"velero\" is not set to runAsNonRoot"
  },
  {
    "id": "00482",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubectl\" has cpu request 0"
  },
  {
    "id": "00483",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"velero\" has cpu request 0"
  },
  {
    "id": "00484",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubectl\" has memory limit 0"
  },
  {
    "id": "00485",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"velero\" has memory limit 0"
  },
  {
    "id": "00486",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pulsar-admin\" is using an invalid container image, \"apachepulsar/pulsar:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00487",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pulsar-admin\" does not have a read-only root file system"
  },
  {
    "id": "00488",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pulsar-admin\" is not set to runAsNonRoot"
  },
  {
    "id": "00489",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pulsar-admin\" has cpu request 0"
  },
  {
    "id": "00490",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pulsar-admin\" has memory limit 0"
  },
  {
    "id": "00491",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"datasets-lib-unb-ca\" is using an invalid container image, \"||DEPLOYMENTIMAGE||\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00492",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"datasets-lib-unb-ca\" does not have a read-only root file system"
  },
  {
    "id": "00493",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"datasets-lib-unb-ca\" is not set to runAsNonRoot"
  },
  {
    "id": "00494",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"datasets-lib-unb-ca\" has cpu request 0"
  },
  {
    "id": "00495",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"datasets-lib-unb-ca\" has memory limit 0"
  },
  {
    "id": "00496",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"metrics\" does not have a read-only root file system"
  },
  {
    "id": "00497",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "00498",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"metrics\" has cpu request 0"
  },
  {
    "id": "00499",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"metrics\" has memory limit 0"
  },
  {
    "id": "00500",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "00501",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "00502",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "00503",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "00504",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"healthcheck-ready\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00505",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"holder-vcs-add-profiles\" is using an invalid container image, \"alpine:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00506",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00507",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"healthcheck-ready\" does not have a read-only root file system"
  },
  {
    "id": "00508",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"holder-vcs-add-profiles\" does not have a read-only root file system"
  },
  {
    "id": "00509",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait\" does not have a read-only root file system"
  },
  {
    "id": "00510",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"healthcheck-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "00511",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"holder-vcs-add-profiles\" is not set to runAsNonRoot"
  },
  {
    "id": "00512",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait\" is not set to runAsNonRoot"
  },
  {
    "id": "00513",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"healthcheck-ready\" has cpu request 0"
  },
  {
    "id": "00514",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"holder-vcs-add-profiles\" has cpu request 0"
  },
  {
    "id": "00515",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait\" has cpu request 0"
  },
  {
    "id": "00516",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"healthcheck-ready\" has memory limit 0"
  },
  {
    "id": "00517",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"holder-vcs-add-profiles\" has memory limit 0"
  },
  {
    "id": "00518",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait\" has memory limit 0"
  },
  {
    "id": "00519",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00520",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  }
]