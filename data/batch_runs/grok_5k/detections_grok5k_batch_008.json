[
  {
    "id": "00321",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/013_pod_release-name-vault-server-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-server-test\" has cpu request 0"
  },
  {
    "id": "00322",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/013_pod_release-name-vault-server-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-server-test\" has memory limit 0"
  },
  {
    "id": "00323",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/009_deployment_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      serviceAccountName: release-name-ingress-nginx\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"controller\" does not have a read-only root file system"
  },
  {
    "id": "00324",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/009_deployment_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      serviceAccountName: release-name-ingress-nginx\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "00325",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/017_job_release-name-ingress-nginx-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      serviceAccountName: release-name-ingress-nginx-admission\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"create\" has cpu request 0"
  },
  {
    "id": "00326",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/017_job_release-name-ingress-nginx-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      serviceAccountName: release-name-ingress-nginx-admission\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"create\" has memory limit 0"
  },
  {
    "id": "00327",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/018_job_release-name-ingress-nginx-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      serviceAccountName: release-name-ingress-nginx-admission\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"patch\" has cpu request 0"
  },
  {
    "id": "00328",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/018_job_release-name-ingress-nginx-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      serviceAccountName: release-name-ingress-nginx-admission\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"patch\" has memory limit 0"
  },
  {
    "id": "00329",
    "manifest_path": "data/manifests/artifacthub/istio-official/istiod/015_deployment_istiod.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    istio: pilot\n    release: release-name\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: 'false'\n        operator.istio.io/component: Pilot\n        istio: pilot\n        istio.io/dataplane-mode: none\n        app.kubernetes.io/name: istiod\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/part-of: istio\n        app.kubernetes.io/version: 1.27.1\n        helm.sh/chart: istiod-1.27.1\n      annotations:\n        prometheus.io/port: '15014'\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: istiod\n      containers:\n      - name: discovery\n        image: docker.io/istio/pilot:1.27.1\n        args:\n        - discovery\n        - --monitoringAddr=:15014\n        - --log_output_level=default:info\n        - --domain\n        - cluster.local\n        - --keepaliveMaxServerConnectionAge\n        - 30m\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: http-debug\n        - containerPort: 15010\n          protocol: TCP\n          name: grpc-xds\n        - containerPort: 15012\n          protocol: TCP\n          name: tls-xds\n        - containerPort: 15017\n          protocol: TCP\n          name: https-webhooks\n        - containerPort: 15014\n          protocol: TCP\n          name: http-monitoring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 3\n          timeoutSeconds: 5\n        env:\n        - name: REVISION\n          value: default\n        - name: PILOT_CERT_PROVIDER\n          value: istiod\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.serviceAccountName\n        - name: KUBECONFIG\n          value: /var/run/secrets/remote/config\n        - name: CA_TRUSTED_NODE_ACCOUNTS\n          value: default/ztunnel\n        - name: PILOT_TRACE_SAMPLING\n          value: '1'\n        - name: PILOT_ENABLE_ANALYSIS\n          value: 'false'\n        - name: CLUSTER_ID\n          value: Kubernetes\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: PLATFORM\n          value: ''\n        resources:\n          requests:\n            cpu: 500m\n            memory: 2048Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: istio-token\n          mountPath: /var/run/secrets/tokens\n          readOnly: true\n        - name: local-certs\n          mountPath: /var/run/secrets/istio-dns\n        - name: cacerts\n          mountPath: /etc/cacerts\n          readOnly: true\n        - name: istio-kubeconfig\n          mountPath: /var/run/secrets/remote\n          readOnly: true\n        - name: istio-csr-dns-cert\n          mountPath: /var/run/secrets/istiod/tls\n          readOnly: true\n        - name: istio-csr-ca-configmap\n          mountPath: /var/run/secrets/istiod/ca\n          readOnly: true\n      volumes:\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              audience: istio-ca\n              expirationSeconds: 43200\n              path: istio-token\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n      - name: istio-csr-dns-cert\n        secret:\n          secretName: istiod-tls\n          optional: true\n      - name: istio-csr-ca-configmap\n        configMap:\n          name: istio-ca-root-cert\n          defaultMode: 420\n          optional: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"discovery\" has memory limit 0"
  },
  {
    "id": "00330",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable SECRETS in container \"jenkins\" found"
  },
  {
    "id": "00331",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"config-reload\" has cpu request 0"
  },
  {
    "id": "00332",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"config-reload-init\" has cpu request 0"
  },
  {
    "id": "00333",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"config-reload\" has memory limit 0"
  },
  {
    "id": "00334",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"config-reload-init\" has memory limit 0"
  },
  {
    "id": "00335",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-ui-test\" does not have a read-only root file system"
  },
  {
    "id": "00336",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-framework\" does not have a read-only root file system"
  },
  {
    "id": "00337",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"release-name-ui-test\" is not set to runAsNonRoot"
  },
  {
    "id": "00338",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-framework\" is not set to runAsNonRoot"
  },
  {
    "id": "00339",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-ui-test\" has cpu request 0"
  },
  {
    "id": "00340",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-framework\" has cpu request 0"
  },
  {
    "id": "00341",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-ui-test\" has memory limit 0"
  },
  {
    "id": "00342",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-framework\" has memory limit 0"
  },
  {
    "id": "00343",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clear-stale-pid\" has cpu request 0"
  },
  {
    "id": "00344",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"proxy\" has cpu request 0"
  },
  {
    "id": "00345",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clear-stale-pid\" has memory limit 0"
  },
  {
    "id": "00346",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"proxy\" has memory limit 0"
  },
  {
    "id": "00347",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-manager\" does not have a read-only root file system"
  },
  {
    "id": "00348",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pre-pull-share-manager-image\" does not have a read-only root file system"
  },
  {
    "id": "00349",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"longhorn-manager\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "00350",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"longhorn-manager\" is privileged"
  },
  {
    "id": "00351",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "00352",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pre-pull-share-manager-image\" is not set to runAsNonRoot"
  },
  {
    "id": "00353",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-manager\" has cpu request 0"
  },
  {
    "id": "00354",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pre-pull-share-manager-image\" has cpu request 0"
  },
  {
    "id": "00355",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-manager\" has memory limit 0"
  },
  {
    "id": "00356",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pre-pull-share-manager-image\" has memory limit 0"
  },
  {
    "id": "00357",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-driver-deployer\" does not have a read-only root file system"
  },
  {
    "id": "00358",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-longhorn-manager\" does not have a read-only root file system"
  },
  {
    "id": "00359",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-driver-deployer\" is not set to runAsNonRoot"
  },
  {
    "id": "00360",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-longhorn-manager\" is not set to runAsNonRoot"
  }
]