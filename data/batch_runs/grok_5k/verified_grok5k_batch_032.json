[
  {
    "id": "01281",
    "policy_id": "read_only_root_fs",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01282",
    "policy_id": "read_only_root_fs",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01283",
    "policy_id": "run_as_non_root",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01284",
    "policy_id": "run_as_non_root",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01285",
    "policy_id": "set_requests_limits",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01286",
    "policy_id": "set_requests_limits",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01287",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:stable\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator:stable\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01288",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:stable\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator:stable\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        securityContext:\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01289",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:stable\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator:stable\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01290",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:stable\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator:stable\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01291",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:stable\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator:stable\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01292",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:stable\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      containers:\n      - name: main\n        image: loadgenerator:stable\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01293",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:stable\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      containers:\n      - name: main\n        image: loadgenerator:stable\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01294",
    "policy_id": "read_only_root_fs",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01295",
    "policy_id": "run_as_non_root",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01296",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01297",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01298",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n    securityContext:\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      privileged: false\n      allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01299",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01300",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01301",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01302",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01303",
    "policy_id": "read_only_root_fs",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": false,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "no containers found in manifest",
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01304",
    "policy_id": "run_as_non_root",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": false,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "no containers found in manifest",
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01305",
    "policy_id": "set_requests_limits",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": false,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "no containers found in manifest",
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01306",
    "policy_id": "set_requests_limits",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": false,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "no containers found in manifest",
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01307",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini:stable\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n    securityContext:\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      privileged: false\n      allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01308",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini:stable\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01309",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini:stable\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01310",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini:stable\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01311",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini:stable\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01312",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cos\n  labels:\n    app.kubernetes.io/component: cos\n    app.kubernetes.io/instance: cos\n    app.kubernetes.io/name: cos\n    app.kubernetes.io/part-of: cos\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cos\n      app.kubernetes.io/instance: cos\n      app.kubernetes.io/name: cos\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cos\n        app.kubernetes.io/instance: cos\n        app.kubernetes.io/name: cos\n    spec:\n      containers:\n      - image: nginx:1.20-alpine\n        name: cos\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01313",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cos\n  labels:\n    app.kubernetes.io/component: cos\n    app.kubernetes.io/instance: cos\n    app.kubernetes.io/name: cos\n    app.kubernetes.io/part-of: cos\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cos\n      app.kubernetes.io/instance: cos\n      app.kubernetes.io/name: cos\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cos\n        app.kubernetes.io/instance: cos\n        app.kubernetes.io/name: cos\n    spec:\n      containers:\n      - image: nginx:1.20-alpine\n        name: cos\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n",
    "errors": []
  },
  {
    "id": "01314",
    "policy_id": "no_latest_tag",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01315",
    "policy_id": "read_only_root_fs",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01316",
    "policy_id": "run_as_non_root",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01317",
    "policy_id": "set_requests_limits",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01318",
    "policy_id": "set_requests_limits",
    "accepted": false,
    "ok_schema": false,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": null,
    "errors": [
      "kubectl dry-run failed"
    ]
  },
  {
    "id": "01319",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "01320",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "ok_safety": true,
    "ok_rescan": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          allowPrivilegeEscalation: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  }
]