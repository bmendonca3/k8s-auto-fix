[
  {
    "id": "03601",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tmp-container\" is using an invalid container image, \"contacts1-server:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03602",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"migration\" does not have a read-only root file system"
  },
  {
    "id": "03603",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tmp-container\" does not have a read-only root file system"
  },
  {
    "id": "03604",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"migration\" is not set to runAsNonRoot"
  },
  {
    "id": "03605",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tmp-container\" is not set to runAsNonRoot"
  },
  {
    "id": "03606",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"migration\" has cpu request 0"
  },
  {
    "id": "03607",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tmp-container\" has cpu request 0"
  },
  {
    "id": "03608",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"migration\" has memory limit 0"
  },
  {
    "id": "03609",
    "manifest_path": "data/manifests/the_stack_sample/sample_1598.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: contacts1-migration\nspec:\n  initContainers:\n  - name: tmp-container\n    image: contacts1-server:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - cp\n    - -r\n    - /db/migrations\n    - /atlas-migrations\n  containers:\n  - name: migration\n    env:\n    - name: CONFIG_FILE\n      value: defaults\n    image: infoblox/migrate:latest\n    imagePullPolicy: Always\n    volumeMounts:\n    - mountPath: /atlas-migrations\n      name: migrations\n    command:\n    - /migrate\n    args:\n    - --verbose\n    - --source\n    - file://atlas-migrations/migrations\n    - --database.address\n    - postgres.default:5432\n    - --database.name\n    - contacts1\n    - --database.user\n    - postgres\n    - --database.password\n    - postgres\n    - up\n  volumes:\n  - name: migrations\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tmp-container\" has memory limit 0"
  },
  {
    "id": "03610",
    "manifest_path": "data/manifests/the_stack_sample/sample_1599.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2360\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03611",
    "manifest_path": "data/manifests/the_stack_sample/sample_1599.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2360\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03612",
    "manifest_path": "data/manifests/the_stack_sample/sample_1599.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2360\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03613",
    "manifest_path": "data/manifests/the_stack_sample/sample_1599.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2360\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03614",
    "manifest_path": "data/manifests/the_stack_sample/sample_1599.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2360\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03615",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"priv-exec-replicaset\" is using an invalid container image, \"ubuntu\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03616",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"priv-exec-replicaset\" does not have a read-only root file system"
  },
  {
    "id": "03617",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"priv-exec-replicaset\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "03618",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"priv-exec-replicaset\" is privileged"
  },
  {
    "id": "03619",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"priv-exec-replicaset\" is not set to runAsNonRoot"
  },
  {
    "id": "03620",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"priv-exec-replicaset\" has cpu request 0"
  },
  {
    "id": "03621",
    "manifest_path": "data/manifests/the_stack_sample/sample_1604.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: priv-exec-replicaset\n  labels:\n    app: pentest\n    type: replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      type: replicaset\n  template:\n    metadata:\n      labels:\n        app: pentest\n        type: replicaset\n    spec:\n      containers:\n      - name: priv-exec-replicaset\n        image: ubuntu\n        securityContext:\n          privileged: true\n        command:\n        - /bin/sh\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"priv-exec-replicaset\" has memory limit 0"
  },
  {
    "id": "03622",
    "manifest_path": "data/manifests/the_stack_sample/sample_1605.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-c2195422bf\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prow-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "03623",
    "manifest_path": "data/manifests/the_stack_sample/sample_1605.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-c2195422bf\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prow-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "03624",
    "manifest_path": "data/manifests/the_stack_sample/sample_1605.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-c2195422bf\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prow-controller-manager\" has cpu request 0"
  },
  {
    "id": "03625",
    "manifest_path": "data/manifests/the_stack_sample/sample_1605.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-c2195422bf\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prow-controller-manager\" has memory limit 0"
  },
  {
    "id": "03626",
    "manifest_path": "data/manifests/the_stack_sample/sample_1606.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: ServerIP\n          value: 192.168.1.100\n        - name: WEBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pihole\n              key: ADMIN_PASSWORD\n        image: pihole/pihole\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 80\n          name: pihole\n          protocol: TCP\n        - containerPort: 53\n          name: dns\n          protocol: TCP\n        - containerPort: 53\n          name: dns-udp\n          protocol: UDP\n        - containerPort: 443\n          name: pihole-ssl\n          protocol: TCP\n        - containerPort: 67\n          name: client-udp\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /etc/pihole\n          name: config\n        - mountPath: /etc/dnsmasq.d/02-custom.conf\n          name: custom-dnsmasq\n          subPath: 02-custom.conf\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: pihole-pvc\n      - configMap:\n          defaultMode: 420\n          name: pihole-custom-dnsmasq\n        name: custom-dnsmasq\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pihole\" is using an invalid container image, \"pihole/pihole\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03627",
    "manifest_path": "data/manifests/the_stack_sample/sample_1606.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: ServerIP\n          value: 192.168.1.100\n        - name: WEBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pihole\n              key: ADMIN_PASSWORD\n        image: pihole/pihole\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 80\n          name: pihole\n          protocol: TCP\n        - containerPort: 53\n          name: dns\n          protocol: TCP\n        - containerPort: 53\n          name: dns-udp\n          protocol: UDP\n        - containerPort: 443\n          name: pihole-ssl\n          protocol: TCP\n        - containerPort: 67\n          name: client-udp\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /etc/pihole\n          name: config\n        - mountPath: /etc/dnsmasq.d/02-custom.conf\n          name: custom-dnsmasq\n          subPath: 02-custom.conf\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: pihole-pvc\n      - configMap:\n          defaultMode: 420\n          name: pihole-custom-dnsmasq\n        name: custom-dnsmasq\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pihole\" does not have a read-only root file system"
  },
  {
    "id": "03628",
    "manifest_path": "data/manifests/the_stack_sample/sample_1606.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: ServerIP\n          value: 192.168.1.100\n        - name: WEBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pihole\n              key: ADMIN_PASSWORD\n        image: pihole/pihole\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 80\n          name: pihole\n          protocol: TCP\n        - containerPort: 53\n          name: dns\n          protocol: TCP\n        - containerPort: 53\n          name: dns-udp\n          protocol: UDP\n        - containerPort: 443\n          name: pihole-ssl\n          protocol: TCP\n        - containerPort: 67\n          name: client-udp\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /etc/pihole\n          name: config\n        - mountPath: /etc/dnsmasq.d/02-custom.conf\n          name: custom-dnsmasq\n          subPath: 02-custom.conf\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: pihole-pvc\n      - configMap:\n          defaultMode: 420\n          name: pihole-custom-dnsmasq\n        name: custom-dnsmasq\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pihole\" is not set to runAsNonRoot"
  },
  {
    "id": "03629",
    "manifest_path": "data/manifests/the_stack_sample/sample_1606.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: ServerIP\n          value: 192.168.1.100\n        - name: WEBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pihole\n              key: ADMIN_PASSWORD\n        image: pihole/pihole\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 80\n          name: pihole\n          protocol: TCP\n        - containerPort: 53\n          name: dns\n          protocol: TCP\n        - containerPort: 53\n          name: dns-udp\n          protocol: UDP\n        - containerPort: 443\n          name: pihole-ssl\n          protocol: TCP\n        - containerPort: 67\n          name: client-udp\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /etc/pihole\n          name: config\n        - mountPath: /etc/dnsmasq.d/02-custom.conf\n          name: custom-dnsmasq\n          subPath: 02-custom.conf\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: pihole-pvc\n      - configMap:\n          defaultMode: 420\n          name: pihole-custom-dnsmasq\n        name: custom-dnsmasq\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pihole\" has cpu request 0"
  },
  {
    "id": "03630",
    "manifest_path": "data/manifests/the_stack_sample/sample_1606.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: ServerIP\n          value: 192.168.1.100\n        - name: WEBPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pihole\n              key: ADMIN_PASSWORD\n        image: pihole/pihole\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - containerPort: 80\n          name: pihole\n          protocol: TCP\n        - containerPort: 53\n          name: dns\n          protocol: TCP\n        - containerPort: 53\n          name: dns-udp\n          protocol: UDP\n        - containerPort: 443\n          name: pihole-ssl\n          protocol: TCP\n        - containerPort: 67\n          name: client-udp\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /etc/pihole\n          name: config\n        - mountPath: /etc/dnsmasq.d/02-custom.conf\n          name: custom-dnsmasq\n          subPath: 02-custom.conf\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: pihole-pvc\n      - configMap:\n          defaultMode: 420\n          name: pihole-custom-dnsmasq\n        name: custom-dnsmasq\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pihole\" has memory limit 0"
  },
  {
    "id": "03631",
    "manifest_path": "data/manifests/the_stack_sample/sample_1607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: proxy-deployment-canary\n  labels:\n    app: proxy-canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: proxy-canary\n  template:\n    metadata:\n      labels:\n        app: proxy-canary\n    spec:\n      containers:\n      - name: proxy-canary\n        image: gcr.io/GCP_PROJECT/IMAGE_NAME:bazel\n        ports:\n        - containerPort: 30000\n          name: health-check\n        - containerPort: 30001\n          name: whois\n        - containerPort: 30002\n          name: epp\n        - containerPort: 30010\n          name: http-whois\n        - containerPort: 30011\n          name: https-whois\n        readinessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        args:\n        - --env\n        - sandbox_canary\n        - --log\n        env:\n        - name: POD_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTAINER_NAME\n          value: proxy-canary\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"proxy-canary\" does not have a read-only root file system"
  },
  {
    "id": "03632",
    "manifest_path": "data/manifests/the_stack_sample/sample_1607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: proxy-deployment-canary\n  labels:\n    app: proxy-canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: proxy-canary\n  template:\n    metadata:\n      labels:\n        app: proxy-canary\n    spec:\n      containers:\n      - name: proxy-canary\n        image: gcr.io/GCP_PROJECT/IMAGE_NAME:bazel\n        ports:\n        - containerPort: 30000\n          name: health-check\n        - containerPort: 30001\n          name: whois\n        - containerPort: 30002\n          name: epp\n        - containerPort: 30010\n          name: http-whois\n        - containerPort: 30011\n          name: https-whois\n        readinessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        args:\n        - --env\n        - sandbox_canary\n        - --log\n        env:\n        - name: POD_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTAINER_NAME\n          value: proxy-canary\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"proxy-canary\" is not set to runAsNonRoot"
  },
  {
    "id": "03633",
    "manifest_path": "data/manifests/the_stack_sample/sample_1607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: proxy-deployment-canary\n  labels:\n    app: proxy-canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: proxy-canary\n  template:\n    metadata:\n      labels:\n        app: proxy-canary\n    spec:\n      containers:\n      - name: proxy-canary\n        image: gcr.io/GCP_PROJECT/IMAGE_NAME:bazel\n        ports:\n        - containerPort: 30000\n          name: health-check\n        - containerPort: 30001\n          name: whois\n        - containerPort: 30002\n          name: epp\n        - containerPort: 30010\n          name: http-whois\n        - containerPort: 30011\n          name: https-whois\n        readinessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        args:\n        - --env\n        - sandbox_canary\n        - --log\n        env:\n        - name: POD_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTAINER_NAME\n          value: proxy-canary\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"proxy-canary\" has cpu request 0"
  },
  {
    "id": "03634",
    "manifest_path": "data/manifests/the_stack_sample/sample_1607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: proxy-deployment-canary\n  labels:\n    app: proxy-canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: proxy-canary\n  template:\n    metadata:\n      labels:\n        app: proxy-canary\n    spec:\n      containers:\n      - name: proxy-canary\n        image: gcr.io/GCP_PROJECT/IMAGE_NAME:bazel\n        ports:\n        - containerPort: 30000\n          name: health-check\n        - containerPort: 30001\n          name: whois\n        - containerPort: 30002\n          name: epp\n        - containerPort: 30010\n          name: http-whois\n        - containerPort: 30011\n          name: https-whois\n        readinessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: health-check\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        args:\n        - --env\n        - sandbox_canary\n        - --log\n        env:\n        - name: POD_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTAINER_NAME\n          value: proxy-canary\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"proxy-canary\" has memory limit 0"
  },
  {
    "id": "03635",
    "manifest_path": "data/manifests/the_stack_sample/sample_1609.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.38\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: ffe957e0de307ee6ef1778ba06a6b8d84591e23040e3e9cd38b5b063ff9194e3\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.38\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: bvboca\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.38\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-keeper\" does not have a read-only root file system"
  },
  {
    "id": "03636",
    "manifest_path": "data/manifests/the_stack_sample/sample_1609.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.38\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: ffe957e0de307ee6ef1778ba06a6b8d84591e23040e3e9cd38b5b063ff9194e3\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.38\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: bvboca\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.38\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-keeper\" is not set to runAsNonRoot"
  },
  {
    "id": "03637",
    "manifest_path": "data/manifests/the_stack_sample/sample_1610.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v${k8s_ver}_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --apiserver-count=${api_server_count}\n    - --etcd-servers=${etcd_endpoints}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=10.21.0.0/16\n    - --secure-port=443\n    - --insecure-port=8080\n    - --insecure-bind-address=0.0.0.0\n    - --audit-log-maxage=30\n    - --audit-log-maxsize=100\n    - --audit-log-path=/var/log/apiserver/audit.log\n    - --feature-gates=AdvancedAuditing=false\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,Initializers\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --runtime-config=batch/v2alpha1,extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true,admissionregistration.k8s.io/v1alpha1=true\n    - --anonymous-auth=false\n    - --authorization-mode=RBAC\n    - --etcd-quorum-read=true\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem\n    - --token-auth-file=/etc/kubernetes/ssl/token_auth.csv\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/auth\n      name: auth-kubernetes\n      readOnly: true\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: /var/log/apiserver\n      name: apiserver-logs\n      readOnly: false\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/auth\n    name: auth-kubernetes\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /etc/ssl/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: /var/log/apiserver\n    name: apiserver-logs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-apiserver\" does not have a read-only root file system"
  },
  {
    "id": "03638",
    "manifest_path": "data/manifests/the_stack_sample/sample_1610.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v${k8s_ver}_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --apiserver-count=${api_server_count}\n    - --etcd-servers=${etcd_endpoints}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=10.21.0.0/16\n    - --secure-port=443\n    - --insecure-port=8080\n    - --insecure-bind-address=0.0.0.0\n    - --audit-log-maxage=30\n    - --audit-log-maxsize=100\n    - --audit-log-path=/var/log/apiserver/audit.log\n    - --feature-gates=AdvancedAuditing=false\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,Initializers\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --runtime-config=batch/v2alpha1,extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true,admissionregistration.k8s.io/v1alpha1=true\n    - --anonymous-auth=false\n    - --authorization-mode=RBAC\n    - --etcd-quorum-read=true\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem\n    - --token-auth-file=/etc/kubernetes/ssl/token_auth.csv\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/auth\n      name: auth-kubernetes\n      readOnly: true\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: /var/log/apiserver\n      name: apiserver-logs\n      readOnly: false\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/auth\n    name: auth-kubernetes\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /etc/ssl/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: /var/log/apiserver\n    name: apiserver-logs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-apiserver\" is not set to runAsNonRoot"
  },
  {
    "id": "03639",
    "manifest_path": "data/manifests/the_stack_sample/sample_1610.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v${k8s_ver}_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --apiserver-count=${api_server_count}\n    - --etcd-servers=${etcd_endpoints}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=10.21.0.0/16\n    - --secure-port=443\n    - --insecure-port=8080\n    - --insecure-bind-address=0.0.0.0\n    - --audit-log-maxage=30\n    - --audit-log-maxsize=100\n    - --audit-log-path=/var/log/apiserver/audit.log\n    - --feature-gates=AdvancedAuditing=false\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,Initializers\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --runtime-config=batch/v2alpha1,extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true,admissionregistration.k8s.io/v1alpha1=true\n    - --anonymous-auth=false\n    - --authorization-mode=RBAC\n    - --etcd-quorum-read=true\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem\n    - --token-auth-file=/etc/kubernetes/ssl/token_auth.csv\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/auth\n      name: auth-kubernetes\n      readOnly: true\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: /var/log/apiserver\n      name: apiserver-logs\n      readOnly: false\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/auth\n    name: auth-kubernetes\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /etc/ssl/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: /var/log/apiserver\n    name: apiserver-logs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-apiserver\" has cpu request 0"
  },
  {
    "id": "03640",
    "manifest_path": "data/manifests/the_stack_sample/sample_1610.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: quay.io/coreos/hyperkube:v${k8s_ver}_coreos.0\n    command:\n    - /hyperkube\n    - apiserver\n    - --apiserver-count=${api_server_count}\n    - --etcd-servers=${etcd_endpoints}\n    - --allow-privileged=true\n    - --service-cluster-ip-range=10.21.0.0/16\n    - --secure-port=443\n    - --insecure-port=8080\n    - --insecure-bind-address=0.0.0.0\n    - --audit-log-maxage=30\n    - --audit-log-maxsize=100\n    - --audit-log-path=/var/log/apiserver/audit.log\n    - --feature-gates=AdvancedAuditing=false\n    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,Initializers\n    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n    - --runtime-config=batch/v2alpha1,extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true,admissionregistration.k8s.io/v1alpha1=true\n    - --anonymous-auth=false\n    - --authorization-mode=RBAC\n    - --etcd-quorum-read=true\n    - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem\n    - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem\n    - --token-auth-file=/etc/kubernetes/ssl/token_auth.csv\n    ports:\n    - containerPort: 443\n      hostPort: 443\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: /etc/kubernetes/auth\n      name: auth-kubernetes\n      readOnly: true\n    - mountPath: /etc/kubernetes/ssl\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: /etc/ssl/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: /var/log/apiserver\n      name: apiserver-logs\n      readOnly: false\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/auth\n    name: auth-kubernetes\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: /etc/ssl/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: /var/log/apiserver\n    name: apiserver-logs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-apiserver\" has memory limit 0"
  }
]