[
  {
    "id": "04401",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"database\" is not set to runAsNonRoot"
  },
  {
    "id": "04402",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"database\" has cpu request 0"
  },
  {
    "id": "04403",
    "manifest_path": "data/manifests/the_stack_sample/sample_2020.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: '1'\n  labels:\n    crunchy-pgha-scope: some-name-required\n    deployment-name: some-name-required\n    name: some-name-required\n    pg-cluster: some-name-required\n    pgo-pg-database: 'true'\n    pgo-version: 1.2.0\n    pgouser: admin\n    service-name: some-name-required\n    vendor: crunchydata\n  name: some-name-required\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      deployment-name: some-name-required\n      pg-cluster: some-name-required\n      pgo-pg-database: 'true'\n      vendor: crunchydata\n  template:\n    metadata:\n      annotations:\n        keep-backups: 'false'\n        keep-data: 'false'\n      labels:\n        crunchy-pgha-scope: some-name-required\n        deployment-name: some-name-required\n        name: some-name-required\n        pg-cluster: some-name-required\n        pg-pod-anti-affinity: required\n        pgo-pg-database: 'true'\n        pgo-version: 1.2.0\n        pgouser: admin\n        vendor: crunchydata\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: vendor\n                operator: In\n                values:\n                - crunchydata\n              - key: pg-pod-anti-affinity\n                operator: In\n                values:\n                - required\n                - require\n              - key: pg-cluster\n                operator: In\n                values:\n                - some-name-required\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - env:\n        - name: MODE\n          value: postgres\n        - name: PGHA_PG_PORT\n          value: '5432'\n        - name: PGHA_USER\n          value: postgres\n        - name: PGHA_INIT\n          valueFrom:\n            configMapKeyRef:\n              key: init\n              name: some-name-required-pgha-config\n        - name: PATRONI_POSTGRESQL_DATA_DIR\n          value: /pgdata/some-name-required\n        - name: PGBACKREST_STANZA\n          value: db\n        - name: PGBACKREST_REPO1_HOST\n          value: some-name-required-backrest-shared-repo\n        - name: BACKREST_SKIP_CREATE_STANZA\n          value: 'true'\n        - name: PGHA_PGBACKREST\n          value: 'true'\n        - name: PGBACKREST_REPO1_PATH\n          value: /backrestrepo/some-name-required-backrest-shared-repo\n        - name: PGBACKREST_DB_PATH\n          value: /pgdata/some-name-required\n        - name: ENABLE_SSHD\n          value: 'true'\n        - name: PGBACKREST_LOG_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_SOCKET_PATH\n          value: /tmp\n        - name: PGBACKREST_PG1_PORT\n          value: '5432'\n        - name: PGBACKREST_REPO1_TYPE\n          value: posix\n        - name: PGHA_PGBACKREST_LOCAL_S3_STORAGE\n          value: 'false'\n        - name: PGHA_PGBACKREST_LOCAL_GCS_STORAGE\n          value: 'false'\n        - name: PGHA_DATABASE\n          value: some-name-required\n        - name: PGHA_REPLICA_REINIT_ON_START_FAIL\n          value: 'true'\n        - name: PGHA_SYNC_REPLICATION\n          value: 'false'\n        - name: PGHA_TLS_ENABLED\n          value: 'false'\n        - name: PGHA_TLS_ONLY\n          value: 'false'\n        - name: PGHA_PASSWORD_TYPE\n        - name: PGHA_STANDBY\n          value: 'false'\n        - name: PATRONI_KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PATRONI_KUBERNETES_SCOPE_LABEL\n          value: crunchy-pgha-scope\n        - name: PATRONI_SCOPE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels['crunchy-pgha-scope']\n        - name: PATRONI_KUBERNETES_LABELS\n          value: '{vendor: \"crunchydata\"}'\n        - name: PATRONI_LOG_LEVEL\n          value: INFO\n        - name: PGHOST\n          value: /tmp\n        - name: LD_PRELOAD\n          value: /usr/lib64/libnss_wrapper.so\n        - name: NSS_WRAPPER_PASSWD\n          value: /tmp/nss_wrapper/postgres/passwd\n        - name: NSS_WRAPPER_GROUP\n          value: /tmp/nss_wrapper/postgres/group\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-liveness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 10\n        name: database\n        ports:\n        - containerPort: 5432\n          name: postgres\n          protocol: TCP\n        - containerPort: 8009\n          name: patroni\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - /opt/crunchy/bin/postgres-ha/health/pgha-readiness.sh\n          failureThreshold: 3\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /pgdata\n          name: pgdata\n        - mountPath: /pgconf/pguser\n          name: user-volume\n        - mountPath: /pgconf/pgreplicator\n          name: primary-volume\n        - mountPath: /pgconf/pgsuper\n          name: root-volume\n        - mountPath: /sshd\n          name: sshd\n          readOnly: true\n        - mountPath: /etc/ssh\n          name: ssh-config\n          readOnly: true\n        - mountPath: /pgconf\n          name: pgconf-volume\n        - mountPath: /dev/shm\n          name: dshm\n        - mountPath: /etc/pgbackrest/conf.d\n          name: pgbackrest-config\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /tmp\n          name: tmp\n      securityContext:\n        supplementalGroups:\n        - 1001\n      serviceAccount: pgo-pg\n      serviceAccountName: pgo-pg\n      volumes:\n      - name: pgdata\n        persistentVolumeClaim:\n          claimName: some-name-required\n      - name: user-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-some-name-secret\n      - name: primary-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-primaryuser-secret\n      - name: sshd\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-backrest-repo-config\n      - name: ssh-config\n        secret:\n          defaultMode: 420\n          items:\n          - key: config\n            path: ssh_config\n          secretName: some-name-required-backrest-repo-config\n      - name: root-volume\n        secret:\n          defaultMode: 420\n          secretName: some-name-required-postgres-secret\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 64Mi\n        name: report\n      - emptyDir:\n          medium: Memory\n        name: dshm\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 16Mi\n        name: tmp\n      - name: pgbackrest-config\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-config-backrest\n              optional: true\n          - secret:\n              name: some-name-required-config-backrest\n              optional: true\n      - name: pgconf-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - configMap:\n              name: some-name-required-pgha-config\n              optional: true\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - path: cpu_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: limits.cpu\n          - path: cpu_request\n            resourceFieldRef:\n              containerName: database\n              divisor: 1m\n              resource: requests.cpu\n          - path: mem_limit\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: limits.memory\n          - path: mem_request\n            resourceFieldRef:\n              containerName: database\n              divisor: '0'\n              resource: requests.memory\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"database\" has memory limit 0"
  },
  {
    "id": "04404",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"manager\" is using an invalid container image, \"controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04405",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy\" does not have a read-only root file system"
  },
  {
    "id": "04406",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "04407",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "04408",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "04409",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-rbac-proxy\" has cpu request 0"
  },
  {
    "id": "04410",
    "manifest_path": "data/manifests/the_stack_sample/sample_2022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/core: capi-operator\n    control-plane: controller-manager\n  name: capi-operator-controller-manager\n  namespace: openshift-cluster-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      clusterctl.cluster.x-k8s.io/core: capi-operator\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        clusterctl.cluster.x-k8s.io/core: capi-operator\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n      - args:\n        - --metrics-bind-addr=127.0.0.1:8080\n        - --leader-elect\n        command:\n        - /manager\n        image: controller:latest\n        name: manager\n        resources:\n          limits:\n            cpu: 100m\n            memory: 150Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy\" has memory limit 0"
  },
  {
    "id": "04411",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ibmmq-client\" is using an invalid container image, \"mqkeda/sample-app:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04412",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ibmmq-client\" does not have a read-only root file system"
  },
  {
    "id": "04413",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ibmmq-client\" is not set to runAsNonRoot"
  },
  {
    "id": "04414",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ibmmq-client\" has cpu request 0"
  },
  {
    "id": "04415",
    "manifest_path": "data/manifests/the_stack_sample/sample_2023.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ibmmq-producer\nspec:\n  template:\n    spec:\n      containers:\n      - name: ibmmq-client\n        image: mqkeda/sample-app:latest\n        imagePullPolicy: Always\n        command:\n        - /src/send\n        args:\n        - '75'\n        - '1'\n        env:\n        - name: APP_USER\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_USER\n        - name: APP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: ibmmq-secret\n              key: APP_PASSWORD\n        - name: QMGR\n          value: MQoC\n        - name: QUEUE_NAME\n          value: DEMO.QUEUE\n        - name: HOST\n          value: mqoc-419f.qm.eu-gb.mq.appdomain.cloud\n        - name: PORT\n          value: '31175'\n        - name: CHANNEL\n          value: CLOUD.APP.SVRCONN\n        - name: TOPIC_NAME\n          value: dev/\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ibmmq-client\" has memory limit 0"
  },
  {
    "id": "04416",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "04417",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "04418",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "04419",
    "manifest_path": "data/manifests/the_stack_sample/sample_2032.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200227-045f82e5a\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "04420",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04421",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04422",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "04423",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "04424",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "04425",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "04426",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "04427",
    "manifest_path": "data/manifests/the_stack_sample/sample_2036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes6\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - name: volume-rbd\n    rbd:\n      image: testing\n      monitors:\n      - testing\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "04428",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"jaeger-agent\" is using an invalid container image, \"jaegertracing/jaeger-agent\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04429",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-agent\" does not have a read-only root file system"
  },
  {
    "id": "04430",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress\" does not have a read-only root file system"
  },
  {
    "id": "04431",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"nginx-ingress\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "04432",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "04433",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-agent\" has cpu request 0"
  },
  {
    "id": "04434",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress\" has cpu request 0"
  },
  {
    "id": "04435",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-agent\" has memory limit 0"
  },
  {
    "id": "04436",
    "manifest_path": "data/manifests/the_stack_sample/sample_2038.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n      annotations:\n        co.elastic.logs/module: nginx\n        co.elastic.logs/fileset.stdout: access\n        co.elastic.logs/fileset.stderr: error\n        co.elastic.metrics/module: nginx\n        co.elastic.metrics/hosts: ${data.host}:${data.port}\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: wangsiming519/nginx-ingress:opentracing_1.7.0\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        securityContext:\n          allowPrivilegeEscalation: true\n          runAsUser: 101\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        args:\n        - --reporter.grpc.host-port=grpc-apm-service.default.svc.cluster.local:14250\n        - --reporter.type=grpc\n        - --reporter.grpc.tls.enabled=true\n        - --reporter.grpc.tls.skip-host-verify=true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress\" has memory limit 0"
  },
  {
    "id": "04437",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04438",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04439",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04440",
    "manifest_path": "data/manifests/the_stack_sample/sample_2039.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5490\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  }
]